<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:media="http://search.yahoo.com/mrss/">

<channel>
	<title>NVIDIA Blog</title>
	<atom:link href="https://blogs.nvidia.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.nvidia.com/</link>
	<description></description>
	<lastBuildDate>Thu, 16 Nov 2023 17:08:32 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.3.2</generator>
	<item>
		<title>Three Ways Generative AI Can Bolster Cybersecurity</title>
		<link>https://blogs.nvidia.com/blog/generative-ai-cybersecurity/</link>
		
		<dc:creator><![CDATA[David Reber Jr.]]></dc:creator>
		<pubDate>Thu, 16 Nov 2023 16:00:37 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Cybersecurity]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[New GPU Uses]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=68187</guid>

					<description><![CDATA[Human analysts can no longer effectively defend against the increasing speed and complexity of cybersecurity attacks. The amount of data is simply too large to screen manually. Generative AI, the most transformative tool of our time, enables a kind of digital jiu jitsu. It lets companies shift the force of data that threatens to overwhelm <a class="read-more" href="https://blogs.nvidia.com/blog/generative-ai-cybersecurity/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Human analysts can no longer effectively defend against the increasing speed and complexity of cybersecurity attacks. The amount of data is simply too large to screen manually.</p>
<p><a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">Generative AI</a>, the most transformative tool of our time, enables a kind of digital jiu jitsu. It lets companies shift the force of data that threatens to overwhelm them into a force that makes their defenses stronger.</p>
<p>Business leaders seem ready for the opportunity at hand. In a <a href="https://www.ibm.com/thought-leadership/institute-business-value/en-us/c-suite-study/ceo">recent survey</a>, CEOs said cybersecurity is one of their top three concerns, and they see generative AI as a lead technology that will deliver competitive advantages.</p>
<p>Generative AI brings both risks and benefits. An earlier blog outlined <a href="https://blogs.nvidia.com/blog/ai-security-steps/">six steps to start the process of securing enterprise AI</a>.</p>
<p>Here are three ways generative AI can bolster cybersecurity.</p>
<h2><b>Begin With Developers</b></h2>
<p>First, give developers a security copilot.</p>
<p>Everyone plays a role in security, but not everyone is a security expert. So, this is one of the most strategic places to begin.</p>
<p>The best place to start bolstering security is on the front end, where developers are writing software. An AI-powered assistant, trained as a security expert, can help them ensure their code follows best practices in security.</p>
<p>The AI software assistant can get smarter every day if it‚Äôs fed previously reviewed code. It can learn from prior work to help guide developers on best practices.</p>
<p>To give users a leg up, NVIDIA is creating a <a href="https://www.nvidia.com/en-us/ai-data-science/ai-workflows/generative-ai-chatbots/">workflow for building such co-pilots or chatbots</a>. This particular workflow uses components from <a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/">NVIDIA NeMo</a>, a framework for building and customizing <a href="https://www.nvidia.com/en-us/glossary/data-science/large-language-models/">large language models</a>.</p>
<p>Whether users customize their own models or use a commercial service, a security assistant is just the first step in applying generative AI to cybersecurity.</p>
<h2><b>An Agent to Analyze Vulnerabilities</b></h2>
<p>Second, let generative AI help navigate the sea of known software vulnerabilities.</p>
<p>At any moment, companies must choose among thousands of patches to mitigate known exploits. That‚Äôs because every piece of code can have roots in dozens if not thousands of different software branches and open-source projects.</p>
<p>An LLM focused on vulnerability analysis can help prioritize which patches a company should implement first. It‚Äôs a particularly powerful security assistant because it reads all the software libraries a company uses as well as its policies on the features and APIs it supports.</p>
<p>To test this concept, NVIDIA built a pipeline to analyze software containers for vulnerabilities. The agent identified areas that needed patching with high accuracy, speeding the work of human analysts up to 4x.</p>
<p>The takeaway is clear. It‚Äôs time to enlist generative AI as a first responder in vulnerability analysis.</p>
<h2><b>Fill the Data Gap</b></h2>
<p>Finally, use LLMs to help fill the growing data gap in cybersecurity.</p>
<p>Users rarely share information about data breaches because they‚Äôre so sensitive. That makes it difficult to anticipate exploits.</p>
<p>Enter LLMs. Generative AI models can create <a href="https://blogs.nvidia.com/blog/what-is-synthetic-data/">synthetic data</a> to simulate never-before-seen attack patterns. Such synthetic data can also fill gaps in training data so machine-learning systems learn how to defend against exploits before they happen.</p>
<h2><b>Staging Safe Simulations</b></h2>
<p>Don‚Äôt wait for attackers to demonstrate what‚Äôs possible. Create safe simulations to learn how they might try to penetrate corporate defenses.</p>
<p>This kind of proactive defense is the hallmark of a strong security program. Adversaries are already using generative AI in their attacks. It‚Äôs time users harness this powerful technology for cybersecurity defense.</p>
<p>To show what‚Äôs possible, another <a href="https://www.nvidia.com/en-us/ai-data-science/ai-workflows/spear-phishing/">AI workflow</a> uses generative AI to defend against spear phishing ‚Äî the carefully targeted bogus emails that cost companies an estimated $2.4 billion in 2021 alone.</p>
<p>This workflow generated synthetic emails to make sure it had plenty of good examples of spear phishing messages. The AI model trained on that data learned to understand the intent of incoming emails through natural language processing capabilities in <a href="https://developer.nvidia.com/morpheus-cybersecurity">NVIDIA Morpheus</a>, a framework for AI-powered cybersecurity.</p>
<p>The resulting model caught 21% more spear phishing emails than existing tools. Check out our <a href="https://developer.nvidia.com/blog/generative-ai-and-accelerated-computing-for-spear-phishing-detection/">developer blog</a> or watch the video below to learn more.</p>
<p><iframe title="Improve Spear Phishing Detection with Generative AI" width="500" height="281" src="https://www.youtube.com/embed/57dEPP67XrY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Wherever users choose to start this work, automation is crucial, given the shortage of cybersecurity experts and the thousands upon thousands of users and use cases that companies need to protect.</p>
<p>These three tools ‚Äî software assistants, virtual vulnerability analysts and synthetic data simulations ‚Äî are great starting points for applying generative AI to a security journey that continues every day.</p>
<p>But this is just the beginning. Companies need to integrate generative AI into all layers of their defenses.</p>
<p>Attend a <a href="https://www.nvidia.com/en-us/events/llm-developer-day/">webinar</a> for more details on how to get started.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/new-KV.jpg"
			type="image/jpeg"
			width="1000"
			height="563"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/new-KV-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Three Ways Generative AI Can Bolster Cybersecurity]]></media:title>
			<media:description type="html">Image showing worker using generative AI to bolster cybersecurity</media:description>
			</media:content>
			</item>
		<item>
		<title>Into the Omniverse: OpenUSD Enhancements for Autodesk Maya Make 3D Workflows a Ferret-Tale</title>
		<link>https://blogs.nvidia.com/blog/openusd-enhancements-for-autodesk-maya/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Thu, 16 Nov 2023 14:00:53 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[Into the Omniverse]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Universal Scene Description]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=68127</guid>

					<description><![CDATA[3D artists can improve the productivity and efficiency of their generative AI-enabled content-creation workflows thanks to the latest updates to popular OpenUSD software.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p><i>Editor‚Äôs note: This post is part of </i><a href="https://www.nvidia.com/en-us/omniverse/news/"><i>Into the Omniverse</i></a><i>, a series focused on how artists, developers and enterprises can transform their workflows using the latest advances in </i><a href="https://www.nvidia.com/en-us/omniverse/usd/"><i>OpenUSD</i></a><i> and </i><a href="https://www.nvidia.com/en-us/omniverse/usd/"><i>NVIDIA Omniverse</i></a><i>.</i></p>
<p>In 3D art and design, efficient workflows are essential for quickly bringing creative visions to life.</p>
<p><a href="https://www.nvidia.com/en-us/omniverse/usd/">Universal Scene Description</a>, aka OpenUSD, is a framework that enhances these workflows by providing a unified, extensible ecosystem for describing, composing, simulating and collaborating within 3D worlds. OpenUSD is a key technology in Autodesk‚Äôs suite of products and solutions, across media and entertainment; architecture, engineering and construction; and product design and manufacturing.</p>
<p>Unveiled at the <a href="https://conferences.autodesk.com/flow/autodesk/au2023/web/page/overview" target="_blank" rel="noopener">AU 2023</a> conference this week, the latest OpenUSD updates to Autodesk Maya enable artists and technical professionals to create and manipulate OpenUSD assets with greater control and efficiency, while also ensuring more efficient and accurate 3D workflows.</p>
<h2><b></b><b>Bridging the Digital and Real Worlds With Maya and OpenUSD</b></h2>
<p>Many creators are using Maya and OpenUSD to propel their 3D workflows.</p>
<p><a href="https://www.linkedin.com/in/karol-osinski-3dartist" target="_blank" rel="noopener">Karol Osinski</a> is a 3D artist at <a href="http://www.s20m.com" target="_blank" rel="noopener">S20M</a>, an architectural and design firm that specializes in tackling unique, bold and elegant projects. When it comes to creating architectural visualizations, Osinski says the biggest challenge is matching the digital world to the real one.</p>
<p>Using USD and creative tools such as Maya, SideFX Houdini and Epic Games‚Äô Unreal Engine, Osinski creates high-quality visuals for clients while accelerating his architectural workflows.</p>
<p style="text-align: center;"><img decoding="async" fetchpriority="high" class=" wp-image-68143 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/ezgif.com-video-to-gif-38.gif" alt="" width="644" height="363" /><i>Osinski‚Äôs panoramic view from the 20th floor terrace in the Upper East Side</i></p>
<p>‚ÄúOpenUSD provides the possibility of bridging different tools like never before,‚Äù said Osinski. ‚ÄúI love how accessible USD is for first-time users and how it opens opportunities to make designs very complex.‚Äù</p>
<p><a href="https://blogs.nvidia.com/blog/sir-wade-autodesk-maya-blender-davinci-resolve" target="_blank" rel="noopener">‚ÄúSir Wade‚Äù Neistadt</a>, an animator and YouTube creator, aims to make animation and 3D education more accessible through his video tutorials and industry training. The first step of his unique animation workflow is to act out his animations on camera. He then translates them in Maya to begin his animation work before using USD to export them to other 3D software, including Blender, for finishing touches.</p>
<p style="text-align: center;"><img decoding="async" class=" wp-image-68137 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-1.png" alt="" width="722" height="435" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-1.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-1-400x241.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-1-672x405.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-1-768x463.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-1-746x450.png 746w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-1-356x215.png 356w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-1-166x100.png 166w" sizes="(max-width: 722px) 100vw, 722px" /><i>The making of Sir Wade‚Äôs VFX robot animation</i></p>
<p>3D artists at NVIDIA are also experiencing the power of Maya and OpenUSD. Technical specialist <a href="https://www.linkedin.com/in/lee-fraser/" target="_blank" rel="noopener">Lee Fraser</a> led the ‚ÄúFerret-Tale Project‚Äù to showcase character creation and animation workflows enabled by OpenUSD and <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">generative AI</a>.</p>
<p>To create the demo, Fraser and his team collaborated across 3D applications like Blender, Autodesk Maya and Reallusion Character Creator through <a href="https://www.nvidia.com/en-us/omniverse/ecosystem/">OpenUSD Connectors</a>. This allowed them to reduce the data prep and import and export time that‚Äôs usually required when working with multiple data sources.</p>
<p>‚ÄúMy favorite thing about using OpenUSD is not having to think about where the 3D files I use originated from,‚Äù Fraser said. ‚ÄúIt was also easy to use USD layers to experiment with applying different animation clips with different characters.‚Äù</p>
<p><iframe loading="lazy" title="The Making of an OpenUSD and Generative AI Ferret-Tale" width="500" height="281" src="https://www.youtube.com/embed/mYcTbOV334Q?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Members of the creative community joined a recent livestream to share their workflows using Autodesk tools, OpenUSD and <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a>, a development platform for connecting and building OpenUSD-based tools and applications.</p>
<p>Whether adjusting lighting conditions in an environment or looking at building designs from the street view, designers in architecture, engineering, construction and operations are advancing their work with AI. Learn more by watching the replay:</p>
<p><iframe loading="lazy" title="Architecting 3D Environments With Autodesk and OpenUSD" width="500" height="281" src="https://www.youtube.com/embed/jJhwq3uO_Uk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<h2><b>Shaping the Future of 3D With More Efficient Workflows</b></h2>
<p>AU 2023 attendees experienced how Autodesk is <a href="https://www.autodesk.com/customer-value/me/maya">enhancing Maya</a> with its new OpenUSD plug-in to provide additional practical workflows for various production processes. The software‚Äôs latest features include:</p>
<ul>
<li><strong>Simplified asset sharing:</strong> Designers can now use relative paths when creating OpenUSD stages, allowing for easy asset sharing between different systems. This includes support for sublayers, references, payloads and textures.</li>
<li><strong>Enhanced control:</strong> Plug-in developers and technical directors can overwrite the default prim writers in Maya USD to gain complete control over their OpenUSD exports.</li>
</ul>
<p>Plus, Autodesk introduced impressive capabilities to <a href="https://help-staging.autodesk.com/view/MAYAUL/2024/ENU/?guid=GUID-3C97F007-F02A-43AD-A4A4-3590E120DA1D">LookdevX in Maya</a>, a look-development tool that lets users create OpenUSD shade graphs and custom materials in Maya. These new features include:</p>
<ul>
<li><b>Streamlined shader creation:</b> Users can employ a unified shader workflow, replacing the need for multiple shaders. They can select their desired shader type within the parameters panel, with intuitive error messages guiding them to the correct selection.</li>
<li><b>Efficient operations:</b> Creators can copy, paste and duplicate shaders and materials using the Outliner and LookdevX tool sets, with the option to include or exclude connections.</li>
<li><b>Seamless color management:</b> LookdevX in Maya integrates with color managers in other digital content creation apps to ensure accurate color representation. Color management data is precisely embedded in USD files for accurate reading.</li>
<li><b>Advanced graphing:</b> Users can explore advanced graphing options with the integrated component workflow, supporting multichannel Extended Dynamic Range (EXR) workflows within USD, MaterialX or Arnold shading graphs.</li>
<li><b>Efficient troubleshooting: </b>Solo nodes enable faster look-development workflows and efficient graph troubleshooting. Users can inspect renders of upstream nodes, supporting both Autodesk Arnold and MaterialX graphs, including materials, shaders and compounds.</li>
</ul>
<p style="text-align: center;"><img decoding="async" loading="lazy" class=" wp-image-68131 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/Picture1.png" alt="" width="587" height="330" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/Picture1.png 480w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Picture1-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Picture1-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Picture1-178x100.png 178w" sizes="(max-width: 587px) 100vw, 587px" /><i>Access to default prim options in Maya UI</i></p>
<h2><b>Get Plugged Into the World of OpenUSD</b></h2>
<p>Anyone can build their own <a href="https://developer.nvidia.com/omniverse">Omniverse extension or Connector</a> to enhance their 3D workflows and tools. Explore the Omniverse ecosystem‚Äôs <a href="https://www.nvidia.com/en-us/omniverse/ecosystem/">growing catalog</a> of connections, extensions, foundation applications and third-party tools.</p>
<p>Autodesk and NVIDIA are founding members of the <a href="https://aousd.org/">Alliance for OpenUSD</a> (AOUSD), together strengthening an open future with USD. To learn more, explore the <a href="https://forum.aousd.org/">AOUSD forum</a> and check out <a href="https://developer.nvidia.com/usd">resources on OpenUSD</a>.</p>
<p>Share your Autodesk Maya and Omniverse work through November as part of the <a href="https://forums.developer.nvidia.com/t/the-new-community-challenge-is-here-we-are-kicking-off-the-seasonalartchallenge/268346">#SeasonalArtChallenge</a>. Use the hashtag to submit an autumn harvest-themed scene for a chance to be featured on the @NVIDIAStudio and @NVIDIAOmniverse social channels.</p>
<p><i>Get started with NVIDIA Omniverse by downloading the standard license </i><a href="https://www.nvidia.com/en-us/omniverse/download/"><i>free</i></a><i>, or learn how </i><a href="https://www.nvidia.com/en-us/omniverse/enterprise/"><i>Omniverse Enterprise</i><i> can connect your team</i></a><i>.¬†</i></p>
<p><i>Developers can check out these </i><a href="https://docs.omniverse.nvidia.com/dev-guide/latest/index.html"><i>Omniverse resources</i></a><i> to begin building on the platform.¬†</i></p>
<p><i>Stay up to date on the platform by subscribing to the </i><a href="https://nvda.ws/3u5KPv1"><i>newsletter</i></a><i> and following NVIDIA Omniverse on </i><a href="https://www.instagram.com/nvidiaomniverse/"><i>Instagram</i></a><i>, </i><a href="https://www.linkedin.com/showcase/nvidia-omniverse"><i>LinkedIn</i></a><i>, </i><a href="https://medium.com/@nvidiaomniverse"><i>Medium</i></a><i>, </i><a href="https://www.threads.net/@nvidiaomniverse"><i>Threads</i></a><i> and </i><a href="https://twitter.com/nvidiaomniverse"><i>Twitter</i></a><i>.</i></p>
<p><i>For more, check out our </i><a href="https://forums.developer.nvidia.com/c/omniverse/300"><i>forums</i></a><i>, </i><a href="https://discord.com/invite/XWQNJDNuaC"><i>Discord server</i></a><i>, </i><a href="https://www.twitch.tv/nvidiaomniverse"><i>Twitch</i></a><i> and </i><a href="https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA"><i>YouTube</i></a><i> channels..</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/Copy-of-nv-ov-ito-1280x680_nov-autodesk.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/Copy-of-nv-ov-ito-1280x680_nov-autodesk-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Into the Omniverse: OpenUSD Enhancements for Autodesk Maya Make 3D Workflows a Ferret-Tale]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>More Games, More Wins: PC Game Pass Included With Six-Month GeForce NOW Memberships</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-nov-16/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 16 Nov 2023 14:00:44 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=68198</guid>

					<description><![CDATA[The fastest way to give the gift of cloud gaming starts this GFN Thursday: For a limited time, every six-month GeForce NOW Ultimate membership includes three months of PC Game Pass. Also, the newest GeForce NOW app update is rolling out to members, including Xbox Game Syncing and more improvements. Plus, take advantage of a <a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-nov-16/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>The fastest way to give the gift of cloud gaming starts this GFN Thursday: For a limited time, every six-month <a href="https://www.nvidia.com/en-us/geforce-now/memberships/">GeForce NOW Ultimate membership</a> includes three months of PC Game Pass.</p>
<p>Also, the newest GeForce NOW app update is rolling out to members, including Xbox Game Syncing and more improvements.</p>
<p>Plus, take advantage of a heroic, new members-only <i>Guild Wars 2</i> reward. It‚Äôs all topped off by support for 18 more games in the GeForce NOW library this week.</p>
<h2><b>Give the Gift of Gaming</b></h2>
<figure id="attachment_68212" aria-describedby="caption-attachment-68212" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-68212" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-PC_Game_Pass_Titles-672x336.jpg" alt="PC Game Pass bundle" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-PC_Game_Pass_Titles-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-PC_Game_Pass_Titles-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-PC_Game_Pass_Titles-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-PC_Game_Pass_Titles-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-PC_Game_Pass_Titles-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-PC_Game_Pass_Titles-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-PC_Game_Pass_Titles-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-PC_Game_Pass_Titles-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-PC_Game_Pass_Titles.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-68212" class="wp-caption-text"><em>Pair PC Game Pass with a GeForce NOW Ultimate bundle for the ultimate gaming gift.</em></figcaption></figure>
<p>Unwrap the gift of gaming: For a limited time, gamers who sign up for the six-month GeForce NOW Ultimate membership will also receive three free months of PC Game Pass ‚Äî a $30 value.</p>
<p>With it, Ultimate members can play a collection of high-quality Xbox PC titles with the power of a GeForce RTX 4080 rig in the cloud. Jump into the action in iconic franchises like <i>Age of Empires</i>, <i>DOOM</i>, <i>Forza</i> and more, with support for more titles added every GFN Thursday.</p>
<p>Seamlessly launch supported favorites across nearly any device at up to 4K and 120 frames per second or at up to 240 fps with <a href="https://www.nvidia.com/en-us/geforce/technologies/reflex/">NVIDIA Reflex</a> technology in supported titles for lowest-latency streaming.</p>
<p>This special offer is only here for a limited time, so <a href="https://www.nvidia.com/en-us/geforce-now/holiday">upgrade</a> today.</p>
<h2><b>Sync‚Äôd Up</b></h2>
<figure id="attachment_68209" aria-describedby="caption-attachment-68209" style="width: 606px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-68209" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Xbox_Ubisoft-606x500.jpg" alt="Xbox and Ubisoft+ game library sync" width="606" height="500" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Xbox_Ubisoft-606x500.jpg 606w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Xbox_Ubisoft-400x330.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Xbox_Ubisoft-545x450.jpg 545w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Xbox_Ubisoft-261x215.jpg 261w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Xbox_Ubisoft-121x100.jpg 121w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Xbox_Ubisoft.jpg 623w" sizes="(max-width: 606px) 100vw, 606px" /><figcaption id="caption-attachment-68209" class="wp-caption-text"><em>Look who just joined the party!</em></figcaption></figure>
<p>With so many games ready to stream, it might be hard to decide what to play next. The latest GeForce NOW app update, currently rolling out to members, is here to help.</p>
<p>Members can now connect their Xbox accounts to GeForce NOW to sync the games they own to their GeForce NOW library. Game syncing lets members connect their digital game store accounts to GeForce NOW, so all of their supported games are part of their streaming library. Syncing an Xbox account will also add any supported titles a member has access to via PC Game Pass ‚Äî perfect for members taking advantage of the latest Ultimate bundle.</p>
<p>The new update also adds benefits for Ubisoft+ subscribers. With a linked Ubisoft+ account, members can now launch supported Ubisoft+ games they already own from the GeForce NOW app, and the game will be automatically added to ‚ÄúMy Library.‚Äù Get more details on <a href="https://nvidia.custhelp.com/app/answers/detail/a_id/5286">Ubisoft account</a> linking.</p>
<p>Version 2.0.58 also includes an expansion of the new game session diagnostic tools to help members ensure they‚Äôre streaming at optimal quality. It adds codec information to the in-stream statistics overlay and includes other miscellaneous bug fixes. The update should be available for all members soon.</p>
<h2><b>A Heroic Offering</b></h2>
<figure id="attachment_68206" aria-describedby="caption-attachment-68206" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-68206" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Guild_Wars_2_reward-672x336.jpg" alt="Guild Wars 2 reward on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Guild_Wars_2_reward-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Guild_Wars_2_reward-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Guild_Wars_2_reward-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Guild_Wars_2_reward-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Guild_Wars_2_reward-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Guild_Wars_2_reward-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Guild_Wars_2_reward-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Guild_Wars_2_reward-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Guild_Wars_2_reward.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-68206" class="wp-caption-text"><em>Rewards fit for a hero.</em></figcaption></figure>
<p>This week, members can receive <i>Guild Wars 2</i> ‚ÄúHeroic Edition,‚Äù which includes a treasure trove of goodies, such as the base game, Legacy Armor, an 18-slot inventory expansion and four heroic Boosters. It‚Äôs the perfect way to jump into ArenaNet‚Äôs critically acclaimed, free-to-play, massively multiplayer online role-playing game.</p>
<p>It‚Äôs easy to get membership rewards for streaming games on the cloud. Visit the <a href="https://www.nvidia.com/en-us/geforce-now/rewards/">GeForce NOW Rewards portal</a> and update the settings to receive special offers and in-game goodies.</p>
<p>Members can also sign up for the GeForce NOW newsletter, which includes reward notifications, by logging into their <a href="https://www.nvidia.com/en-us/account/email-preferences">NVIDIA account</a> and selecting ‚ÄúPreferences‚Äù from the header. Check the ‚ÄúGaming &amp; Entertainment‚Äù box and ‚ÄúGeForce NOW‚Äù under topic preferences.</p>
<h2><b>Ready, Set, Go</b></h2>
<figure id="attachment_68203" aria-describedby="caption-attachment-68203" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-68203" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Remnant_2_DLC-672x336.jpg" alt="Remnant II DLC on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Remnant_2_DLC-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Remnant_2_DLC-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Remnant_2_DLC-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Remnant_2_DLC-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Remnant_2_DLC-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Remnant_2_DLC-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Remnant_2_DLC-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Remnant_2_DLC-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Remnant_2_DLC.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-68203" class="wp-caption-text"><em>A new DLC awakens.</em></figcaption></figure>
<p>The first downloadable content for Gearbox‚Äôs <i>Remnant 2</i> arrives in the cloud. <i>The Awakened King </i>brings a new storyline, area, archetype and more to the dark fantasy co-op shooter ‚Äî stream it today to experience the awakening of the One True King as he seeks revenge against all who oppose him.</p>
<p>Catch even more action with the 18 newly supported games in the cloud:</p>
<ul>
<li><i>Spirittea </i>(New release on <a href="https://store.steampowered.com/app/1931010?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Nov. 13)</li>
<li><i>KarmaZoo </i>(New release on <a href="https://store.steampowered.com/app/1661630?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Nov. 14)</li>
<li><i>Naheulbeuk‚Äôs Dungeon Master </i>(New release on <a href="https://store.steampowered.com/app/2005160?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Nov. 15)</li>
<li><i>Warhammer Age of Sigmar: Realms of Ruin </i>(New release on <a href="https://store.steampowered.com/app/1844380?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Nov. 17)</li>
<li><i>Arcana of Paradise ‚ÄîThe Tower </i>(<a href="https://store.steampowered.com/app/2089500/Arcana_of_Paradise_The_Tower/">Steam</a>)</li>
<li><i>Blazing Sails: Pirate Battle Royale</i> (<a href="https://www.epicgames.com/store/p/blazing-sails?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
<li><i>Disney Dreamlight Valley</i> (<a href="https://www.xbox.com/games/store/disney-dreamlight-valley/9NSF0BGH8D86?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Hello Neighbor 2</i> (<a href="https://www.xbox.com/games/store/hello-neighbor-2/9N961B11FJ4W?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Overcooked! 2 </i>(<a href="https://www.xbox.com/en-us/games/store/Overcooked-2/BVJLKDG2TX8H">Xbox</a>, available on PC Game Pass)</li>
<li><i>RoboCop: Rogue City </i>(New release on <a href="https://www.epicgames.com/store/p/robocop-rogue-city?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
<li><i>Roboquest </i>(<a href="https://www.xbox.com/games/store/roboquest-game-preview/9p47s7njgwzl?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Rune Factory 4 Special </i>(<a href="https://www.xbox.com/games/store/rune-factory-4-special---windows-edition/9N73PLHJN878?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass)</li>
<li><i>Settlement Survival </i>(<a href="https://store.steampowered.com/app/1509510?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>SOULVARS </i>(<a href="https://store.steampowered.com/app/2087910?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>State of Decay: Year-One Survival Edition</i> (<a href="https://store.steampowered.com/app/329430?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>The Wonderful One: After School Hero </i>(<a href="https://store.steampowered.com/app/2399600?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Wolfenstein: The New Order </i>(<a href="https://www.xbox.com/games/store/wolfenstein-the-new-order-pc/9p75cbj9wt9w?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Wolfenstein: The Old Blood </i>(<a href="https://store.steampowered.com/app/350080?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a href="https://www.epicgames.com/store/p/wolfenstein-the-old-blood?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>, <a href="https://www.xbox.com/games/store/wolfenstein-the-old-blood-pc/9pbb4qhmsdkr?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox </a>and available on PC Game Pass)</li>
</ul>
<p>What are you looking forward to streaming? Let us know on <a href="https://www.twitter.com/nvidiagfn">Twitter</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="in" dir="ltr">hÃ¥eÃ¥rÃ¥eÃ¥&#39;Ã¥sÃ¥ soon&#39;s the deal&#8230;</p>
<p>stay tuned <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f440.png" alt="üëÄ" class="wp-smiley" style="height: 1em; max-height: 1em;" /><img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f384.png" alt="üéÑ" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f329.png" alt="üå©" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a href="https://twitter.com/NVIDIAGFN/status/1724834650688758068?ref_src=twsrc%5Etfw">November 15, 2023</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/gfn-thursday-date-nv-blog-1280x680-no-cta.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/gfn-thursday-date-nv-blog-1280x680-no-cta-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[More Games, More Wins: PC Game Pass Included With Six-Month GeForce NOW Memberships]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Ringing in the Future: NVIDIA and Amdocs Bring Custom Generative AI to Global Telco Industry</title>
		<link>https://blogs.nvidia.com/blog/amdocs-telco-industry/</link>
		
		<dc:creator><![CDATA[Lilac Ilan]]></dc:creator>
		<pubDate>Wed, 15 Nov 2023 16:00:46 +0000</pubDate>
				<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[NVIDIA DGX]]></category>
		<category><![CDATA[Telecommunications]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67989</guid>

					<description><![CDATA[The telecommunications industry ‚Äî the backbone of today‚Äôs interconnected world ‚Äî is valued at a staggering $1.7 trillion globally, according to IDC. It‚Äôs a massive operation, as telcos process hundreds of petabytes of data in their networks each day. That magnitude is only increasing, as the total amount of data transacted globally is forecast to <a class="read-more" href="https://blogs.nvidia.com/blog/amdocs-telco-industry/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>The telecommunications industry ‚Äî the backbone of today‚Äôs interconnected world ‚Äî is valued at a staggering $1.7 trillion globally, <a href="https://www.idc.com/getdoc.jsp?containerId=prUS51360523">according to IDC</a>.</p>
<p>It‚Äôs a massive operation, as telcos process hundreds of petabytes of data in their networks each day. That magnitude is only increasing, as the total amount of data transacted globally is forecast to grow to more than <a href="https://www.statista.com/statistics/871513/worldwide-data-created/">180 zettabytes by 2025</a>.</p>
<p>To meet this demand for data processing and analysis, telcos are turning to <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">generative AI</a>, which is improving efficiency and productivity across industries.</p>
<p>NVIDIA announced an <a href="https://nvidianews.nvidia.com/news/nvidia-introduces-generative-ai-foundry-service-on-microsoft-azure-for-enterprises-and-startups-worldwide">AI foundry service</a> ‚Äî a collection of NVIDIA AI Foundation Models, <a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/">NVIDIA NeMo</a> framework and tools, and <a href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a> AI supercomputing and services ‚Äî that gives enterprises an end-to-end solution for creating and optimizing custom generative AI models.</p>
<p>Using the AI foundry service, Amdocs, a leading provider of software and services for communications and media providers, will optimize enterprise-grade <a href="https://www.nvidia.com/en-us/glossary/data-science/large-language-models/">large language models</a> for the telco and media industries to efficiently deploy generative AI use cases across their businesses, from customer experiences to network operations and provisioning. The LLMs will run on NVIDIA accelerated computing as part of the <a href="https://www.amdocs.com/products-services/amaiz-telco-grade-genai-framework">Amdocs amAIz framework</a>.</p>
<p>The collaboration builds on the previously announced <a href="https://www.amdocs.com/news-press/amdocs-and-microsoft-expand-strategic-partnership-reimagine-telco-experience">Amdocs-Microsoft partnership</a>, enabling service providers to adopt these applications in secure, trusted environments, including on premises and in the cloud.</p>
<h2><b>Custom Models for Custom Results</b></h2>
<p>While preliminary applications of <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">generative AI</a> used broad datasets, enterprises have become increasingly focused on developing custom models to perform specialized, industry-specific skills.</p>
<p>By training models on proprietary data, telcos can deliver tailored solutions that produce more accurate results for their use cases.</p>
<p>To simplify the development, tuning and deployment of such custom models, Amdocs is integrating the new NVIDIA AI foundry service.</p>
<p>Equipped with these new generative AI capabilities ‚Äî including <a href="https://blogs.nvidia.com/blog/ai-chatbot-guardrails-nemo/">guardrail</a> features ‚Äî service providers can enhance performance, optimize resource utilization and flexibly scale to meet future needs.</p>
<h2><b>Amdocs‚Äô Global Telco Ecosystem Footprint</b></h2>
<p>More than 350 of the world‚Äôs leading telecom and media companies across 90 countries take advantage of Amdocs services each day, including 27 of the world‚Äôs top 30 service providers, according to OMDIA.<sup>(1)</sup> Powering more than 1.7 billion daily digital journeys, Amdocs platforms impact more than 3 billion people around the world.</p>
<p>NVIDIA and Amdocs are exploring several generative AI use cases to simplify and improve operations by providing secure, cost-effective, and high-performance generative AI capabilities.</p>
<p>Initial use cases span customer care, including accelerating resolution of customer inquiries by drawing information from across company data.</p>
<p>And in network operations, the companies are exploring ways to generate solutions to address configuration, coverage or performance issues as they arise.</p>
<p><sup>(1)</sup> Source: OMDIA 2022 revenue estimates, excludes China.</p>
<p><i>Stay up to date on the latest </i><a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/news/"><i>NVIDIA generative AI news and technologies</i></a><i> and</i><a href="https://azure.microsoft.com/en-us/blog/"> <i>Microsoft Azure AI News</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/Amdocs-logo.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/Amdocs-logo-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Ringing in the Future: NVIDIA and Amdocs Bring Custom Generative AI to Global Telco Industry]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>In the Fast Lane: NVIDIA Announces Omniverse Cloud Services on Microsoft Azure to Accelerate Automotive Digitalization</title>
		<link>https://blogs.nvidia.com/blog/omniverse-cloud-services-microsoft-azure/</link>
		
		<dc:creator><![CDATA[Danny Shapiro]]></dc:creator>
		<pubDate>Wed, 15 Nov 2023 16:00:39 +0000</pubDate>
				<category><![CDATA[Autonomous Machines]]></category>
		<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Driving]]></category>
		<category><![CDATA[NVIDIA DRIVE Sim]]></category>
		<category><![CDATA[NVIDIA Isaac Sim]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Omniverse Enterprise]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=68080</guid>

					<description><![CDATA[Automotive companies are transforming every phase of their product lifecycle ‚Äî evolving their primarily physical, manual processes into software-driven, AI-enhanced digital systems. To help them save costs and reduce lead times, NVIDIA is announcing two new simulation engines on Omniverse Cloud: the virtual factory simulation engine and the autonomous vehicle (AV) simulation engine. Omniverse Cloud, <a class="read-more" href="https://blogs.nvidia.com/blog/omniverse-cloud-services-microsoft-azure/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Automotive companies are transforming every phase of their product lifecycle ‚Äî evolving their primarily physical, manual processes into software-driven, AI-enhanced digital systems.</p>
<p>To help them save costs and reduce lead times, NVIDIA is announcing two new simulation engines on <a href="https://www.nvidia.com/en-us/omniverse/cloud/">Omniverse Cloud</a>: the virtual factory simulation engine and the autonomous vehicle (AV) simulation engine.</p>
<p>Omniverse Cloud, a platform-as-a-service for developing and deploying applications for industrial digitalization, is hosted on Microsoft Azure. This one-stop shop enables automakers worldwide to unify digitalization across their core product and business processes. It allows enterprises to achieve faster production and more efficient operations, improving time to market and enhancing sustainability initiatives.</p>
<p>For design, engineering and manufacturing teams, digitalization streamlines their work, converting once primarily manual industrial processes into efficient systems for concept and styling; AV development, testing and validation; and factory planning.</p>
<h2><b>Virtual Factory Simulation Engine</b></h2>
<p>The Omniverse Cloud virtual factory simulation engine is a collection of customizable developer applications and services that enable factory planning teams to connect large-scale industrial datasets while collaborating, navigating and reviewing them in real time.</p>
<p>Design teams working with 3D data can assemble virtual factories and share their work with thousands of planners who can view, annotate and update the full-fidelity factory dataset from lightweight devices. By simulating virtual factories on Omniverse Cloud, automakers can increase throughput and production quality while saving years of effort and millions of dollars that would result from making changes once construction is underway.</p>
<p>On Omniverse Cloud, teams can create interoperability between existing software applications such as Autodesk Factory Planning, which supports the entire lifecycle for building, mechanical, electrical, and plumbing and factory lines, as well as Siemens‚Äô NX, Process Simulate and Teamcenter Visualization software and the JT file format. They can share knowledge and data in real time in live, virtual factory reviews across 2D devices or in <a href="https://blogs.nvidia.com/blog/what-is-extended-reality/">extended reality</a>.</p>
<p><a href="https://www.telekom.com/en/media/media-information/archive/t-systems-announces-digital-twin-offering-powered-by-nvidia-omniverse-1051446">T-Systems</a>, a leading IT solutions provider for Europe‚Äôs largest automotive manufacturers, is building and deploying a custom virtual factory application that its customers can deploy in Omniverse Cloud.</p>
<p><a href="https://www.softserveinc.com/en-us/our-partners/nvidia-omniverse">SoftServe</a>, an elite member of the NVIDIA Service Delivery Partner program, is also developing custom factory simulation and visualization solutions on this Omniverse Cloud engine, covering factory design, production planning and control.</p>
<h2><b>AV Simulation Engine</b></h2>
<p>The AV simulation engine is a service that delivers physically based sensor simulation, enabling AV and robotics developers to run autonomous systems in a closed-loop virtual environment.</p>
<p>The next generation of AV architectures will be built on large, unified AI models that combine layers of the vehicle stack, including perception, planning and control. Such new architectures call for an integrated approach to development.</p>
<p>With previous architectures, developers could train and test these layers independently, as they were governed by different models. For example, simulation could be used to develop a vehicle‚Äôs planning and control system, which only needs basic information about objects in a scene ‚Äî such as the speed and distance of surrounding vehicles ‚Äî while perception networks could be trained and tested on recorded sensor data.</p>
<p>However, using simulation to develop an advanced unified AV architecture requires sensor data as the input. For a simulator to be effective, it must be able to simulate vehicle sensors, such as cameras, radars and lidars, with high fidelity.</p>
<p>To address this challenge, NVIDIA is bringing state-of-the-art sensor simulation pipelines used in <a href="https://www.nvidia.com/en-us/self-driving-cars/simulation/">DRIVE Sim</a> and <a href="https://developer.nvidia.com/isaac-sim">Isaac Sim</a> to Omniverse Cloud on Microsoft Azure.</p>
<p>Omniverse Cloud sensor simulation provides AV and robotics workflows with high-fidelity, physically based simulation for cameras, radars, lidars and other types of sensors. It can be connected to existing simulation applications, whether developed in-house or provided by a third party, via Omniverse Cloud application programming interfaces for integration into workflows.</p>
<h2><b>Fast Track to Digitalization</b></h2>
<p>The factory simulation engine is now available to customers via an Omniverse Cloud enterprise private offer through the <a href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps/nvidia.nvidia-omniverse-cloud?tab=Overview">Azure Marketplace</a>, which provides access to <a href="https://www.nvidia.com/en-us/data-center/products/ovx/">NVIDIA OVX</a> systems and fully managed Omniverse software, reference applications and workflows. The sensor simulation engine is coming soon.</p>
<p>Enterprises can now also deploy <a href="https://www.nvidia.com/en-us/omniverse/enterprise/">Omniverse Enterprise</a> on new <a href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps/nvidia.nvidia-omniverse-workstation?tab=Overview">optimized Azure virtual machines</a>.</p>
<p><i>Learn more on </i><a href="https://ignite.microsoft.com/en-US/partners/3c7c8276-fad8-4276-8977-5a7d4d383a45"><i>NVIDIA‚Äôs Microsoft Ignite showcase page</i></a><i>. </i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/Omniverse-cloud-services-simulation.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/Omniverse-cloud-services-simulation-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[In the Fast Lane: NVIDIA Announces Omniverse Cloud Services on Microsoft Azure to Accelerate Automotive Digitalization]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>New NVIDIA H100, H200 Tensor Core GPU Instances Coming to Microsoft Azure to Accelerate AI Workloads</title>
		<link>https://blogs.nvidia.com/blog/microsoft-azure-hopper-gpu-instances/</link>
		
		<dc:creator><![CDATA[Dave Salvator]]></dc:creator>
		<pubDate>Wed, 15 Nov 2023 16:00:37 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Hardware]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[NVIDIA Hopper Architecture]]></category>
		<category><![CDATA[NVLink]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=68115</guid>

					<description><![CDATA[As NVIDIA continues to collaborate with Microsoft to build state-of-the-art AI infrastructure, Microsoft is introducing additional H100-based virtual machines to Microsoft Azure to accelerate demanding AI workloads. At its Ignite conference in Seattle today, Microsoft announced its new NC H100 v5 VM series for Azure, the industry‚Äôs first cloud instances featuring NVIDIA H100 NVL GPUs. <a class="read-more" href="https://blogs.nvidia.com/blog/microsoft-azure-hopper-gpu-instances/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>As NVIDIA continues to collaborate with Microsoft to build state-of-the-art AI infrastructure, Microsoft is introducing additional H100-based virtual machines to Microsoft Azure to accelerate demanding AI workloads.</p>
<p>At its Ignite conference in Seattle today, Microsoft announced its new NC H100 v5 VM series for Azure, the industry‚Äôs first cloud instances featuring NVIDIA H100 NVL GPUs.</p>
<p>This offering brings together a pair of PCIe-based H100 GPUs connected via <a href="https://www.nvidia.com/en-us/data-center/nvlink/">NVIDIA NVLink</a>, with nearly 4 petaflops of AI compute and 188GB of faster HBM3 memory. The NVIDIA H100 NVL GPU can deliver up to 12x higher performance on GPT-3 175B over the previous generation and is ideal for inference and mainstream training workloads.</p>
<p>Additionally, Microsoft announced plans to add the <a href="https://www.nvidia.com/en-us/data-center/h200/">NVIDIA H200 Tensor Core GPU</a> to its Azure fleet next year to support larger model inferencing with no increase in latency. This new offering is purpose-built to accelerate the largest AI workloads, including <a href="https://www.nvidia.com/en-us/glossary/data-science/large-language-models/">LLMs</a> and <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">generative AI</a> models.</p>
<p>The H200 GPU brings dramatic increases both in memory capacity and bandwidth using the latest-generation HBM3e memory. Compared to the H100, this new GPU will offer 141GB of HBM3e memory (1.8x more) and 4.8 TB/s of peak memory bandwidth (a 1.4x increase).</p>
<h2><b>Cloud Computing Gets Confidential</b></h2>
<p>Further expanding availability of NVIDIA-accelerated generative AI computing for Azure customers, Microsoft announced another NVIDIA-powered instance: the NCC H100 v5.</p>
<p>These Azure confidential VMs with NVIDIA H100 Tensor Core GPUs allow customers to protect the confidentiality and integrity of their data and applications in use, in memory, while accessing the unsurpassed acceleration of H100 GPUs. These GPU-enhanced confidential VMs will be coming soon to private preview.</p>
<p>To learn more about the new confidential VMs with NVIDIA H100 Tensor Core GPUs, and sign up for the preview, read the blog.</p>
<p><i>Learn more about NVIDIA-powered Azure instances on the </i><a href="https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu"><i>GPU VM information page</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-h100-sxm-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/nvidia-h100-sxm-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[New NVIDIA H100, H200 Tensor Core GPU Instances Coming to Microsoft Azure to Accelerate AI Workloads]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Fast-Tracks Custom Generative AI Model Development for Enterprises</title>
		<link>https://blogs.nvidia.com/blog/custom-generative-ai-model-development/</link>
		
		<dc:creator><![CDATA[Erik Pounds]]></dc:creator>
		<pubDate>Wed, 15 Nov 2023 16:00:32 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Software]]></category>
		<category><![CDATA[Cloud Services]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[TensorRT]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=68048</guid>

					<description><![CDATA[Today‚Äôs landscape of free, open-source large language models (LLMs) is like an all-you-can-eat buffet for enterprises. This abundance can be overwhelming for developers building custom generative AI applications, as they need to navigate unique project and business requirements, including compatibility, security and the data used to train the models. NVIDIA AI Foundation Models ‚Äî a <a class="read-more" href="https://blogs.nvidia.com/blog/custom-generative-ai-model-development/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Today‚Äôs landscape of free, open-source <a href="https://www.nvidia.com/en-us/glossary/data-science/large-language-models/">large language models</a> (LLMs) is like an all-you-can-eat buffet for enterprises. This abundance can be overwhelming for developers building custom generative AI applications, as they need to navigate unique project and business requirements, including compatibility, security and the data used to train the models.</p>
<p><a href="https://www.nvidia.com/en-us/ai-data-science/foundation-models/">NVIDIA AI Foundation Models</a> ‚Äî a curated collection of enterprise-grade pretrained models ‚Äî give developers a running start for bringing custom generative AI to their enterprise applications.</p>
<h2><b>NVIDIA-Optimized Foundation Models Speed Up Innovation¬†</b></h2>
<p>NVIDIA AI Foundation Models can be experienced through a simple user interface or API, directly from a browser. Additionally, these models can be accessed from NVIDIA AI Foundation Endpoints to test model performance from within their enterprise applications.</p>
<p>Available models include leading community models such as Llama 2, Stable Diffusion XL and Mistral, which are formatted to help developers streamline customization with proprietary data. Additionally, models have been optimized with <a href="https://developer.nvidia.com/tensorrt#inference">NVIDIA TensorRT-LLM</a> to deliver the highest throughput and lowest latency and to run at scale on any NVIDIA GPU-accelerated stack. For instance, the Llama 2 model optimized with TensorRT-LLM runs nearly <a href="https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/">2x faster</a> on NVIDIA H100.</p>
<p>The new NVIDIA family of <a href="https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/">Nemotron-3 8B foundation models</a> supports the creation of today‚Äôs most advanced enterprise chat and Q&amp;A applications for a broad range of industries, including healthcare, telecommunications and financial services.</p>
<p>The models are a starting point for customers building secure, production-ready generative AI applications, are trained on responsibly sourced datasets and operate at comparable performance to much larger models. This makes them ideal for enterprise deployments.</p>
<p>Multilingual capabilities are a key differentiator of the Nemotron-3 8B models. Out of the box, the models are proficient in over 50 languages, including English, German, Russian, Spanish, French, Japanese, Chinese, Korean, Italian and Dutch.</p>
<h2><b>Fast-Track Customization to Deployment</b></h2>
<p>Enterprises leveraging generative AI across business functions need an AI foundry to customize models for their unique applications. NVIDIA‚Äôs AI foundry features three elements ‚Äî NVIDIA AI Foundation Models, <a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/">NVIDIA NeMo</a> framework and tools, and <a href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a> AI supercomputing services. Together, these provide an end-to-end enterprise offering for creating custom generative AI models.</p>
<p>Importantly, enterprises own their customized models and can deploy them virtually anywhere on accelerated computing with enterprise-grade security, stability and support using <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> software.</p>
<p>NVIDIA AI Foundation Models are freely available <a href="https://catalog.ngc.nvidia.com/ai-foundation-models">to experiment with now on the NVIDIA NGC catalog</a> and Hugging Face, and are also hosted in the Microsoft Azure AI model catalog.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-AI-Foundation-Models.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-AI-Foundation-Models-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Fast-Tracks Custom Generative AI Model Development for Enterprises]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>What Is Retrieval-Augmented Generation?</title>
		<link>https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/</link>
		
		<dc:creator><![CDATA[Rick Merritt]]></dc:creator>
		<pubDate>Wed, 15 Nov 2023 16:00:25 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Explainer]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Events]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Inference]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[New GPU Uses]]></category>
		<category><![CDATA[TensorRT]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67360</guid>

					<description><![CDATA[To understand the latest advance in generative AI, imagine a courtroom. Judges hear and decide cases based on their general understanding of the law. Sometimes a case ‚Äî like a malpractice suit or a labor dispute ‚Äî¬† requires special expertise, so judges send court clerks to a law library, looking for precedents and specific cases <a class="read-more" href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>To understand the latest advance in <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">generative AI</a>, imagine a courtroom.</p>
<p>Judges hear and decide cases based on their general understanding of the law. Sometimes a case ‚Äî like a malpractice suit or a labor dispute ‚Äî¬† requires special expertise, so judges send court clerks to a law library, looking for precedents and specific cases they can cite.</p>
<p>Like a good judge, large language models (<a href="https://www.nvidia.com/en-us/glossary/data-science/large-language-models/">LLMs</a>) can respond to a wide variety of human queries. But to deliver authoritative answers that cite sources, the model needs an assistant to do some research.</p>
<p>The court clerk of AI is a process called retrieval-augmented generation, or RAG for short.</p>
<h2><b>The Story of the Name</b></h2>
<p>Patrick Lewis, lead author of the <a href="https://arxiv.org/pdf/2005.11401.pdf">2020 paper that coined the term</a>, apologized for the unflattering acronym that now describes a growing family of methods across hundreds of papers and dozens of commercial services he believes represent the future of generative AI.</p>
<figure id="attachment_68128" aria-describedby="caption-attachment-68128" style="width: 250px" class="wp-caption alignleft"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/Patrick-Lewis-RAG-lead-author.jpg"><img decoding="async" loading="lazy" class="wp-image-68128" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/Patrick-Lewis-RAG-lead-author-150x150.jpg" alt="Picture of Patrick Lewis, lead author of RAG paper" width="250" height="267" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/Patrick-Lewis-RAG-lead-author-202x215.jpg 202w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Patrick-Lewis-RAG-lead-author-94x100.jpg 94w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Patrick-Lewis-RAG-lead-author.jpg 375w" sizes="(max-width: 250px) 100vw, 250px" /></a><figcaption id="caption-attachment-68128" class="wp-caption-text">Patrick Lewis</figcaption></figure>
<p>‚ÄúWe definitely would have put more thought into the name had we known our work would become so widespread,‚Äù Lewis said in an interview from Singapore, where he was sharing his ideas with a regional conference of database developers.</p>
<p>‚ÄúWe always planned to have a nicer sounding name, but when it came time to write the paper, no one had a better idea,‚Äù said Lewis, who now leads a RAG team at AI startup Cohere.</p>
<h2><b>So, What Is Retrieval-Augmented Generation?</b></h2>
<p>Retrieval-augmented generation is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from external sources.</p>
<p>In other words, it fills a gap in how LLMs work. Under the hood, LLMs are neural networks, typically measured by how many parameters they contain. An LLM‚Äôs parameters essentially represent the general patterns of how humans use words to form sentences.</p>
<p>That deep understanding, sometimes called parameterized knowledge, makes LLMs useful in responding to general prompts at light speed. However, it does not serve users who want a deeper dive into a current or more specific topic.</p>
<h2><b>Combining Internal, External Resources</b></h2>
<p>Lewis and colleagues developed retrieval-augmented generation to link generative AI services to external resources, especially ones rich in the latest technical details.</p>
<p>The paper, with coauthors from the former Facebook AI Research (now Meta AI), University College London and New York University, called RAG ‚Äúa general-purpose fine-tuning recipe‚Äù because it can be used by nearly any LLM to connect with practically any external resource.</p>
<h2><b>Building User Trust</b></h2>
<p>Retrieval-augmented generation gives models sources they can cite, like footnotes in a research paper, so users can check any claims. That builds trust.</p>
<p>What‚Äôs more, the technique can help models clear up ambiguity in a user query. It also reduces the possibility a model will make a wrong guess, a phenomenon sometimes called hallucination.</p>
<p>Another great advantage of RAG is it‚Äôs relatively easy. A <a href="https://ai.meta.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/">blog</a> by Lewis and three of the paper‚Äôs coauthors said developers can implement the process with as few as <a href="https://huggingface.co/facebook/rag-token-nq">five lines of code</a>.</p>
<p>That makes the method faster and less expensive than retraining a model with additional datasets. And it lets users hot-swap new sources on the fly.</p>
<h2><b>How People Are Using Retrieval-Augmented Generation¬†</b></h2>
<p>With retrieval-augmented generation, users can essentially have conversations with data repositories, opening up new kinds of experiences. This means the applications for RAG could be multiple times the number of available datasets.</p>
<p>For example, a generative AI model supplemented with a medical index could be a great assistant for a doctor or nurse. Financial analysts would benefit from an assistant linked to market data.</p>
<p>In fact, almost any business can turn its technical or policy manuals, videos or logs into resources called knowledge bases that can enhance LLMs. These sources can enable use cases such as customer or field support, employee training and developer productivity.</p>
<p>The broad potential is why companies including <a href="https://aws.amazon.com/blogs/machine-learning/simplify-access-to-internal-information-using-retrieval-augmented-generation-and-langchain-agents/">AWS</a>, <a href="https://research.ibm.com/blog/retrieval-augmented-generation-RAG">IBM</a>, <a href="https://www.glean.com/">Glean</a>, Google, Microsoft, NVIDIA, <a href="https://www.oracle.com/artificial-intelligence/generative-ai/retrieval-augmented-generation-rag/">Oracle</a> and <a href="https://www.pinecone.io/learn/retrieval-augmented-generation/">Pinecone</a> are adopting RAG.</p>
<h2><b>Getting Started With Retrieval-Augmented Generation¬†</b></h2>
<p>To help users get started, NVIDIA developed a <a href="https://docs.nvidia.com/ai-enterprise/workflows-generative-ai/0.1.0/technical-brief.html">reference architecture for retrieval-augmented generation</a>. It includes a sample chatbot and the elements users need to create their own applications with this new method.</p>
<p>The workflow uses <a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/">NVIDIA NeMo</a>, a framework for developing and customizing generative AI models, as well as software like <a href="https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/">NVIDIA Triton Inference Server</a> and <a href="https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/">NVIDIA TensorRT-LLM</a> for running generative AI models in production.</p>
<p>The software components are all part of <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a>, a software platform that accelerates development and deployment of production-ready AI with the security, support and stability businesses need.</p>
<p>Getting the best performance for RAG workflows requires massive amounts of memory and compute to move and process data. The <a href="https://nvidianews.nvidia.com/news/gh200-grace-hopper-superchip-with-hbm3e-memory">NVIDIA GH200 Grace Hopper Superchip</a>, with its 288GB of fast HBM3e memory and 8 petaflops of compute, is ideal ‚Äî it can deliver a 150x speedup over using a CPU.</p>
<p>Once companies get familiar with RAG, they can combine a variety of off-the-shelf or custom LLMs with internal or external knowledge bases to create a wide range of assistants that help their employees and customers.</p>
<p>RAG doesn‚Äôt require a data center. LLMs are debuting on Windows PCs, thanks to NVIDIA software that enables all sorts of applications users can access even on their laptops.</p>
<figure id="attachment_68134" aria-describedby="caption-attachment-68134" style="width: 1200px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/Using-RAG-on-PCs.jpg"><img decoding="async" loading="lazy" class="size-full wp-image-68134" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/Using-RAG-on-PCs.jpg" alt="Chart shows running RAG on a PC" width="1200" height="659" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/Using-RAG-on-PCs.jpg 1200w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Using-RAG-on-PCs-400x220.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Using-RAG-on-PCs-672x369.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Using-RAG-on-PCs-768x422.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Using-RAG-on-PCs-819x450.jpg 819w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Using-RAG-on-PCs-392x215.jpg 392w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Using-RAG-on-PCs-182x100.jpg 182w" sizes="(max-width: 1200px) 100vw, 1200px" /></a><figcaption id="caption-attachment-68134" class="wp-caption-text">An example application for RAG on a PC.</figcaption></figure>
<p>PCs equipped with NVIDIA RTX GPUs can now run some AI models locally. By using RAG on a PC, users can link to a private knowledge source ‚Äì whether that be emails, notes or articles ‚Äì to improve responses. The user can then feel confident that their data source, prompts and response all remain private and secure.</p>
<p>A<a href="https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fblogs.nvidia.com%2Fblog%2F2023%2F10%2F17%2Ftensorrt-llm-windows-stable-diffusion-rtx%2F&amp;data=05%7C01%7Crmerritt%40nvidia.com%7Cdfd2267cb8344597f73408dbd0e6fc36%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638333462716560839%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=V5FClNyBybzOnm3%2FKxiTT4i9aoT0GdLWzdLfnF8LBk0%3D&amp;reserved=0"> recent blog</a> provides an example of RAG accelerated by TensorRT-LLM for Windows to get better results fast.</p>
<h2><b>The History of Retrieval-Augmented Generation¬†</b></h2>
<p>The roots of the technique go back at least to the early 1970s. That‚Äôs when researchers in information retrieval prototyped what they called question-answering systems, apps that use natural language processing (<a href="https://www.nvidia.com/en-us/glossary/natural-language-processing/">NLP</a>) to access text, initially in narrow topics such as baseball.</p>
<p>The concepts behind this kind of text mining have remained fairly constant over the years. But the machine learning engines driving them have grown significantly, increasing their usefulness and popularity.</p>
<p>In the mid-1990s, the Ask Jeeves service, now Ask.com, popularized question answering with its mascot of a well-dressed valet. IBM‚Äôs Watson became a TV celebrity in 2011 when it handily beat two human champions on the <i>Jeopardy!</i> game show.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/Ask-Jeeves-2.jpg"><img decoding="async" loading="lazy" class="aligncenter size-full wp-image-68140" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/Ask-Jeeves-2.jpg" alt="Picture of Ask Jeeves, an early RAG-like web service" width="620" height="334" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/Ask-Jeeves-2.jpg 620w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Ask-Jeeves-2-400x215.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Ask-Jeeves-2-399x215.jpg 399w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Ask-Jeeves-2-186x100.jpg 186w" sizes="(max-width: 620px) 100vw, 620px" /></a></p>
<p>Today, LLMs are taking question-answering systems to a whole new level.</p>
<h2><b>Insights From a London Lab</b></h2>
<p>The seminal 2020 paper arrived as Lewis was pursuing a doctorate in NLP at University College London and working for Meta at a new London AI lab. The team was searching for ways to pack more knowledge into an LLM‚Äôs parameters and using a benchmark it developed to measure its progress.</p>
<p>Building on earlier methods and inspired by <a href="https://arxiv.org/pdf/2002.08909.pdf">a paper</a> from Google researchers, the group ‚Äúhad this compelling vision of a trained system that had a retrieval index in the middle of it, so it could learn and generate any text output you wanted,‚Äù Lewis recalled.</p>
<figure id="attachment_68146" aria-describedby="caption-attachment-68146" style="width: 1280px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/IBM-Watson-wins-Jeopardy-YT.jpg"><img decoding="async" loading="lazy" class="wp-image-68146 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/IBM-Watson-wins-Jeopardy-YT.jpg" alt="Picture of IBM Watson winning on &quot;Jeopardy&quot; TV show, popularizing a RAG-like AI service" width="1280" height="720" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/IBM-Watson-wins-Jeopardy-YT.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/11/IBM-Watson-wins-Jeopardy-YT-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/IBM-Watson-wins-Jeopardy-YT-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/IBM-Watson-wins-Jeopardy-YT-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/IBM-Watson-wins-Jeopardy-YT-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/11/IBM-Watson-wins-Jeopardy-YT-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/11/IBM-Watson-wins-Jeopardy-YT-178x100.jpg 178w" sizes="(max-width: 1280px) 100vw, 1280px" /></a><figcaption id="caption-attachment-68146" class="wp-caption-text">The IBM Watson question-answering system became a celebrity when it won big on the TV game show Jeopardy!</figcaption></figure>
<p>When Lewis plugged into the work in progress a promising retrieval system from another Meta team, the first results were unexpectedly impressive.</p>
<p>‚ÄúI showed my supervisor and he said, ‚ÄòWhoa, take the win. This sort of thing doesn‚Äôt happen very often,‚Äô because these workflows can be hard to set up correctly the first time,‚Äù he said.</p>
<p>Lewis also credits major contributions from team members Ethan Perez and Douwe Kiela, then of New York University and Facebook AI Research, respectively.</p>
<p>When complete, the work, which ran on a cluster of NVIDIA GPUs, showed how to make generative AI models more authoritative and trustworthy. It‚Äôs since been cited by hundreds of papers that amplified and extended the concepts in what continues to be an active area of research.</p>
<h2><b>How Retrieval-Augmented Generation Works</b></h2>
<p>At a high level, here‚Äôs how an <a href="https://docs.nvidia.com/ai-enterprise/workflows-generative-ai/0.1.0/technical-brief.html">NVIDIA technical brief</a> describes the RAG process.</p>
<p>When users ask an LLM a question, the AI model sends the query to another model that converts it into a numeric format so machines can read it. The numeric version of the query is sometimes called an embedding or a vector.</p>
<figure id="attachment_68152" aria-describedby="caption-attachment-68152" style="width: 2048px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-scaled.jpg"><img decoding="async" loading="lazy" class="wp-image-68152 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-scaled.jpg" alt="NVIDIA diagram of how RAG works with LLMs" width="2048" height="901" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-400x176.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-672x296.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-768x338.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-1536x676.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-842x370.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-406x179.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-188x83.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-RAG-diagram-1280x563.jpg 1280w" sizes="(max-width: 2048px) 100vw, 2048px" /></a><figcaption id="caption-attachment-68152" class="wp-caption-text">Retrieval-augmented generation combines LLMs with embedding models and vector databases.</figcaption></figure>
<p>The embedding model then compares these numeric values to vectors in a machine-readable index of an available knowledge base. When it finds a match or multiple matches, it retrieves the related data, converts it to human-readable words and passes it back to the LLM.</p>
<p>Finally, the LLM combines the retrieved words and its own response to the query into a final answer it presents to the user, potentially citing sources the embedding model found.</p>
<h2><b>Keeping Sources Current</b></h2>
<p>In the background, the embedding model continuously creates and updates machine-readable indices, sometimes called vector databases, for new and updated knowledge bases as they become available.</p>
<figure id="attachment_68149" aria-describedby="caption-attachment-68149" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/LangChain-2-LLM-with-a-retriveal-process.jpg"><img decoding="async" loading="lazy" class="size-large wp-image-68149" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/LangChain-2-LLM-with-a-retriveal-process-672x268.jpg" alt="Chart of a RAG process described by LangChain" width="672" height="268" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/LangChain-2-LLM-with-a-retriveal-process-672x268.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/LangChain-2-LLM-with-a-retriveal-process-400x159.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/LangChain-2-LLM-with-a-retriveal-process-768x306.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/LangChain-2-LLM-with-a-retriveal-process-842x335.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/11/LangChain-2-LLM-with-a-retriveal-process-406x162.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/11/LangChain-2-LLM-with-a-retriveal-process-188x75.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/11/LangChain-2-LLM-with-a-retriveal-process-1280x510.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/11/LangChain-2-LLM-with-a-retriveal-process.jpg 1386w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-68149" class="wp-caption-text">Retrieval-augmented generation combines LLMs with embedding models and vector databases.</figcaption></figure>
<p>Many developers find LangChain, an open-source library, can be particularly useful in chaining together LLMs, embedding models and knowledge bases. NVIDIA uses LangChain in its reference architecture for retrieval-augmented generation.</p>
<p>The LangChain community provides its own <a href="https://blog.langchain.dev/tutorial-chatgpt-over-your-data/">description of a RAG process</a>.</p>
<p>Looking forward, the future of generative AI lies in creatively chaining all sorts of LLMs and knowledge bases together to create new kinds of assistants that deliver authoritative results users can verify.</p>
<p>Get a hands on using retrieval-augmented generation with an AI chatbot in this <a href="https://www.nvidia.com/en-us/launchpad/ai/generative-ai-knowledge-base-chatbot/">NVIDIA LaunchPad lab</a>.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/Retrieval-Augmented-Generation-RAG-KV-1.jpg"
			type="image/jpeg"
			width="1920"
			height="1025"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/Retrieval-Augmented-Generation-RAG-KV-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[What Is Retrieval-Augmented Generation?]]></media:title>
			<media:description type="html">NVIDIA artist&#039;s concept of retrieval-augmented generation aka RAG</media:description>
			</media:content>
			</item>
		<item>
		<title>Igniting the Future: TensorRT-LLM Release Accelerates AI Inference Performance, Adds Support for New Models Running on RTX-Powered Windows 11 PCs</title>
		<link>https://blogs.nvidia.com/blog/ignite-rtx-ai-tensorrt-llm-chat-api/</link>
		
		<dc:creator><![CDATA[Jesse Clayton]]></dc:creator>
		<pubDate>Wed, 15 Nov 2023 16:00:20 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[TensorRT]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=68161</guid>

					<description><![CDATA[Artificial intelligence on Windows 11 PCs marks a pivotal moment in tech history, revolutionizing experiences for gamers, creators, streamers, office workers, students and even casual PC users. It offers unprecedented opportunities to enhance productivity for users of the more than 100 million Windows PCs and workstations that are powered by RTX GPUs. And NVIDIA RTX <a class="read-more" href="https://blogs.nvidia.com/blog/ignite-rtx-ai-tensorrt-llm-chat-api/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Artificial intelligence on Windows 11 PCs marks a pivotal moment in tech history, revolutionizing experiences for gamers, creators, streamers, office workers, students and even casual PC users.</p>
<p>It offers unprecedented opportunities to enhance productivity for users of the more than 100 million Windows PCs and workstations that are powered by RTX GPUs. And NVIDIA RTX technology is making it even easier for developers to create AI applications to change the way people use computers.</p>
<p>New optimizations, models and resources announced at Microsoft Ignite will help developers deliver new end-user experiences, quicker.</p>
<p>An upcoming update to <a href="https://developer.nvidia.com/tensorrt">TensorRT-LLM</a> ‚Äî open-source software that increases AI inference performance ‚Äî will add support for new <a href="https://www.nvidia.com/en-us/glossary/data-science/large-language-models/">large language models</a> and make demanding AI workloads more accessible on desktops and laptops with RTX GPUs starting at 8GB of VRAM.</p>
<p>TensorRT-LLM for Windows will soon be compatible with OpenAI‚Äôs popular Chat API through a new wrapper. This will enable hundreds of developer projects and applications to run locally on a PC with RTX, instead of in the cloud ‚Äî so users can keep private and proprietary data on Windows 11 PCs.</p>
<p>Custom generative AI requires time and energy to maintain projects. The process can become incredibly complex and time-consuming, especially when trying to collaborate and deploy across multiple environments and platforms.</p>
<p><a href="https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/">AI Workbench</a> is a unified, easy-to-use toolkit that allows developers to quickly create, test and customize pretrained generative AI models and LLMs on a PC or workstation. It provides developers a single platform to organize their AI projects and tune models to specific use cases.</p>
<p>This enables seamless collaboration and deployment for developers to create cost-effective, scalable generative AI models quickly. <a href="https://developer.nvidia.com/ai-workbench-early-access">Join the early access list</a> to be among the first to gain access to this growing initiative and to receive future updates.</p>
<p>To support AI developers, NVIDIA and Microsoft will release <a href="#directml-llama">DirectML enhancements</a> to accelerate one of the most popular foundational AI models, Llama 2. Developers now have more options for cross-vendor deployment, in addition to setting <a href="https://blogs.nvidia.com/blog/tensorrt-llm-windows-stable-diffusion-rtx/">a new standard for performance</a>.</p>
<h2><b>Portable AI</b></h2>
<p>Last month, NVIDIA announced TensorRT-LLM for Windows, a library for accelerating LLM inference.</p>
<p>The next TensorRT-LLM release, v0.6.0 coming later this month, will bring improved inference performance ‚Äî up to 5x faster ‚Äî and enable support for additional popular LLMs, including the new Mistral 7B and Nemotron-3 8B. Versions of these LLMs will run on any GeForce RTX 30 Series and 40 Series GPU with 8GB of RAM or more, making fast, accurate, local LLM capabilities accessible even in some of the most portable Windows devices.</p>
<figure id="attachment_68166" aria-describedby="caption-attachment-68166" style="width: 508px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-68166" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/TensorRT-LLM-Windows-Inference-Perf-Chart-508x500.jpg" alt="TensorRT-LLM V0.6 Windows Perf Chart" width="508" height="500" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/TensorRT-LLM-Windows-Inference-Perf-Chart-508x500.jpg 508w, https://blogs.nvidia.com/wp-content/uploads/2023/11/TensorRT-LLM-Windows-Inference-Perf-Chart-400x394.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/TensorRT-LLM-Windows-Inference-Perf-Chart-768x756.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/TensorRT-LLM-Windows-Inference-Perf-Chart-1536x1513.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/11/TensorRT-LLM-Windows-Inference-Perf-Chart-457x450.jpg 457w, https://blogs.nvidia.com/wp-content/uploads/2023/11/TensorRT-LLM-Windows-Inference-Perf-Chart-218x215.jpg 218w, https://blogs.nvidia.com/wp-content/uploads/2023/11/TensorRT-LLM-Windows-Inference-Perf-Chart-102x100.jpg 102w, https://blogs.nvidia.com/wp-content/uploads/2023/11/TensorRT-LLM-Windows-Inference-Perf-Chart-1280x1261.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/11/TensorRT-LLM-Windows-Inference-Perf-Chart.jpg 1660w" sizes="(max-width: 508px) 100vw, 508px" /><figcaption id="caption-attachment-68166" class="wp-caption-text"><em>Up to 5X performance with the new TensorRT-LLM v0.6.0.</em></figcaption></figure>
<p>The new release of TensorRT-LLM will be available for install on the <a href="https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/">/NVIDIA/TensorRT-LLM</a> GitHub repo. New optimized models will be available on <a href="http://ngc.nvidia.com">ngc.nvidia.com</a>.</p>
<h2><b>Conversing With Confidence¬†</b></h2>
<p>Developers and enthusiasts worldwide use OpenAI‚Äôs Chat API for a wide range of applications ‚Äî from summarizing web content and drafting documents and emails to analyzing and visualizing data and creating presentations.</p>
<p>One challenge with such cloud-based AIs is that they require users to upload their input data, making them impractical for private or proprietary data or for working with large datasets.</p>
<p>To address this challenge, NVIDIA is soon enabling TensorRT-LLM for Windows to offer a similar API interface to OpenAI&#8217;s widely popular ChatAPI, through a new wrapper, offering a similar workflow to developers whether they are designing models and applications to run locally on a PC with RTX or in the cloud. By changing just one or two lines of code, hundreds of AI-powered developer projects and applications can now benefit from fast, local AI. Users can keep their data on their PCs and not worry about uploading datasets to the cloud.</p>
<p><iframe loading="lazy" title="TensorRT-LLM OpenAI Chat API Integration" width="500" height="281" src="https://www.youtube.com/embed/-P17YXulhDc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Perhaps the best part is that many of these projects and applications are open source, making it easy for developers to leverage and extend their capabilities to fuel the adoption of generative AI on Windows, powered by RTX.</p>
<p>The wrapper will work with any LLM that‚Äôs been optimized for TensorRT-LLM (for example, Llama 2, Mistral and Nemotron-3 8B) and is being released as a reference project on GitHub, alongside other developer resources for working with LLMs on RTX.</p>
<h2 id="directml-llama"><b>Model Acceleration</b></h2>
<p>Developers can now leverage cutting-edge AI models and deploy with a cross-vendor API. As part of an ongoing commitment to empower developers, NVIDIA and Microsoft have been working together to accelerate Llama on RTX via the DirectML API.</p>
<p>Building on the announcements for the fastest inference performance for these models announced last month, this new option for cross-vendor deployment makes it easier than ever to bring AI capabilities to PC.</p>
<p>Developers and enthusiasts can experience the latest optimizations by downloading the latest ONNX runtime and following the installation instructions from Microsoft, and installing the <a href="https://www.nvidia.com/download/index.aspx">latest driver from NVIDIA</a>, which will be available on Nov. 21.</p>
<p>These new optimizations, models and resources will accelerate the development and deployment of AI features and applications to the 100 million RTX PCs worldwide, joining the more than 400 AI-powered apps and games already accelerated by RTX GPUs.</p>
<p>As models become even more accessible and developers bring more generative AI-powered functionality to RTX-powered Windows PCs, RTX GPUs will be critical for enabling users to take advantage of this powerful technology.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/RTX_for_AI-Nov_15.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/RTX_for_AI-Nov_15-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Igniting the Future: TensorRT-LLM Release Accelerates AI Inference Performance, Adds Support for New Models Running on RTX-Powered Windows 11 PCs]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Challenge Accepted: Animator Sir Wade Neistadt Leads Robotic Revolution in Record Time This Week ‚ÄòIn the NVIDIA Studio‚Äô</title>
		<link>https://blogs.nvidia.com/blog/sir-wade-autodesk-maya-blender-davinci-resolve/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Tue, 14 Nov 2023 14:00:25 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[In the NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA Studio]]></category>
		<category><![CDATA[Rendering]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=68083</guid>

					<description><![CDATA[Character animator Sir Wade Neistadt works to make animation and 3D education more accessible for aspiring and professional artists alike through video tutorials and industry training.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p><i>Editor‚Äôs note: This post is part of our weekly </i><a href="https://blogs.nvidia.com/blog/tag/in-the-nvidia-studio/"><i>In the NVIDIA Studio</i></a><i> series, which celebrates featured artists, offers creative tips and tricks, and demonstrates how </i><a href="https://www.nvidia.com/en-us/studio/"><i>NVIDIA Studio</i></a><i> technology improves creative workflows. We‚Äôre also deep diving on new </i><a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/"><i>GeForce RTX 40 Series GPU</i></a><i> features, technologies and resources, and how they dramatically accelerate content creation.</i></p>
<p>Character animator Sir Wade Neistadt works to make animation and 3D education more accessible for aspiring and professional artists alike through video tutorials and industry training.</p>
<p>The YouTube creator, who goes by Sir Wade, also likes a challenge. When electronics company Razer recently asked him to create something unique and creative using the new <a href="https://www.razer.com/gaming-laptops/razer-blade-18">Razer Blade 18</a> laptop with <a href="https://www.nvidia.com/en-us/geforce/laptops/">GeForce RTX 4090 graphics</a>, Sir Wade obliged.</p>
<p>‚ÄúI said yes because I thought it‚Äôd be a great opportunity to try something creatively risky and make something I didn‚Äôt yet know how to achieve,‚Äù the artist said.</p>
<p><iframe loading="lazy" title="I Created a VFX Robot Animation in Blender + Maya" width="500" height="281" src="https://www.youtube.com/embed/BJ4WiyNpyLM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<h2><b>I, Robot</b></h2>
<p>One of the hardest parts of getting started on a project is needing to be creative on demand, said Sir Wade. For the Razer piece, the animator started by asking himself two questions: ‚ÄúWhat am I inspired by?‚Äù and ‚ÄúWhat do I have to work with?‚Äù</p>
<p>Sir Wade finds inspiration in games, technology, movies, people-watching and conversations. Fond of tech ‚Äî and having eyed <a href="https://prorigs.com/characters/">characters from the ProRigs library</a> for some time ‚Äî he decided his short animation should feature robots.</p>
<p>When creating a concept for the animation, Sir Wade took an unorthodox approach, skipping the popular step of 2D sketching. Instead, he captured video references by acting out the animations himself.</p>
<p>This gave Sir Wade the opportunity to quickly try a bunch of movements and preview body mechanics for the animation phase. Since ProRigs characters are rigs based on Autodesk Maya, he naturally began his animation work using this 3D software.</p>
<figure id="attachment_68090" aria-describedby="caption-attachment-68090" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-you-shall-not-render-pass-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-68090" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-you-shall-not-render-pass-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-you-shall-not-render-pass-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-you-shall-not-render-pass-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-you-shall-not-render-pass-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-you-shall-not-render-pass-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-you-shall-not-render-pass-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-you-shall-not-render-pass-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-you-shall-not-render-pass-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-68090" class="wp-caption-text">‚ÄúYOU SHALL NOT (RENDER) PASS.‚Äù</figcaption></figure>
<p>His initial approach was straightforward: mimicking the main robot character‚Äôs movements with the edited reference footage. This worked fairly well, as <a href="https://www.nvidia.com/en-us/geforce/rtx/">NVIDIA RTX</a>-accelerated ray tracing and AI denoising with the default Autodesk Arnold renderer resulted in smooth viewport movement and photorealistic visuals.</p>
<p>Then, Sir Wade continued tinkering with the piece, focusing on how the robot‚Äôs arm plates crashed into each other and how its feet moved. This was a great challenge, but he kept moving on the project. The featured artist would advise, ‚ÄúDon‚Äôt wait for everything to be perfect.‚Äù</p>
<figure id="attachment_68093" aria-describedby="caption-attachment-68093" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-68093" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-672x405.png" alt="" width="672" height="405" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-672x405.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-400x241.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-768x463.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-746x450.png 746w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-356x215.png 356w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w-166x100.png 166w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-maya-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-68093" class="wp-caption-text">The video reference footage captured earlier paid off later in Sir Wade‚Äôs creative workflow.</figcaption></figure>
<p>Next, Sir Wade exported files into Blender software with the <a href="https://www.nvidia.com/en-us/omniverse/usd/">Universal Scene Description (OpenUSD)</a> framework, unlocking an open and extensible ecosystem, including the ability to make edits in <a href="https://www.nvidia.com/en-us/omniverse/creators/">NVIDIA Omniverse</a>, a development platform for building and connecting 3D tools and applications. The edits could then be captured in the original native files, eliminating the need for tedious uploading, downloading and file reformatting.</p>
<figure id="attachment_68096" aria-describedby="caption-attachment-68096" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/render-studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-68096" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/render-studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/render-studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/render-studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/render-studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/render-studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/11/render-studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/11/render-studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/11/render-studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-68096" class="wp-caption-text">AI-powered RTX-accelerated OptiX ray tracing in the viewport allowed Sir Wade to manipulate the scene with ease.</figcaption></figure>
<p>Sir Wade browsed the Kitbash3D digital platform with the new asset browser Cargo to compile kits, models and materials, and drag them into Blender with ease. It‚Äôs important at this stage to get base-level models in the scene, he said, so the environment can be further refined.</p>
<figure id="attachment_68099" aria-describedby="caption-attachment-68099" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-laptop-view-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-68099" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-laptop-view-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-laptop-view-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-laptop-view-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-laptop-view-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-laptop-view-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-laptop-view-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-laptop-view-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-laptop-view-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-68099" class="wp-caption-text">Dubbed the ‚Äúultimate desktop replacement,‚Äù the Razer Blade 18 offers NVIDIA GeForce RTX 4090 graphics.</figcaption></figure>
<p>Sir Wade raved about the Razer Blade 18‚Äôs quad-high-definition (QHD+) 18&#8243; screen and 16:10 aspect ratio, which gives him more room to create, as well as its color-calibrated display, which ensures uploads to social media are as accurate as possible and require minimal color correction.</p>
<p>The preinstalled NVIDIA Studio Drivers, free to RTX GPU owners, are extensively tested with the most popular creative software to deliver maximum stability and performance.</p>
<div class="simplePullQuote right"><p>‚ÄúThis is by far the best laptop I‚Äôve ever used for this type of work.‚Äù ‚Äî Sir Wade Neistadt</p>
</div>
<p>Returning to the action, Sir Wade used an emission shader to form the projectiles aimed at the robot. He also tweaked various textures, such as surface imperfections, to make the robot feel more weathered and battle-worn, before moving on to visual effects (VFX).</p>
<p>The artist used basic primitives as particle emitters in Blender to achieve the look of bursting particles over a limited number of frames. This, combined with the robot and floor surfaces containing surface nodes, creates sparks when the robot moves or gets hit by objects.</p>
<p>Sir Wade‚Äôs GeForce RTX 4090 Laptop GPU with Blender Cycles RTX-accelerated OptiX ray tracing in the viewport provides interactive, photorealistic rendering for modeling and animation.</p>
<figure id="attachment_68102" aria-describedby="caption-attachment-68102" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-0436-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-68102" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-0436-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-0436-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-0436-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-0436-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-0436-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-0436-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-0436-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-0436-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-68102" class="wp-caption-text">Particle and collusion effects in Blender enable compelling VFX.</figcaption></figure>
<p>To further experiment with VFX, Sir Wade imported the project into the EmberGen simulation tool to test out various preset and physics effects.</p>
<figure id="attachment_68105" aria-describedby="caption-attachment-68105" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-embergen-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-68105" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-embergen-1280w-672x404.png" alt="" width="672" height="404" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-embergen-1280w-672x404.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-embergen-1280w-400x241.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-embergen-1280w-768x462.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-embergen-1280w-748x450.png 748w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-embergen-1280w-357x215.png 357w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-embergen-1280w-166x100.png 166w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-embergen-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-68105" class="wp-caption-text">VFX in EmberGen.</figcaption></figure>
<p>He added dust and debris VFX, and exported the scene as an OpenVDB file back to Blender to perfect the lighting.</p>
<figure id="attachment_68108" aria-describedby="caption-attachment-68108" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-blender-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-68108" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-blender-1280w-672x399.png" alt="" width="672" height="399" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-blender-1280w-672x399.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-blender-1280w-400x238.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-blender-1280w-768x456.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-blender-1280w-758x450.png 758w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-blender-1280w-362x215.png 362w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-blender-1280w-168x100.png 168w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-sir-wade-neistadt-wk83-robot-bts-blender-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-68108" class="wp-caption-text">Final lighting elements in Blender.</figcaption></figure>
<div class="simplePullQuote right"><p>‚ÄúI chose an NVIDIA RTX GPU-powered system for its reliable speed, performance and stability, as I had a very limited window to complete this project.‚Äù ‚Äî Sir Wade Neistadt</p>
</div>
<p>Finally, Sir Wade completed sound-design effects in Blackmagic Design‚Äôs DaVinci Resolve software.</p>
<p>Sir Wade‚Äôs video tutorials resonate with diverse audiences because of their fresh approach to solving problems and individualistic flair.</p>
<p>‚ÄúCreativity for me doesn‚Äôt come naturally like for other artists,‚Äù Sir Wade explained. ‚ÄúI reverse engineer the process by seeing a tool or a concept, evaluating what‚Äôs interesting, then either figuring out a way to use it uniquely or explaining the discovery in a relatable way.‚Äù</p>
<figure id="attachment_68111" aria-describedby="caption-attachment-68111" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-68111" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-672x252.png" alt="" width="672" height="252" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-672x252.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-400x150.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-768x288.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-842x316.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-406x152.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w-188x71.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/11/studio-itns-sir-wade-neistadt-wk83-artist-feature-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-68111" class="wp-caption-text">Sir Wade Neistadt.</figcaption></figure>
<p>Check out Sir Wade‚Äôs animation workshops on his <a href="http://courses.sirwade.com">website</a>.</p>
<p>Less than two days remain in Sir Wade‚Äôs <a href="https://courses.sirwade.com/challenge">Fall 2023 Animation Challenge</a>. Download the challenge template and Maya character rig files, and submit a custom 3D scene to win an NVIDIA RTX GPU or other prizes by end of day on Wednesday, Nov. 15.</p>
<p><i>Follow NVIDIA Studio on </i><a href="https://www.instagram.com/nvidiastudio/"><i>Instagram</i></a><i>, </i><a href="https://twitter.com/NVIDIAStudio"><i>Twitter</i></a><i> and </i><a href="https://www.facebook.com/NVIDIAStudio/"><i>Facebook</i></a><i>. Access tutorials on the </i><a href="https://www.youtube.com/channel/UCDeQdW6Lt6nhq3mLM4oLGWw"><i>Studio YouTube channel</i></a><i> and get updates directly in your inbox by subscribing to the </i><a href="https://www.nvidia.com/en-us/studio/?nvmid=subscribe-creators-mail-icon"><i>Studio newsletter</i></a><i>.</i><b>¬†</b></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/nv-blog-header-preview-1280x680-2.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/nv-blog-header-preview-1280x680-2-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Challenge Accepted: Animator Sir Wade Neistadt Leads Robotic Revolution in Record Time This Week ‚ÄòIn the NVIDIA Studio‚Äô]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>New Class of Accelerated, Efficient AI Systems Mark the Next Era of Supercomputing</title>
		<link>https://blogs.nvidia.com/blog/efficient-ai-supercomputers-sc23/</link>
		
		<dc:creator><![CDATA[Rick Merritt]]></dc:creator>
		<pubDate>Mon, 13 Nov 2023 20:00:50 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Supercomputing]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Customer Stories]]></category>
		<category><![CDATA[Events]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Hardware]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[New GPU Uses]]></category>
		<category><![CDATA[NVIDIA Hopper Architecture]]></category>
		<category><![CDATA[Science]]></category>
		<category><![CDATA[Scientific Visualization]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=68046</guid>

					<description><![CDATA[NVIDIA today unveiled at SC23 the next wave of technologies that will lift scientific and industrial research centers worldwide to new levels of performance and energy efficiency. ‚ÄúNVIDIA hardware and software innovations are creating a new class of AI supercomputers,‚Äù said Ian Buck, vice president of the company‚Äôs high performance computing and hyperscale data center <a class="read-more" href="https://blogs.nvidia.com/blog/efficient-ai-supercomputers-sc23/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>NVIDIA today unveiled at SC23 the next wave of technologies that will lift scientific and industrial research centers worldwide to new levels of performance and energy efficiency.</p>
<p>‚ÄúNVIDIA hardware and software innovations are creating a new class of AI supercomputers,‚Äù said Ian Buck, vice president of the company‚Äôs high performance computing and hyperscale data center business, in a special address at the conference.</p>
<p>Some of the systems will pack memory-enhanced <a href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">NVIDIA Hopper accelerators</a>, others a new <a href="https://nvidianews.nvidia.com/news/nvidia-grace-hopper-superchip-powers-jupiter-defining-a-new-class-of-supercomputers-to-propel-ai-for-scientific-discovery">NVIDIA Grace Hopper systems architecture</a>. All will use the expanded parallelism to run a full stack of accelerated software for <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">generative AI</a>, HPC and hybrid <a href="https://blogs.nvidia.com/blog/what-is-quantum-computing/">quantum computing</a>.</p>
<p>Buck described the new <a href="https://nvidianews.nvidia.com/news/nvidia-supercharges-hopper-the-worlds-leading-ai-computing-platform">NVIDIA HGX H200</a> as ‚Äúthe world‚Äôs leading AI computing platform.‚Äù</p>
<figure id="attachment_68062" aria-describedby="caption-attachment-68062" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/H200-image.jpg"><img decoding="async" loading="lazy" class="size-large wp-image-68062" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/H200-image-672x411.jpg" alt="Image of H200 GPU system" width="672" height="411" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/H200-image-672x411.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/H200-image-400x244.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/H200-image-768x469.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/H200-image-737x450.jpg 737w, https://blogs.nvidia.com/wp-content/uploads/2023/11/H200-image-352x215.jpg 352w, https://blogs.nvidia.com/wp-content/uploads/2023/11/H200-image-164x100.jpg 164w, https://blogs.nvidia.com/wp-content/uploads/2023/11/H200-image.jpg 1231w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-68062" class="wp-caption-text">NVIDIA H200 Tensor Core GPUs pack HBM3e memory to run growing generative AI models.</figcaption></figure>
<p>It packs up to 141GB of HBM3e, the first AI accelerator to use the ultrafast technology. Running models like GPT-3, NVIDIA H200 Tensor Core GPUs provide an 18x performance increase over prior-generation accelerators.</p>
<p><span>Among other </span><a href="https://github.com/NVIDIA/TensorRT-LLM/blob/release/0.5.0/docs/source/blogs/H200launch.md"><span>generative AI benchmarks</span></a><span>, they zip through 12,000 tokens per second on a Llama2-13B large language model (</span><a href="https://www.nvidia.com/en-us/glossary/data-science/large-language-models/"><span>LLM</span></a><span>). </span></p>
<p>Buck¬†also revealed a server platform that links four <a href="https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/">NVIDIA GH200 Grace Hopper Superchips</a> on an <a href="https://blogs.nvidia.com/blog/what-is-nvidia-nvlink/">NVIDIA NVLink</a> interconnect. The quad configuration puts in a single compute node a whopping 288 Arm Neoverse cores and 16 petaflops of AI performance with up to 2.3 terabytes of high-speed memory.</p>
<figure id="attachment_68065" aria-describedby="caption-attachment-68065" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/Quad-Grace-Hopper-node-configuration.jpg"><img decoding="async" loading="lazy" class="size-large wp-image-68065" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/Quad-Grace-Hopper-node-configuration-672x405.jpg" alt="Image of quad GH200 server node" width="672" height="405" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/Quad-Grace-Hopper-node-configuration-672x405.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Quad-Grace-Hopper-node-configuration-400x241.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Quad-Grace-Hopper-node-configuration-768x463.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Quad-Grace-Hopper-node-configuration-747x450.jpg 747w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Quad-Grace-Hopper-node-configuration-357x215.jpg 357w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Quad-Grace-Hopper-node-configuration-166x100.jpg 166w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Quad-Grace-Hopper-node-configuration.jpg 1256w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-68065" class="wp-caption-text">Server nodes based on the four GH200 Superchips will deliver 16 petaflops of AI performance.</figcaption></figure>
<p>Demonstrating its efficiency, one GH200 Superchip using the <a href="https://github.com/NVIDIA/TensorRT-LLM">NVIDIA TensorRT-LLM</a> open-source library is 100x faster than a dual-socket x86 CPU system and nearly 2x more energy efficient than an X86 + H100 GPU server.</p>
<p>‚ÄúAccelerated computing is sustainable computing,‚Äù Buck said. ‚ÄúBy harnessing the power of accelerated computing and generative AI, together we can drive innovation across industries while reducing our impact on the environment.‚Äù</p>
<h2><b>NVIDIA Powers 38 of 49 New TOP500 Systems</b></h2>
<p>The latest TOP500 list of the world‚Äôs fastest supercomputers reflects the shift toward accelerated, energy-efficient supercomputing.</p>
<p>Thanks to new systems powered by <a href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPUs</a>, NVIDIA now delivers more than 2.5 exaflops of HPC performance across these world-leading systems, up from 1.6 exaflops in the May rankings. NVIDIA‚Äôs contribution on the top 10 alone reaches nearly an exaflop of HPC and 72 exaflops of AI performance.</p>
<p>The new list contains the highest number of systems ever using NVIDIA technologies, 379 vs. 372 in May, including 38 of 49 new supercomputers on the list.</p>
<p>Microsoft Azure leads the newcomers with its Eagle system using H100 GPUs in NDv5 instances to hit No. 3 with 561 petaflops. Mare Nostrum5 in Barcelona ranked No. 8, and NVIDIA Eos ‚Äî which recently set <a href="https://blogs.nvidia.com/blog/scaling-ai-training-mlperf/">new AI training records</a> on the MLPerf benchmarks ‚Äî came in at No. 9.</p>
<p>Showing their energy efficiency, NVIDIA GPUs power 23 of the top 30 systems on the Green500. And they retained the No. 1 spot with the H100 GPU-based Henri system, which delivers 65.09 gigaflops per watt for the Flatiron Institute in New York.</p>
<h2><b>Gen AI Explores COVID</b></h2>
<p><span>Showing what‚Äôs possible</span>, the Argonne National Laboratory used <a href="https://www.nvidia.com/en-gb/gpu-cloud/bionemo/">NVIDIA BioNeMo</a>, a generative AI platform for biomolecular LLMs, to develop <a href="https://blogs.nvidia.com/blog/generative-ai-covid-genome-sequences/">GenSLMs</a>, a model that can generate gene sequences that closely resemble real-world variants of the coronavirus. Using NVIDIA GPUs and data from 1.5 million COVID genome sequences, it can also rapidly identify new virus variants.</p>
<p>The work<a href="https://blogs.nvidia.com/blog/genomic-large-language-model-predicts-covid-variants/"> won the Gordon Bell special prize</a> last year and was trained on supercomputers, including Argonne‚Äôs<a href="https://nvidianews.nvidia.com/news/nvidia-turbocharges-extreme-scale-ai-for-argonne-national-laboratorys-polaris-supercomputer"> Polaris</a> system, the U.S. Department of Energy‚Äôs<a href="https://blogs.nvidia.com/blog/nersc-perlmutter-ai-supercomputer/"> Perlmutter</a> and NVIDIA‚Äôs<a href="https://blogs.nvidia.com/blog/making-selene-pandemic-ai/"> Selene</a>.</p>
<p>It‚Äôs ‚Äújust the tip of the iceberg ‚Äî the future is brimming with possibilities, as generative AI continues to redefine the landscape of scientific exploration,‚Äù said Kimberly Powell, vice president of healthcare at NVIDIA, in the special address.</p>
<h2><b>Saving Time, Money and Energy</b></h2>
<p>Using the latest technologies, accelerated workloads can see an order-of-magnitude reduction in system cost and energy used, Buck said.</p>
<p>For example, Siemens teamed with Mercedes to analyze aerodynamics and related acoustics for its new electric EQE vehicles. The simulations that took weeks on CPU clusters ran significantly faster using the latest NVIDIA H100 GPUs. In addition, Hopper GPUs let them reduce costs by 3x and reduce energy consumption by 4x (below).</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/Sustainable-computing-chart.jpg"><img decoding="async" loading="lazy" class="aligncenter size-large wp-image-68068" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/Sustainable-computing-chart-672x351.jpg" alt="Chart showing the performance and energy efficiency of H100 GPUs" width="672" height="351" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/Sustainable-computing-chart-672x351.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Sustainable-computing-chart-400x209.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Sustainable-computing-chart-768x401.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Sustainable-computing-chart-842x439.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Sustainable-computing-chart-406x212.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Sustainable-computing-chart-188x98.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Sustainable-computing-chart-1280x668.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Sustainable-computing-chart.jpg 1319w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<h2><b>Switching on 200 Exaflops Beginning Next Year</b></h2>
<p>Scientific and industrial advances will come from every corner of the globe where the latest systems are <a href="https://blogs.nvidia.com/blog/gh200-grace-hopper-superchip-powers-ai-supercomputers/">being deployed</a>.</p>
<p>‚ÄúWe already see a combined 200 <a href="https://blogs.nvidia.com/blog/what-is-an-exaflop/">exaflops</a> of AI on Grace Hopper supercomputers going to production 2024,‚Äù Buck said.</p>
<p>They include the massive <a href="https://nvidianews.nvidia.com/news/nvidia-grace-hopper-superchip-powers-jupiter-defining-a-new-class-of-supercomputers-to-propel-ai-for-scientific-discovery">JUPITER supercomputer</a> at Germany‚Äôs J√ºlich center. It can deliver 93 exaflops of performance for AI training and 1 exaflop for HPC applications, while consuming only 18.2 megawatts of power.</p>
<figure id="attachment_68071" aria-describedby="caption-attachment-68071" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/Chart-on-350-exaflops-of-AI-performance-deployed.jpg"><img decoding="async" loading="lazy" class="size-large wp-image-68071" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/Chart-on-350-exaflops-of-AI-performance-deployed-672x403.jpg" alt="Chart of deployed performance of supercomputers using NVIDIA GPUs through 2024" width="672" height="403" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/Chart-on-350-exaflops-of-AI-performance-deployed-672x403.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Chart-on-350-exaflops-of-AI-performance-deployed-400x240.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Chart-on-350-exaflops-of-AI-performance-deployed-768x460.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Chart-on-350-exaflops-of-AI-performance-deployed-751x450.jpg 751w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Chart-on-350-exaflops-of-AI-performance-deployed-359x215.jpg 359w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Chart-on-350-exaflops-of-AI-performance-deployed-167x100.jpg 167w, https://blogs.nvidia.com/wp-content/uploads/2023/11/Chart-on-350-exaflops-of-AI-performance-deployed.jpg 1201w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-68071" class="wp-caption-text">Research centers are poised to switch on a tsunami of GH200 performance.</figcaption></figure>
<p>Based on Eviden‚Äôs BullSequana XH3000 liquid-cooled system, JUPITER will use the NVIDIA quad GH200 system architecture and <a href="https://www.nvidia.com/en-us/networking/quantum2/">NVIDIA Quantum-2 InfiniBand</a> networking for climate and weather predictions, drug discovery, hybrid quantum computing and digital twins. JUPITER quad GH200 nodes will be configured with 864GB of high-speed memory.</p>
<p>It‚Äôs one of several new supercomputers using Grace Hopper that NVIDIA announced at SC23.</p>
<p>The HPE Cray EX2500 system from Hewlett Packard Enterprise will use the quad GH200 to power many AI supercomputers coming online next year.</p>
<p>For example, HPE uses the quad GH200 to power OFP-II, an advanced HPC system in Japan shared by the University of Tsukuba and the University of Tokyo, as well as the DeltaAI system, which will triple computing capacity for the U.S. National Center for Supercomputing Applications.</p>
<p>HPE is also building the <a href="https://blogs.nvidia.com/blog/special-address-isc-2022-hpc/">Venado</a> system for the Los Alamos National Laboratory, the first GH200 to be deployed in the U.S. In addition, HPE is building GH200 supercomputers in the Middle East, Switzerland and the U.K.</p>
<h2><b>Grace Hopper in Texas and Beyond</b></h2>
<p>At the Texas Advanced Computing Center (TACC), Dell Technologies is building the Vista supercomputer with NVIDIA Grace Hopper and Grace CPU Superchips.</p>
<p>More than 100 global enterprises and organizations, including NASA Ames Research Center and Total Energies, have already purchased Grace Hopper early-access systems, Buck said.</p>
<p>They join previously announced GH200 users such as <a href="https://blogs.nvidia.com/blog/computex-keynote-generative-ai/">SoftBank</a> and the <a href="https://blogs.nvidia.com/blog/uk-largest-ai-supercomputer/">University of Bristol</a>, as well as the massive <a href="https://blogs.nvidia.com/blog/supercomputing-ai-eurohpc/">Leonardo</a> system with 14,000 NVIDIA A100 GPUs that delivers 10 exaflops of AI performance for Italy‚Äôs Cineca consortium.</p>
<h2><b>The View From Supercomputing Centers</b></h2>
<p>Leaders from supercomputing centers around the world shared their plans and work in progress with the latest systems.</p>
<p>‚ÄúWe‚Äôve been collaborating with MeteoSwiss ECMWP as well as scientists from ETH EXCLAIM and NVIDIA‚Äôs Earth-2 project to create an infrastructure that will push the envelope in all dimensions of big data analytics and extreme scale computing,‚Äù said Thomas Schultess, director of the Swiss National Supercomputing Centre of work on the Alps supercomputer.</p>
<p>‚ÄúThere‚Äôs really impressive energy-efficiency gains across our stacks,‚Äù Dan Stanzione, executive director of TACC, said of Vista.</p>
<p>It‚Äôs ‚Äúreally the stepping stone to move users from the kinds of systems we‚Äôve done in the past to looking at this new Grace Arm CPU and Hopper GPU tightly coupled combination and ‚Ä¶ we‚Äôre looking to scale out by probably a factor of 10 or 15 from what we are deploying with Vista when we deploy Horizon in a couple years,‚Äù he said.</p>
<h2><b>Accelerating the Quantum Journey</b></h2>
<p>Researchers are also using today‚Äôs accelerated systems to pioneer a path to tomorrow‚Äôs supercomputers.</p>
<p>In Germany, JUPITER ‚Äúwill revolutionize scientific research across climate, materials, drug discovery and quantum computing,‚Äù said Kristel Michelson, who leads Julich‚Äôs research group on quantum information processing.</p>
<p>‚ÄúJUPITER‚Äôs architecture also allows for the seamless integration of quantum algorithms with parallel HPC algorithms, and this is mandatory for effective quantum HPC hybrid simulations,‚Äù she said.</p>
<h2><b>CUDA Quantum Drives Progress</b></h2>
<p>The special address also showed how <a href="https://developer.nvidia.com/cuda-quantum">NVIDIA CUDA Quantum</a> ‚Äî a platform for programming CPUs, GPUs and quantum computers also known as <a href="https://blogs.nvidia.com/blog/what-is-a-qpu/">QPUs</a> ‚Äî is advancing research in quantum computing.</p>
<p>For example, researchers at <a href="https://blogs.nvidia.com/blog/basf-cuda-quantum-momentum/">BASF</a>, the world‚Äôs largest chemical company, pioneered a new hybrid quantum-classical method for simulating chemicals that can shield humans against harmful metals. They join researchers at Brookhaven National Laboratory and HPE who are separately pushing the frontiers of science with CUDA Quantum.</p>
<p>NVIDIA also announced a collaboration with Classiq, a developer of quantum programming tools, to create a life sciences research center at the Tel Aviv Sourasky Medical Center, Israel‚Äôs largest teaching hospital.¬† The center will use Classiq‚Äôs software and CUDA Quantum running on an <a href="https://www.nvidia.com/en-us/data-center/dgx-h100/">NVIDIA DGX H100 system</a>.</p>
<p>Separately, Quantum Machines will deploy the first <a href="https://www.nvidia.com/en-us/data-center/dgx-quantum/">NVIDIA DGX Quantum</a>, a system using Grace Hopper Superchips, at the Israel National Quantum Center that aims to drive advances across scientific fields. The DGX system will be connected to a superconducting QPU by Quantware and a photonic QPU from ORCA Computing, both powered by CUDA Quantum.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/quantum-computing-partners.jpg"><img decoding="async" loading="lazy" class="aligncenter size-large wp-image-68074" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/quantum-computing-partners-672x402.jpg" alt="Logos of NVIDIA CUDA Quantum partners" width="672" height="402" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/quantum-computing-partners-672x402.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/quantum-computing-partners-400x239.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/quantum-computing-partners-768x459.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/quantum-computing-partners-752x450.jpg 752w, https://blogs.nvidia.com/wp-content/uploads/2023/11/quantum-computing-partners-359x215.jpg 359w, https://blogs.nvidia.com/wp-content/uploads/2023/11/quantum-computing-partners-167x100.jpg 167w, https://blogs.nvidia.com/wp-content/uploads/2023/11/quantum-computing-partners.jpg 1227w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>‚ÄúIn just two years, our NVIDIA quantum computing platform has amassed over 120 partners [above], a testament to its open, innovative platform,‚Äù Buck said.</p>
<p>Overall, the work across many fields of discovery reveals a new trend that combines accelerated computing at data center scale with NVIDIA‚Äôs full-stack innovation.</p>
<p>‚ÄúAccelerated computing is paving the path for sustainable computing with advancements that provide not just amazing technology but a more sustainable and impactful future,‚Äù he concluded.</p>
<p>Watch NVIDIA‚Äôs<a href="http://www.nvidia.com/sc23"> SC23 special address</a> below.</p>
<p><iframe loading="lazy" title="NVIDIA SC23 Special Address" width="500" height="281" src="https://www.youtube.com/embed/6g0v3tMK2LU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>&nbsp;</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/JUPITER-supercomputer-2-in-Germany.jpg"
			type="image/jpeg"
			width="1389"
			height="737"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/JUPITER-supercomputer-2-in-Germany-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[New Class of Accelerated, Efficient AI Systems Mark the Next Era of Supercomputing]]></media:title>
			<media:description type="html">Image of JUPITER supercomputer in Germany</media:description>
			</media:content>
			</item>
		<item>
		<title>Gen AI for the Genome: LLM Predicts Characteristics of COVID Variants</title>
		<link>https://blogs.nvidia.com/blog/generative-ai-covid-genome-sequences/</link>
		
		<dc:creator><![CDATA[Isha Salian]]></dc:creator>
		<pubDate>Mon, 13 Nov 2023 14:00:46 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Research]]></category>
		<category><![CDATA[Supercomputing]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[COVID-19]]></category>
		<category><![CDATA[Healthcare and Life Sciences]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[NVIDIA DGX]]></category>
		<category><![CDATA[NVIDIA Research]]></category>
		<category><![CDATA[Science]]></category>
		<category><![CDATA[Social Impact]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=68026</guid>

					<description><![CDATA[A widely acclaimed large language model for genomic data has demonstrated its ability to generate gene sequences that closely resemble real-world variants of SARS-CoV-2, the virus behind COVID-19. Called GenSLMs, the model, which last year won the Gordon Bell special prize for high performance computing-based COVID-19 research, was trained on a dataset of nucleotide sequences <a class="read-more" href="https://blogs.nvidia.com/blog/generative-ai-covid-genome-sequences/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>A widely acclaimed large language model for genomic data has demonstrated its ability to generate gene sequences that closely resemble real-world variants of SARS-CoV-2, the virus behind COVID-19.</p>
<p>Called GenSLMs, the model, which last year <a href="https://blogs.nvidia.com/blog/genomic-large-language-model-predicts-covid-variants/">won the Gordon Bell special prize</a> for high performance computing-based COVID-19 research, was trained on a dataset of nucleotide sequences ‚Äî the building blocks of DNA and RNA. It was developed by researchers from Argonne National Laboratory, NVIDIA, the University of Chicago and a score of other academic and commercial collaborators.</p>
<p>When the researchers looked back at the nucleotide sequences generated by GenSLMs, they discovered that specific characteristics of the AI-generated sequences closely matched the real-world Eris and Pirola subvariants that have been prevalent this year ‚Äî even though the AI was only trained on COVID-19 virus genomes from the first year of the pandemic.</p>
<p>‚ÄúOur model‚Äôs generative process is extremely naive, lacking any specific information or constraints around what a new COVID variant should look like,‚Äù said Arvind Ramanathan, lead researcher on the project and a computational biologist at Argonne. ‚ÄúThe AI‚Äôs ability to predict the kinds of gene mutations present in recent COVID strains ‚Äî despite having only seen the Alpha and Beta variants during training ‚Äî is a strong validation of its capabilities.‚Äù</p>
<p>In addition to generating its own sequences, GenSLMs can also classify and cluster different COVID genome sequences by distinguishing between variants. In a <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/genslm">demo available on NGC</a>, NVIDIA‚Äôs hub for accelerated software, users can explore visualizations of GenSLMs‚Äô analysis of the evolutionary patterns of various proteins within the COVID viral genome.</p>
<div style="width: 1920px;" class="wp-video"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video class="wp-video-shortcode" id="video-68026-1" width="1920" height="1080" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/GenSLM.mp4?_=1" /><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/GenSLM.mp4">https://blogs.nvidia.com/wp-content/uploads/2023/11/GenSLM.mp4</a></video></div>
<p>&nbsp;</p>
<h2><b>Reading Between the Lines, Uncovering Evolutionary Patterns</b></h2>
<p>A key feature of GenSLMs is its ability to interpret long strings of nucleotides ‚Äî represented with sequences of the letters A, T, G and C in DNA, or A, U, G and C in RNA ‚Äî in the same way an LLM trained on English text would interpret a sentence. This capability enables the model to understand the relationship between different areas of the genome, which in coronaviruses consists of around 30,000 nucleotides.</p>
<p>In the <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/ai-foundation/models/genslm">NGC demo</a>, users can choose from among eight different COVID variants to understand how the AI model tracks mutations across various proteins of the viral genome. The visualization depicts evolutionary couplings across the viral proteins ‚Äî highlighting which snippets of the genome are likely to be seen in a given variant.</p>
<p>‚ÄúUnderstanding how different parts of the genome are co-evolving gives us clues about how the virus may develop new vulnerabilities or new forms of resistance,‚Äù Ramanathan said. ‚ÄúLooking at the model‚Äôs understanding of which mutations are particularly strong in a variant may help scientists with downstream tasks like determining how a specific strain can evade the human immune system.‚Äù</p>
<div style="width: 1080px;" class="wp-video"><video class="wp-video-shortcode" id="video-68026-2" width="1080" height="1080" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/SLM.mp4?_=2" /><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/SLM.mp4">https://blogs.nvidia.com/wp-content/uploads/2023/11/SLM.mp4</a></video></div>
<p>&nbsp;</p>
<p>GenSLMs was trained on more than 110 million prokaryotic genome sequences and fine-tuned with a global dataset of around 1.5 million COVID viral sequences using open-source data from the Bacterial and Viral Bioinformatics Resource Center. In the future, the model could be fine-tuned on the genomes of other viruses or bacteria, enabling new research applications.</p>
<p>To train the model, the researchers used <a href="https://www.nvidia.com/en-us/data-center/tensor-cores/">NVIDIA A100 Tensor Core GPU</a>-powered supercomputers, including Argonne‚Äôs <a href="https://nvidianews.nvidia.com/news/nvidia-turbocharges-extreme-scale-ai-for-argonne-national-laboratorys-polaris-supercomputer">Polaris</a> system, the U.S. Department of Energy‚Äôs <a href="https://blogs.nvidia.com/blog/nersc-perlmutter-ai-supercomputer/">Perlmutter</a> and NVIDIA‚Äôs <a href="https://blogs.nvidia.com/blog/making-selene-pandemic-ai/">Selene</a>.</p>
<p>The GenSLMs research team‚Äôs Gordon Bell special prize was awarded at last year‚Äôs SC22 supercomputing conference. At this week‚Äôs SC23, in Denver, NVIDIA is sharing a new range of groundbreaking work in the field of accelerated computing. <a href="https://www.nvidia.com/en-us/events/supercomputing/">View the full schedule</a> and catch the replay of NVIDIA&#8217;s special address below.</p>
<p><iframe loading="lazy" title="NVIDIA SC23 Special Address" width="500" height="281" src="https://www.youtube.com/embed/6g0v3tMK2LU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>NVIDIA Research comprises hundreds of scientists and engineers worldwide, with teams focused on topics including AI, computer graphics, computer vision, self-driving cars and robotics. Learn more about <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a> and subscribe to <a href="https://www.nvidia.com/en-us/industries/healthcare-life-sciences/healthcare-news-sign-up/">NVIDIA healthcare news</a>.</p>
<p><i>Main image courtesy of Argonne National Laboratory‚Äôs Bharat Kale.¬†</i></p>
<p><i>This research was supported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of the U.S. DOE Office of Science and the National Nuclear Security Administration. Research was supported by the DOE through the National Virtual Biotechnology Laboratory, a consortium of DOE national laboratories focused on response to COVID-19, with funding from the Coronavirus CARES Act.</i></p>
</div>]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2023/11/GenSLM.mp4" length="5494935" type="video/mp4" />
<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2023/11/SLM.mp4" length="10414507" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/research-corp-blog-genslm-demo-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/research-corp-blog-genslm-demo-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Gen AI for the Genome: LLM Predicts Characteristics of COVID Variants]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Researchers Poised for Advances With NVIDIA CUDA Quantum</title>
		<link>https://blogs.nvidia.com/blog/basf-cuda-quantum-momentum/</link>
		
		<dc:creator><![CDATA[Sam Stanwyck]]></dc:creator>
		<pubDate>Mon, 13 Nov 2023 14:00:16 +0000</pubDate>
				<category><![CDATA[Software]]></category>
		<category><![CDATA[Supercomputing]]></category>
		<category><![CDATA[Customer Stories]]></category>
		<category><![CDATA[Events]]></category>
		<category><![CDATA[GPU Computing]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[New GPU Uses]]></category>
		<category><![CDATA[Science]]></category>
		<category><![CDATA[Scientific Visualization]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=68042</guid>

					<description><![CDATA[Michael Kuehn and Davide Vodola are taking to new heights work that‚Äôs pioneering quantum computing for the world‚Äôs largest chemical company. The BASF researchers are demonstrating how a quantum algorithm can see what no traditional simulation can ‚Äî key attributes of NTA, a compound with applications that include removing toxic metals like iron from a <a class="read-more" href="https://blogs.nvidia.com/blog/basf-cuda-quantum-momentum/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Michael Kuehn and Davide Vodola are taking to new heights work that‚Äôs pioneering <a href="https://blogs.nvidia.com/blog/what-is-quantum-computing/">quantum computing</a> for the world‚Äôs largest chemical company.</p>
<p>The BASF researchers are demonstrating how a quantum algorithm can see what no traditional simulation can ‚Äî key attributes of NTA, a compound with applications that include removing toxic metals like iron from a city‚Äôs wastewater.</p>
<p>The quantum computing team at BASF simulated on GPUs how the equivalent of 24 qubits ‚Äî the processing engines of a quantum computer ‚Äî can tackle the challenge.</p>
<p>Many corporate R&amp;D centers would consider that a major achievement, but they pressed on, and recently ran their first 60 qubit simulations on NVIDIA‚Äôs Eos H100 Supercomputer.</p>
<p>‚ÄúIt‚Äôs the largest simulation of a molecule using a quantum algorithm we‚Äôve ever run,‚Äù said Kuehn.</p>
<h2><b>Flexible, Friendly Software</b></h2>
<p>BASF is running the simulation on <a href="https://developer.nvidia.com/cuda-quantum">NVIDIA CUDA Quantum</a>, a platform for programming CPUs, GPUs and quantum computers, also known as <a href="https://blogs.nvidia.com/blog/what-is-a-qpu/">QPUs</a>.</p>
<p>Vodola described it as ‚Äúvery flexible and user friendly, letting us build up a complex quantum circuit simulation from relatively simple building blocks. Without CUDA Quantum, it would be impossible to run this simulation,‚Äù he said.</p>
<p>The work requires a lot of heavy lifting, too, so BASF turned to an <a href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a> service that uses <a href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPUs</a>.</p>
<p>‚ÄúWe need a lot of computing power, and the NVIDIA platform is significantly faster than CPU-based hardware for this kind of simulation,‚Äù said Kuehn.</p>
<p>BASF‚Äôs quantum computing initiative, which Kuehn helped launch, started in 2017. In addition to its work in chemistry, the team is developing use cases for quantum computing in machine learning as well as optimizations for logistics and scheduling.</p>
<h2><b>An Expanding CUDA Quantum Community</b></h2>
<p>Other research groups are also advancing science with CUDA Quantum.</p>
<p>At SUNY Stony Brook, researchers are pushing the boundaries of high-energy physics to simulate complex interactions of subatomic particles. Their work promises new discoveries in fundamental physics.</p>
<p>‚ÄúCUDA Quantum enables us to do quantum simulations that would otherwise be impossible,‚Äù said Dmitri Kharzeev,¬† a SUNY professor and scientist at Brookhaven National Lab.</p>
<p>In addition, a research team at <a href="https://community.hpe.com/t5/advancing-life-work/hpc-for-simulating-quantum-circuits/ba-p/7200536">Hewlett Packard Labs</a> is using the <a href="https://blogs.nvidia.com/blog/nersc-perlmutter-ai-supercomputer/">Perlmutter supercomputer</a> to explore magnetic phase transition in quantum chemistry in one of the largest simulations of its kind. The effort could reveal important and unknown details of physical processes too difficult to model with conventional techniques.</p>
<p>‚ÄúAs quantum computers progress toward useful applications, high-performance classical simulations will be key for prototyping novel quantum algorithms,‚Äù said Kirk Bresniker, a chief architect at Hewlett Packard Labs. ‚ÄúSimulating and learning from quantum data are promising avenues toward tapping quantum computing‚Äôs potential.‚Äù</p>
<h2><b>A Quantum Center for Healthcare</b></h2>
<p>These efforts come as support for CUDA Quantum expands worldwide.</p>
<p>Classiq ‚Äî an Israeli startup that already has more than 400 universities using its novel approach to writing quantum programs ‚Äî <a href="https://classiq.io/insights/classiq-announces-quantum-center-for-life-sciences-in-collaboration-with-nvidia">announced </a>today a new research center at the Tel Aviv Sourasky Medical Center, Israel‚Äôs largest teaching hospital.</p>
<p>Created in collaboration with NVIDIA, it will train experts in life science to write quantum applications that could someday help doctors diagnose diseases or accelerate the discovery of new drugs.</p>
<p>Classiq created quantum design software that automates low-level tasks, so developers don‚Äôt need to know all the complex details of how a quantum computer works. It‚Äôs now being integrated with CUDA Quantum.</p>
<p><a href="https://terraquantum.swiss/news/terra-quantum-nvidia-collaborate-to-unlock-unprecedented-business-performance-with-hybrid-quantum-computing">Terra Quantum</a>, a quantum services company with headquarters in Germany and Switzerland, is developing hybrid quantum applications for life sciences, energy, chemistry and finance that will run on CUDA Quantum. And IQM in Finland is enabling its superconducting QPU to use CUDA Quantum.</p>
<h2><b>Quantum Loves Grace Hopper</b></h2>
<p>Several companies, including Oxford Quantum Circuits, will use <a href="https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/">NVIDIA Grace Hopper Superchips</a> to power their hybrid quantum efforts. Based in Reading, England, Oxford Quantum is using Grace Hopper in a hybrid QPU/GPU system programmed by CUDA Quantum.</p>
<p>Quantum Machines announced that the Israeli National Quantum Center will be the first deployment of <a href="https://www.nvidia.com/en-us/data-center/dgx-quantum/">NVIDIA DGX Quantum</a>, a system using Grace Hopper Superchips. Based in Tel Aviv, the center will tap DGX Quantum to power quantum computers from Quantware, ORCA Computing and more.</p>
<p>In addition, Grace Hopper is being put to work by qBraid, in Chicago, to build a quantum cloud service, and Fermioniq, in Amsterdam, to develop tensor-network algorithms.</p>
<p>The large quantity of shared memory and the memory bandwidth of Grace Hopper make these superchips an excellent fit for memory-hungry quantum simulations.</p>
<p>Get started programming hybrid quantum systems today with the latest release of CUDA Quantum from <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda-quantum">NGC</a>, NVIDIA‚Äôs catalog of accelerated software, or <a href="https://github.com/NVIDIA/cuda-quantum">GitHub</a>.</p>
<p><em>(Picture above source: BASF)</em></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/BASF-flags-x-1280-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1092"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/BASF-flags-x-1280-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Researchers Poised for Advances With NVIDIA CUDA Quantum]]></media:title>
			<media:description type="html">Image of BASF flags</media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Grace Hopper Superchip Powers 40+ AI Supercomputers Across Global Research Centers, System Makers, Cloud Providers</title>
		<link>https://blogs.nvidia.com/blog/gh200-grace-hopper-superchip-powers-ai-supercomputers/</link>
		
		<dc:creator><![CDATA[Angie Lee]]></dc:creator>
		<pubDate>Mon, 13 Nov 2023 14:00:08 +0000</pubDate>
				<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Supercomputing]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Climate]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[NVIDIA Hopper Architecture]]></category>
		<category><![CDATA[Science]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=68040</guid>

					<description><![CDATA[Dozens of new supercomputers for scientific computing will soon hop online, powered by NVIDIA‚Äôs breakthrough GH200 Grace Hopper Superchip for giant-scale AI and high performance computing. The NVIDIA GH200 enables scientists and researchers to tackle the world‚Äôs most challenging problems by accelerating complex AI and HPC applications running terabytes of data. At the SC23 supercomputing <a class="read-more" href="https://blogs.nvidia.com/blog/gh200-grace-hopper-superchip-powers-ai-supercomputers/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Dozens of new supercomputers for scientific computing will soon hop online, powered by NVIDIA‚Äôs breakthrough GH200 Grace Hopper Superchip for giant-scale AI and high performance computing.</p>
<p>The <a href="https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/" target="_blank" rel="noopener">NVIDIA GH200</a> enables scientists and researchers to tackle the world‚Äôs most challenging problems by accelerating complex AI and HPC applications running terabytes of data.</p>
<p>At the <a href="https://www.nvidia.com/en-us/events/supercomputing/" target="_blank" rel="noopener">SC23</a> supercomputing show, NVIDIA today announced that the superchip is coming to more systems worldwide, including from Dell Technologies, Eviden, Hewlett Packard Enterprise (HPE), Lenovo, QCT and Supermicro.</p>
<p>Bringing together the Arm-based <a href="https://www.nvidia.com/en-us/data-center/grace-cpu/" target="_blank" rel="noopener">NVIDIA Grace CPU</a> and <a href="https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/" target="_blank" rel="noopener">Hopper GPU</a> architectures using <a href="https://www.nvidia.com/en-us/data-center/nvlink-c2c/" target="_blank" rel="noopener">NVIDIA NVLink-C2C</a> interconnect technology, GH200 also serves as the engine behind scientific supercomputing centers across the globe.</p>
<p>Combined, these GH200-powered centers represent some 200 <a href="https://blogs.nvidia.com/blog/what-is-an-exaflop/" target="_blank" rel="noopener">exaflops</a> of AI performance to drive scientific innovation.</p>
<h2><b>HPE Cray Supercomputers Integrate NVIDIA Grace Hopper</b></h2>
<p>At the show in Denver, HPE announced it will offer HPE Cray EX2500 supercomputers with the NVIDIA Grace Hopper Superchip. The integrated solution will feature quad GH200 processors, scaling up to tens of thousands of Grace Hopper Superchip nodes to provide organizations with unmatched supercomputing agility and quicker AI training. This configuration will also be part of a supercomputing solution for generative AI that HPE introduced today.</p>
<p>‚ÄúOrganizations are rapidly adopting generative AI to accelerate business transformations and technological breakthroughs,‚Äù said Justin Hotard, executive vice president and general manager of HPC, AI and Labs at HPE. ‚ÄúWorking with NVIDIA, we‚Äôre excited to deliver a full supercomputing solution for generative AI, powered by technologies like Grace Hopper, which will make it easy for customers to accelerate large-scale AI model training and tuning at new levels of efficiency.‚Äù</p>
<h2><b>Next-Generation AI Supercomputing Centers</b></h2>
<p>A vast array of the world‚Äôs supercomputing centers are powered by NVIDIA Grace Hopper systems. Several top centers announced at SC23 that they‚Äôre now integrating GH200 systems for their supercomputers.</p>
<p>Germany‚Äôs J√ºlich Supercomputing Centre will use GH200 superchips in JUPITER, set to become the first exascale supercomputer in Europe. The supercomputer will help tackle urgent scientific challenges, such as mitigating climate change, combating pandemics and bolstering sustainable energy production.</p>
<p>Japan‚Äôs Joint Center for Advanced High Performance Computing ‚Äî established between the Center for Computational Sciences at the University of Tsukuba and the Information Technology Center at the University of Tokyo ‚Äî promotes advanced computational sciences integrated with data analytics, AI and machine learning across academia and industry. Its next-generation supercomputer will be powered by NVIDIA Grace Hopper.</p>
<p>The Texas Advanced Computing Center, based in Austin, Texas, designs and operates some of the world&#8217;s most powerful computing resources. The center will power its Vista supercomputer with NVIDIA GH200 for low power and high-bandwidth memory to deliver more computation while enabling bigger models to run with greater efficiency.</p>
<p>The National Center for Supercomputing Applications at the University of Illinois Urbana-Champaign will tap NVIDIA Grace Hopper superchips to power DeltaAI, an advanced computing and data resource set to triple NCSA‚Äôs AI-focused computing capacity.</p>
<p>And, the University of Bristol recently received funding from the UK government to build <a href="https://blogs.nvidia.com/blog/uk-largest-ai-supercomputer/" target="_blank" rel="noopener">Isambard-AI</a>, set to be the country‚Äôs most powerful supercomputer, which will enable AI-driven breakthroughs in robotics, big data, climate research and drug discovery. The new system, being built by HPE, will be equipped with over 5,000 NVIDIA GH200 Grace Hopper Superchips, providing 21 exaflops of AI supercomputing power capable of making 21 quintillion AI calculations per second.</p>
<p>These systems join previously announced next-generation Grace Hopper systems from the Swiss National Supercomputing Centre, Los Alamos National Laboratory and SoftBank Corp.</p>
<h2><b>GH200 Shipping Globally and Available in Early Access from CSPs</b></h2>
<p>GH200 is available in early access from select cloud service providers such as <a href="https://lambdalabs.com/blog/lambda-cloud-clusters-now-available-with-nvidia-gh200-grace-hopper-superchip?utm_medium=press-release&amp;utm_campaign=2023-11-NVIDIA-announcements" target="_blank" rel="noopener">Lambda</a> and <a href="https://www.vultr.com/news/NVIDIA-GH200-Grace-Hopper-Superchip/">Vultr</a>. <a href="https://blogs.oracle.com/cloud-infrastructure/post/oci-plans-offer-nvidia-grace-hopper-superchip" target="_blank" rel="noopener">Oracle Cloud Infrastructure</a> today announced plans to offer GH200 instances, while <a href="https://www.coreweave.com/blog/kicking-off-sc23-coreweave-collaborates-with-nvidia" target="_blank" rel="noopener">CoreWeave</a> detailed plans for early availability of its GH200 instances starting in Q1 2024.</p>
<p>Other system manufacturers such as ASRock Rack, ASUS, GIGABYTE and Ingrasys will begin shipping servers with the superchips by the end of the year.</p>
<p>NVIDIA Grace Hopper has been adopted in early access for supercomputing initiatives by more than 100 enterprises, organizations and government agencies across the globe, including the NASA Ames Research Center for aeronautics research and global energy company TotalEnergies.</p>
<p>In addition, the GH200 will soon become available through <a href="https://www.nvidia.com/en-us/launchpad/" target="_blank" rel="noopener">NVIDIA LaunchPad</a>, which provides free access to enterprise NVIDIA hardware and software through an internet browser.</p>
<p><i>Learn more about </i><a href="https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/" target="_blank" rel="noopener"><i>Grace Hopper</i></a><i> and other supercomputing breakthroughs by joining </i><a href="https://www.nvidia.com/en-us/events/supercomputing/" target="_blank" rel="noopener"><i>NVIDIA at SC23</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/gh200-momentum.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/gh200-momentum-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Grace Hopper Superchip Powers 40+ AI Supercomputers Across Global Research Centers, System Makers, Cloud Providers]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Scroll Back in Time: AI Deciphers Ancient Roman Riddles</title>
		<link>https://blogs.nvidia.com/blog/ai-deciphers-scroll/</link>
		
		<dc:creator><![CDATA[Brian Caulfield]]></dc:creator>
		<pubDate>Fri, 10 Nov 2023 14:00:28 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67936</guid>

					<description><![CDATA[Thanks to a viral trend sweeping social media, we now know some men think about the Roman Empire every day. And thanks to Luke Farritor, a 21-year-old computer science undergrad at the University of Nebraska-Lincoln, and like-minded AI enthusiasts, there might soon be a lot more to think about. Blending a passion for history with <a class="read-more" href="https://blogs.nvidia.com/blog/ai-deciphers-scroll/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Thanks to a viral trend sweeping social media, we now know some men think about the Roman Empire every day.</p>
<p>And thanks to Luke Farritor, a 21-year-old computer science undergrad at the University of Nebraska-Lincoln, and like-minded AI enthusiasts, there might soon be a lot more to think about.</p>
<p>Blending a passion for history with machine learning skills, Farritor has triumphed in the Vesuvius Challenge, wielding the power of the NVIDIA GeForce GTX 1070 GPU to bring a snippet of ancient text back from the ashes after almost 2,000 years.</p>
<h2>Text Big Thing: Deciphering Rome‚Äôs Hidden History</h2>
<p>The Herculaneum scrolls are a library of ancient texts that were carbonized and preserved by the eruption of Mount Vesuvius in 79 AD, which buried the cities of Pompeii and Herculaneum under a thick layer of ash and pumice.</p>
<p>The competition, which has piqued the interest of historians and technologists across the globe, seeks to extract readable content from the carbonized remains of the scrolls.</p>
<p>In a significant breakthrough, the word ‚ÄúœÄŒøœÅœÜœÖœÅŒ±œÇ,‚Äù which means ‚Äúpurple dye‚Äù or ‚Äúcloths of purple,‚Äù emerged from the ancient texts thanks to the efforts of Farritor.</p>
<figure id="attachment_35262" aria-describedby="caption-attachment-35262" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="wp-image-35262 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2016/12/Herculaneum-scroll-672x448.jpg" alt="The Herculaneum scrolls, wound about 100 times around, are sealed by the heat of the lava." width="672" height="448" srcset="https://blogs.nvidia.com/wp-content/uploads/2016/12/Herculaneum-scroll-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2016/12/Herculaneum-scroll-300x200.jpg 300w, https://blogs.nvidia.com/wp-content/uploads/2016/12/Herculaneum-scroll-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2016/12/Herculaneum-scroll-676x451.jpg 676w, https://blogs.nvidia.com/wp-content/uploads/2016/12/Herculaneum-scroll-328x219.jpg 328w, https://blogs.nvidia.com/wp-content/uploads/2016/12/Herculaneum-scroll-1200x800.jpg 1200w, https://blogs.nvidia.com/wp-content/uploads/2016/12/Herculaneum-scroll.jpg 2000w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-35262" class="wp-caption-text">The Herculaneum scrolls, wound about 100 times around, are sealed by the heat of the eruption of Vesuvius.</figcaption></figure>
<p>His achievement in identifying 10 letters within a small patch of scroll earned him a $40,000 prize.</p>
<p>Close on his heels was Youssef Nader, a biorobotics graduate student, who independently discerned the same word a few months later, meriting a $10,000 prize.</p>
<p>Adding to these notable successes, Casey Handmer, an entrepreneur with a keen eye, secured another $10,000 for his demonstration that significant amounts of ink were waiting to be discovered within the unopened scrolls.</p>
<p>All these discoveries are advancing the work pioneered by W. Brent Seales, chair of the University of Kentucky Computer Science Department, who has dedicated over a decade to developing methods to digitally unfurl and read the delicate Herculaneum scrolls.</p>
<p>Turbocharging these efforts is Nat Friedman, the CEO of GitHub and the organizer of the Vesuvius Challenge, whose commitment to open-source innovation has fostered a community where such historical breakthroughs are possible.</p>
<p>To become the first to decipher text from the scrolls, Farritor, who served as an intern at SpaceX, harnessed the GeForce GTX 1070 to accelerate his work.</p>
<h2>When Rome Meets RAM: Older GPU Helps Uncover Even Older Text</h2>
<p>Introduced in 2016, the GTX 1070 is celebrated among gamers, who have long praised the GPU for its balance of performance and affordability.</p>
<p>Instead of gaming, however, Farritor harnessed the parallel processing capabilities of the GPU to accelerate the ResNet deep learning framework, processing data at speeds unattainable by traditional computing methods.</p>
<p>Farritor is not the only competitor harnessing NVIDIA GPUs, which have proven themselves as indispensable tools to Vesuvius challenge competitors.</p>
<h2>Latin Lingo and Lost Text</h2>
<p>Discovered in the 18th century in the Villa of the Papyri, the Herculaneum scrolls have presented a challenge to researchers. Their fragile state has made them nearly impossible to read without causing damage. The advent of advanced imaging and AI technology changed that.</p>
<p>The project has become a passion for Farritor, who finds himself struggling to recall more of the Latin he studied in high school. ‚ÄúAnd man, like what‚Äôs in the scrolls &#8230; it‚Äôs just the anticipation, you know?‚Äù Farritor said.</p>
<p>The next challenge is to unearth passages from the Herculaneum scrolls that are 144 characters long, echoing the brevity of an original Twitter post.</p>
<p>Engaging over 1,500 experts in a collaborative effort, the endeavor is now more heated than ever.</p>
<p>Private donors have upped the ante, offering a $700,000 prize for those who can retrieve four distinct passages of at least 140 characters this year ‚Äî a testament to the value placed on these ancient texts and the lengths required to reclaim them.</p>
<p>And Farritor‚Äôs eager to keep digging, reeling off the names of lost works of Roman and Greek history that he‚Äôd love to help uncover.</p>
<p>He reports he‚Äôs now thinking about Rome ‚Äî and what his efforts might help discover ‚Äî not just every day now, but ‚Äúevery hour.‚Äù ‚ÄúI think anything that sheds light on that time in human history is gonna be significant,‚Äù Farritor said.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/Vesuvius.jpg"
			type="image/jpeg"
			width="1950"
			height="1213"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/Vesuvius-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Scroll Back in Time: AI Deciphers Ancient Roman Riddles]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Enter a World of Samurai and Demons: GFN Thursday Brings Capcom‚Äôs ‚ÄòOnimusha: Warlords‚Äô to the Cloud</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-nov-09/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 09 Nov 2023 14:00:19 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67993</guid>

					<description><![CDATA[Wield the blade and embrace the way of the samurai for some thrilling action ‚Äî Onimusha: Warlords comes to GeForce NOW this week. Members can experience feudal Japan in this hack-and-slash adventure game in the cloud. It‚Äôs part of an action-packed GFN Thursday, with 16 more games joining the cloud gaming platform‚Äôs library. Forging Destinies <a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-nov-09/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Wield the blade and embrace the way of the samurai for some thrilling action ‚Äî <i>Onimusha: Warlords</i> comes to <a href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a> this week. Members can experience feudal Japan in this hack-and-slash adventure game in the cloud.</p>
<p>It‚Äôs part of an action-packed GFN Thursday, with 16 more games joining the cloud gaming platform‚Äôs library.</p>
<h2><b>Forging Destinies</b></h2>
<figure id="attachment_68004" aria-describedby="caption-attachment-68004" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-68004" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Onimusha_Warlords-672x378.jpg" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Onimusha_Warlords-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Onimusha_Warlords-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Onimusha_Warlords-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Onimusha_Warlords-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Onimusha_Warlords-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Onimusha_Warlords-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Onimusha_Warlords-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Onimusha_Warlords-1280x720.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Onimusha_Warlords.jpg 1920w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-68004" class="wp-caption-text"><em>Vengeance is mine.</em></figcaption></figure>
<p>Capcom‚Äôs popular <i>Onimusha: Warlords</i> is newly supported in the cloud this week, just in time for those tuning into the recently released <a href="https://www.netflix.com/title/81153116">Netflix anime</a> adaptation.</p>
<p>Fight against the evil warlord Nobunaga Oda and his army of demons as samurai Samanosuke Akechi. Explore feudal Japan, wield swords, use ninja techniques and solve puzzles to defeat enemies. The action-adventure hack-and-slash game has been enhanced with improved controls for smoother swordplay mechanics, an updated soundtrack and more.</p>
<p><a href="https://www.nvidia.com/en-us/geforce-now/memberships/">Ultimate</a> members can stream the game in ultrawide resolution with up to eight hours each gaming session for riveting samurai action.</p>
<h2><b>Endless Games</b></h2>
<figure id="attachment_68001" aria-describedby="caption-attachment-68001" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="wp-image-68001 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Endless_Dungeon-672x336.jpg" alt="Endless Dungeons on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Endless_Dungeon-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Endless_Dungeon-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Endless_Dungeon-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Endless_Dungeon-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Endless_Dungeon-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Endless_Dungeon-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Endless_Dungeon-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Endless_Dungeon-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/11/GFN_Thursday-Endless_Dungeon.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-68001" class="wp-caption-text"><em>Monsters, dangers, secrets and treasures, oh my!</em></figcaption></figure>
<p>Roguelite fans and GeForce NOW members have been enjoying Sega‚Äôs <i>Endless Dungeon</i> in the cloud. Recruit a team of shipwrecked heroes, plunge into a long-abandoned space station and protect the crystal against never-ending waves of monsters. Never accept defeat ‚Äî get reloaded and try, try again.</p>
<p>On top of that, check out the 16 newly supported games joining the GeForce NOW library this week:</p>
<ul>
<li><i>The Invincible </i>(New release on <a href="https://store.steampowered.com/app/731040?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Nov. 6)</li>
<li><i>Roboquest </i>(New release on <a href="https://store.steampowered.com/app/692890?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Nov. 7)</li>
<li><i>Stronghold: Definitive Edition </i>(New release on <a href="https://store.steampowered.com/app/2140020?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Nov. 7)</li>
<li><i>Dungeons 4 </i>(New release on <a href="https://store.steampowered.com/app/1643310?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a href="https://www.xbox.com/games/store/dungeons-4-win/9nd9bbmdvd2c?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass, Nov. 9)</li>
<li><i>Space Trash Scavenger </i>(New release on <a href="https://store.steampowered.com/app/1759350?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Nov. 9)</li>
<li><i>Airport CEO </i>(<a href="https://store.steampowered.com/app/673610?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Car Mechanic Simulator 2021 </i>(<a href="https://www.xbox.com/games/store/car-mechanic-simulator-2021/9NH019ZFFHTV?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Farming Simulator 19 </i>(<a href="https://www.xbox.com/games/store/farming-simulator-19-premium-edition-windows-10/9NJJC9Z14WT0?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>GoNNER </i>(<a href="https://www.xbox.com/games/store/gonner-win10/9PBM5VSZDW5C?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>GoNNER2 </i>(<a href="https://www.xbox.com/games/store/gonner2-win10/9P44W70MC99Z?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Jurassic World</i> <i>Evolution 2</i> (<a href="https://www.xbox.com/games/store/jurassic-world-evolution-2/9MWHMJ0SRBXV?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Onimusha: Warlords </i>(<a href="https://store.steampowered.com/app/761600?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Planet of Lana </i>(<a href="https://www.xbox.com/games/store/planet-of-lana/9MXK2RSQJQND?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Q.U.B.E. 10th Anniversary </i>(<a href="https://www.epicgames.com/store/p/qube-10th-anniversary-59e999?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
<li><i>Trailmakers</i> (<a href="https://www.xbox.com/games/store/trailmakers/9NSFGM8J6MBJ?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Turnip Boy Commits Tax Evasion </i>(<a href="https://www.xbox.com/games/store/turnip-boy-commits-tax-evasion/9N0T8V0R7MBC?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
</ul>
<p>What are you planning to play this weekend? Let us know on <a href="https://www.twitter.com/nvidiagfn">Twitter</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">Share a game that more people should be playing. <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f914.png" alt="ü§î" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f329.png" alt="üå©" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a href="https://twitter.com/NVIDIAGFN/status/1722297932538397082?ref_src=twsrc%5Etfw">November 8, 2023</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/gfn-thursday-11-9-nv-blog-1280x680-preview.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/gfn-thursday-11-9-nv-blog-1280x680-preview-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Enter a World of Samurai and Demons: GFN Thursday Brings Capcom‚Äôs ‚ÄòOnimusha: Warlords‚Äô to the Cloud]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Acing the Test: NVIDIA Turbocharges Generative AI Training in MLPerf Benchmarks</title>
		<link>https://blogs.nvidia.com/blog/scaling-ai-training-mlperf/</link>
		
		<dc:creator><![CDATA[Dave Salvator]]></dc:creator>
		<pubDate>Wed, 08 Nov 2023 17:00:10 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Cloud Services]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Genomics]]></category>
		<category><![CDATA[Hardware]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[NVIDIA Hopper Architecture]]></category>
		<category><![CDATA[NVIDIA Quantum-2]]></category>
		<category><![CDATA[Recommender Systems]]></category>
		<category><![CDATA[Science]]></category>
		<category><![CDATA[Supercomputing]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67934</guid>

					<description><![CDATA[NVIDIA‚Äôs AI platform raised the bar for AI training and high performance computing in the latest MLPerf industry benchmarks. Among many new records and milestones, one in generative AI stands out: NVIDIA Eos ‚Äî an AI supercomputer powered by a whopping 10,752 NVIDIA H100 Tensor Core GPUs and NVIDIA Quantum-2 InfiniBand networking ‚Äî completed a <a class="read-more" href="https://blogs.nvidia.com/blog/scaling-ai-training-mlperf/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>NVIDIA‚Äôs AI platform raised the bar for AI training and high performance computing in the latest <a href="https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/">MLPerf</a> industry benchmarks.</p>
<p>Among many new records and milestones, one in <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">generative AI</a> stands out: <a href="https://nvidianews.nvidia.com/news/nvidia-announces-dgx-h100-systems-worlds-most-advanced-enterprise-ai-infrastructure">NVIDIA Eos</a> ‚Äî an AI supercomputer powered by a whopping 10,752 <a href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPUs</a> and <a href="https://www.nvidia.com/en-us/networking/quantum2/">NVIDIA Quantum-2 InfiniBand</a> networking ‚Äî completed a training benchmark based on a GPT-3 model with 175 billion parameters trained on one billion tokens in just 3.9 minutes.</p>
<p>That‚Äôs a nearly 3x gain from 10.9 minutes, the record NVIDIA set when the test <a href="https://blogs.nvidia.com/blog/generative-ai-debut-mlperf/">was introduced</a> less than six months ago.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/1-NV-H100-Leaps-to-New-Heights.jpg"><img decoding="async" loading="lazy" class="aligncenter size-large wp-image-67961" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/1-NV-H100-Leaps-to-New-Heights-672x422.jpg" alt="NVIDIA H100 training results over time on MLPerf benchmarks" width="672" height="422" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/1-NV-H100-Leaps-to-New-Heights-672x422.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/1-NV-H100-Leaps-to-New-Heights-400x251.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/1-NV-H100-Leaps-to-New-Heights-768x483.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/1-NV-H100-Leaps-to-New-Heights-716x450.jpg 716w, https://blogs.nvidia.com/wp-content/uploads/2023/11/1-NV-H100-Leaps-to-New-Heights-342x215.jpg 342w, https://blogs.nvidia.com/wp-content/uploads/2023/11/1-NV-H100-Leaps-to-New-Heights-159x100.jpg 159w, https://blogs.nvidia.com/wp-content/uploads/2023/11/1-NV-H100-Leaps-to-New-Heights.jpg 1200w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>The benchmark uses a portion of the full GPT-3 data set behind the popular ChatGPT service that, by extrapolation, Eos could now train in just eight days, 73x faster than a prior state-of-the-art system using 512 A100 GPUs.</p>
<p>The acceleration in training time reduces costs, saves energy and speeds time-to-market. It‚Äôs heavy lifting that makes large language models widely available so every business can adopt them with tools like <a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/">NVIDIA NeMo</a>, a framework for customizing <a href="https://www.nvidia.com/en-us/glossary/data-science/large-language-models/">LLMs</a>.</p>
<p>In a new generative AI test ‚Äåthis round, 1,024 <a href="https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/">NVIDIA Hopper architecture GPUs</a> completed a training benchmark based on the Stable Diffusion text-to-image model in 2.5 minutes, setting a high bar on this new workload.</p>
<p>By adopting these two tests, MLPerf reinforces its leadership as the industry standard for measuring AI performance, since generative AI is the most transformative technology of our time.</p>
<h2><b>System Scaling Soars</b></h2>
<p>The latest results were due in part to the use of the most accelerators ever applied to an MLPerf benchmark. The 10,752 H100 GPUs far surpassed the scaling in <a href="https://blogs.nvidia.com/blog/generative-ai-debut-mlperf/">AI training</a> in June, when NVIDIA used 3,584 Hopper GPUs.</p>
<p>The 3x scaling in GPU numbers delivered a 2.8x scaling in performance, a 93% efficiency rate thanks in part to software optimizations.</p>
<p>Efficient scaling is a key requirement in generative AI because LLMs are growing by <a href="https://blogs.nvidia.com/blog/huangs-law-dally-hot-chips/">an order of magnitude every year</a>. The latest results show NVIDIA‚Äôs ability to meet this unprecedented challenge for even the world‚Äôs largest data centers.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/2-Near-linear-scaling-of-NV-H100s-on-MLPerf-training-benchmark.jpg"><img decoding="async" loading="lazy" class="aligncenter size-large wp-image-67964" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/2-Near-linear-scaling-of-NV-H100s-on-MLPerf-training-benchmark-672x420.jpg" alt="Chart of near linear scaling of H100 GPUs on MLPerf training" width="672" height="420" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/2-Near-linear-scaling-of-NV-H100s-on-MLPerf-training-benchmark-672x420.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/2-Near-linear-scaling-of-NV-H100s-on-MLPerf-training-benchmark-400x250.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/2-Near-linear-scaling-of-NV-H100s-on-MLPerf-training-benchmark-768x480.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/2-Near-linear-scaling-of-NV-H100s-on-MLPerf-training-benchmark-720x450.jpg 720w, https://blogs.nvidia.com/wp-content/uploads/2023/11/2-Near-linear-scaling-of-NV-H100s-on-MLPerf-training-benchmark-344x215.jpg 344w, https://blogs.nvidia.com/wp-content/uploads/2023/11/2-Near-linear-scaling-of-NV-H100s-on-MLPerf-training-benchmark-160x100.jpg 160w, https://blogs.nvidia.com/wp-content/uploads/2023/11/2-Near-linear-scaling-of-NV-H100s-on-MLPerf-training-benchmark.jpg 1200w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>The achievement is thanks to a full-stack platform of innovations in accelerators, systems and software that both Eos and <a href="https://learn.microsoft.com/en-us/azure/virtual-machines/nd-h100-v5-series">Microsoft Azure</a> used in the latest round.</p>
<p>Eos and Azure both employed 10,752 H100 GPUs in separate submissions. They achieved within 2% of the same performance, demonstrating the efficiency of NVIDIA AI in data center and public-cloud deployments.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/3-Azure-achieves-record-scaling.jpg"><img decoding="async" loading="lazy" class="aligncenter size-large wp-image-67967" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/3-Azure-achieves-record-scaling-672x378.jpg" alt="Chart of record Azure scaling in MLPerf training" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/3-Azure-achieves-record-scaling-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/3-Azure-achieves-record-scaling-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/3-Azure-achieves-record-scaling-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/3-Azure-achieves-record-scaling-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/11/3-Azure-achieves-record-scaling-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/11/3-Azure-achieves-record-scaling-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2023/11/3-Azure-achieves-record-scaling.jpg 1200w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>NVIDIA relies on Eos for a wide array of critical jobs. It helps advance initiatives like <a href="https://www.nvidia.com/en-us/geforce/technologies/dlss/">NVIDIA DLSS</a>, AI-powered software for state-of-the-art computer graphics and NVIDIA Research projects like <a href="https://blogs.nvidia.com/blog/llm-semiconductors-chip-nemo/">ChipNeMo</a>, generative AI tools that help design next-generation GPUs.</p>
<h2><b>Advances Across Workloads</b></h2>
<p>NVIDIA set several new records in this round in addition to making advances in generative AI.</p>
<p>For example, H100 GPUs were 1.6x faster than the prior-round training recommender models widely employed to help users find what they‚Äôre looking for online. Performance was up 1.8x on RetinaNet, a computer vision model.</p>
<p>These increases came from a combination of advances in software and scaled-up hardware.</p>
<p>NVIDIA was once again the only company to run all MLPerf tests. H100 GPUs demonstrated the fastest performance and the greatest scaling in each of the nine benchmarks.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/11/4-Six-NVIDIA-records-in-MLPerf-training.jpg"><img decoding="async" loading="lazy" class="aligncenter size-large wp-image-67970" src="https://blogs.nvidia.com/wp-content/uploads/2023/11/4-Six-NVIDIA-records-in-MLPerf-training-672x366.jpg" alt="List of six new NVIDIA records in MLPerf training" width="672" height="366" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/11/4-Six-NVIDIA-records-in-MLPerf-training-672x366.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/11/4-Six-NVIDIA-records-in-MLPerf-training-400x218.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/11/4-Six-NVIDIA-records-in-MLPerf-training-768x418.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/11/4-Six-NVIDIA-records-in-MLPerf-training-826x450.jpg 826w, https://blogs.nvidia.com/wp-content/uploads/2023/11/4-Six-NVIDIA-records-in-MLPerf-training-395x215.jpg 395w, https://blogs.nvidia.com/wp-content/uploads/2023/11/4-Six-NVIDIA-records-in-MLPerf-training-184x100.jpg 184w, https://blogs.nvidia.com/wp-content/uploads/2023/11/4-Six-NVIDIA-records-in-MLPerf-training-1280x697.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/11/4-Six-NVIDIA-records-in-MLPerf-training.jpg 1300w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>Speedups translate to faster time to market, lower costs and energy savings for users training massive LLMs or customizing them with frameworks like NeMo for the specific needs of their business.</p>
<p>Eleven systems makers used the NVIDIA AI platform in their submissions this round, including ASUS, Dell Technologies, Fujitsu, GIGABYTE, Lenovo, QCT and Supermicro.</p>
<p>NVIDIA partners participate in MLPerf because they know it‚Äôs a valuable tool for customers evaluating AI platforms and vendors.</p>
<h2><b>HPC Benchmarks Expand</b></h2>
<p>In MLPerf HPC, a separate benchmark for AI-assisted simulations on supercomputers, H100 GPUs delivered up to twice the performance of NVIDIA A100 Tensor Core GPUs in <a href="https://blogs.nvidia.com/blog/mlperf-hpc-ai/">the last HPC round</a>. The results showed up to 16x gains since the first MLPerf HPC round in 2019.</p>
<p>The benchmark included a new test that trains OpenFold, a model that predicts the 3D structure of a protein from its sequence of amino acids. OpenFold can do in minutes vital work for healthcare that used to take researchers weeks or months.</p>
<p>Understanding a protein‚Äôs structure is key to finding effective drugs fast because most drugs act on proteins, the cellular machinery that helps control many biological processes.</p>
<p>In the MLPerf HPC test, H100 GPUs trained OpenFold in 7.5 minutes.¬† The OpenFold test is a representative part of the entire AlphaFold training process that two years ago took 11 days using 128 accelerators.</p>
<p>A version of the OpenFold model and the software NVIDIA used to train it will be available soon in <a href="https://www.nvidia.com/en-us/gpu-cloud/bionemo/">NVIDIA BioNeMo</a>, a generative AI platform for drug discovery.</p>
<p>Several partners made submissions on the NVIDIA AI platform in this round. They included Dell Technologies and supercomputing centers at Clemson University, the Texas Advanced Computing Center and ‚Äî with assistance from Hewlett Packard Enterprise (HPE) ‚Äî Lawrence Berkeley National Laboratory.</p>
<h2><b>Benchmarks With Broad Backing</b></h2>
<p>Since its inception in May 2018, the MLPerf benchmarks have enjoyed broad backing from both industry and academia. Organizations that support them include Amazon, Arm, Baidu, Google, Harvard, HPE, Intel, Lenovo, Meta, Microsoft, NVIDIA, Stanford University and the University of Toronto.</p>
<p>MLPerf tests are transparent and objective, so users can rely on the results to make informed buying decisions.</p>
<p>All the software NVIDIA used is available from the MLPerf repository, so all developers can get the same world-class results. These software optimizations get continuously folded into containers available on <a href="https://ngc.nvidia.com/catalog">NGC</a>, NVIDIA‚Äôs software hub for GPU applications.</p>
<p><i>Learn more about </i><a href="https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/"><i>MLPerf</i></a><i> and </i><a href="https://developer.nvidia.com/blog/setting-new-records-at-data-center-scale-using-nvidia-h100-gpus-and-quantum-2-infiniband/"><i>the details</i></a><i> of this round.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-Eos-supercomputer-runs-MLPerf-tests.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/NVIDIA-Eos-supercomputer-runs-MLPerf-tests-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Acing the Test: NVIDIA Turbocharges Generative AI Training in MLPerf Benchmarks]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Partners With APEC Economies to Change Lives, Increase Opportunity, Improve Outcomes</title>
		<link>https://blogs.nvidia.com/blog/apec-summit/</link>
		
		<dc:creator><![CDATA[Ruth Berry]]></dc:creator>
		<pubDate>Wed, 08 Nov 2023 16:00:32 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Deep Learning Institute]]></category>
		<category><![CDATA[Developer Program]]></category>
		<category><![CDATA[Inception]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67976</guid>

					<description><![CDATA[When patients in Vietnam enter a medical facility in distress, doctors use NVIDIA technology to get more accurate scans to diagnose their ailments. In Hong Kong, a different set of doctors leverage generative AI to discover new cures for patients. Improving the health and well-being of citizens and strengthening economies and communities are key themes <a class="read-more" href="https://blogs.nvidia.com/blog/apec-summit/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>When patients in Vietnam enter a medical facility in distress, doctors use NVIDIA technology to get more accurate scans to diagnose their ailments. In Hong Kong, a different set of doctors leverage generative AI to discover new cures for patients.</p>
<p>Improving the health and well-being of citizens and strengthening economies and communities are key themes as world leaders soon gather in San Francisco for the 2023 Asia-Pacific Economic Cooperation (APEC) Summit.</p>
<p>When they meet to discuss bold solutions to improve the lives of their citizens and societies, NVIDIA‚Äôs AI and accelerated computing initiatives are a crucial enabler.</p>
<p>NVIDIA‚Äôs work to improve outcomes for everyday people while tackling future challenges builds on years of deep investment with APEC partners. With a strong presence in countries across the region, including a workforce of thousands and numerous collaborative projects in areas from farming to healthcare to education, NVIDIA is delivering new technologies and workforce training programs to enhance industrial development and advance generative AI research.</p>
<p>Beyond technological advancements, these efforts spur economic growth, create good-paying jobs and improve the health and well-being of people globally.</p>
<h2><b>Research and National Compute Partnerships</b></h2>
<p>NVIDIA has advanced AI research partnerships with several APEC economies. These accelerate scientific breakthroughs in AI and HPC to address national challenges, such as healthcare, skills development and creating more robust local AI ecosystems to protect and advance well-being, prosperity and security. For example:</p>
<ul>
<li><b>Australia‚Äôs</b> national science and research organization, CSIRO, has teamed with NVIDIA to advance Australia&#8217;s AI program across climate action, space exploration, quantum computing and AI education.</li>
<li><b>Singapore‚Äôs</b> National Supercomputing Centre and Ministry of Education have partnered with NVIDIA to drive sovereign AI capabilities with a priority focus on sectors such as healthcare, climate science and digital twins.</li>
<li><b>Thailand</b> was Southeast Asia‚Äôs first country to participate in NVIDIA‚Äôs AI Nations initiative, bringing together the Ministry of Education with a consortium of top universities to advance public-private collaborations in urban planning, public health and autonomous vehicles.</li>
<li>In <b>Vietnam</b>, NVIDIA is partnering with Viettel,¬† the nation‚Äôs largest employer, and Vietnam‚Äôs Academy for Science &amp; Technology to upskill workforces, accelerate the introduction of AI services to industry and deploy next-generation 5G services.</li>
</ul>
<h2><b>Innovation Ecosystems</b></h2>
<p>Startups are at the leading edge of AI innovation, and a robust startup ecosystem is vital to advancing technology within APEC economies.</p>
<p><a href="https://www.nvidia.com/en-us/startups/#nv-accordion-81048a018e-item-2202bf6f03">NVIDIA Inception</a> is a free program to help startups innovate faster. Through it, NVIDIA supports over 5,000 startups across APEC economies, and more than 15,000 globally, by providing cutting-edge technology, connections with venture capitalists and access to the latest technical resources.</p>
<p>In 2023, NVIDIA added nearly 1,000 APEC-area startups to the program. In addition to creating economic opportunities, Inception supports small- and medium-sized enterprises in developing novel solutions to some of society&#8217;s biggest challenges. Here‚Äôs what some of its members are doing:</p>
<ul>
<li>In <b>Malaysia</b>, <a href="https://gotapway.com/">Tapway</a> uses AI to reduce congestion and streamline traffic for more than 1 million daily travelers.</li>
<li>In <b>New Zealand</b>, <a href="https://www.lynker-analytics.com/our-work">Lynker</a> uses geospatial analysis, deep learning and remote sensing for earth observation.¬† Lynker‚Äôs technology measures carbon sequestration on farms, detecting, monitoring and restoring wetlands and enabling more effective disaster relief.</li>
<li>In <b>Thailand</b>, <a href="https://www.altotech.ai/">AltoTech Global</a>, an Inception partner, integrates AI software with Internet of Things devices to optimize energy consumption for hotels, buildings, factories and smart cities. AltoTech‚Äôs ultimate goal is contributing to the net-zero economy and helping customers achieve their net-zero targets.</li>
</ul>
<h2><b>Digital Upskilling and Tools for Growth</b></h2>
<p>The <a href="https://www.nvidia.com/en-us/training/">NVIDIA Deep Learning Institute</a> (DLI) provides AI training and digital upskilling programs that cultivate innovation and create economic opportunities.</p>
<p>DLI‚Äôs training and certification program helps individuals and organizations accelerate skills development and workforce transformation in AI, high performance computing and industrial digitalization.</p>
<p>Hands-on, self-paced and instructor-led courses are created and taught by NVIDIA experts, bringing real-world experience and deep technical know-how to developers and IT professionals.</p>
<p>Through this program, NVIDIA has trained more than 115,000 individuals in APEC economies, including more than 16,000 new trainees this year.</p>
<p>Separately, the <a href="https://developer.nvidia.com/developer-program">NVIDIA Developer Program</a> offers more than 2 million developers in APEC economies access to software development kits, application program interfaces, pretrained AI models and performance analysis tools to help developers create and innovate. Members receive free hands-on training, access to developer forums and early access to new products and services.</p>
<h2><b>Creating a Better Future for All</b></h2>
<p>As nations work together to address common challenges and improve the lives of their citizens, NVIDIA will continue to leverage its world-class technologies to help create a better world for all.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/apec-ceo-2023.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/11/apec-ceo-2023-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Partners With APEC Economies to Change Lives, Increase Opportunity, Improve Outcomes]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
	</channel>
</rss>
