<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:media="http://search.yahoo.com/mrss/">

<channel>
	<title>NVIDIA Blog</title>
	<atom:link href="https://blogs.nvidia.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.nvidia.com/</link>
	<description></description>
	<lastBuildDate>Tue, 30 Jul 2024 00:30:44 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.5.5</generator>
	<item>
		<title>Creators To Have Personalized AI Assistants, Meta CEO Mark Zuckerberg Tells NVIDIA CEO Jensen Huang</title>
		<link>https://blogs.nvidia.com/blog/zuckerberg-huang/</link>
		
		<dc:creator><![CDATA[Brian Caulfield]]></dc:creator>
		<pubDate>Tue, 30 Jul 2024 00:30:28 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73130</guid>

					<description><![CDATA[In a highly anticipated fireside chat at SIGGRAPH 2024, NVIDIA founder and CEO Jensen Huang and Meta founder and CEO Mark Zuckerberg discussed the transformative potential of open source AI and AI assistants. Zuckerberg kicked off the discussion by announcing the launch of AI Studio, a new platform that allows users to create, share and	<a class="read-more" href="https://blogs.nvidia.com/blog/zuckerberg-huang/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>In a highly anticipated fireside chat at SIGGRAPH 2024, NVIDIA founder and CEO Jensen Huang and Meta founder and CEO Mark Zuckerberg discussed the transformative potential of open source AI and AI assistants.</p>
<p>Zuckerberg kicked off the discussion by announcing the launch of <a target="_blank" href="https://about.fb.com/news/2024/07/create-your-own-custom-ai-with-ai-studio/">AI Studio</a>, a new platform that allows users to create, share and discover AI characters, making AI more accessible to millions of creators and small businesses.</p>
<p>“Every single restaurant, every single website will probably, in the future, have these AIs …” Huang said.</p>
<p>“&#8230;just like every business has an email address and a website and a social media account, I think, in the future, every business is going to have an AI,” Zuckerberg responded.</p>
<p>Zuckerberg has gotten it right before. Huang credited Zuckerberg and Meta with being leaders in AI, even if only some have noticed until recently.</p>
<p>“You guys have done amazing AI work,” Huang said, citing advancements from Meta in computer vision, language models, real-time translation. “We all use Pytorch, that comes out of Meta.”</p>
<p><b>The Importance of Open Source in Advancing AI</b><b><br />
</b><br />
Zuckerberg highlighted the importance of open source in advancing AI — with the two business leaders emphasizing the importance of open platforms for innovation. <b><br />
</b><br />
Meta has rapidly emerged as a leader in AI, putting it to work throughout its businesses — most notably with Meta AI, which is used across Facebook, Instagram and WhatsApp — and advancing open-source AI throughout the industry, most recently with the release of Llama 3.1.</p>
<p>The open-source model represents a significant investment of time and training resources. The largest version of Llama boasts 405 billion parameters and was trained on over 16,000 NVIDIA H100 GPUs.</p>
<p>“One of the things that drives quality improvements is it used to be that you have a different model for each type of content,” Zuckerberg explained.</p>
<p>“A  the models get bigger and more general, that gets better and better. So, I kind of dream of one day like you can almost imagine all of Facebook or Instagram being like a single AI model that has unified all these different content types and systems together,” he added.</p>
<p>Zuckerberg sees collaboration as key to more advancements. In a blog post released last week, Zuckerberg wrote that the release of Llama 3.1 promises to be an “inflection point” in adopting open source in AI.</p>
<p>These advancements promise more tools to foster engagement, create compelling and personalized content — such as digital avatars — and build virtual worlds.</p>
<p>More broadly, the advancement of AI across a broad ecosystem promises to supercharge human productivity, for example, by giving every human on earth a digital assistant — or assistants — allowing people to live richer lives that they can interact with quickly and fluidly.</p>
<p>“I feel like I’m collaborating with WhatsApp,” Huang said. “Imagine I’m sitting here typing, and it’s generating the images as I’m going. I go back and change my words, and it’s generating other images.”</p>
<p><b>Vision for the Future</b></p>
<p>Looking ahead, both CEOs shared their visions for the future.</p>
<p>Zuckerberg expressed optimism about bringing AI together with the real world through eyeglasses — nothing his company’s collaboration with eyewear maker Luxotic — that can be used to help transform education, entertainment and work.</p>
<p>Huang emphasized how interacting with AIs is becoming more fluid, moving beyond just text-based interactions.</p>
<p>“Today’s AI is kind of turn-based. You say something, it says something back to you,” Huang said. In the future, AI could contemplate multiple options, or come up with a tree of options and simulate outcomes, making it much more powerful.”</p>
<p>Throughout the conversation, the two leaders playfully bantered about everything from fashion to steak sandwiches, ending the discussion by exchanging leather jackets.</p>
<p>Zuckerberg give Huang with a black leather shearling jacket with an enormous hood.</p>
<p>Huang gave Zuckerberg his own leather jacket, which he got from his wife, Lori, just for SIGGRAPH, quipping that it was just “two hours old.”</p>
<p>“Well this one’s yours,” Zuckerberg said with a smile. “This is worth more because it’s used.”</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/764A8547-blog-press-1280x680-1.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/764A8547-blog-press-1280x680-1-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Creators To Have Personalized AI Assistants, Meta CEO Mark Zuckerberg Tells NVIDIA CEO Jensen Huang]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>‘Everybody Will Have an AI Assistant,’ NVIDIA CEO Tells SIGGRAPH Audience</title>
		<link>https://blogs.nvidia.com/blog/nvidia-ceo-siggraph/</link>
		
		<dc:creator><![CDATA[Brian Caulfield]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 22:43:08 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73382</guid>

					<description><![CDATA[The generative AI revolution — with deep roots in visual computing — is amplifying human creativity even as accelerated computing promises significant gains in energy efficiency, NVIDIA founder and CEO Jensen Huang said Monday. That makes this week&#8217;s SIGGRAPH professional graphics conference, in Denver, the logical venue to discuss what’s next. “Everybody will have an	<a class="read-more" href="https://blogs.nvidia.com/blog/nvidia-ceo-siggraph/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>The generative AI revolution — with deep roots in visual computing — is amplifying human creativity even as accelerated computing promises significant gains in energy efficiency, NVIDIA founder and CEO Jensen Huang said Monday.</p>
<p>That makes this week&#8217;s SIGGRAPH professional graphics conference, in Denver, the logical venue to discuss what’s next.</p>
<p>“Everybody will have an AI assistant,” Huang said. “Every single company, every single job within the company, will have AI assistance.”</p>
<p>But even as generative AI promises to amplify human productivity, Huang said the accelerated computing technology that underpins it promises to make computing more energy efficient.</p>
<p>“Accelerated computing helps you save so much energy, 20 times, 50 times, and doing the same processing,” Huang said. “The first thing we have to do, as a society, is accelerate every application we can: this reduces the amount of energy being used all over the world.”</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1.png"><img fetchpriority="high" decoding="async" class="alignnone size-large wp-image-73389" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>The conversation follows a spate of announcements from NVIDIA today.</p>
<p><a href="https://nvidianews.nvidia.com/news/nvidia-announces-generative-ai-models-and-nim-microservices-for-openusd" target="_blank" rel="noopener">NVIDIA introduced a new suite of NIM microservices</a> tailored for diverse workflows, including OpenUSD, 3D modeling, physics, materials, robotics, industrial digital twins and physical AI.<br />
These advancements aim to enhance developer capabilities, particularly with the <a href="https://blogs.nvidia.com/blog/hugging-face-inference-nim-microservices-dgx-cloud/" target="_blank" rel="noopener">integration of Hugging Face Inference-as-a-Service on DGX Cloud</a>.</p>
<p>In addition, <a href="https://blogs.nvidia.com/blog/edify-shutterstock-getty-images/" target="_blank" rel="noopener">Shutterstock has launched a Generative 3D Service</a>, while <a href="https://blogs.nvidia.com/blog/edify-shutterstock-getty-images/" target="_blank" rel="noopener">Getty Images has upgraded its offerings using NVIDIA Edify technology</a>.</p>
<p>In the realm of AI and graphics, NVIDIA has revealed new <a href="https://blogs.nvidia.com/blog/generative-physical-ai-nim-microservices/" target="_blank" rel="noopener">OpenUSD NIM microservices and reference workflows</a> designed for generative physical AI applications.</p>
<p>This includes a <a href="https://nvidianews.nvidia.com/news/nvidia-accelerates-worldwide-humanoid-robotics-development" target="_blank" rel="noopener">program for accelerating humanoid robotics development</a> through new NIM microservices for robotics simulation and more.</p>
<p>Finally, WPP, the world&#8217;s largest advertising agency, is using <a href="https://blogs.nvidia.com/blog/coca-cola-wpp-omniverse-generative-ai/" target="_blank" rel="noopener">Omniverse-driven generative AI for The Coca-Cola Company</a>, helping drive brand authenticity, showcasing the practical applications of NVIDIA’s advancements in AI technology across various industries.</p>
<p>Huang and Goode started their conversation by exploring how visual computing gave rise to everything from computer games to digital animation to GPU-accelerated computing and, most recently, generative AI powered by industrial-scale AI factories.</p>
<p>All these advancements build on one another. Robotics, for example, requires advanced AI and photorealistic virtual worlds where AI can be trained before being deployed into next-generation humanoid robots.</p>
<p>Huang explained that robotics requires three computers: one to train the AI, one to test the AI in a physically accurate simulation, and one within the robot itself.</p>
<p>“Just about every industry is going to be affected by this, whether it’s scientific computing trying to do a better job predicting the weather with a lot less energy, to augmenting and collaborating with creators to generate images, or generating virtual scenes for industrial visualization,” Huang said. “Robotic self-driving cars are all going to be transformed by generative AI.”</p>
<p>Likewise, <a href="https://www.nvidia.com/en-us/omniverse/" target="_blank" rel="noopener">NVIDIA Omniverse systems</a> — built around the OpenUSD standard — will also be key to harnessing generative AI to create assets that the world’s largest brands can use.</p>
<p>By pulling from brand assets that live in Omniverse, which can capture brand assets, these systems can capture and replicate carefully curated brand magic.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1.png"><img decoding="async" class="alignnone size-large wp-image-73392" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>Finally, all these systems — visual computing, simulation and large-language models — will come together to create digital humans who can help people interact with digital systems of all kinds.</p>
<p>“One of the things that we’re announcing here this week is the concept of digital agents, digital AIs that will augment every single job in the company,” Huang said.</p>
<p>“And so one of the most important use cases that people are discovering is customer service,” Huang said. “In the future, my guess is that it’s going to be human still, but AI in the loop.”</p>
<p>All of this, like any new tool, promises to amplify human productivity and creativity. “Imagine the stories that you’re going to be able to tell with these tools,” Huang said.</p>
<p><iframe title="What’s Next in AI: NVIDIA’s Jensen Huang Talks With WIRED’s Lauren Goode and Meta’s Mark Zuckerberg" width="500" height="281" src="https://www.youtube.com/embed/H0WxJ7caZQU?start=957&#038;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/764A7888-blog-press-1280x680-2.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/764A7888-blog-press-1280x680-2-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[‘Everybody Will Have an AI Assistant,’ NVIDIA CEO Tells SIGGRAPH Audience]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Recipe for Magic: WPP and NVIDIA Omniverse Help The Coca-Cola Company Scale Generative AI Content That Pops With Brand Authenticity</title>
		<link>https://blogs.nvidia.com/blog/coca-cola-wpp-omniverse-generative-ai/</link>
		
		<dc:creator><![CDATA[James Mills]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:49 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<category><![CDATA[SIGGRAPH]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73259</guid>

					<description><![CDATA[When The Coca-Cola Company produces thirst-quenching marketing, the creative elements of campaigns aren’t just left to chance — there’s a recipe for the magic. Now, the beverage company, through its partnership with WPP Open X, is beginning to scale its global campaigns with generative AI from NVIDIA Omniverse and NVIDIA NIM microservices. “With NVIDIA, we	<a class="read-more" href="https://blogs.nvidia.com/blog/coca-cola-wpp-omniverse-generative-ai/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>When The Coca-Cola Company produces thirst-quenching marketing, the creative elements of campaigns aren’t just left to chance — there’s a recipe for the magic. Now, the beverage company, through its partnership with WPP Open X, is beginning to scale its global campaigns with generative AI from <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> and <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> microservices.</p>
<p>“With NVIDIA, we can personalize and customize Coke and meals imagery across 100-plus markets, delivering on hyperlocal relevance with speed and at global scale,” said Samir Bhutada, global vice president of StudioX Digital Transformation at The Coca-Cola Company.</p>
<p>Coca-Cola has been working with WPP to develop <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/">digital twin</a> tools and roll out Prod X — a custom production studio experience created specifically for the beverage maker to use globally.</p>
<p><a target="_blank" href="https://www.wpp.com/news/2024/07/wpp-to-create-generative-3d-worlds-in-collaboration-with-nvidia">WPP announced</a> today at SIGGRAPH that The Coca-Cola Company will be an early adopter for integrating the <a target="_blank" href="https://nvidianews.nvidia.com/news/nvidia-announces-generative-ai-models-and-nim-microservices-for-openusd">new NVIDIA NIM microservices</a> for Universal Scene Description (aka OpenUSD) into its Prod X roadmap. <a target="_blank" href="https://aousd.org/blog/explainer-series-what-is-openusd/">OpenUSD</a> is a 3D framework that enables interoperability between software tools and data types for building virtual worlds. NIM inference microservices provide models as optimized containers.</p>
<p>The USD Search NIM allows WPP to tap into a large archive of models to create on-brand assets, and the USD Code NIM can be used to assemble them into scenes.</p>
<p>These NIM microservices will enable Prod X users to create 3D advertising assets that contain culturally relevant elements on a global scale, using prompt engineering to quickly make adjustments to AI-generated images so that brands can better target their products at local markets.</p>
<p><iframe loading="lazy" title="BTS: how WPP is creating generative 3D worlds using NVIDIA NIM microservices" width="500" height="281" src="https://www.youtube.com/embed/n-vsX-NzHzw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Tapping Into NVIDIA NIM Microservices to Deploy Generative AI </b></h2>
<p><a target="_blank" href="https://www.nvidia.com/en-us/industries/media-and-entertainment/wpp/">WPP</a> said that the NVIDIA NIM microservices will have a lasting impact on the 3D engineering and art world.</p>
<p>The <a target="_blank" href="https://build.nvidia.com/nvidia/usdsearch">USD Search NIM</a> can make WPP’s massive visual asset libraries quickly available via written prompts. The <a target="_blank" href="https://build.nvidia.com/nvidia/usdcode-llama3-70b-instruct">USD Code NIM</a> allows developers to enter prompts and get Python code to create novel 3D worlds.</p>
<p>“The beauty of the solution is that it compresses multiple phases of the production process into a single interface and process,” said Perry Nightingale, senior vice president of creative AI at WPP, of the new NIM microservices. “It empowers artists to get more out of the technology and create better work.”</p>
<h2><b>Redefining Content Production With Production Studio</b></h2>
<p>WPP recently announced the release of <a target="_blank" href="https://www.wpp.com/news/2024/06/wpp-unveils-ai-powered-production-studio">Production Studio</a> on WPP Open, the company’s intelligent marketing operating system powered by AI. Co-developed with its production company, Hogarth, Production Studio taps into the Omniverse development platform and OpenUSD for its generative AI-enabled <a target="_blank" href="https://www.nvidia.com/en-us/use-cases/3d-product-configurator/">product configurator</a> workflows.</p>
<p>Production Studio can streamline and automate multilingual text, image and video creation, simplifying content creation for advertisers and marketers, and directly addresses the challenges advertisers continue to face in producing brand-compliant and product-accurate content at scale.</p>
<p>“Our groundbreaking research with NVIDIA Omniverse for the past few years, and the research and development associated with having built our own core USD pipeline and decades of experience in 3D workflows, is what made it possible for us to stand up a tailored experience like this for The Coca-Cola Company,” said Priti Mhatre, managing director for strategic consulting and AI at Hogarth.</p>
<p><a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/"><i>SIGGRAPH</i></a><i> attendees can hear more about WPP’s efforts by joining the company’s session on “</i><a target="_blank" href="https://s2024.conference-program.org/presentation/?id=ind_120&amp;sess=sess421"><i>Robotics, Generative AI, and OpenUSD: How WPP Is Building the Future of Creativity</i></a><i>.”</i></p>
<p><i>NVIDIA founder and CEO Jensen Huang will also be featured at the event in fireside chats with Meta founder and CEO Mark Zuckerberg and WIRED Senior Writer Lauren Goode. Watch the talks and other sessions from </i><a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/"><i>NVIDIA at SIGGRAPH 2024 on demand</i></a><i>.</i></p>
<p><em>Photo credit: WPP, The Coca-Cola Company</em></p>
<p><i>See </i><a target="_blank" href="https://www.nvidia.com/en-eu/about-nvidia/terms-of-service/"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/TCCC_master_08LATAM_taco_v001_1-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1152"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/TCCC_master_08LATAM_taco_v001_1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Recipe for Magic: WPP and NVIDIA Omniverse Help The Coca-Cola Company Scale Generative AI Content That Pops With Brand Authenticity]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Reality Reimagined: NVIDIA Introduces fVDB to Build Bigger Digital Models of the World</title>
		<link>https://blogs.nvidia.com/blog/fvdb-bigger-digital-models/</link>
		
		<dc:creator><![CDATA[Ken Museth]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:47 +0000</pubDate>
				<category><![CDATA[Software]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<category><![CDATA[Universal Scene Description]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73236</guid>

					<description><![CDATA[NVIDIA announced at SIGGRAPH fVDB, a new deep-learning framework for generating AI-ready virtual representations of the real world. fVDB is built on top of OpenVDB, the industry-standard library for simulating and rendering sparse volumetric data such as water, fire, smoke and clouds. Generative physical AI, such as autonomous vehicles and robots that inhabit the real	<a class="read-more" href="https://blogs.nvidia.com/blog/fvdb-bigger-digital-models/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>NVIDIA announced at SIGGRAPH <a target="_blank" href="https://developer.nvidia.com/fvdb">fVDB</a>, a new deep-learning framework for generating AI-ready virtual representations of the real world.</p>
<p>fVDB is built on top of <a target="_blank" href="https://www.openvdb.org/">OpenVDB</a>, the industry-standard library for simulating and rendering sparse volumetric data such as water, fire, smoke and clouds.</p>
<p><a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-physical-ai/">Generative physical AI</a>, such as autonomous vehicles and robots that inhabit the real world, need to have “spatial intelligence” — the ability to understand and operate in 3D space.</p>
<p>Capturing the large scale and super-fine details of the world around us is essential. But converting reality into a virtual representation to train AI is hard.</p>
<p>Raw data for real-world environments can be collected through many different techniques, like neural radiance fields (<a href="https://blogs.nvidia.com/blog/neural-radiance-fields-3d-models/">NeRFs</a>) and lidar. fVDB translates this data into massive, AI-ready environments rendered in real time.</p>
<p><iframe loading="lazy" title="Train physical AI with real-world simulations using fVDB." width="500" height="281" src="https://www.youtube.com/embed/rx55wDWqSrw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>Building on a decade of innovation in the OpenVDB standard, the introduction of<a target="_blank" href="https://arxiv.org/abs/2407.01781"> fVDB</a> at SIGGRAPH represents a significant leap forward in how industries can benefit from <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/">digital twins</a> of the real world.</p>
<p>Reality-scale virtual environments are used for training autonomous agents. City-scale 3D models are captured by drones for climate science and disaster planning. Today, 3D generative AI is even used to plan urban spaces and smart cities.</p>
<p>fVDB enables industries to tap into spatial intelligence on a larger scale and with higher resolution than ever before, making physical AI even smarter.</p>
<p>The framework builds NVIDIA-accelerated AI operators on top of <a target="_blank" href="https://developer.nvidia.com/nanovdb">NanoVDB</a>, a GPU-accelerated data structure for efficient 3D simulations. These operators include convolution, pooling, attention and meshing, all of which are designed for high-performance 3D deep learning applications.</p>
<p>AI operators allow businesses to build complex neural networks for spatial intelligence, like large-scale point cloud reconstruction and 3D generative modeling.</p>
<p>fVDB is the result of a long-running effort by NVIDIA’s research team and is already used to support <a target="_blank" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>, <a target="_blank" href="https://developer.nvidia.com/drive">NVIDIA DRIVE</a> and <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> projects that require high-fidelity models of large, complex real-world spaces.</p>
<h2><b>Key Advantages of fVDB</b></h2>
<ul>
<li style="font-weight: 300;" aria-level="1">Larger: 4x larger spatial scale than prior frameworks</li>
<li style="font-weight: 300;" aria-level="1">Faster: 3.5x faster than prior frameworks</li>
<li style="font-weight: 300;" aria-level="1">Interoperable: Businesses can fully tap into massive real-world datasets. fVDB reads VDB datasets into full-sized 3D environments. AI-ready and real-time rendered for building physical AI with spatial intelligence.</li>
<li style="font-weight: 300;" aria-level="1">More powerful: 10x more operators than prior frameworks. fVDB simplifies processes by combining functionalities that previously required multiple deep-learning libraries.</li>
</ul>
<p>fVDB will soon be available as <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> inference microservices. A trio of the microservices will enable businesses to incorporate fVDB into <a target="_blank" href="https://nvidianews.nvidia.com/news/nvidia-expands-openusd-to-generative-ai-for-robotics-industrial-digitalization-markets">OpenUSD workflows</a>, generating AI-ready OpenUSD geometry in NVIDIA Omniverse, a development platform for industrial digitalization and generative physical AI applications. They are:</p>
<ul>
<li style="font-weight: 300;" aria-level="1">fVDB Mesh Generation NIM — Generates digital 3D environments of the real world</li>
<li style="font-weight: 300;" aria-level="1">fVDB NeRF-XL NIM — Generates large-scale NeRFs in OpenUSD using Omniverse Cloud APIs</li>
<li style="font-weight: 300;" aria-level="1">fVDB Physics Super-Res NIM — Performs super-resolution to generate an OpenUSD-based, high-resolution physics simulation</li>
</ul>
<p>Over the past decade, <a target="_blank" href="https://www.openvdb.org/about/">OpenVDB</a><span>, housed at the </span><a target="_blank" href="https://www.aswf.io/blog/fvdb/"><span>Academy Software Foundation</span></a><span>,</span> has earned multiple <a href="https://blogs.nvidia.com/blog/academy-awards-vfx-openusd/">Academy Awards</a> as a core technology used throughout the visual-effects industry. It has since grown beyond entertainment to industrial and scientific uses, like industrial design and robotics.</p>
<p>NVIDIA continues to enhance the open-source OpenVDB library. Four years ago, the company introduced <a target="_blank" href="https://developer.nvidia.com/nanovdb">NanoVDB</a>, which added GPU support to OpenVDB. This delivered an order-of-magnitude speed-up, enabling faster performance and easier development, and opening the door to real-time simulation and rendering.</p>
<p>Two years ago, NVIDIA introduced <a target="_blank" href="https://developer.nvidia.com/rendering-technologies/neuralvdb">NeuralVDB</a>, which builds machine learning on top of NanoVDB to compress the memory footprint of VDB volumes up to 100x, allowing creators, developers and researchers to interact with extremely large and complex datasets.</p>
<p>fVDB builds AI operators on top of NanoVDB to unlock spatial intelligence at the scale of reality. Apply to the <a target="_blank" href="https://developer.nvidia.com/fvdb/early-access-form">early-access program</a> for the fVDB PyTorch extension. fVDB will also be available as part of the <a target="_blank" href="https://github.com/AcademySoftwareFoundation/openvdb">OpenVDB GitHub repository</a>.</p>
<p>Dive deeper into <a target="_blank" href="https://developer.nvidia.com/blog/building-spatial-intelligence-from-real-world-3d-data-using-deep-learning-framework-fvdb/">fVDB in this technical blog</a> and watch how accelerated computing and generative AI are transforming industries and creating new opportunities for innovation and growth in NVIDIA founder and CEO Jensen Huang’s two <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">fireside chats</a> at SIGGRAPH.</p>
<p><i>See </i><a target="_blank" href="https://www.nvidia.com/en-eu/about-nvidia/terms-of-service/"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/DisasterRelief0.jpg"
			type="image/jpeg"
			width="1920"
			height="1080"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/DisasterRelief0-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Reality Reimagined: NVIDIA Introduces fVDB to Build Bigger Digital Models of the World]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Supercharges Digital Marketing With Greater Control Over Generative AI</title>
		<link>https://blogs.nvidia.com/blog/nvidia-supercharges-marketing-agencies-generative-ai/</link>
		
		<dc:creator><![CDATA[James Mills]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:46 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<category><![CDATA[Universal Scene Description]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73289</guid>

					<description><![CDATA[The world’s brands and agencies are using generative AI to create advertising and marketing content, but it doesn’t always provide the desired outputs. NVIDIA offers a comprehensive set of technologies — bringing together generative AI, NVIDIA NIM microservices, NVIDIA Omniverse and Universal Scene Description (OpenUSD) — to allow developers to build applications and workflows that	<a class="read-more" href="https://blogs.nvidia.com/blog/nvidia-supercharges-marketing-agencies-generative-ai/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>The world’s brands and agencies are using generative AI to create advertising and marketing content, but it doesn’t always provide the desired outputs.</p>
<p>NVIDIA offers a comprehensive set of technologies — bringing together generative AI, NVIDIA NIM microservices, NVIDIA Omniverse and <a target="_blank" href="https://aousd.org/blog/explainer-series-what-is-openusd/">Universal Scene Description</a> (OpenUSD) — to allow developers to build applications and workflows that enable brand-accurate, targeted and efficient advertising at scale.</p>
<p>Developers can use the <a target="_blank" href="https://build.stg.ngc.nvidia.com/nvidia/usdsearch">USD Search NIM microservice to provide artists access to a vast archive of </a>OpenUSD-based, brand-approved assets — such as products, props and environments — <a target="_blank" href="https://build.stg.ngc.nvidia.com/nvidia/usdsearch">and</a> when integrated with the <a target="_blank" href="https://build.stg.ngc.nvidia.com/nvidia/usdcode-llama3-70b-instruct">USD Code NIM</a> microservice, assembly of these scenes can be accelerated. Teams can also use the <a target="_blank" href="https://www.nvidia.com/en-us/gpu-cloud/picasso/">NVIDIA Edify</a>-powered <a href="https://blogs.nvidia.com/blog/edify-shutterstock-getty-images/">Shutterstock Generative 3D</a> service to rapidly generate 3D new assets using AI.</p>
<p>The scenes, once constructed, can be rendered to a 2D image and used as input to direct an AI-powered image generator to create precise, brand-accurate visuals.</p>
<p>Global agencies, developers and production studios are tapping these technologies to revolutionize every aspect of the advertising process, from creative production and content supply chain to dynamic creative optimization.</p>
<p><a target="_blank" href="https://www.wpp.com/news/2024/07/wpp-to-create-generative-3d-worlds-in-collaboration-with-nvidia">WPP announced at SIGGRAPH</a> its adoption of the technologies, <a href="https://blogs.nvidia.com/blog/coca-cola-wpp-omniverse-generative-ai">naming The Coca-Cola Company</a> the first brand to embrace generative AI with Omniverse and NVIDIA NIM microservices.</p>
<p><iframe loading="lazy" title="BTS: how WPP is creating generative 3D worlds using NVIDIA NIM microservices" width="500" height="281" src="https://www.youtube.com/embed/n-vsX-NzHzw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Agencies and Service Providers Increase Adoption of Omniverse</b></h2>
<p>The NVIDIA Omniverse development platform has seen widespread adoption for its ability to build accurate <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/">digital twins</a> of products. These virtual replicas allow brands and agencies to create ultra-photorealistic and physically accurate 3D <a target="_blank" href="https://www.nvidia.com/en-us/use-cases/3d-product-configurator/">product configurators</a>, helping to increase personalization, customer engagement and loyalty, and average selling prices, and reducing return rates.</p>
<p>Digital twins can also serve many purposes and be updated to meet shifting consumer preferences with minimal time, cost and effort, helping flexibly scale content production.</p>
<h2><b>Agencies and Service Providers Increase Adoption of Omniverse</b></h2>
<p>The NVIDIA Omniverse development platform has seen widespread adoption for its ability to build accurate <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/">digital twins</a> of products. These virtual replicas allow brands and agencies to create ultra-photorealistic and physically accurate 3D <a target="_blank" href="https://www.nvidia.com/en-us/use-cases/3d-product-configurator/">product configurators</a>, helping to increase personalization, customer engagement and loyalty, and average selling prices, and reducing return rates.</p>
<p>Digital twins can also serve many purposes and be updated to meet shifting consumer preferences with minimal time, cost and effort, helping flexibly scale content production.</p>
<p>&nbsp;</p>
<figure id="attachment_73362" aria-describedby="caption-attachment-73362" style="width: 500px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="size-full wp-image-73362" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/monks-hatch.gif" alt="" width="500" height="500" /><figcaption id="caption-attachment-73362" class="wp-caption-text">Image courtesy of Monks, Hatch.</figcaption></figure>
<p>Global marketing and technology services company Monks developed Monks.Flow, an AI-centric professional managed service that uses the Omniverse platform to help brands virtually explore different customizable product designs and unlock scale and hyper-personalization across any customer journey.</p>
<p>“NVIDIA Omniverse and OpenUSD’s interoperability accelerates connectivity between marketing, technology and product development,” said Lewis Smithingham, executive vice president of strategic industries at Monks. “Combining Omniverse with Monks’ streamlined marketing and technology services, we infuse AI throughout the product development pipeline and help accelerate technological and creative possibilities for clients.”</p>
<p>Collective World, a creative and technology company, is an early adopter of real-time 3D, OpenUSD and NVIDIA Omniverse, using them to create high-quality digital campaigns for customers like Unilever and EE. The technologies allow Collective to develop digital twins, delivering consistent, high-quality product content at scale to streamline advertising and marketing campaigns.</p>
<p>Building on its use of NVIDIA technologies, Collective World announced at SIGGRAPH that it has joined the <a target="_blank" href="https://www.nvidia.com/en-us/about-nvidia/partners/">NVIDIA Partner Network</a>.</p>
<figure id="attachment_73356" aria-describedby="caption-attachment-73356" style="width: 672px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="wp-image-73356 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-672x378.jpg" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-1280x720.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse.jpg 1999w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73356" class="wp-caption-text">Product digital twin configurator and content generation tool built by Collective on NVIDIA Omniverse.</figcaption></figure>
<p>INDG is using Omniverse to introduce new capabilities into Grip, its popular software tool. Grip uses OpenUSD and generative AI to streamline and enhance the creation process, delivering stunning, high-fidelity marketing content faster than ever.</p>
<p>“This integration helps bring significant efficiencies to every brand by delivering seamless interoperability and enabling real-time visualization,” said Frans Vriendsendorp, CEO of INDG. “Harnessing the potential of USD to eliminate the lock-in to proprietary formats, the combination of Grip and Omniverse is helping set new standards in the realm of digital content creation.”</p>
<figure id="attachment_73359" aria-describedby="caption-attachment-73359" style="width: 672px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="wp-image-73359 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-1536x864.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-1280x720.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image.png 1999w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73359" class="wp-caption-text">Image generated with Grip, copyright Beiersdorf</figcaption></figure>
<p>To get started building applications and services using OpenUSD, Omniverse and NVIDIA AI, check out the <a target="_blank" href="https://www.nvidia.com/en-us/use-cases/3d-product-configurator/">product configurator developer resources</a> and the <a target="_blank" href="https://resources.nvidia.com/en-us-omniverse-product-configurator/ov-genai-workflow-reference-architecture?xs=674987&amp;ncid=no-ncid">generative AI workflow for content creation reference architecture</a>, or submit a <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/contact/">contact form</a> to learn more or connect with NVIDIA’s ecosystem of service providers.</p>
<p><i>Watch NVIDIA founder and CEO Jensen Huang’s fireside chats, as well as other on-demand sessions from </i><a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/"><i>NVIDIA at SIGGRAPH</i></a><i>.</i></p>
<p><i>Stay up to date by subscribing to our </i><a target="_blank" href="https://nvda.ws/3u5KPv1"><i>newsletter</i></a><i>, and following NVIDIA Omniverse on </i><a target="_blank" href="https://www.instagram.com/nvidiaomniverse/"><i>Instagram</i></a><i>, </i><a target="_blank" href="https://www.linkedin.com/showcase/nvidia-omniverse/"><i>LinkedIn</i></a><i>, </i><a target="_blank" href="https://medium.com/@nvidiaomniverse"><i>Medium</i></a><i> and </i><a target="_blank" href="https://twitter.com/nvidiaomniverse"><i>X</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/digital-marketing-agencies-featured-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/digital-marketing-agencies-featured-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Supercharges Digital Marketing With Greater Control Over Generative AI]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>New NVIDIA Digital Human Technologies Enhance Customer Interactions Across Industries</title>
		<link>https://blogs.nvidia.com/blog/digital-humans-siggraph-2024/</link>
		
		<dc:creator><![CDATA[Ike Nnoli]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:41 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Maxine]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73273</guid>

					<description><![CDATA[Generative AI is unlocking new ways for enterprises to engage customers through digital human avatars. At SIGGRAPH, NVIDIA previewed &#8220;James,&#8221; an interactive digital human that can connect with people using emotions, humor and more. James is based on a customer-service workflow using NVIDIA ACE, a reference design for creating custom, hyperrealistic, interactive avatars. Users will	<a class="read-more" href="https://blogs.nvidia.com/blog/digital-humans-siggraph-2024/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-ai/">Generative AI</a> is unlocking new ways for enterprises to engage customers through digital human avatars.</p>
<p>At <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">SIGGRAPH</a>, NVIDIA previewed &#8220;James,&#8221; an interactive digital human that can connect with people using emotions, humor and more. James is based on a customer-service workflow using <a target="_blank" href="https://developer.nvidia.com/ace">NVIDIA ACE</a>, a reference design for creating custom, hyperrealistic, interactive avatars. Users will soon be able to talk with James in real time at <a target="_blank" href="https://build.nvidia.com/explore/discover">ai.nvidia.com</a>.</p>
<p>NVIDIA also showcased at the computer graphics conference the latest advancements to the <a target="_blank" href="https://developer.nvidia.com/maxine">NVIDIA Maxine AI platform</a>, including Maxine 3D and Audio2Face-2D for an immersive telepresence experience.</p>
<p>Developers can use Maxine and NVIDIA ACE digital human technologies to make customer interactions with digital interfaces more engaging and natural. ACE technologies enable digital human development with AI models for speech and translation, vision, intelligence, lifelike animation and behavior, and realistic appearance.</p>
<p>Companies across industries are using Maxine and ACE to deliver immersive virtual customer experiences.</p>
<h2><b>Meet James, a Digital Brand Ambassador</b></h2>
<div style="width: 1280px;" class="wp-video"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video class="wp-video-shortcode" id="video-73273-1" width="1280" height="720" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/AceCut-2.mp4?_=1" /><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/AceCut-2.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/07/AceCut-2.mp4</a></video></div>
<p>Built on top of <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> microservices, James is a virtual assistant that can provide contextually accurate responses.</p>
<p>Using retrieval-augmented generation (<a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">RAG</a>), James can accurately tell users about the latest NVIDIA technologies. ACE allows developers to use their own data to create domain-specific avatars that can communicate relevant information to customers.</p>
<p>James is powered by the latest <a target="_blank" href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX</a> rendering technologies for advanced, lifelike animations. His natural-sounding voice is powered by <a target="_blank" href="https://elevenlabs.io/blog/nvidia-ace-at-computex">ElevenLabs</a>. NVIDIA ACE lets developers customize animation, voice and language when building avatars tailored for different use cases.</p>
<h2><b>NVIDIA Maxine Enhances Digital Humans in Telepresence</b></h2>
<p>Maxine, a platform for deploying cutting-edge AI features that enhance the audio and video quality of digital humans, enables the use of real-time, photorealistic 2D and 3D avatars with video-conferencing devices.</p>
<p>Maxine 3D converts 2D video portrait inputs into 3D avatars, allowing the integration of highly realistic digital humans in video conferencing and other two-way communication applications. The technology will soon be available in early access.</p>
<p>Audio2Face-2D, currently in early access, animates static portraits based on audio input, creating dynamic, speaking digital humans from a single image. Try the technology at <a target="_blank" href="https://build.nvidia.com/nvidia/audio2face-2d">ai.nvidia.com</a>.</p>
<h2><b>Companies Embracing Digital Human Applications</b></h2>
<p>HTC, Looking Glass, Reply and UneeQ are among the latest companies using NVIDIA ACE and Maxine across a broad range of use cases, including customer service agents, and telepresence experiences in entertainment, retail and hospitality.</p>
<p>At SIGGRAPH, digital human technology developer <a target="_blank" href="http://www.digitalhumans.com/blog/uneeq-joining-nvidia-to-showcase-next-generation-digital-humans-at-siggraph-2024">UneeQ</a> is showcasing two new demos.</p>
<p>The first spotlights cloud-rendered digital humans powered by NVIDIA GPUs with local, in-browser computer vision for enhanced scalability and privacy, and animated using the Audio2Face-3D NVIDIA NIM microservice. UneeQ’s Synapse technology processes anonymized user data and feeds it to a <a target="_blank" href="https://www.nvidia.com/en-us/glossary/large-language-models/">large language model</a> (LLM) for more accurate, responsive interactions.</p>
<p>The second demo runs on a single NVIDIA RTX GPU-powered laptop, featuring an advanced digital human powered by Gemma 7B LLM, RAG and the NVIDIA Audio2Face-3D NIM microservice.</p>
<p>Both demos showcase UneeQ’s NVIDIA-powered efforts to develop digital humans that can react to users’ facial expressions and actions, pushing the boundaries of realism in virtual customer service experiences.</p>
<p><iframe loading="lazy" title="UneeQ and NVIDIA showcase next-generation digital humans driven by Synanim and Audio2Face" width="500" height="281" src="https://www.youtube.com/embed/LD4ak_O0oLk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p><a target="_blank" href="https://blog.vive.com/us/htc-and-nvidia-partner-at-siggraph-elevating-viverse-avatar-interactions-with-audio2face-technology">HTC Viverse</a> has integrated the Audio2Face-3D NVIDIA NIM microservice into its VIVERSE AI agent for dynamic facial animation and lip sync, allowing for more natural and immersive user interactions.</p>
<p>Hologram technology company <a target="_blank" href="https://blog.lookingglassfactory.com/p/b3e1da47-f231-4fb6-8ee0-1fef3818ff2f/">Looking Glass</a>’ Magic Mirror demo at SIGGRAPH uses a simple camera setup and Maxine’s advanced 3D AI capabilities to generate a real-time holographic feed of users’ faces on its newly launched, group-viewable Looking Glass 16-inch and 32-inch Spatial Displays.</p>
<p><a target="_blank" href="https://www.reply.com/en">Reply</a> is unveiling an enhanced version of <a target="_blank" href="https://www.reply.com/en/artificial-intelligence/futura-the-ai-driven-tour-expert">Futura</a>, its cutting-edge digital human developed for Costa Crociere’s Costa Smeralda cruise ship. Powered by <a target="_blank" href="https://build.nvidia.com/nvidia/audio2face">Audio2Face-3D NVIDIA NIM</a> and <a target="_blank" href="https://build.nvidia.com/nvidia/parakeet-ctc-riva">Riva ASR NIM</a> microservices, Futura’s speech-synthesis capabilities tap advanced technologies including GPT-4o, LlamaIndex for RAG and Microsoft Azure text-to-speech services.</p>
<p>Futura also incorporates Reply’s proprietary <a target="_blank" href="https://www.reply.com/en/artificial-intelligence/affective-computing">affective computing technology</a>, alongside Hume AI and MorphCast, for comprehensive emotion recognition. Built using Unreal Engine 5.4.3 and MetaHuman Creator with NVIDIA ACE-powered facial animation, Futura supports six languages. The intelligent assistant can help plan personalized port visits, suggest tailored itineraries and facilitate tour bookings.</p>
<p>In addition, Futura refines recommendations based on guest feedback and uses a specially created knowledge base to provide informative city presentations, enhancing tourist itineraries. Futura aims to enhance customer service and offer immersive interactions in real-world scenarios, leading to streamlined operations and driving business growth.</p>
<p><i>Learn more about </i><a target="_blank" href="https://developer.nvidia.com/ace"><i>NVIDIA ACE</i></a><i> and </i><a target="_blank" href="https://developer.nvidia.com/maxine"><i>NVIDIA Maxine</i></a><i>. </i></p>
<p><i>Discover how accelerated computing and generative AI are transforming industries and creating new opportunities for innovation by watching NVIDIA founder and CEO Jensen Huang’s </i><a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/"><i>fireside chats</i></a><i> at SIGGRAPH.</i></p>
<p><i>See </i><a target="_blank" href="https://www.nvidia.com/en-eu/about-nvidia/terms-of-service/"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2024/07/AceCut-2.mp4" length="400181" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/sigg24-social-ace-kv-1200x628-1.jpg"
			type="image/jpeg"
			width="1200"
			height="628"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/sigg24-social-ace-kv-1200x628-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[New NVIDIA Digital Human Technologies Enhance Customer Interactions Across Industries]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Hugging Face Offers Developers Inference-as-a-Service Powered by NVIDIA NIM</title>
		<link>https://blogs.nvidia.com/blog/hugging-face-inference-nim-microservices-dgx-cloud/</link>
		
		<dc:creator><![CDATA[Alexis Bjorlin]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:41 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[DGX Cloud]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73334</guid>

					<description><![CDATA[One of the world’s largest AI communities — comprising 4 million developers on the Hugging Face platform — is gaining easy access to NVIDIA-accelerated inference on some of the most popular AI models. New inference-as-a-service capabilities will enable developers to rapidly deploy leading large language models such as the Llama 3 family and Mistral AI	<a class="read-more" href="https://blogs.nvidia.com/blog/hugging-face-inference-nim-microservices-dgx-cloud/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>One of the world’s largest AI communities — comprising 4 million developers on the Hugging Face platform — is gaining easy access to NVIDIA-accelerated inference on some of the most popular AI models.</p>
<p>New inference-as-a-service capabilities will enable developers to rapidly deploy leading <a target="_blank" href="https://www.nvidia.com/en-us/glossary/large-language-models/">large language models</a> such as the Llama 3 family and Mistral AI models with optimization from <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> microservices running on <a target="_blank" href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a>.</p>
<p>Announced today at the <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">SIGGRAPH</a> conference, the service will help developers quickly prototype with open-source AI models hosted on the Hugging Face Hub and deploy them in production. Enterprise Hub users can tap serverless inference for increased flexibility, minimal infrastructure overhead and optimized performance with NVIDIA NIM.</p>
<p>The inference service complements <a target="_blank" href="https://nvidianews.nvidia.com/news/nvidia-and-hugging-face-to-connect-millions-of-developers-to-generative-ai-supercomputing">Train on DGX Cloud</a>, an AI training service already available on Hugging Face.</p>
<p>Developers facing a growing number of open-source models can benefit from a hub where they can easily compare options. These training and inference tools give Hugging Face developers new ways to experiment with, test and deploy cutting-edge models on NVIDIA-accelerated infrastructure. They’re made easily accessible using the “Train” and “Deploy” drop-down menus on Hugging Face model cards, letting users get started with just a few clicks.</p>
<p>Get started with <a href="https://huggingface.co/blog/inference-dgx-cloud" target="_blank" rel="noopener">inference-as-a-service powered by NVIDIA NIM</a>.</p>
<p><b>Beyond a Token Gesture — NVIDIA NIM Brings Big Benefits</b></p>
<p><a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> is a collection of AI microservices — including NVIDIA AI foundation models and open-source community models — optimized for inference using industry-standard application programming interfaces, or APIs.</p>
<p>NIM offers users higher efficiency in processing tokens — the units of data used and generated by a language model. The optimized microservices also improve the efficiency of the underlying NVIDIA DGX Cloud infrastructure, which can increase the speed of critical AI applications.</p>
<p>This means developers see faster, more robust results from an AI model accessed as a NIM compared with other versions of the model. The 70-billion-parameter version of Llama 3, for example, delivers up to 5x higher throughput when accessed as a NIM compared with off-the-shelf deployment on <a target="_blank" href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPU-powered systems</a>.</p>
<p><b>Near-Instant Access to DGX Cloud Provides Accessible AI Acceleration</b></p>
<p>The NVIDIA DGX Cloud platform is purpose-built for <a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a>, offering developers easy access to reliable accelerated computing infrastructure that can help them bring production-ready applications to market faster.</p>
<p>The platform provides scalable GPU resources that support every step of AI development, from prototype to production, without requiring developers to make long-term AI infrastructure commitments.</p>
<p>Hugging Face inference-as-a-service on NVIDIA DGX Cloud powered by NIM microservices offers easy access to compute resources that are optimized for AI deployment, enabling users to experiment with the latest AI models in an enterprise-grade environment.</p>
<p><b>More on NVIDIA NIM at SIGGRAPH </b></p>
<p>At SIGGRAPH, NVIDIA also introduced <a target="_blank" href="https://nvidianews.nvidia.com/news/nvidia-announces-generative-ai-models-and-nim-microservices-for-openusd">generative AI models and NIM microservices for the OpenUSD framework</a> to accelerate developers’ abilities to build highly accurate virtual worlds for the next evolution of AI.</p>
<p>To experience more than 100 NVIDIA NIM microservices with applications across industries, visit <a target="_blank" href="http://ai.nvidia.com/">ai.nvidia.com</a>.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-hugging-face-logos.jpg"
			type="image/jpeg"
			width="1920"
			height="1080"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-hugging-face-logos-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Hugging Face Offers Developers Inference-as-a-Service Powered by NVIDIA NIM]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>AI Gets Physical: New NVIDIA NIM Microservices Bring Generative AI to Digital Environments</title>
		<link>https://blogs.nvidia.com/blog/generative-physical-ai-nim-microservices/</link>
		
		<dc:creator><![CDATA[Adam Scraba]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:23 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Robotics]]></category>
		<category><![CDATA[Computer Vision]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[Industrial and Manufacturing]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<category><![CDATA[Smart Spaces]]></category>
		<category><![CDATA[Synthetic Data Generation]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73301</guid>

					<description><![CDATA[Millions of people already use generative AI to assist in writing and learning. Now, the technology can also help them more effectively navigate the physical world. NVIDIA announced at SIGGRAPH generative physical AI advancements including the NVIDIA Metropolis reference workflow for building interactive visual AI agents and new NVIDIA NIM microservices that will help developers	<a class="read-more" href="https://blogs.nvidia.com/blog/generative-physical-ai-nim-microservices/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Millions of people already use <a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a> to assist in writing and learning. Now, the technology can also help them more effectively navigate the physical world.</p>
<p>NVIDIA announced at SIGGRAPH <a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-physical-ai">generative physical AI</a> advancements including the <a target="_blank" href="https://www.nvidia.com/en-us/use-cases/visual-ai-agents/">NVIDIA Metropolis reference workflow</a> for building interactive visual AI agents and new NVIDIA NIM microservices that will help developers train physical machines and improve how they handle complex tasks.</p>
<p>These include <a href="https://blogs.nvidia.com/blog/fvdb-bigger-digital-models/">three fVDB NIM microservices</a> that support NVIDIA’s new deep learning framework for 3D worlds, as well as the <a target="_blank" href="https://nvidianews.nvidia.com/news/nvidia-expands-openusd-to-generative-ai-for-robotics-industrial-digitalization-markets">USD Code, USD Search and USD Validate NIM microservices</a> for working with Universal Scene Description (aka <a target="_blank" href="https://aousd.org/blog/explainer-series-what-is-openusd/">OpenUSD</a>).</p>
<p>The NVIDIA OpenUSD NIM microservices work together with the world’s first generative AI models for OpenUSD development — also developed by NVIDIA — to enable developers to <a target="_blank" href="https://developer.nvidia.com/blog/integrate-generative-ai-into-openusd-workflows-using-new-nvidia-omniverse-developer-tools/">incorporate generative AI copilots and agents into USD workflows</a> and broaden the possibilities of 3D worlds.</p>
<h2><b>NVIDIA NIM Microservices Transform Physical AI Landscapes</b></h2>
<p>Physical AI uses advanced simulations and learning methods to help robots and other industrial automation more effectively perceive, reason and navigate their surroundings. The technology is transforming industries like manufacturing and healthcare, and advancing smart spaces with robots, factory and warehouse technologies, surgical AI agents and cars that can operate more autonomously and precisely.</p>
<p>NVIDIA offers a broad range of <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NIM microservices</a> customized for specific models and industry domains. NVIDIA’s suite of NIM microservices tailored for physical AI supports capabilities for speech and translation, vision and intelligence, and realistic animation and behavior.</p>
<h2><b>Turning Visual AI Agents Into Visionaries With NVIDIA NIM</b></h2>
<p><a target="_blank" href="https://www.nvidia.com/en-us/use-cases/visual-ai-agents/">Visual AI agents</a> use computer vision capabilities to perceive and interact with the physical world and perform reasoning tasks.</p>
<p>Highly perceptive and interactive visual AI agents are powered by a new class of generative AI models called <a target="_blank" href="https://build.nvidia.com/explore/vision">vision language models (VLMs)</a>, which bridge digital perception and real-world interaction in physical AI workloads to enable enhanced decision-making, accuracy, interactivity and performance. With VLMs, developers can build vision AI agents that can more effectively handle challenging tasks, even in complex environments.</p>
<p>Generative AI-powered visual AI agents are rapidly being deployed across hospitals, factories, warehouses, retail stores, airports, traffic intersections and more.</p>
<p>To help physical AI developers more easily build high-performing, custom visual AI agents, NVIDIA offers NIM microservices and reference workflows for physical AI. The NVIDIA Metropolis reference workflow provides a simple, structured approach for customizing, building and deploying visual AI agents, as detailed in <a target="_blank" href="https://developer.nvidia.com/blog/build-vlm-powered-visual-ai-agents-using-nvidia-nim-and-nvidia-via-microservices/">the blog</a>.</p>
<h2><b>NVIDIA NIM Helps</b><b> K2K </b><b>Make Palermo More Efficient, Safe and Secure</b></h2>
<p>City traffic managers in Palermo, Italy, deployed visual AI agents using NVIDIA NIM to uncover physical insights that help them better manage roadways.</p>
<p>K2K, an <a target="_blank" href="https://nvidia.com/en-us/autonomous-machines/intelligent-video-analytics-platform/">NVIDIA Metropolis</a> partner, is leading the effort, integrating NVIDIA NIM microservices and VLMs into AI agents that analyze the city’s live traffic cameras in real time. City officials can ask the agents questions in natural language and receive fast, accurate insights on street activity and suggestions on how to improve the city’s operations, like adjusting traffic light timing.</p>
<p><img loading="lazy" decoding="async" class="alignnone wp-image-73295 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-1536x864.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-1280x720.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui.png 1600w" sizes="(max-width: 672px) 100vw, 672px" /></p>
<p>Leading global electronics giants <a href="https://blogs.nvidia.com/blog/foxconn-digital-twin-ai/">Foxconn</a> and <a href="https://blogs.nvidia.com/blog/computex-metropolis-nim/">Pegatron</a> have adopted physical AI, NIM microservices and Metropolis reference workflows to more efficiently design and run their massive manufacturing operations.</p>
<p>The companies are building <a target="_blank" href="https://www.nvidia.com/en-us/use-cases/ai-for-virtual-factory-solutions/">virtual factories</a> in simulation to save significant time and costs. They’re also running more thorough tests and refinements for their physical AI — including AI multi-camera and visual AI agents — in digital twins before real-world deployment, improving worker safety and leading to operational efficiencies.</p>
<h2><b>Bridging the Simulation-to-Reality Gap With Synthetic Data Generation</b></h2>
<p>Many AI-driven businesses are now adopting a “simulation-first” approach for generative physical AI projects involving real-world industrial automation.</p>
<p>Manufacturing, factory logistics and robotics companies need to manage intricate human-worker interactions, advanced facilities and expensive equipment. NVIDIA physical AI software, tools and platforms — including physical AI and VLM NIM microservices, reference workflows and <a target="_blank" href="https://developer.nvidia.com/fVDB">fVDB</a> — can help them streamline the highly complex engineering required to create digital representations or virtual environments that accurately mimic real-world conditions.</p>
<p>VLMs are seeing widespread adoption across industries because of their ability to generate highly realistic imagery. However, these models can be challenging to train because of the immense volume of data required to create an accurate physical AI model.</p>
<p><a href="https://blogs.nvidia.com/blog/what-is-synthetic-data/">Synthetic data</a> generated from <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/">digital twins</a> using computer simulations offers a powerful alternative to real-world datasets, which can be expensive — and sometimes impossible — to acquire for model training, depending on the use case.</p>
<p>Tools like NVIDIA NIM microservices and <a target="_blank" href="https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator.html">Omniverse Replicator</a> let developers <a target="_blank" href="https://developer.nvidia.com/blog/how-to-build-a-generative-ai-enabled-synthetic-data-pipeline-with-openusd/">build generative AI-enabled synthetic data pipelines</a> to accelerate the creation of robust, diverse datasets for training physical AI. This enhances the adaptability and performance of models such as VLMs, enabling them to generalize more effectively across industries and use cases.</p>
<h2><b>Availability</b></h2>
<p>Developers can access state-of-the-art, open and NVIDIA-built foundation AI models and NIM microservices at <a target="_blank" href="http://ai.nvidia.com">ai.nvidia.com</a>. The Metropolis NIM reference workflow is available in the <a target="_blank" href="https://github.com/NVIDIA/metropolis-nim-workflows/tree/main">GitHub repository</a>, and Metropolis VIA microservices are available for download in <a target="_blank" href="https://developer.nvidia.com/visual-insight-agent-early-access">developer preview</a>.</p>
<p>OpenUSD NIM microservices are available in preview through the <a target="_blank" href="http://ai.nvidia.com">NVIDIA API catalog</a>.</p>
<p>Watch how accelerated computing and generative AI are transforming industries and creating new opportunities for innovation and growth in NVIDIA founder and CEO Jensen Huang’s <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">fireside chats</a> at SIGGRAPH.</p>
<p><i>See </i><a target="_blank" href="https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fabout-nvidia%2Flegal-info%2F&amp;data=05%7C02%7Clpham%40nvidia.com%7Cd59f2f66f51e4deaac8008dc94b3ef0f%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638548747745016311%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&amp;sdata=dm8Os%2B4LtHW2ehZrPaxn38bsutMQBDeUdQuxrIa2y1Y%3D&amp;reserved=0"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceblog.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceblog-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[AI Gets Physical: New NVIDIA NIM Microservices Bring Generative AI to Digital Environments]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>For Your Edification: Shutterstock Releases Generative 3D, Getty Images Upgrades Service Powered by NVIDIA</title>
		<link>https://blogs.nvidia.com/blog/edify-shutterstock-getty-images/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:04 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[Customer Stories]]></category>
		<category><![CDATA[Events]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73113</guid>

					<description><![CDATA[Designers and artists have new and improved ways to boost their productivity with generative AI trained on licensed data. Shutterstock, a leading platform for creative content, launched its Generative 3D service in commercial beta. It lets creators quickly prototype 3D assets and generate 360 HDRi backgrounds that light scenes, using just text or image prompts.	<a class="read-more" href="https://blogs.nvidia.com/blog/edify-shutterstock-getty-images/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Designers and artists have new and improved ways to boost their productivity with generative AI trained on licensed data.</p>
<p>Shutterstock, a leading platform for creative content, launched its <a target="_blank" href="https://www.shutterstock.com/discover/generative-ai-3d">Generative 3D service</a> in commercial beta. It lets creators quickly prototype 3D assets and generate 360 HDRi backgrounds that light scenes, using just text or image prompts.</p>
<p>Getty Images, a premier visual content creator and marketplace, turbocharged its <a target="_blank" href="https://www.gettyimages.com/ai/generation/about">Generative AI by Getty Images</a> service so it creates images twice as fast, improves output quality, brings advanced controls and enables fine-tuning.</p>
<p>The services are built with NVIDIA’s visual AI foundry using <a target="_blank" href="https://www.nvidia.com/en-us/gpu-cloud/picasso/">NVIDIA Edify</a>, a multimodal generative AI architecture. The AI models are then optimized and packaged for maximum performance with <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a>, a set of accelerated microservices for AI inference.</p>
<p>Edify enables service providers to train responsible generative models on their licensed data and scale them quickly with <a target="_blank" href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a>, the cloud-first way to get the best of NVIDIA AI.</p>
<h2><b>Generative AI Speeds 3D Modeling</b></h2>
<p>Available now for enterprises in commercial beta, Shutterstock’s service lets designers and artists quickly create 3D objects that help them prototype or populate virtual environments. For example, tapping generative AI, they can quickly create the silverware and plates on a dining room table so they can focus on designing the characters around it.</p>
<p>The 3D assets the service generates are ready to edit using digital content creation tools, and available in a variety of popular file formats. Their clean geometry and layout gives artists an advanced starting point for adding their own flair.</p>
<div class="mceTemp"></div>
<figure id="attachment_73330" aria-describedby="caption-attachment-73330" style="width: 1080px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="wp-image-73330 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/chairmesh.gif" alt="" width="1080" height="608" /><figcaption id="caption-attachment-73330" class="wp-caption-text">An example of a 3D mesh from Shutterstock Generative 3D.</figcaption></figure>
<p>The AI model first delivers a preview of a single asset in as little as 10 seconds. If users like it, the preview can be turned into a higher-quality 3D asset, complete with physically based rendering materials like concrete, wood or leather.</p>
<p>At this year’s <a target="_blank" href="https://s2024.siggraph.org/">SIGGRAPH</a> computer graphics conference, designers will see just how fast they can make their ideas come to life.</p>
<p>Shutterstock will demo a workflow in Blender that lets artists generate objects directly within their 3D environment. In the Shutterstock booth at SIGGRAPH, HP will show 3D prints and physical prototypes of the kinds of assets attendees can design on the show floor using Generative 3D.</p>
<p>Shutterstock is also working with global marketing and communications services company <a target="_blank" href="https://www.nvidia.com/en-us/industries/media-and-entertainment/wpp/">WPP</a> to bring ideas to life with Edify 3D generation for virtual production (see video below).</p>
<p><iframe loading="lazy" title="NVIDIA Shutterstock Gen3D" src="https://player.vimeo.com/video/983328464?h=2a6e6255af&amp;dnt=1&amp;app_id=122963" width="500" height="281" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write"></iframe></p>
<p>Explore Generative 3D by Shutterstock on the company’s <a target="_blank" href="https://www.shutterstock.com/discover/generative-ai-3d">website</a>, or test-drive the application programming interface (API) at <a target="_blank" href="http://build.nvidia.com/shutterstock/edify-shutterstock-3d-txt23d-2pt7b">build.nvidia.com/</a>.</p>
<h2><b>Virtual Lighting Gets Real</b></h2>
<p>Lighting a virtual scene with accurate reflections can be a complicated task. Creatives need to operate expensive 360-degree camera rigs and go on set to create backgrounds from scratch, or search vast libraries for something that approximates what they want.</p>
<p>With Shutterstock’s Generative 3D service, users can now simply describe the exact environment they need in text or with an image, and out comes a high-dynamic-range panoramic image, aka 360 HDRi, in brilliant 16K resolution. (See video below.)</p>
<div style="width: 960px;" class="wp-video"><video class="wp-video-shortcode" id="video-73113-2" width="960" height="540" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/3_HDRi_SD_30fps_fast.mp4?_=2" /><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/3_HDRi_SD_30fps_fast.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/07/3_HDRi_SD_30fps_fast.mp4</a></video></div>
<p>Want that beautiful new sports car shown in a desert, a tropical beach or maybe on a winding mountain road? With generative AI, designers can shift gears fast.</p>
<p>Three companies plan to integrate Shutterstock’s 360 HDRi APIs directly into their workflows — WPP, CGI studio Katana and Dassault Systèmes, developer of the 3DEXCITE applications for creating high-end visualizations and 3D content for virtual worlds.</p>
<figure id="attachment_73327" aria-describedby="caption-attachment-73327" style="width: 1400px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="size-full wp-image-73327" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/genaibygettyimages.gif" alt="" width="1400" height="700" /><figcaption id="caption-attachment-73327" class="wp-caption-text">Examples from Generative AI by Getty Images.</figcaption></figure>
<h2><b>Great Images Get a Custom Fit</b></h2>
<p>Generative AI by Getty Images has upgraded to a more powerful Edify AI model with a portfolio of new features that let artists control image composition and style.</p>
<p>Want a red beach ball floating above that perfect shot of a coral reef in Fiji? Getty Images’ service can get it done in a snap.</p>
<p>The new model is twice as fast, boosts image quality and prompt accuracy, and lets users control camera settings like the depth of field or focal length of a shot. Users can generate four images in about six seconds and scale them up to 4K resolution.</p>
<figure id="attachment_73324" aria-describedby="caption-attachment-73324" style="width: 1400px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="size-full wp-image-73324" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls.jpg" alt="An example of the camera controls in Generative AI by Getty Images." width="1400" height="700" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls.jpg 1400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-1280x640.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption id="caption-attachment-73324" class="wp-caption-text">An example of the camera controls in Generative AI by Getty Images.</figcaption></figure>
<p>In addition, the commercially safe <a href="https://blogs.nvidia.com/blog/what-are-foundation-models/">foundational model</a> now serves as the basis for a fine-tuning capability that lets companies customize the AI with their own data. That lets them generate images tailored to the creative style of their specific brands.</p>
<p>New controls in the service support the use of a sketch or depth map to guide the composition or structure of an image.</p>
<p>Creatives at Omnicom, a global leader in marketing and sales solutions, are using Getty Images’ service to streamline advertising workflows and safely create on-brand content. The collaboration with Getty Images is part of Omnicom’s strategy to infuse generative AI into every facet of its business, helping teams move from ideas to outcomes faster.</p>
<p>Generative AI by Getty Images is available through the <a target="_blank" href="https://www.gettyimages.com/">Getty Images </a>and <a target="_blank" href="https://www.istockphoto.com/">iStock</a> websites, and <a target="_blank" href="https://www.gettyimages.com/enterprise/contact-sales?form=GI_NVIDIAStrategicParnership">via an API</a>.</p>
<p>For more about NVIDIA’s offerings, read about the <a target="_blank" href="https://www.nvidia.com/en-us/gpu-cloud/picasso/">AI foundry</a> for visual generative AI built on <a target="_blank" href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a>, and try it on <a target="_blank" href="https://build.nvidia.com/explore/visual-design">ai.nvidia.com</a>.</p>
<p>To get the big picture, listen to NVIDIA founder and CEO Jensen Huang in two <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">fireside chats</a> at SIGGRAPH.</p>
<p><i>See </i><a target="_blank" href="https://www.nvidia.com/en-eu/about-nvidia/terms-of-service/"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2024/07/3_HDRi_SD_30fps_fast.mp4" length="9811620" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/Key-visual-of-Shutterstock-Getty-Images-Gen-AI-servcies.jpg"
			type="image/jpeg"
			width="1920"
			height="1028"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/Key-visual-of-Shutterstock-Getty-Images-Gen-AI-servcies-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[For Your Edification: Shutterstock Releases Generative 3D, Getty Images Upgrades Service Powered by NVIDIA]]></media:title>
			<media:description type="html">Image of generative AI services from Shutterstock and Getty Images</media:description>
			</media:content>
			</item>
		<item>
		<title>Unleash the Dragonborn: ‘Elder Scrolls V: Skyrim Special Edition’ Joins GeForce NOW</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-elder-scrolls-skyrim/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 25 Jul 2024 13:00:02 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73239</guid>

					<description><![CDATA[“Hey, you. You’re finally awake.” It’s the summer of Elder Scrolls — whether a seasoned Dragonborn or a new adventurer, dive into the legendary world of Tamriel this GFN Thursday as The Elder Scrolls V: Skyrim Special Edition joins the cloud. Epic adventures await, along with nine new games joining the GeForce NOW library this	<a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-elder-scrolls-skyrim/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>“Hey, you. You’re finally awake.”</p>
<p>It’s the summer of <i>Elder Scrolls</i> — whether a seasoned Dragonborn or a new adventurer, dive into the legendary world of Tamriel this GFN Thursday as <i>The Elder Scrolls V: Skyrim Special Edition</i> joins the cloud.</p>
<p>Epic adventures await, along with nine new games joining the <a target="_blank" href="https://www.nvidia.com/en-us/geforce-now/games/">GeForce NOW library</a> this week.</p>
<p>Plus make sure to catch the <a href="https://blogs.nvidia.com/blog/geforce-now-thursday-summer-sale-2024">GeForce NOW Summer Sale</a> for 50% off new Ultimate and Priority memberships.</p>
<h2><b>Unleash the Dragonborn</b></h2>
<figure id="attachment_73243" aria-describedby="caption-attachment-73243" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-73243" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-672x378.jpg" alt="Skyrim on GeForce NOW" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73243" class="wp-caption-text"><em>Taking an arrow to the knee won’t stop gamers from questing in the cloud.</em></figcaption></figure>
<p>Experience the legendary adventures, breathtaking landscapes and immersive storytelling of the iconic role-playing game <i>The Elder Scrolls V: Skyrim Special Edition</i> from Bethesda Game Studios — now accessible on any device from the cloud. Become the Dragonborn and defeat Alduin the World-Eater, a dragon prophesied to destroy the world.<i> </i></p>
<p>Explore a vast landscape, complete quests and improve skills to develop characters in the open world of <i>Skyrim</i>. The <i>Special Edition</i> includes add-ons with all-new features, including remastered art and effects. It also brings the adventure of Bethesda Game Studios creations, including new quests, environments, characters, dialogue, armor and weapons.</p>
<p>Get ready to embark on unforgettable quests, battle fearsome foes and uncover the rich lore of the <i>Elder Scrolls</i> universe, all with the power and convenience of <a target="_blank" href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a>. “Fus Ro Dah” with an <a target="_blank" href="https://www.nvidia.com/en-us/geforce-now/memberships/">Ultimate membership</a> to stream at up to 4K resolution and 120 frames per second with up to eight-hour gaming sessions for the ultimate immersive experience throughout the realms of Tamriel.</p>
<h2><b>All Hands on Deck</b></h2>
<figure id="attachment_73249" aria-describedby="caption-attachment-73249" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-73249" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-672x336.jpg" alt="World of Warships members rewards on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73249" class="wp-caption-text"><em>Get those sea legs ready for a reward.</em></figcaption></figure>
<p>Wargaming is bringing back an in-game event exclusively for GeForce NOW members this week.</p>
<p>Through Tuesday, July 30, members who complete the quest while streaming <i>World of Warships</i> can earn up to five GeForce NOW one-day Priority codes — one for each day of the challenge. Aspiring admirals can learn more on the <i>World of Warships</i> <a target="_blank" href="https://worldofwarships.com/en/news/">blog</a> and <a target="_blank" href="https://x.com/WorldofWarships">social channels</a>.</p>
<h2><b>Shiny and New</b></h2>
<figure id="attachment_73246" aria-describedby="caption-attachment-73246" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-73246" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-672x336.jpg" alt="Conscript on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73246" class="wp-caption-text"><em>Rendezvous with death.</em></figcaption></figure>
<p>Take on classic survival horror in <i>CONSCRIPT</i> from Jordan Mochi and Team17. Inspired by legendary games in the genre, the game is set in 1916 during the Great War. <i>CONSCRIPT </i>blends all the punishing mechanics of older horror games into a cohesive, tense and unique experience. Play as a French soldier searching for his missing-in-action brother during the Battle of Verdun. Search through twisted trenches, navigate overrun forts and cross no-man’s-land to find him.</p>
<p>Here’s the full list of new games this week:</p>
<ul>
<li><i>Cataclismo </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1422440?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 22</li>
<li><i>CONSCRIPT </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1286990?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 23)</li>
<li><i>F1 Manager 2024 </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2591280?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 23)</li>
<li><i>EARTH DEFENSE FORCE 6 </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2291060?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 25)</li>
<li><i>The Elder Scrolls V: Skyrim </i>(<a target="_blank" href="https://store.steampowered.com/app/72850?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>The Elder Scrolls V: Skyrim Special Edition</i> (<a target="_blank" href="https://store.steampowered.com/app/489830?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a target="_blank" href="https://www.epicgames.com/store/p/skyrim?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a> and <a target="_blank" href="https://www.xbox.com/games/store/the-elder-scrolls-v-skyrim-special-edition-pc/9p03jgq4s1gc?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Gang Beasts </i>(<a target="_blank" href="https://store.steampowered.com/app/285900?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a target="_blank" href="https://www.xbox.com/games/store/gang-beasts/BPQZT43FWD49?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Kingdoms and Castles </i>(<a target="_blank" href="https://store.steampowered.com/app/569480?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>The Settlers: New Allies </i>(<a target="_blank" href="https://store.steampowered.com/app/2750080?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
</ul>
<p>What are you planning to play this weekend? Let us know on <a target="_blank" href="https://www.twitter.com/nvidiagfn">X</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="und" dir="ltr"><img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f4c8.png" alt="📈" class="wp-smiley" style="height: 1em; max-height: 1em;" /><img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f199.png" alt="🆙" class="wp-smiley" style="height: 1em; max-height: 1em;" /> ʏᴏᴜʀ ᴄʟᴏᴜᴅ ɢᴀᴍɪɴɢ ꜱᴋɪʟʟ ɪɴᴄʀᴇᴀꜱᴇᴅ</p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f329.png" alt="🌩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a target="_blank" href="https://twitter.com/NVIDIAGFN/status/1816141289769033834?ref_src=twsrc%5Etfw">July 24, 2024</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<p>&nbsp;</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/gfn-thursday-7-11-nv-blog-1280x680-no-copy-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/gfn-thursday-7-11-nv-blog-1280x680-no-copy-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Unleash the Dragonborn: ‘Elder Scrolls V: Skyrim Special Edition’ Joins GeForce NOW]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Demystifying AI-Assisted Artistry With Adobe Apps Using NVIDIA RTX</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-adobe-firefly-creative-cloud-rtx/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Wed, 24 Jul 2024 13:00:56 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[Rendering]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73159</guid>

					<description><![CDATA[Adobe Creative Cloud applications, which tap NVIDIA RTX GPUs, are designed to enhance the creativity of users, empowering them to work faster and focus on their craft.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><i>Editor’s note: This post is part of the </i><a href="https://blogs.nvidia.com/blog/tag/ai-decoded/"><i>AI Decoded series</i></a><i>, which demystifies AI by making the technology more accessible, and showcases new hardware, software, tools and accelerations for RTX PC users.</i></p>
<p>Adobe Creative Cloud applications, which tap <a target="_blank" href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX GPUs</a>, are designed to enhance the creativity of users, empowering them to work faster and focus on their craft.</p>
<p>These tools seamlessly integrate into existing creator workflows, enabling greater productivity and delivering power and precision.</p>
<h2><b>Look to the Light</b></h2>
<p><a target="_blank" href="https://helpx.adobe.com/creative-cloud/generative-ai-overview.html#generative-ai-with-adobe-creative-cloud">Generative AI</a> creates new data in forms such as images or text by learning from existing data. It effectively visualizes and generates content to match what a user describes and helps open up fresh avenues for creativity.</p>
<p>Adobe Firefly is Adobe’s family of creative generative AI models that offer new ways to ideate and create while assisting creative workflows using generative AI. They’re designed to be safe for commercial use and were trained, using NVIDIA GPUs, on licensed content, like Adobe Stock Images, and public domain content where copyright has expired.</p>
<p><iframe loading="lazy" title="Adobe Firefly: A New Era of Creativity | Adobe" width="500" height="281" src="https://www.youtube.com/embed/f_2KsIwoV4Q?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>Firefly features are integrated in Adobe’s most popular creative apps.</p>
<p><iframe loading="lazy" title="Bring Illustrations to Life with Adobe AI Effects powered by NVIDIA GPUs on PC &amp; in the Cloud" width="500" height="281" src="https://www.youtube.com/embed/7ELrpfVY4P4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>Adobe Photoshop features the <a target="_blank" href="https://www.adobe.com/products/photoshop/generative-fill.html">Generative Fill tool,</a> which uses simple description prompts to easily add content from images. With the latest Reference Image feature currently in beta, users can also upload a sample image to get image results closer to their desired output.</p>
<figure id="attachment_73163" aria-describedby="caption-attachment-73163" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees.png"><img loading="lazy" decoding="async" class="wp-image-73163 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-672x259.png" alt="" width="672" height="259" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-672x259.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-400x154.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-768x296.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-842x325.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-406x157.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-188x73.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73163" class="wp-caption-text">Use Generative Fill to add content and Reference Image to refine it.</figcaption></figure>
<p>Generative Expand allows artists to extend the border of their image with the Crop tool, filling in bigger canvases with new content that automatically blends in with the existing image.</p>
<figure id="attachment_73166" aria-describedby="caption-attachment-73166" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill.png"><img loading="lazy" decoding="async" class="wp-image-73166 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73166" class="wp-caption-text">Bigger canvas? Not a problem.</figcaption></figure>
<p>RTX-accelerated Neural Filters, such as Photo Restoration, enable complex adjustments such as colorizing black-and-white photos and performing style transfers using AI. The Smart Portrait filter, which allows non-destructive editing with filters, is based on work from <a target="_blank" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>.</p>
<p><iframe loading="lazy" title="Accelerating AI in Photoshop Neural Filters with RTX A2000" width="500" height="281" src="https://www.youtube.com/embed/WwCe9Woy1jw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>The brand-new Generative Shape Fill (beta) in Adobe Illustrator, powered by the latest Adobe Firefly Vector Model, allows users to accelerate design workflows by quickly filling shapes with detail and color in their own styles. With Generative Shape Fill, designers can easily match the style and color of their own artwork to create a wide variety of editable and scalable vector graphic options.</p>
<figure id="attachment_73170" aria-describedby="caption-attachment-73170" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe.jpg"><img loading="lazy" decoding="async" class="wp-image-73170 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-672x294.jpg" alt="" width="672" height="294" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-672x294.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-400x175.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-768x336.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-842x369.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-406x178.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-188x82.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe.jpg 1000w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73170" class="wp-caption-text">Generative AI.</figcaption></figure>
<p><a target="_blank" href="https://www.adobe.com/products/illustrator/generative-recolor.html">Adobe Illustrator’s Generative Recolor</a> feature lets creators type in a text prompt to explore custom color palettes and themes for their vector artwork in seconds.</p>
<figure id="attachment_73173" aria-describedby="caption-attachment-73173" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color.png"><img loading="lazy" decoding="async" class="wp-image-73173 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-672x327.png" alt="" width="672" height="327" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-672x327.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-400x195.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-768x374.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-842x410.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-406x198.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-188x92.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color.png 1109w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73173" class="wp-caption-text">Color us impressed.</figcaption></figure>
<p>NVIDIA will continue working with Adobe to support advanced generative AI models, with a focus on deep integration into the apps the world’s leading creators use.</p>
<h2><b>Making Moves on Video</b></h2>
<p>Adobe Premiere Pro is one of the most popular and powerful video editing solutions.</p>
<p>Its Enhance Speech tool, accelerated by RTX, uses AI to remove unwanted noise and improve the quality of dialogue clips so they sound professionally recorded. It’s up to 4.5x faster on RTX PCs.</p>
<figure id="attachment_73176" aria-describedby="caption-attachment-73176" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/enhance-speech.png"><img loading="lazy" decoding="async" class="wp-image-73176 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/enhance-speech.png" alt="" width="672" height="445" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/enhance-speech.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/enhance-speech-400x265.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/enhance-speech-325x215.png 325w, https://blogs.nvidia.com/wp-content/uploads/2024/07/enhance-speech-151x100.png 151w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73176" class="wp-caption-text">Adobe Premiere Pro’s AI-powered Enhance Speech tool removes unwanted noise and improves dialogue quality.</figcaption></figure>
<p><a target="_blank" href="https://helpx.adobe.com/premiere-pro/using/auto-reframe.html">Auto Reframe</a>, another Adobe Premiere feature, uses GPU acceleration to identify and track the most relevant elements in a video, and intelligently reframes video content for different aspect ratios. Scene Edit Detection automatically finds the original edit points in a video, a necessary step before the video editing stage begins.</p>
<p><iframe loading="lazy" title="How to detect a cut with Scene Edit Detection in Premiere Pro" width="500" height="281" src="https://www.youtube.com/embed/DYor3a5DKBA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Visual Effects</b></h2>
<p>Separating a foreground object from a background is a crucial step in many visual effects and compositing workflows.</p>
<p>Adobe After Effects has a new feature that uses a matte to isolate an object, enabling capabilities including background replacement and the selective application of effects to the foreground.</p>
<p>Using the <a target="_blank" href="https://helpx.adobe.com/after-effects/using/roto-brush-refine-matte.html">Roto Brush</a> tool, artists can draw strokes on representative areas of the foreground and background elements. After Effects uses that information to create a segmentation boundary between the foreground and background elements, delivering cleaner cutouts with fewer clicks.</p>
<p><iframe loading="lazy" title="How to use Next-Gen Rotobrush 3 in Adobe After Effects" width="500" height="281" src="https://www.youtube.com/embed/VJ3cNt0yEhQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Creating 3D Product Shots</b></h2>
<p>The Substance 3D Collection is Adobe’s solution for 3D material authoring, texturing and rendering, enabling users to rapidly create stunningly photorealistic 3D content, including models, materials and lighting.</p>
<p>Visualizing products and designs in the context of a space is compelling, but it can be time-consuming to find the right environment for the objects to live in. Substance 3D Stager’s<a target="_blank" href="https://helpx.adobe.com/substance-3d-stager/features/generative-background.html"> Generative Background</a> feature, powered by Adobe Firefly, solves this issue by letting artists quickly explore generated backgrounds to composite 3D models.</p>
<p>Once an environment is selected, Stager can automatically match the perspective and lighting to the generated background.</p>
<p><iframe loading="lazy" title="Generate Background Images Fast in Substance 3D Stager &amp; Firefly | Adobe Substance 3D" width="500" height="281" src="https://www.youtube.com/embed/woUUPrSiVPc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Material Authoring With AI</b></h2>
<p>Adobe Substance 3D Sampler, also part of the Substance 3D Collection, is designed to transform images of surfaces and objects into photorealistic physically based rendering (PBR) materials, 3D models and high-dynamic range environment lights. With the recent introduction of<a target="_blank" href="https://helpx.adobe.com/substance-3d-sampler/release-notes/version-4-4---substance-3d-sampler.html"> new generative workflows powered by Adobe Firefly</a>, Sampler is making it easier than ever for artists to explore variations when creating materials for everything from product visualization projects to the latest AAA games.</p>
<p>Sampler’s Text-to-Texture feature allows users to generate tiled images from detailed text prompts. These generated images can then be edited and transformed into photorealistic PBR materials using the machine learning-powered Image-to-Material feature or any Sampler filter.</p>
<p><iframe loading="lazy" title="How to Use Generative Tools in Sampler for Fast Iterations | Adobe Substance 3D" width="500" height="281" src="https://www.youtube.com/embed/YD3tPlO5h3g?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>Image-to-Texture similarly enables the creation of tiled textures from reference images, providing an alternate way to prompt and generate variations from existing visual content.</p>
<figure id="attachment_73179" aria-describedby="caption-attachment-73179" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture.png"><img loading="lazy" decoding="async" class="wp-image-73179 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-801x450.png 801w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-383x215.png 383w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture.png 975w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73179" class="wp-caption-text">Adobe 3D Sampler’s Image-to-Texture feature.</figcaption></figure>
<p>Sampler’s Text-to-Pattern feature uses text prompts to generate tiling patterns, which can be used as base colors or inputs for various filters, such as the Cloth Weave filter for creating original fabric materials.</p>
<p>All of these generative AI features in the Substance 3D Collection, supercharged with RTX GPUs, are designed to help 3D creators ideate and create faster.</p>
<h2><b>Photo-tastic Features</b></h2>
<p>Adobe Lightroom’s AI-powered <a target="_blank" href="https://helpx.adobe.com/lightroom-cc/using/enhance-details.html">Raw Details</a> feature produces crisp detail and more accurate renditions of edges, improves color rendering and reduces artifacts, enhancing the image without changing its original resolution. This feature is handy for large displays and prints, where fine details are visible.</p>
<figure id="attachment_73182" aria-describedby="caption-attachment-73182" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance.png"><img loading="lazy" decoding="async" class="wp-image-73182 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-672x412.png" alt="" width="672" height="412" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-672x412.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-400x245.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-768x471.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-734x450.png 734w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-351x215.png 351w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-163x100.png 163w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance.png 926w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73182" class="wp-caption-text">Enhance, enhance, enhance.</figcaption></figure>
<p><a target="_blank" href="https://helpx.adobe.com/lightroom-cc/using/enhance-details.html">Super Resolution</a> helps create an enhanced image with similar results as Raw Details but with 2x the linear resolution. This means that the enhanced image will have 2x the width and height of the original image — or 4x the total pixel count. This is especially useful for increasing the resolution of cropped imagery.</p>
<p><iframe loading="lazy" title="NVIDIA RTX Accelerates AI Super Resolution in Adobe Photoshop" width="500" height="281" src="https://www.youtube.com/embed/UURFgE43PUg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>For faster editing, AI-powered, RTX-accelerated masking tools like Select Subject, which isolates people from an image, and Select Sky, which captures skies, enable users to create complex masks with the click of a button.</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-73188" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/magic.mask_.gif" alt="" width="640" height="360" /></p>
<p>Visit Adobe’s <a target="_blank" href="https://www.adobe.com/ai/overview/features.html">AI features page</a> for a complete list of AI features using RTX.</p>
<p><i>Looking for more AI-powered content creation apps? Consider </i><a target="_blank" href="https://www.nvidia.com/en-us/geforce/broadcasting/broadcast-app/"><i>NVIDIA Broadcast</i></a><i>, which transforms any room into a home studio, free for RTX GPU owners. </i></p>
<p><i>Generative AI is transforming gaming, videoconferencing and interactive experiences of all kinds. Make sense of what’s new and what’s next by subscribing to the </i><a target="_blank" href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><i>AI Decoded newsletter</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/adobe-apps-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/adobe-apps-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Demystifying AI-Assisted Artistry With Adobe Apps Using NVIDIA RTX]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>How NVIDIA AI Foundry Lets Enterprises Forge Custom Generative AI Models</title>
		<link>https://blogs.nvidia.com/blog/ai-foundry-enterprise-generative-ai/</link>
		
		<dc:creator><![CDATA[Kari Briski]]></dc:creator>
		<pubDate>Tue, 23 Jul 2024 15:15:59 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Software]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[DGX Cloud]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73148</guid>

					<description><![CDATA[Businesses seeking to harness the power of AI need customized models tailored to their specific industry needs. NVIDIA AI Foundry is a service that enables enterprises to use data, accelerated computing and software tools to create and deploy custom models that can supercharge their generative AI initiatives. Just as TSMC manufactures chips designed by other	<a class="read-more" href="https://blogs.nvidia.com/blog/ai-foundry-enterprise-generative-ai/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Businesses seeking to harness the power of AI need customized models tailored to their specific industry needs.</p>
<p><a target="_blank" href="https://nvidianews.nvidia.com/news/nvidia-ai-foundry-custom-llama-generative-models">NVIDIA AI Foundry</a> is a service that enables enterprises to use data, accelerated computing and software tools to create and deploy custom models that can supercharge their generative AI initiatives.</p>
<p>Just as TSMC manufactures chips designed by other companies, NVIDIA AI Foundry provides the infrastructure and tools for other companies to develop and <a target="_blank" href="https://developer.nvidia.com/blog/customize-generative-ai-models-for-enterprise-applications-with-llama-3-1/">customize AI models</a> — using DGX Cloud, foundation models, NVIDIA NeMo software, NVIDIA expertise, as well as ecosystem tools and support.</p>
<p>The key difference is the product: TSMC produces physical semiconductor chips, while NVIDIA AI Foundry helps create custom models. Both enable innovation and connect to a vast ecosystem of tools and partners.</p>
<p>Enterprises can use AI Foundry to customize NVIDIA and open community models, including the new <a target="_blank" href="https://ai.meta.com/blog/meta-llama-3-1/">Llama 3.1</a> collection, as well as <a href="https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/">NVIDIA Nemotron</a>, CodeGemma by Google DeepMind, CodeLlama, Gemma by Google DeepMind, Mistral, Mixtral, Phi-3, StarCoder2 and others.</p>
<h2><b>Industry Pioneers Drive AI Innovation</b></h2>
<p>Industry leaders <a target="_blank" href="https://www.amdocs.com/insights/press-release/amdocs-unveils-generative-ai-milestones-bringing-enhanced-efficiencies">Amdocs</a>, Capital One, Getty Images, KT, Hyundai Motor Company, <a target="_blank" href="https://nvidianews.nvidia.com/news/sap-nvidia-generative-ai-enterprise-applications">SAP</a>, ServiceNow and Snowflake are among the first using NVIDIA AI Foundry. These pioneers are setting the stage for a new era of AI-driven innovation in enterprise software, technology, communications and media.</p>
<p>“Organizations deploying AI can gain a competitive edge with custom models that incorporate industry and business knowledge,” said Jeremy Barnes, vice president of AI Product at ServiceNow. “ServiceNow is using NVIDIA AI Foundry to fine-tune and deploy models that can integrate easily within customers’ existing workflows.”</p>
<h2><b>The Pillars of NVIDIA AI Foundry </b></h2>
<p>NVIDIA AI Foundry is supported by the key pillars of foundation models, enterprise software, accelerated computing, expert support and a broad partner ecosystem.</p>
<p>Its software includes AI foundation models from NVIDIA and the AI community as well as the complete <a target="_blank" href="https://www.nvidia.com/en-us/ai-data-science/products/nemo/">NVIDIA NeMo</a> software platform for fast-tracking model development.</p>
<p>The computing muscle of NVIDIA AI Foundry is <a target="_blank" href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a>, a network of accelerated compute resources co-engineered with the world’s leading public clouds — Amazon Web Services, Google Cloud and Oracle Cloud Infrastructure. With DGX Cloud, AI Foundry customers can develop and fine-tune custom generative AI applications with unprecedented ease and efficiency, and scale their AI initiatives as needed without significant upfront investments in hardware. This flexibility is crucial for businesses looking to stay agile in a rapidly changing market.</p>
<p>If an NVIDIA AI Foundry customer needs assistance, NVIDIA AI Enterprise experts are on hand to help. NVIDIA experts can walk customers through each of the steps required to build, fine-tune and deploy their models with proprietary data, ensuring the models tightly align with their business requirements.</p>
<p>NVIDIA AI Foundry customers have access to a global ecosystem of partners that can provide a full range of support. Accenture, Deloitte, Infosys, Tata Consultancy Services and Wipro are among the NVIDIA partners that offer AI Foundry consulting services that encompass design, implementation and management of AI-driven digital transformation projects. <a target="_blank" href="https://newsroom.accenture.com/news/2024/accenture-pioneers-custom-llama-llm-models-with-nvidia-ai-foundry">Accenture</a> is first to offer its own AI Foundry-based offering for custom model development, the Accenture AI Refinery framework.</p>
<p>Additionally, service delivery partners such as Data Monsters, Quantiphi, Slalom and SoftServe help enterprises navigate the complexities of integrating AI into their existing IT landscapes, ensuring that AI applications are scalable, secure and aligned with business objectives.</p>
<p>Customers can develop NVIDIA AI Foundry models for production using AIOps and MLOps platforms from NVIDIA partners, including ActiveFence, AutoAlign, Cleanlab, DataDog, Dataiku, Dataloop, DataRobot, Deepchecks, Domino Data Lab, <a target="_blank" href="https://www.fiddler.ai/blog/steer-and-observe-llms-with-nvidia-nemo-guardrails-and-fiddler">Fiddler AI</a>, Giskard, New Relic, Scale, Tumeryk and Weights &amp; Biases.</p>
<p>Customers can output their AI Foundry models as <a target="_blank" href="http://v">NVIDIA NIM</a> inference microservices — which include the custom model, optimized engines and a standard API — to run on their preferred accelerated infrastructure.</p>
<p>Inferencing solutions like <a target="_blank" href="https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/">NVIDIA TensorRT-LLM</a> deliver improved efficiency for Llama 3.1 models to minimize latency and maximize throughput. This enables enterprises to generate tokens faster while reducing total cost of running the models in production. Enterprise-grade support and security is provided by the <a target="_blank" href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> software suite.</p>
<figure id="attachment_73149" aria-describedby="caption-attachment-73149" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/Llama-3.1-Perf.png"><img loading="lazy" decoding="async" class="size-large wp-image-73149" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/Llama-3.1-Perf-672x362.png" alt="" width="672" height="362" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/Llama-3.1-Perf-672x362.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Llama-3.1-Perf-400x215.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Llama-3.1-Perf-768x414.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Llama-3.1-Perf-836x450.png 836w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Llama-3.1-Perf-399x215.png 399w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Llama-3.1-Perf-186x100.png 186w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Llama-3.1-Perf.png 1140w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73149" class="wp-caption-text">NVIDIA NIM and TensorRT-LLM minimize inference latency and maximize throughput for Llama 3.1 models to generate tokens faster.</figcaption></figure>
<p>The broad range of deployment options includes <a target="_blank" href="https://www.nvidia.com/en-us/data-center/products/certified-systems/">NVIDIA-Certified Systems</a> from global server manufacturing partners including Cisco, Dell Technologies, Hewlett Packard Enterprise, Lenovo and Supermicro, as well as cloud instances from Amazon Web Services, Google Cloud and Oracle Cloud Infrastructure.</p>
<p>Additionally, <a target="_blank" href="https://www.together.ai/blog/nvidia-ai-foundry-partnership">Together AI</a>, a leading AI acceleration cloud, today announced it will enable its ecosystem of over 100,000 developers and enterprises to use its NVIDIA GPU-accelerated inference stack to deploy Llama 3.1 endpoints and other open models on DGX Cloud.</p>
<p>“Every enterprise running generative AI applications wants a faster user experience, with greater efficiency and lower cost,” said Vipul Ved Prakash, founder and CEO of Together AI. “Now, developers and enterprises using the Together Inference Engine can maximize performance, scalability and security on NVIDIA DGX Cloud.”</p>
<h2><b>NVIDIA NeMo Speeds and Simplifies Custom Model Development</b></h2>
<p>With <a target="_blank" href="https://www.nvidia.com/en-us/ai-data-science/products/nemo/">NVIDIA NeMo</a> integrated into AI Foundry, developers have at their fingertips the tools needed to curate data, customize foundation models and evaluate performance. NeMo technologies include:</p>
<ul>
<li><b>NeMo Curator</b> is a GPU-accelerated data-curation library that improves generative AI model performance by preparing large-scale, high-quality datasets for pretraining and fine-tuning.</li>
<li><b>NeMo Customizer</b> is a high-performance, scalable microservice that simplifies fine-tuning and alignment of LLMs for domain-specific use cases.</li>
<li><b>NeMo Evaluator</b> provides automatic assessment of generative AI models across academic and custom benchmarks on any accelerated cloud or data center.</li>
<li><b>NeMo Guardrails</b> orchestrates dialog management, supporting accuracy, appropriateness and security in smart applications with large language models to provide safeguards for generative AI applications.</li>
</ul>
<p>Using the NeMo platform in NVIDIA AI Foundry, businesses can create custom AI models that are precisely tailored to their needs. This customization allows for better alignment with strategic objectives, improved accuracy in decision-making and enhanced operational efficiency. For instance, companies can develop models that understand industry-specific jargon, comply with regulatory requirements and integrate seamlessly with existing workflows.</p>
<p>“As a next step of our partnership, SAP plans to use NVIDIA’s NeMo platform to help businesses to accelerate AI-driven productivity powered by SAP Business AI,” said Philipp Herzig, chief AI officer at SAP.</p>
<p>Enterprises can deploy their custom AI models in production with <a href="https://blogs.nvidia.com/blog/nemo-retriever-microservices">NVIDIA NeMo Retriever NIM</a> inference microservices. These help developers fetch proprietary data to generate knowledgeable responses for their AI applications with <a target="_blank" href="https://developer.nvidia.com/blog/develop-production-grade-text-retrieval-pipelines-for-rag-with-nvidia-nemo-retriever">retrieval-augmented generation </a>(<a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">RAG</a>).</p>
<p>“Safe, trustworthy AI is a non-negotiable for enterprises harnessing generative AI, with retrieval accuracy directly impacting the relevance and quality of generated responses in RAG systems,” said Baris Gultekin, Head of AI, Snowflake. &#8220;Snowflake Cortex AI leverages NeMo Retriever, a component of NVIDIA AI Foundry, to further provide enterprises with easy, efficient, and trusted answers using their custom data.”</p>
<h2><b>Custom Models Drive Competitive Advantage</b></h2>
<p>One of the key advantages of NVIDIA AI Foundry is its ability to address the unique challenges faced by enterprises in adopting AI. Generic AI models can fall short of meeting specific business needs and data security requirements. Custom AI models, on the other hand, offer superior flexibility, adaptability and performance, making them ideal for enterprises seeking to gain a competitive edge.</p>
<p><i>Learn more about how </i><a target="_blank" href="https://www.nvidia.com/en-us/ai/foundry/"><i>NVIDIA AI Foundry</i></a><i> allows enterprises to boost productivity and innovation.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/ai-foundry.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/ai-foundry-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[How NVIDIA AI Foundry Lets Enterprises Forge Custom Generative AI Models]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>AI, Go Fetch! New NVIDIA NeMo Retriever Microservices Boost LLM Accuracy and Throughput</title>
		<link>https://blogs.nvidia.com/blog/nemo-retriever-microservices/</link>
		
		<dc:creator><![CDATA[Erik Pounds]]></dc:creator>
		<pubDate>Tue, 23 Jul 2024 15:15:16 +0000</pubDate>
				<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Data Science]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Riva]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73134</guid>

					<description><![CDATA[Generative AI applications have little, or sometimes negative, value without accuracy — and accuracy is rooted in data. To help developers efficiently fetch the best proprietary data to generate knowledgeable responses for their AI applications, NVIDIA today announced four new NVIDIA NeMo Retriever NIM inference microservices. Combined with NVIDIA NIM inference microservices for the Llama	<a class="read-more" href="https://blogs.nvidia.com/blog/nemo-retriever-microservices/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-ai/">Generative AI</a> applications have little, or sometimes negative, value without accuracy — and accuracy is rooted in data.</p>
<p>To help developers efficiently fetch the best proprietary data to generate knowledgeable responses for their AI applications, NVIDIA today announced four new <a target="_blank" href="https://www.nvidia.com/en-us/ai-data-science/products/nemo/">NVIDIA NeMo Retriever NIM</a> inference microservices.</p>
<p>Combined with <a target="_blank" href="https://nvidianews.nvidia.com/news/nvidia-ai-foundry-custom-llama-generative-models">NVIDIA NIM inference microservices for the Llama 3.1</a> model collection, also announced today, <a target="_blank" href="https://developer.nvidia.com/blog/develop-production-grade-text-retrieval-pipelines-for-rag-with-nvidia-nemo-retriever">NeMo Retriever NIM microservices</a> enable enterprises to scale to <a target="_blank" href="https://developer.nvidia.com/blog/build-an-agentic-rag-pipeline-with-llama-3-1-and-nvidia-nemo-retriever-nims">agentic AI workflows</a> — where AI applications operate accurately with minimal intervention or supervision — while delivering the highest accuracy retrieval-augmented generation, or <a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">RAG</a>.</p>
<p>NeMo Retriever allows organizations to seamlessly connect custom models to diverse business data and deliver highly accurate responses for AI applications using RAG. In essence, the production-ready microservices enable highly accurate information retrieval for building highly accurate AI applications.</p>
<p>For example, NeMo Retriever can boost model accuracy and throughput for developers creating AI agents and customer service chatbots, analyzing security vulnerabilities or extracting insights from complex supply chain information.</p>
<p>NIM inference microservices enable high-performance, easy-to-use, enterprise-grade inferencing. And with NeMo Retriever NIM microservices, developers can benefit from all of this — superpowered by their data.</p>
<p>These new NeMo Retriever <a target="_blank" href="https://www.nvidia.com/en-eu/glossary/vector-database/#:~:text=What%20is%20an%20Embedding%20Model%3F">embedding</a> and reranking NIM microservices are now generally available:</p>
<ul>
<li style="font-weight: 300;" aria-level="1">NV-EmbedQA-E5-v5, a popular community base embedding model optimized for text question-answering retrieval</li>
<li style="font-weight: 300;" aria-level="1">NV-EmbedQA-Mistral7B-v2, a popular multilingual community base model fine-tuned for text embedding for high-accuracy question answering</li>
<li style="font-weight: 300;" aria-level="1">Snowflake-Arctic-Embed-L, an optimized community model, and</li>
<li style="font-weight: 300;" aria-level="1">NV-RerankQA-Mistral4B-v3, a popular community base model fine-tuned for text reranking for high-accuracy question answering.</li>
</ul>
<p>They join the collection of NIM microservices easily accessible through the <a target="_blank" href="https://build.nvidia.com/explore/retrieval">NVIDIA API catalog</a>.</p>
<h2><b>Embedding and Reranking Models</b></h2>
<p>NeMo Retriever NIM microservices comprise two model types — embedding and reranking — with open and commercial offerings that ensure transparency and reliability.</p>
<figure id="attachment_73135" aria-describedby="caption-attachment-73135" style="width: 823px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-full wp-image-73135" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-rag-example-pipeline.png" alt="A diagram showing a user prompt inquiring about a bill, retrieving the most accurate response. " width="823" height="413" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-rag-example-pipeline.png 823w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-rag-example-pipeline-400x201.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-rag-example-pipeline-672x337.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-rag-example-pipeline-768x385.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-rag-example-pipeline-406x204.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-rag-example-pipeline-188x94.png 188w" sizes="(max-width: 823px) 100vw, 823px" /><figcaption id="caption-attachment-73135" class="wp-caption-text">Example RAG pipeline using NVIDIA NIM microservices for Llama 3.1 and NeMo Retriever embedding and reranking NIM microservices for a customer service AI chatbot application.</figcaption></figure>
<p>An <a target="_blank" href="https://www.nvidia.com/en-us/glossary/vector-database/#:~:text=What%20is%20an%20Embedding%20Model%3F">embedding model</a> transforms diverse data — such as text, images, charts and video — into numerical vectors, stored in a vector database, while capturing their meaning and nuance. Embedding models are fast and computationally less expensive than traditional large language models, or LLMs.</p>
<p>A reranking model ingests data and a query, then scores the data according to its relevance to the query. Such models offer significant accuracy improvements while being computationally complex and slower than embedding models.</p>
<p>NeMo Retriever provides the best of both worlds. By casting a wide net of data to be retrieved with an embedding NIM, then using a reranking NIM to trim the results for relevancy, developers tapping NeMo Retriever can build a pipeline that ensures the most helpful, accurate results for their enterprise.</p>
<p>With NeMo Retriever, developers get access to state-of-the-art open, commercial models for building text Q&amp;A retrieval pipelines that provide the highest accuracy. When compared with alternate models, NeMo Retriever NIM microservices provided 30% fewer inaccurate answers for enterprise question answering.</p>
<figure id="attachment_73138" aria-describedby="caption-attachment-73138" style="width: 811px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-full wp-image-73138" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-embedding-reranking-comparison.png" alt="Bar chart showing lexical search (45%), alternative embedder (63%), compared with NeMo Retriever embedding NIM (73%) and NeMo Retriever embedding + reranking NIM microservices (75%)." width="811" height="321" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-embedding-reranking-comparison.png 811w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-embedding-reranking-comparison-400x158.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-embedding-reranking-comparison-672x266.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-embedding-reranking-comparison-768x304.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-embedding-reranking-comparison-406x161.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-embedding-reranking-comparison-188x74.png 188w" sizes="(max-width: 811px) 100vw, 811px" /><figcaption id="caption-attachment-73138" class="wp-caption-text">Comparison of NeMo Retriever embedding NIM and embedding plus reranking NIM microservices performance versus lexical search and an alternative embedder.</figcaption></figure>
<h2><b>Top Use Cases</b></h2>
<p>From RAG and AI agent solutions to data-driven analytics and more, NeMo Retriever powers a wide range of AI applications.</p>
<p>The microservices can be used to build <a target="_blank" href="https://www.nvidia.com/en-us/ai-data-science/ai-workflows/generative-ai-chatbots/">intelligent chatbots</a> that provide accurate, context-aware responses. They can help analyze vast amounts of data to <a target="_blank" href="https://www.nvidia.com/en-us/ai-data-science/ai-workflows/security-vulnerability-analysis/">identify security vulnerabilities</a>. They can assist in extracting insights from complex <a target="_blank" href="https://www.youtube.com/watch?v=a9O0JipIrb4">supply chain information</a>. And they can boost AI-enabled <a target="_blank" href="https://www.nvidia.com/en-us/ai-data-science/ai-workflows/retail-shopping-advisor/">retail shopping advisors</a> that offer natural, personalized shopping experiences, among other tasks.</p>
<p><a target="_blank" href="https://www.nvidia.com/en-us/ai-data-science/ai-workflows/">NVIDIA AI workflows</a> for these use cases provide an easy, supported starting point for developing generative AI-powered technologies.</p>
<p>Dozens of NVIDIA data platform partners are working with NeMo Retriever NIM microservices to boost their AI models’ accuracy and throughput.</p>
<p><a target="_blank" href="https://www.datastax.com/blog/datastax-ai-paas-integrated-with-nvidia-nemo-retriever">DataStax</a> has integrated NeMo Retriever embedding NIM microservices in its Astra DB and Hyper-Converged platforms, enabling the company to bring accurate, generative AI-enhanced RAG capabilities to customers with faster time to market.</p>
<p>Cohesity will integrate NVIDIA NeMo Retriever microservices with its AI product, Cohesity Gaia, to help customers put their data to work to power insightful, transformative generative AI applications through RAG.</p>
<p>Kinetica will use NVIDIA NeMo Retriever to develop LLM agents that can interact with complex networks in natural language to respond more quickly to outages or breaches — turning insights into immediate action.</p>
<p>NetApp is collaborating with NVIDIA to connect NeMo Retriever microservices to exabytes of data on its intelligent data infrastructure. Every NetApp ONTAP customer will be able to seamlessly “talk to their data” to access proprietary business insights without having to compromise the security or privacy of their data.</p>
<p>NVIDIA global system integrator partners including Accenture, Deloitte, Infosys, LTTS, Tata Consultancy Services, Tech Mahindra and Wipro, as well as service delivery partners Data Monsters, EXLService (Ireland) Limited, Latentview, Quantiphi, Slalom, SoftServe and Tredence, are developing services to help enterprises add NeMo Retriever NIM microservices into their AI pipelines.</p>
<h2><b>Use With Other NIM Microservices</b></h2>
<p>NeMo Retriever NIM microservices can be used with NVIDIA Riva NIM microservices, which  supercharge <a href="https://blogs.nvidia.com/blog/speech-ai-for-industries/">speech AI</a> applications across industries — enhancing customer service and enlivening digital humans.</p>
<p>New models that will soon be available as Riva NIM microservices include: FastPitch and HiFi-GAN for <a target="_blank" href="https://www.nvidia.com/en-us/glossary/text-to-speech/">text-to-speech</a> applications; Megatron for multilingual neural machine translation; and the record-breaking <a target="_blank" href="https://developer.nvidia.com/blog/nvidia-speech-and-translation-ai-models-set-records-for-speed-and-accuracy/">NVIDIA Parakeet</a> family of models for <a target="_blank" href="https://developer.nvidia.com/blog/essential-guide-to-automatic-speech-recognition-technology/#what_is_automatic_speech_recognition">automatic speech recognition</a>.</p>
<p>NVIDIA NIM microservices can be used all together or separately, offering developers a modular approach to building AI applications. In addition, the microservices can be integrated with community models, NVIDIA models or users’ custom models — in the cloud, on premises or in hybrid environments — providing developers with further flexibility.</p>
<p>NVIDIA NIM microservices are available at <a target="_blank" href="http://ai.nvidia.com">ai.nvidia.com</a>. Enterprises can deploy AI applications in production with NIM through the <a target="_blank" href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> software platform.</p>
<p>NIM microservices can run on customers’ preferred accelerated infrastructure, including cloud instances from Amazon Web Services, Google Cloud, Microsoft Azure and Oracle Cloud Infrastructure, as well as <a target="_blank" href="https://www.nvidia.com/en-us/data-center/products/certified-systems/">NVIDIA-Certified Systems</a> from global server manufacturing partners including Cisco, Dell Technologies, Hewlett Packard Enterprise, Lenovo and Supermicro.</p>
<p><a target="_blank" href="https://developer.nvidia.com/developer-program">NVIDIA Developer Program</a> members will soon be able to access NIM for free for research, development and testing on their preferred infrastructure.</p>
<p><i>Learn more about the latest in generative AI and accelerated computing by joining </i><a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/"><i>NVIDIA at SIGGRAPH</i></a><i>, the premier computer graphics conference, running July 28-Aug. 1 in Denver. </i></p>
<p><i>See </i><a target="_blank" href="https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fabout-nvidia%2Flegal-info%2F&amp;data=05%7C02%7Clpham%40nvidia.com%7Cd59f2f66f51e4deaac8008dc94b3ef0f%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638548747745016311%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&amp;sdata=dm8Os%2B4LtHW2ehZrPaxn38bsutMQBDeUdQuxrIa2y1Y%3D&amp;reserved=0"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-nim-featured.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/nemo-retriever-nim-featured-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[AI, Go Fetch! New NVIDIA NeMo Retriever Microservices Boost LLM Accuracy and Throughput]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA’s AI Masters Sweep KDD Cup 2024 Data Science Competition</title>
		<link>https://blogs.nvidia.com/blog/nvidia-ai-masters-kdd-cup-2024/</link>
		
		<dc:creator><![CDATA[Brian Caulfield]]></dc:creator>
		<pubDate>Mon, 22 Jul 2024 22:47:07 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Data Science]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73114</guid>

					<description><![CDATA[Team NVIDIA has triumphed at the Amazon KDD Cup 2024, securing first place Friday across all five competition tracks. The team — consisting of NVIDIANs Ahmet Erdem, Benedikt Schifferer, Chris Deotte, Gilberto Titericz, Ivan Sorokin and Simon Jegou — demonstrated its prowess in generative AI, winning in categories that included text generation, multiple-choice questions, name	<a class="read-more" href="https://blogs.nvidia.com/blog/nvidia-ai-masters-kdd-cup-2024/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Team NVIDIA has triumphed at the Amazon <a href="https://kdd2024.kdd.org/" target="_blank" rel="noopener">KDD Cup 2024</a>, securing first place Friday across all five competition tracks.</p>
<p>The team — consisting of NVIDIANs <a href="https://www.linkedin.com/in/aerdem4/&amp;sa=D&amp;source=docs&amp;ust=1721670200192732&amp;usg=AOvVaw097v0q1qfYZH3Wca9bat0d" target="_blank" rel="noopener">Ahmet Erdem</a>, <a href="https://www.linkedin.com/in/benedikt-schifferer/" target="_blank" rel="noopener">Benedikt Schifferer</a>, <a href="https://www.kaggle.com/cdeotte" target="_blank" rel="noopener">Chris Deotte</a>, <a href="https://www.linkedin.com/in/giba1/" target="_blank" rel="noopener">Gilberto Titericz</a>, <a href="https://www.linkedin.com/in/lytic/" target="_blank" rel="noopener">Ivan Sorokin</a> and <a href="https://www.linkedin.com/in/simon-jegou/" target="_blank" rel="noopener">Simon Jegou</a> — demonstrated its prowess in generative AI, winning in categories that included text generation, multiple-choice questions, name entity recognition, ranking, and retrieval.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/image1.png"><img loading="lazy" decoding="async" class="alignnone size-large wp-image-73115" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-672x156.png" alt="" width="672" height="156" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-672x156.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-400x93.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-768x178.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1536x356.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-842x195.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-406x94.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-188x44.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1280x296.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1.png 1999w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>The competition, themed “<a href="https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms" target="_blank" rel="noopener">Multi-Task Online Shopping Challenge for LLMs</a>,” asked participants to solve various challenges using limited datasets.</p>
<p>“The new trend in LLM competitions is that they don’t give you training data,” said Deotte, a senior data scientist at NVIDIA. “They give you 96 example questions — not enough to train a model — so we came up with 500,000 questions on our own.”</p>
<p>Deotte explained that the NVIDIA team generated a variety of questions by writing some themselves, using a <a href="https://www.nvidia.com/en-us/glossary/large-language-models/" target="_blank" rel="noopener">large language model</a> to create others, and transforming existing e-commerce datasets.</p>
<p>“Once we had our questions, it was straightforward to use existing frameworks to fine-tune a language model,” he said.</p>
<p>The competition organizers hid the test questions to ensure participants couldn’t exploit previously known answers. This approach encourages models that generalize well to any question about e-commerce, proving the model’s capability to handle real-world scenarios effectively.</p>
<p>Despite these constraints, Team NVIDIA’s innovative approach outperformed all competitors by using Qwen2-72B, a just-released LLM with 72 billion parameters, fine-tuned on eight NVIDIA A100 Tensor Core GPUs, and employing QLoRA, a technique for fine-tuning models with datasets.</p>
<h2><strong>About the KDD Cup 2024</strong></h2>
<p>The KDD Cup, organized by the Association for Computing Machinery’s Special Interest Group on Knowledge Discovery and Data Mining, or ACM SIGKDD, is a prestigious annual competition that promotes research and development in the field.</p>
<p>This year’s challenge, hosted by Amazon, focused on mimicking the complexities of online shopping with the goal of making it a more intuitive and satisfying experience using large language models. Organizers utilized the test dataset ShopBench — a benchmark that replicates the massive challenge for online shopping with 57 tasks and about 20,000 questions derived from real-world Amazon shopping data — to evaluate participants&#8217; models.</p>
<p>The ShopBench benchmark focused on four key shopping skills, along with a fifth “all-in-one” challenge:</p>
<ol>
<li>Shopping Concept Understanding: Decoding complex shopping concepts and terminologies.</li>
<li>Shopping Knowledge Reasoning: Making informed decisions with shopping knowledge.</li>
<li>User Behavior Alignment: Understanding dynamic customer behavior.</li>
<li>Multilingual Abilities: Shopping across languages.</li>
<li>All-Around: Solving all tasks from the previous tracks in a unified solution.</li>
</ol>
<h2><strong>NVIDIA’s Winning Solution</strong></h2>
<p>NVIDIA’s winning solution involved creating a single model for each track.</p>
<p>The team fine-tuned the just-released Qwen2-72B model using eight NVIDIA A100 Tensor Core GPUs for approximately 24 hours. The GPUs provided fast and efficient processing, significantly reducing the time required for fine-tuning.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/image2.jpg"><img loading="lazy" decoding="async" class="alignnone size-large wp-image-73118" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/image2-672x191.jpg" alt="" width="672" height="191" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/image2-672x191.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image2-400x114.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image2-768x219.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image2-1536x438.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image2-842x240.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image2-406x116.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image2-188x54.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image2-1280x365.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image2.jpg 1811w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>First, the team generated training datasets based on the provided examples and synthesized additional data using Llama 3 70B hosted on <a href="http://build.nvidia.com" target="_blank" rel="noopener">build.nvidia.com</a>.</p>
<p>Next, they employed QLoRA (Quantized Low-Rank Adaptation), a training process using the data created in step one. QLoRA modifies a smaller subset of the model’s weights, allowing efficient training and fine-tuning.</p>
<p>The model was then quantized — making it smaller and able to run on a system with a smaller hard drive and less memory — with AWQ 4-bit and used the vLLM inference library to predict the test datasets on four NVIDIA T4 Tensor Core GPUs within the time constraints.</p>
<p>This approach secured the top spot in each individual track and the overall first place in the competition—a clean sweep for NVIDIA for the second year in a row.</p>
<p>The team plans to submit a detailed paper on its solution next month and plans to present its findings at KDD 2024 in Barcelona.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/sigg24-llm-image-AI-Masters.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/sigg24-llm-image-AI-Masters-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA’s AI Masters Sweep KDD Cup 2024 Data Science Competition]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Sustainable Strides: How AI and Accelerated Computing Are Driving Energy Efficiency</title>
		<link>https://blogs.nvidia.com/blog/accelerated-ai-energy-efficiency/</link>
		
		<dc:creator><![CDATA[Dion Harris]]></dc:creator>
		<pubDate>Mon, 22 Jul 2024 12:00:18 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Corporate Sustainability]]></category>
		<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Corporate Responsibility]]></category>
		<category><![CDATA[Energy]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73050</guid>

					<description><![CDATA[AI and accelerated computing — twin engines NVIDIA continuously improves — are delivering energy efficiency for many industries. It’s progress the wider community is starting to acknowledge. “Even if the predictions that data centers will soon account for 4% of global energy consumption become a reality, AI is having a major impact on reducing the	<a class="read-more" href="https://blogs.nvidia.com/blog/accelerated-ai-energy-efficiency/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>AI and <a href="https://blogs.nvidia.com/blog/what-is-accelerated-computing/" target="_blank" rel="noopener">accelerated computing</a> — twin engines NVIDIA continuously improves — are delivering <a href="https://www.nvidia.com/en-us/glossary/energy-efficiency/" target="_blank" rel="noopener">energy efficiency</a> for many industries.</p>
<p>It’s progress the wider community is starting to acknowledge.</p>
<p>“Even if the predictions that data centers will soon account for 4% of global energy consumption become a reality, AI is having a major impact on reducing the remaining 96% of energy consumption,” said a <a href="https://lisboncouncil.net/wp-content/uploads/2024/04/LISBON_COUNCIL_Research_Sustainable_Computing_For_A_Sustainable_Planet.pdf" target="_blank" rel="noopener">report</a> from Lisbon Council Research, a nonprofit formed in 2003 that studies economic and social issues.</p>
<p>The article from the Brussels-based research group is among a handful of big-picture AI policy studies starting to emerge. It uses Italy’s <a href="https://blogs.nvidia.com/blog/supercomputing-ai-eurohpc/" target="_blank" rel="noopener">Leonardo supercomputer</a>, accelerated with nearly 14,000 NVIDIA GPUs, as an example of a system advancing work in fields from automobile design and drug discovery to weather forecasting.</p>
<figure id="attachment_73091" aria-describedby="caption-attachment-73091" style="width: 600px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/image10.png"><img loading="lazy" decoding="async" class="wp-image-73091 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/image10.png" alt="" width="600" height="395" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/image10.png 600w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image10-400x263.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image10-327x215.png 327w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image10-152x100.png 152w" sizes="(max-width: 600px) 100vw, 600px" /></a><figcaption id="caption-attachment-73091" class="wp-caption-text">Energy-efficiency gains over time for the most efficient supercomputer on the TOP500 list. Source: TOP500.org</figcaption></figure>
<h2><b>Why Accelerated Computing Is Sustainable Computing</b></h2>
<p>Accelerated computing uses the parallel processing of NVIDIA GPUs to do more work in less time. As a result, it consumes less energy than general-purpose servers that employ CPUs built to handle one task at a time.</p>
<p>That’s why accelerated computing is <a href="https://www.nvidia.com/en-us/data-center/sustainable-computing/?ncid=so-link-848252-vt04" target="_blank" rel="noopener">sustainable computing</a>.</p>
<figure id="attachment_73075" aria-describedby="caption-attachment-73075" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/power-to-time-gpu-vs-cpu-graph.jpg"><img loading="lazy" decoding="async" class="wp-image-73075 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/power-to-time-gpu-vs-cpu-graph-672x297.jpg" alt="" width="672" height="297" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/power-to-time-gpu-vs-cpu-graph-672x297.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/power-to-time-gpu-vs-cpu-graph-400x177.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/power-to-time-gpu-vs-cpu-graph-768x340.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/power-to-time-gpu-vs-cpu-graph-842x373.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/power-to-time-gpu-vs-cpu-graph-406x180.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/power-to-time-gpu-vs-cpu-graph-188x83.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/power-to-time-gpu-vs-cpu-graph.jpg 1146w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73075" class="wp-caption-text">Accelerated systems use parallel processing on GPUs to do more work in less time, consuming less energy than CPUs.</figcaption></figure>
<p>The gains are even greater when <a href="https://blogs.nvidia.com/blog/why-gpus-are-great-for-ai/" target="_blank" rel="noopener">accelerated systems apply AI</a>, an inherently parallel form of computing that’s the most transformative technology of our time.</p>
<p>“When it comes to frontier applications like machine learning or deep learning, the performance of GPUs is an order of magnitude better than that of CPUs,” the report said.</p>
<p>By transitioning from CPU-only operations to GPU-accelerated systems, HPC and AI workloads can save over 40 terawatt-hours of energy annually, equivalent to the electricity needs of nearly 5 million U.S. homes.</p>
<figure id="attachment_73051" aria-describedby="caption-attachment-73051" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/traditional-vs-nvidia-servers.jpg"><img loading="lazy" decoding="async" class="wp-image-73051 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/traditional-vs-nvidia-servers-672x263.jpg" alt="" width="672" height="263" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/traditional-vs-nvidia-servers-672x263.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/traditional-vs-nvidia-servers-400x156.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/traditional-vs-nvidia-servers-768x300.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/traditional-vs-nvidia-servers-1536x600.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/traditional-vs-nvidia-servers-842x329.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/traditional-vs-nvidia-servers-406x159.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/traditional-vs-nvidia-servers-188x73.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/traditional-vs-nvidia-servers-1280x500.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/traditional-vs-nvidia-servers.jpg 1699w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73051" class="wp-caption-text">NVIDIA offers a combination of GPUs, CPUs, and DPUs tailored to maximize energy efficiency with accelerated computing.</figcaption></figure>
<h2><b>User Experiences With Accelerated AI</b></h2>
<p>Users worldwide are documenting energy-efficiency gains with AI and accelerated computing.</p>
<p>In financial services, <a href="https://blogs.nvidia.com/blog/grace-hopper-murex-mx-3/" target="_blank" rel="noopener">Murex</a> — a Paris-based company with a trading and risk-management platform used daily by more than 60,000 people — tested the <a href="https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/" target="_blank" rel="noopener">NVIDIA Grace Hopper Superchip</a>. On its workloads, the CPU-GPU combo delivered a 4x reduction in energy consumption and a 7x reduction in time to completion compared with CPU-only systems (see chart below).</p>
<p>“On risk calculations, Grace is not only the fastest processor, but also far more power-efficient, making green IT a reality in the trading world,” said Pierre Spatz, head of quantitative research at Murex.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/image6.jpg"><img loading="lazy" decoding="async" class="aligncenter wp-image-73094 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/image6.jpg" alt="" width="600" height="371" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/image6.jpg 600w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image6-400x247.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image6-348x215.jpg 348w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image6-162x100.jpg 162w" sizes="(max-width: 600px) 100vw, 600px" /></a></p>
<p>In manufacturing, Taiwan-based <a href="https://blogs.nvidia.com/blog/digital-twins-modulus-wistron/" target="_blank" rel="noopener">Wistron</a> built a digital copy of a room where <a href="https://www.nvidia.com/en-us/data-center/dgx-platform/" target="_blank" rel="noopener">NVIDIA DGX systems</a> undergo thermal stress tests to improve operations at the site. It used <a href="https://www.nvidia.com/en-us/omniverse/" target="_blank" rel="noopener">NVIDIA Omniverse</a>, a platform for industrial digitization, with a surrogate model, a version of AI that emulates simulations.</p>
<p>The digital twin, linked to thousands of networked sensors, enabled Wistron to increase the facility’s overall energy efficiency by up to 10%. That amounts to reducing electricity consumption by 120,000 kWh per year and carbon emissions by a whopping 60,000 kilograms.</p>
<h2><b>Up to 80% Fewer Carbon Emissions</b></h2>
<p><a href="https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/apache-spark-3/" target="_blank" rel="noopener">The RAPIDS Accelerator for Apache Spark</a> can reduce the carbon footprint for data analytics, a widely used form of machine learning, by as much as 80% while delivering 5x average speedups and 4x reductions in computing costs, according to <a href="https://blogs.nvidia.com/blog/spark-rapids-energy-efficiency/" target="_blank" rel="noopener">a recent benchmark</a>.</p>
<p>Thousands of companies — about 80% of the Fortune 500 — use <a href="https://www.nvidia.com/en-us/glossary/data-science/apache-spark/" target="_blank" rel="noopener">Apache Spark</a> to analyze their growing mountains of data. Companies using NVIDIA’s Spark accelerator include Adobe, <a href="https://blogs.nvidia.com/blog/att-data-science-rapids/" target="_blank" rel="noopener">AT&amp;T</a> and the <a href="https://blogs.nvidia.com/blog/cloudera-spark-irs-gpus/" target="_blank" rel="noopener">U.S. Internal Revenue Service</a>.</p>
<p><iframe loading="lazy" title="Accelerating Spark with NVIDIA GPUs on Cloudera Data Platform" width="500" height="281" src="https://www.youtube.com/embed/DuuNX6QiJ9Y?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>In healthcare, <a href="https://blogs.nvidia.com/blog/insilico-medicine-uses-generative-ai-to-accelerate-drug-discovery/" target="_blank" rel="noopener">Insilico Medicine</a> discovered and put into phase 2 clinical trials a drug candidate for a relatively rare respiratory disease, thanks to its NVIDIA-powered AI platform.</p>
<p>Using traditional methods, the work would have cost <a href="https://www.knowledgeportalia.org/costs-r-d#:~:text=Breaking%20down%20the%20total%20costs,million%20and%20%241%2C460%20million%20capitalized." target="_blank" rel="noopener">more than $400 million</a> and taken <a href="http://phrma-docs.phrma.org/sites/default/files/pdf/rd_brochure_022307.pdf" target="_blank" rel="noopener">up to six years</a>. But with <a href="https://www.nvidia.com/en-us/glossary/generative-ai/" target="_blank" rel="noopener">generative AI</a>, Insilico hit the milestone for one-tenth of the cost in one-third of the time.</p>
<p>“This is a significant milestone not only for us, but for everyone in the field of AI-accelerated drug discovery,” said Alex Zhavoronkov, CEO of Insilico Medicine.</p>
<p>This is just a sampler of results that users of accelerated computing and AI are pursuing at companies such as <a href="https://blogs.nvidia.com/blog/genomics-ai-amgen-superpod/" target="_blank" rel="noopener">Amgen</a>, <a href="https://blogs.nvidia.com/blog/bmw-nvidia-isaac-factory-logistics/" target="_blank" rel="noopener">BMW</a>, <a href="https://blogs.nvidia.com/blog/foxconn-digital-twin-ai/" target="_blank" rel="noopener">Foxconn</a>, <a href="https://developer.nvidia.com/blog/gpu-inference-momentum-continues-to-build/" target="_blank" rel="noopener">PayPal</a> and many more.</p>
<h2><b>Speeding Science With Accelerated AI </b></h2>
<p>In basic research, the National Energy Research Scientific Computing Center (<a href="https://www.nersc.gov/" target="_blank" rel="noopener">NERSC</a>), the U.S. Department of Energy’s lead facility for open science, <a href="https://blogs.nvidia.com/blog/gpu-energy-efficiency-nersc/" target="_blank" rel="noopener">measured results</a> on a server with four <a href="https://www.nvidia.com/en-us/data-center/a100/" target="_blank" rel="noopener">NVIDIA A100 Tensor Core GPUs</a> compared with dual-socket x86 CPU servers across four of its key high-performance computing and AI applications.</p>
<p>Researchers found that the apps, when accelerated with the NVIDIA A100 GPUs, saw <a href="https://www.nvidia.com/en-us/glossary/energy-efficiency/" target="_blank" rel="noopener">energy efficiency</a> rise 5x on average (see below). One application, for weather forecasting, logged gains of nearly 10x.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/energy-consumed-per-job.jpg"><img loading="lazy" decoding="async" class="aligncenter wp-image-73063 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/energy-consumed-per-job-672x409.jpg" alt="" width="672" height="409" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/energy-consumed-per-job-672x409.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/energy-consumed-per-job-400x243.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/energy-consumed-per-job-768x467.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/energy-consumed-per-job-739x450.jpg 739w, https://blogs.nvidia.com/wp-content/uploads/2024/07/energy-consumed-per-job-353x215.jpg 353w, https://blogs.nvidia.com/wp-content/uploads/2024/07/energy-consumed-per-job-164x100.jpg 164w, https://blogs.nvidia.com/wp-content/uploads/2024/07/energy-consumed-per-job.jpg 1216w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>Scientists and researchers worldwide depend on AI and accelerated computing to achieve high performance and efficiency.</p>
<p>In a <a href="https://blogs.nvidia.com/blog/green500-energy-efficient-supercomputers/" target="_blank" rel="noopener">recent ranking</a> of the world’s most energy-efficient supercomputers, known as the <a href="https://top500.org/lists/green500/2024/06/" target="_blank" rel="noopener">Green500</a>, NVIDIA-powered systems swept the top six spots, and 40 of the top 50.</p>
<h2><b>Underestimated Energy Savings</b></h2>
<p>The many gains across industries and science are sometimes overlooked in forecasts that extrapolate only the energy consumption of training the largest AI models. That misses the benefits from most of an AI model’s life when it’s consuming relatively little energy, delivering the kinds of efficiencies users described above.</p>
<p>In an analysis citing dozens of sources, a <a href="https://www2.datainnovation.org/2024-ai-energy-use.pdf" target="_blank" rel="noopener">recent study</a> debunked as misleading and inflated projections based on training models.</p>
<p>“Just as the early predictions about the energy footprints of e-commerce and video streaming ultimately proved to be exaggerated, so too will those estimates about AI likely be wrong,” said the report from the Information Technology and Innovation Foundation (ITIF), a Washington-based think tank.</p>
<p>The report notes as much as 90% of the cost — and all the efficiency gains — of running an AI model are in deploying it in applications after it’s trained.</p>
<p>“Given the enormous opportunities to use AI to benefit the economy and society — including transitioning to a low-carbon future — it is imperative that policymakers and the media do a better job of vetting the claims they entertain about AI’s environmental impact,” said the report’s author, who described his findings in a <a href="https://blogs.nvidia.com/blog/itif-daniel-castro/" target="_blank" rel="noopener">recent podcast</a>.</p>
<h2><b>Others Cite AI’s Energy Benefits</b></h2>
<p>Policy analysts from the R Street Institute, also in Washington, D.C., agreed.</p>
<p>“Rather than a pause, policymakers need to help realize the potential for gains from AI,” the group wrote in a 1,200-word <a href="https://www.rstreet.org/commentary/accelerated-computing-artificial-intelligence-and-the-computational-revolution/" target="_blank" rel="noopener">article</a>.</p>
<p>“Accelerated computing and the rise of AI hold great promise for the future, with significant societal benefits in terms of economic growth and social welfare,” it said, citing demonstrated benefits of AI in <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10302890/" target="_blank" rel="noopener">drug discovery</a>, <a href="https://www.bloomberg.com/news/features/2023-05-31/jpmorgan-s-push-into-finance-ai-has-wall-street-rushing-to-catch-up#xj4y7vzkg" target="_blank" rel="noopener">banking</a>, <a href="https://business.fiu.edu/graduate/insights/artificial-intelligence-in-the-stock-market.cfm" target="_blank" rel="noopener">stock trading</a> and <a href="https://www.forbes.com/sites/forbestechcouncil/2023/04/17/harnessing-the-power-of-ai-in-the-insurance-sector/?sh=7b17b48335d6" target="_blank" rel="noopener">insurance</a>.</p>
<p>AI can make the electric grid, manufacturing and transportation sectors more efficient, it added.</p>
<h2><b>AI Supports Sustainability Efforts</b></h2>
<p>The reports also cited the potential of accelerated AI to fight climate change and promote sustainability.</p>
<p>“AI can enhance the accuracy of <a href="https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020MS002109" target="_blank" rel="noopener">weather modeling</a> to improve public safety as well as generate more accurate predictions of <a href="https://medium.com/@xtomsmith/ai-in-agriculture-improving-crop-yield-and-farming-efficiency-4c02bfa334ed" target="_blank" rel="noopener">crop yields</a>. The power of AI can also contribute to … developing more precise <a href="https://www.jhuapl.edu/news/news-releases/230331-johns-hopkins-scientists-leverage-ai-to-discover-climate-tipping-points" target="_blank" rel="noopener">climate models</a>,” R Street said.</p>
<p>The Lisbon report added that AI plays “a crucial role in the innovation needed to address climate change” for work such as discovering more efficient battery materials.</p>
<h2><b>How AI Can Help the Environment</b></h2>
<p>ITIF called on governments to adopt AI as a tool in efforts to decarbonize their operations.</p>
<p>Public and private organizations are already applying NVIDIA AI to <a href="https://blogs.nvidia.com/blog/coral-reef-decline-curee-robot-jetson-isaac-omniverse/" target="_blank" rel="noopener">protect coral reefs</a>, improve <a href="https://blogs.nvidia.com/blog/ai-wildfires-california/" target="_blank" rel="noopener">tracking of wildfires</a> and <a href="https://blogs.nvidia.com/blog/weather-forecast-corrdiff/" target="_blank" rel="noopener">extreme weather</a>, and <a href="https://blogs.nvidia.com/blog/mondavi-monarch-smart-electric-jetson-tractor/" target="_blank" rel="noopener">enhance sustainable agriculture</a>.</p>
<p>For its part, NVIDIA is working with <a href="https://blogs.nvidia.com/blog/earth-day-2024-climate-tech-ai-startups/" target="_blank" rel="noopener">hundreds of startups</a> employing AI to address climate issues. NVIDIA also announced plans for <a href="https://www.nvidia.com/en-us/high-performance-computing/earth-2/" target="_blank" rel="noopener">Earth-2</a>, expected to be the world’s most powerful AI supercomputer dedicated to climate science.</p>
<h2><b>Enhancing Energy Efficiency Across the Stack</b></h2>
<p>Since its founding in 1993, NVIDIA has worked on energy efficiency across all its products — GPUs, CPUs, <a href="https://blogs.nvidia.com/blog/whats-a-dpu-data-processing-unit/" target="_blank" rel="noopener">DPUs</a>, networks, systems and software, as well as platforms such as Omniverse.</p>
<p>In AI, the brunt of an AI model’s life is in inference, delivering insights that help users achieve new efficiencies. The <a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing" target="_blank" rel="noopener">NVIDIA GB200 Grace Blackwell Superchip</a> has demonstrated 25x energy efficiency over the prior NVIDIA Hopper GPU generation in AI inference.</p>
<p>Over the last eight years, NVIDIA GPUs have advanced a whopping 45,000x in their energy efficiency running large language models (see chart below).</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/llm-inference-energy-efficient.jpg"><img loading="lazy" decoding="async" class="aligncenter wp-image-73069 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/llm-inference-energy-efficient-672x368.jpg" alt="" width="672" height="368" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/llm-inference-energy-efficient-672x368.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/llm-inference-energy-efficient-400x219.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/llm-inference-energy-efficient-768x420.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/llm-inference-energy-efficient-1536x841.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/llm-inference-energy-efficient-822x450.jpg 822w, https://blogs.nvidia.com/wp-content/uploads/2024/07/llm-inference-energy-efficient-393x215.jpg 393w, https://blogs.nvidia.com/wp-content/uploads/2024/07/llm-inference-energy-efficient-183x100.jpg 183w, https://blogs.nvidia.com/wp-content/uploads/2024/07/llm-inference-energy-efficient-1280x701.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/llm-inference-energy-efficient.jpg 1999w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>Recent innovations in software include <a href="https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/" target="_blank" rel="noopener">TensorRT-LLM</a>. It can help GPUs reduce 3x the energy consumption of LLM inference.</p>
<p>Here’s an eye-popping stat: If the efficiency of cars improved as much as NVIDIA has advanced the efficiency of AI on its accelerated computing platform, cars would get 280,000 miles per gallon. That means you could drive to the moon on less than a gallon of gas.</p>
<p>The analysis applies to the fuel efficiency of cars NVIDIA’s whopping 10,000x efficiency gain in AI training and inference from 2016 to 2025 (see chart below).</p>
<figure id="attachment_73066" aria-describedby="caption-attachment-73066" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/graph.jpg"><img loading="lazy" decoding="async" class="wp-image-73066 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/graph-672x412.jpg" alt="" width="672" height="412" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/graph-672x412.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/graph-400x245.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/graph-768x471.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/graph-734x450.jpg 734w, https://blogs.nvidia.com/wp-content/uploads/2024/07/graph-351x215.jpg 351w, https://blogs.nvidia.com/wp-content/uploads/2024/07/graph-163x100.jpg 163w, https://blogs.nvidia.com/wp-content/uploads/2024/07/graph-1280x785.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/graph.jpg 1531w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73066" class="wp-caption-text">How the big AI efficiency leap from the NVIDIA P100 GPU to the NVIDIA Grace Blackwell compares to car fuel-efficiency gains.</figcaption></figure>
<h2><b>Driving Data Center Efficiency</b></h2>
<p>NVIDIA delivers many optimizations through system-level innovations. For example, <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/documents/datasheet-nvidia-bluefield-3-dpu.pdf" target="_blank" rel="noopener">NVIDIA BlueField-3 DPUs</a> can <a href="https://images.nvidia.com/content/APAC/assets/in/Increasing-Data-Center-Power-Efficiency-with-the-NVIDIA-BlueField-DPU.pdf" target="_blank" rel="noopener">reduce power consumption</a> up to 30% by offloading essential data center networking and infrastructure functions from less efficient CPUs.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/dpu-drives-data-center-efficiency.jpg"><img loading="lazy" decoding="async" class="aligncenter wp-image-73060 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/dpu-drives-data-center-efficiency-672x375.jpg" alt="" width="672" height="375" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/dpu-drives-data-center-efficiency-672x375.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/dpu-drives-data-center-efficiency-400x224.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/dpu-drives-data-center-efficiency-768x429.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/dpu-drives-data-center-efficiency-1536x858.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/dpu-drives-data-center-efficiency-805x450.jpg 805w, https://blogs.nvidia.com/wp-content/uploads/2024/07/dpu-drives-data-center-efficiency-385x215.jpg 385w, https://blogs.nvidia.com/wp-content/uploads/2024/07/dpu-drives-data-center-efficiency-179x100.jpg 179w, https://blogs.nvidia.com/wp-content/uploads/2024/07/dpu-drives-data-center-efficiency-1280x715.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/dpu-drives-data-center-efficiency.jpg 1999w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>Last year, NVIDIA received <a href="https://blogs.nvidia.com/blog/liquid-cooling-doe-challenge/" target="_blank" rel="noopener">a $5 million grant</a> from the U.S. Department of Energy — the largest of 15 grants from a pool of more than 100 applications — to design a new liquid-cooling technology for data centers. It will run 20% more efficiently than today’s air-cooled approaches and has a smaller carbon footprint.</p>
<p>These are just some of the ways NVIDIA contributes to the energy efficiency of data centers.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/modern-energy-efficient-supercomputeres-run-on-the-NVIDIA-platform.jpg"><img loading="lazy" decoding="async" class="aligncenter wp-image-73072 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/modern-energy-efficient-supercomputeres-run-on-the-NVIDIA-platform-672x364.jpg" alt="" width="672" height="364" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/modern-energy-efficient-supercomputeres-run-on-the-NVIDIA-platform-672x364.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/modern-energy-efficient-supercomputeres-run-on-the-NVIDIA-platform-400x217.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/modern-energy-efficient-supercomputeres-run-on-the-NVIDIA-platform-768x416.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/modern-energy-efficient-supercomputeres-run-on-the-NVIDIA-platform-1536x833.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/modern-energy-efficient-supercomputeres-run-on-the-NVIDIA-platform-830x450.jpg 830w, https://blogs.nvidia.com/wp-content/uploads/2024/07/modern-energy-efficient-supercomputeres-run-on-the-NVIDIA-platform-396x215.jpg 396w, https://blogs.nvidia.com/wp-content/uploads/2024/07/modern-energy-efficient-supercomputeres-run-on-the-NVIDIA-platform-184x100.jpg 184w, https://blogs.nvidia.com/wp-content/uploads/2024/07/modern-energy-efficient-supercomputeres-run-on-the-NVIDIA-platform-1280x694.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/modern-energy-efficient-supercomputeres-run-on-the-NVIDIA-platform.jpg 1999w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>Data centers are among the most efficient users of energy and one of the largest consumers of renewable energy.</p>
<p>The ITIF report notes that between 2010 and 2018, global data centers experienced a 550% increase in compute instances and a 2,400% increase in storage capacity, but only a 6% increase in energy use, thanks to improvements across hardware and software.</p>
<p>NVIDIA continues to drive energy efficiency for accelerated AI, helping users in science, government and industry accelerate their journeys toward sustainable computing.</p>
<p><i>Try NVIDIA’s </i><a href="https://www.nvidia.com/en-us/data-center/sustainable-computing/energy-efficiency-calculator/?ncid=so-link-822651-vt04" target="_blank" rel="noopener"><i>energy-efficiency calculator</i></a><i> to find ways to improve energy efficiency. And check out NVIDIA’s </i><a href="https://www.nvidia.com/en-us/data-center/sustainable-computing/?ncid=so-link-848252-vt04" target="_blank" rel="noopener"><i>sustainable computing site</i></a><i> and </i><a href="https://images.nvidia.com/aem-dam/Solutions/documents/FY2024-NVIDIA-Corporate-Sustainability-Report.pdf" target="_blank" rel="noopener"><i>corporate sustainability report</i></a><i> for more information. </i></p>
<p><iframe loading="lazy" title="This Is Enterprise Accelerated Computing | NVIDIA" width="500" height="281" src="https://www.youtube.com/embed/64ohN9sRi_M?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/Slide1.jpeg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/Slide1-842x450.jpeg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Sustainable Strides: How AI and Accelerated Computing Are Driving Energy Efficiency]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Byte-Sized Courses: NVIDIA Offers Self-Paced Career Development in AI and Data Science</title>
		<link>https://blogs.nvidia.com/blog/ai-data-science-career-development/</link>
		
		<dc:creator><![CDATA[Andy Bui]]></dc:creator>
		<pubDate>Fri, 19 Jul 2024 16:00:14 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Deep Learning Institute]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73041</guid>

					<description><![CDATA[AI has seen unprecedented growth — spurring the need for new training and education resources for students and industry professionals. NVIDIA’s latest on-demand webinar, Essential Training and Tips to Accelerate Your Career in AI, featured a panel discussion with industry experts on fostering career growth and learning in AI and other advanced technologies. Over 1,800	<a class="read-more" href="https://blogs.nvidia.com/blog/ai-data-science-career-development/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>AI has seen unprecedented growth — spurring the need for new training and education resources for students and industry professionals.</p>
<p>NVIDIA’s latest on-demand webinar, <a href="https://nvidia.com/en-us/on-demand/session/other2024-aicareer" target="_blank" rel="noopener">Essential Training and Tips to Accelerate Your Career in AI</a>, featured a panel discussion with industry experts on fostering career growth and learning in AI and other advanced technologies.</p>
<p>Over 1,800 attendees gained insights on how to kick-start their careers and use NVIDIA’s technologies and resources to accelerate their professional development.</p>
<h2><b>Opportunities in AI</b></h2>
<p>AI’s impact is touching nearly every industry, presenting new career opportunities for professionals of all backgrounds.</p>
<p>Lauren Silveira, a university recruiting program manager at NVIDIA, challenged attendees to take their unique education and experience and apply it in the AI field.</p>
<p>“You don’t have to work directly in AI to impact the industry,” said Silveira. “I knew I wouldn’t be a doctor or engineer — that wasn’t in my career path — but I could create opportunities for those that wanted to pursue those dreams.”</p>
<p>Kevin McFall, a principal instructor for the <a href="https://learn.nvidia.com/" target="_blank" rel="noopener">NVIDIA Deep Learning Institute</a>, offered some advice for those looking to navigate a career in AI and advanced technologies but finding themselves overwhelmed or unsure of where to start.</p>
<p>“Don’t try to do it all by yourself,” he said. “Don’t get focused on building everything from scratch — the best skill that you can have is being able to take pieces of code or inspiration from different resources and plug them together to make a whole.”</p>
<p>A main takeaway from the panelists was that students and industry professionals can significantly enhance their capabilities by leveraging tools and resources in addition to their networks.</p>
<p>Every individual can access a variety of free software development kits, community resources and specialized courses in areas like robotics, CUDA and OpenUSD through the <a href="https://developer.nvidia.com/" target="_blank" rel="noopener">NVIDIA Developer Program</a>. Additionally, they can kick off projects with the <a href="https://developer.nvidia.com/cuda-code-samples" target="_blank" rel="noopener">CUDA code sample library</a> and explore specialized guides such as “<a href="https://developer.nvidia.com/blog/a-simple-guide-to-deploying-generative-ai-with-nvidia-nim/" target="_blank" rel="noopener">A Simple Guide to Deploying Generative AI With NVIDIA NIM</a>”.</p>
<h2><b>Spinning a Network</b></h2>
<p>Staying up to date on the rapidly expanding technology industry involves more than just keeping up with the latest education and certifications.</p>
<p>Sabrina Koumoin, a senior software engineer at NVIDIA, spoke on the importance of networking. She believes people can find like-minded peers and mentors to gain inspiration from by sharing their <a href="https://blogs.nvidia.com/blog/sabrina-koumoin/?dysig_tid=7040046f2624437aa2bf3105f501a6a5" target="_blank" rel="noopener">personal learning journeys</a> or projects on social platforms like LinkedIn.</p>
<p>A self-taught coder, Koumoin also advocates for active engagement and education accessibility. Outside of work, she hosted multiple coding bootcamps for people looking to break into tech.</p>
<p>“It’s a way to show that learning technical skills can be engaging, not intimidating,” she said.</p>
<p>David Ajoku, founder and CEO at Demystifyd and Aware.ai, also emphasized the importance of using LinkedIn to build connections, demonstrate key accomplishments and show passion.</p>
<p>He outlined a three-step strategy to enhance your LinkedIn presence, designed to help you stand out, gain deeper insights into your preferred companies and boldly share your aspirations and interests:</p>
<ol>
<li>Think about a company you’d like to work for and what draws you to it.</li>
<li>Research thoroughly, focusing on its main activities, mission and goals.</li>
<li>Be bold — create a series of posts informing your network about your career journey and what advancements interest you in the chosen company.</li>
</ol>
<p>One attendee asked about how AI might evolve over the next decade and what skills professionals should focus on to stay relevant. Louis Stewart, head of strategic initiatives at NVIDIA, replied that crafting a personal narrative and growth journey is just as important as ensuring certifications and skills are up to date.</p>
<p>“Be intentional and purposeful — have an end in mind,” he said. “That’s how you connect with future potential companies and people — it’s a skill you have to develop to stay ahead.”</p>
<h2><b>Deep Dive Into Learning</b></h2>
<p>NVIDIA offers a variety of programs and resources to equip the next generation of AI professionals with the skills and training needed to excel in a career in AI.</p>
<p>NVIDIA’s <a href="https://www.nvidia.com/en-us/learn/ai-learning-essentials/" target="_blank" rel="noopener">AI Learning Essentials</a> is designed to give individuals the knowledge, skills and certifications they need to be prepared for the workforce and the fast moving field of AI. It includes free access to self-paced introductory courses and webinars on topics such as <a href="https://www.nvidia.com/en-us/glossary/generative-ai/" target="_blank" rel="noopener">generative AI</a>, <a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/" target="_blank" rel="noopener">retrieval-augmented generation</a> (RAG) and <a href="https://blogs.nvidia.com/blog/what-is-cuda-2/" target="_blank" rel="noopener">CUDA</a>.</p>
<p>The <a href="https://www.nvidia.com/en-us/training/" target="_blank" rel="noopener">NVIDIA Deep Learning Institute</a> (DLI) provides a diverse range of resources, including learning materials, self-paced and live trainings, and educator programs spanning AI, accelerated computing and data science, graphics simulation and more. They also offer technical workshops for students currently enrolled in universities.</p>
<p>DLI provides comprehensive training for generative AI, RAG, <a href="https://www.nvidia.com/en-us/ai/" target="_blank" rel="noopener">NVIDIA NIM inference microservices</a> and large language models. Offerings also include certifications for <a href="https://www.nvidia.com/en-us/learn/certification/generative-ai-llm-associate/" target="_blank" rel="noopener">generative AI LLMs</a> and <a href="https://www.nvidia.com/en-us/learn/certification/generative-ai-multimodal-associate/" target="_blank" rel="noopener">generative AI multimodal</a> that help learners showcase their expertise and stand out from the crowd.</p>
<p><i>Get started with </i><a href="https://www.nvidia.com/en-us/learn/ai-learning-essentials/" target="_blank" rel="noopener"><i>AI Learning Essentials</i></a><i>, the </i><a href="https://www.nvidia.com/en-us/training/" target="_blank" rel="noopener"><i>NVIDIA Deep Learning Institute</i></a><i> and </i><a href="https://www.nvidia.com/en-us/on-demand/" target="_blank" rel="noopener"><i>on-demand resources</i></a><i>. </i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/IMG_7082-1.jpg"
			type="image/jpeg"
			width="1268"
			height="712"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/IMG_7082-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Byte-Sized Courses: NVIDIA Offers Self-Paced Career Development in AI and Data Science]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Magnetic Marvels: NVIDIA&#8217;s Supercomputers Spin a Quantum Tale</title>
		<link>https://blogs.nvidia.com/blog/quantum-research-gpus/</link>
		
		<dc:creator><![CDATA[Esperanza Cuenca Gómez]]></dc:creator>
		<pubDate>Fri, 19 Jul 2024 15:00:10 +0000</pubDate>
				<category><![CDATA[Research]]></category>
		<category><![CDATA[Supercomputing]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73003</guid>

					<description><![CDATA[Groundbreaking research underlines NVIDIA’s critical role in advancing quantum computing. ]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Research published earlier this month in the science journal <i>Nature </i>used NVIDIA-powered supercomputers to validate a pathway toward the commercialization of <a target="_blank" href="https://www.nvidia.com/en-us/solutions/quantum-computing/">quantum computing</a>.</p>
<p>The research, led by Nobel laureate Giorgio Parisi and Massimo Bernaschi, director of technology at the National Research Council of Italy and a CUDA Fellow, focuses on quantum annealing, a method that may one day tackle complex optimization problems that are extraordinarily challenging to conventional computers.</p>
<p>To conduct their research, the team utilized 2 million GPU computing hours at the Leonardo facility (Cineca, in Bologna, Italy), nearly 160,000 GPU computing hours on the Meluxina-GPU cluster, in Luxembourg, and 10,000 GPU hours from the Spanish Supercomputing Network. Additionally, they accessed the Dariah cluster, in Lecce, Italy.</p>
<p>They used these state-of-the-art resources to simulate the behavior of a certain kind of quantum computing system known as a quantum annealer.</p>
<p>Quantum computers fundamentally rethink how information is computed to enable entirely new solutions.</p>
<p><iframe loading="lazy" title="YouTube video player" src="https://www.youtube.com/embed/7D1Toq1hhJU?si=ZmKsNJuDWubQujg-" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p>Unlike classical computers, which process information in binary — 0s and 1s — quantum computers use quantum bits or qubits that can allow information to be processed in entirely new ways.</p>
<p>Quantum annealers are a special type of quantum computer that, though not universally useful, may have advantages for solving certain types of optimization problems.</p>
<p>The paper, “<a target="_blank" href="https://www.nature.com/articles/s41586-024-07647-y">The Quantum Transition of the Two-Dimensional Ising Spin Glass</a>,” represents a significant step in understanding the phase transition — a change in the properties of a quantum system — of Ising spin glass, a disordered magnetic material in a two-dimensional plane, a critical problem in computational physics.</p>
<p>The paper addresses the problem of how the properties of magnetic particles arranged in a two-dimensional plane can abruptly change their behavior.</p>
<p>The study also shows how GPU-powered systems play a key role in developing approaches to quantum computing.</p>
<p>GPU-accelerated simulations allow researchers to understand the complex systems&#8217; behavior in developing quantum computers, illuminating the most promising paths forward.</p>
<p>Quantum annealers, like the systems developed by the pioneering quantum computing company D-Wave, operate by methodically decreasing a magnetic field that is applied to a set of magnetically susceptible particles.</p>
<p>When strong enough, the applied field will act to align the magnetic orientation of the particles — similar to how iron filings will uniformly stand to attention near a bar magnet.</p>
<p>If the strength of the field is varied slowly enough, the magnetic particles will arrange themselves to minimize the energy of the final arrangement.</p>
<p>Finding this stable, minimum-energy state is crucial in a particularly complex and disordered magnetic system known as a spin glass since quantum annealers can encode certain kinds of problems into the spin glass’s minimum-energy configuration.</p>
<p>Finding the stable arrangement of the spin glass then solves the problem.</p>
<p>Understanding these systems helps scientists develop better algorithms for solving difficult problems by mimicking how nature deals with complexity and disorder.</p>
<p>That’s crucial for advancing quantum annealing and its applications in solving extremely difficult computational problems that currently have no known efficient solution — problems that are pervasive in fields ranging from logistics to cryptography.</p>
<p>Unlike gate-model quantum computers, which operate by applying a sequence of quantum gates, quantum annealers allow a quantum system to evolve freely in time.</p>
<p>This is not a universal computer — a device capable of performing any computation given sufficient time and resources — but may have advantages for solving particular sets of optimization problems in application areas such as vehicle routing, portfolio optimization and protein folding.</p>
<p>Through extensive simulations performed on NVIDIA GPUs, the researchers learned how key parameters of the spin glasses making up quantum annealers change during their operation, allowing a better understanding of how to use these systems to achieve a quantum speedup on important problems.</p>
<p>Much of the work for this groundbreaking paper was <a target="_blank" href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s61293/.">first presented</a> at NVIDIA’s GTC 2024 technology conference. <a target="_blank" href="https://www.nature.com/articles/s41586-024-07647-y">Read the full paper</a> and learn more about <a target="_blank" href="https://www.youtube.com/watch?v=7D1Toq1hhJU">NVIDIA’s work in quantum computing</a>.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/quantum-computing-press-isc24-pr-1-1920x1080-3282051.jpg"
			type="image/jpeg"
			width="1920"
			height="1080"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/quantum-computing-press-isc24-pr-1-1920x1080-3282051-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Magnetic Marvels: NVIDIA’s Supercomputers Spin a Quantum Tale]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Mistral AI and NVIDIA Unveil Mistral NeMo 12B, a Cutting-Edge Enterprise AI Model</title>
		<link>https://blogs.nvidia.com/blog/mistral-nvidia-ai-model/</link>
		
		<dc:creator><![CDATA[Kari Briski]]></dc:creator>
		<pubDate>Thu, 18 Jul 2024 14:00:59 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73008</guid>

					<description><![CDATA[Mistral AI and NVIDIA today released a new state-of-the-art language model, Mistral NeMo 12B, that developers can easily customize and deploy for enterprise applications supporting chatbots, multilingual tasks, coding and summarization. By combining Mistral AI’s expertise in training data with NVIDIA’s optimized hardware and software ecosystem, the Mistral NeMo model offers high performance for diverse	<a class="read-more" href="https://blogs.nvidia.com/blog/mistral-nvidia-ai-model/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Mistral AI and NVIDIA today released a new state-of-the-art language model, <a target="_blank" href="https://mistral.ai/news/mistral-nemo">Mistral NeMo</a> 12B, that developers can easily customize and deploy for enterprise applications supporting chatbots, multilingual tasks, coding and summarization.</p>
<p>By combining Mistral AI’s expertise in training data with NVIDIA’s optimized hardware and software ecosystem, the Mistral NeMo model offers high performance for diverse applications.</p>
<p>&#8220;We are fortunate to collaborate with the NVIDIA team, leveraging their top-tier hardware and software,” said Guillaume Lample, cofounder and chief scientist of Mistral AI. “Together, we have developed a model with unprecedented accuracy, flexibility, high-efficiency and enterprise-grade support and security thanks to NVIDIA AI Enterprise deployment.”</p>
<p>Mistral NeMo was trained on the <a target="_blank" href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a> AI platform, which offers dedicated, scalable access to the latest NVIDIA architecture.</p>
<p><a target="_blank" href="https://developer.nvidia.com/tensorrt">NVIDIA TensorRT-LLM</a> for accelerated inference performance on large language models and the <a target="_blank" href="https://www.nvidia.com/en-us/ai-data-science/products/nemo/">NVIDIA NeMo</a> development platform for building custom generative AI models were also used to advance and optimize the process.</p>
<p>This collaboration underscores NVIDIA’s commitment to supporting the model-builder ecosystem.</p>
<h2><b>Delivering Unprecedented Accuracy, Flexibility and Efficiency </b></h2>
<p>Excelling in multi-turn conversations, math, common sense reasoning, world knowledge and coding, this enterprise-grade AI model delivers precise, reliable performance across diverse tasks.</p>
<p>With a 128K context length, Mistral NeMo processes extensive and complex information more coherently and accurately, ensuring contextually relevant outputs.</p>
<p>Released under the Apache 2.0 license, which fosters innovation and supports the broader AI community, Mistral NeMo is a 12-billion-parameter model. Additionally, the model uses the FP8 data format for model inference, which reduces memory size and speeds deployment without any degradation to accuracy.</p>
<p>That means the model learns tasks better and handles diverse scenarios more effectively, making it ideal for enterprise use cases.</p>
<p>Mistral NeMo comes packaged as an <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> inference microservice, offering performance-optimized inference with NVIDIA TensorRT-LLM engines.</p>
<p>This containerized format allows for easy deployment anywhere, providing enhanced flexibility for various applications.</p>
<p>As a result, models can be deployed anywhere in minutes, rather than several days.</p>
<p>NIM features enterprise-grade software that’s part of <a target="_blank" href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a>, with dedicated feature branches, rigorous validation processes, and enterprise-grade security and support.</p>
<p>It includes comprehensive support, direct access to an NVIDIA AI expert and defined service-level agreements, delivering reliable and consistent performance.</p>
<p>The open model license allows enterprises to integrate Mistral NeMo into commercial applications seamlessly.</p>
<p>Designed to fit on the memory of a single NVIDIA L40S, NVIDIA GeForce RTX 4090 or NVIDIA RTX 4500 GPU, the Mistral NeMo NIM offers high efficiency, low compute cost, and enhanced security and privacy.</p>
<h2><b>Advanced Model Development and Customization </b></h2>
<p>The combined expertise of Mistral AI and NVIDIA engineers has optimized training and inference for Mistral NeMo.</p>
<p>Trained with Mistral AI’s expertise, especially on multilinguality, code and multi-turn content, the model benefits from accelerated training on NVIDIA’s full stack.</p>
<p>It’s designed for optimal performance, utilizing efficient model parallelism techniques, scalability and mixed precision with Megatron-LM.</p>
<p>The model was trained using<a target="_blank" href="https://github.com/NVIDIA/Megatron-LM"> Megatron-LM</a>, part of NVIDIA <a target="_blank" href="https://www.nvidia.com/en-us/ai-data-science/products/nemo/">NeMo</a>, with 3,072 H100 80GB Tensor Core GPUs on DGX Cloud, composed of NVIDIA AI architecture, including accelerated computing, network fabric and software to increase training efficiency.</p>
<h2><b>Availability and Deployment</b></h2>
<p>With the flexibility to run anywhere — cloud, data center or RTX workstation — Mistral NeMo is ready to revolutionize AI applications across various platforms.</p>
<p>Experience Mistral NeMo as an NVIDIA NIM today via <a target="_blank" href="http://ai.nvidia.com">ai.nvidia.com</a>, with a downloadable NIM coming soon.</p>
<p><i>See </i><a target="_blank" href="https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fabout-nvidia%2Flegal-info%2F&amp;data=05%7C02%7Clpham%40nvidia.com%7Cd59f2f66f51e4deaac8008dc94b3ef0f%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638548747745016311%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&amp;sdata=dm8Os%2B4LtHW2ehZrPaxn38bsutMQBDeUdQuxrIa2y1Y%3D&amp;reserved=0"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/ngc-corp-blog-community-model-confidential-model-1280x680-2.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/ngc-corp-blog-community-model-confidential-model-1280x680-2-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Mistral AI and NVIDIA Unveil Mistral NeMo 12B, a Cutting-Edge Enterprise AI Model]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
	</channel>
</rss>
