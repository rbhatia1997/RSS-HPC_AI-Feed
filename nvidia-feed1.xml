<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:media="http://search.yahoo.com/mrss/">

<channel>
	<title>NVIDIA Blog</title>
	<atom:link href="https://blogs.nvidia.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.nvidia.com/</link>
	<description></description>
	<lastBuildDate>Wed, 24 Apr 2024 23:15:55 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.5.2</generator>
	<item>
		<title>How Virtual Factories Are Making Industrial Digitalization a Reality</title>
		<link>https://blogs.nvidia.com/blog/virtual-factories-industrial-digitalization/</link>
		
		<dc:creator><![CDATA[James McKenna]]></dc:creator>
		<pubDate>Wed, 24 Apr 2024 15:00:40 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Robotics]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71266</guid>

					<description><![CDATA[To address the shift to electric vehicles, increased semiconductor demand, manufacturing onshoring, and ambitions for greater sustainability, manufacturers are investing in new factory developments and re-engineering their existing facilities. These projects often run over budget and schedule, due to complex and manual planning processes, legacy technology infrastructure, and disconnected tools, data and teams. To address		<a class="read-more" href="https://blogs.nvidia.com/blog/virtual-factories-industrial-digitalization/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>To address the shift to electric vehicles, increased semiconductor demand, manufacturing onshoring, and ambitions for greater sustainability, manufacturers are investing in new factory developments and re-engineering their existing facilities.</p>
<p>These projects often run over budget and schedule, due to complex and manual planning processes, legacy technology infrastructure, and disconnected tools, data and teams.</p>
<p>To address these challenges, manufacturers are embracing digitalization and virtual factories, powered by technologies like <a href="https://blogs.nvidia.com/blog/what-is-a-digital-twin/">digital twins,</a> the <a href="https://www.nvidia.com/en-us/omniverse/usd/">Universal Scene Description (OpenUSD)</a><a href="https://blogs.nvidia.com/blog/what-is-a-digital-twin/"> ecosystem</a> and <a href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a>, that enable new possibilities from planning to operations.</p>
<h2>What Is a Virtual Factory?</h2>
<p>A virtual factory is a physically accurate representation of a real factory. These digital twins of factories allow manufacturers to model, simulate, analyze and optimize their production processes, resources and operations without the need for a physical prototype or pilot plant.</p>
<h2>Benefits of Virtual Factories</h2>
<p>Virtual factories unlock many benefits and possibilities for manufacturers, including:</p>
<ul>
<li><b>Streamlined Communication: </b>Instead of teams relying on in-person meetings and static planning documents for project alignment, virtual factories streamline communication and ensure that critical design and operations decisions are informed by the most current data.</li>
<li><b>Contextualized Planning</b>: During facility design, construction and commissioning, virtual factories allow project stakeholders to visualize designs in the context of the entire facility and production process. Planning and operations teams can compare and verify built structures with the virtual designs in real time and decrease costs by identifying errors and incorporating feedback early in the review process.</li>
<li><b>Optimized Facility Designs: </b>Connecting virtual factories to simulations of processes and discrete events enables teams to optimize facility designs for production and material flow, ergonomic work design, safety and overall utilization.</li>
<li><b>Intelligent and Optimized Operations: </b>Operations teams can integrate their virtual factories with valuable production data from Internet of Things technology at the <a href="https://blogs.nvidia.com/blog/what-is-edge-computing/">edge</a>, and tap AI to drive further optimizations.</li>
</ul>
<h2>Virtual Factories: A Testing Ground for AI and Robotics</h2>
<p>Robotics developers are increasingly using virtual factories to train and test AI and autonomous systems that run in physical factories. For example, virtual factories can enable developers and manufacturing teams to simulate digital workers and <a href="https://blogs.nvidia.com/blog/isaac-amr-nova-orin-autonomous-mobile-robots/">autonomous mobile robots</a> (AMRs), vision AI agents and sensors to create a centralized map of worker activity throughout a facility. By fusing data from simulated camera streams with multi-camera tracking, developers can generate occupancy maps that inform optimal AMR routes.</p>
<p>Developers can also use these physically accurate virtual factories to train and test AI agents capable of managing their robot fleets, to ensure AI-enabled robots can adapt to real-world unpredictability and to identify streamlined configurations for human-robot collaboration.</p>
<p><iframe title="Fusing Real-Time AI With Digital Twins" width="500" height="281" src="https://www.youtube.com/embed/l5M4sqaRd6w?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2>What Are the Foundations of a Virtual Factory</h2>
<p>Building large-scale, physically accurate virtual factories that unlock these transformational possibilities requires bringing together many tools, data formats and technologies to harmonize the representation of real-world aspects in the digital world.</p>
<p>Originally invented by Pixar Animation Studios, <a href="https://www.nvidia.com/en-us/omniverse/usd/">OpenUSD</a> encompasses a collection of tools and capabilities that enable the data interoperability developers and manufacturers require to achieve their digitalization goals.</p>
<p>OpenUSD’s core superpower is flexible data modeling. 3D input can be accepted from source applications and combined with a variety of data, including from computer-aided design software, live sensors, documentation and maintenance records, through a unified data pipeline. OpenUSD enables developers to share these data types across different simulation tools and AI models, providing insights for all stakeholders. Data can be synced from the factory floor to the digital twin, surfacing real-time insights for factory managers and teams.</p>
<p>By developing virtual factory solutions on OpenUSD, developers can enhance collaboration for factory teams, allowing them to review plans, discuss optimization opportunities and make decisions in real time.</p>
<p>To support and accelerate the development of the OpenUSD ecosystem, Pixar, Adobe, Apple, Autodesk and NVIDIA formed the <a href="https://aousd.org/">Alliance for OpenUSD</a>, which is building open standards for USD in core specification, materials, geometry and more.</p>
<h2>Industrial Use Cases for Virtual Factories</h2>
<p>To unlock the potential of virtual factories, industry leaders including Autodesk, Continental, Pegatron, Rockwell Automation, Siemens and Wistron are developing virtual-factory solutions that interoperate with OpenUSD and <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a>, a platform of application programming interfaces (APIs) and software development kits that enable developers to build applications for complex 3D and industrial digitalization workflows based on OpenUSD.</p>
<p><a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62639/">FlexSim</a>, an Autodesk company, uses OpenUSD to enable factory teams to analyze, visualize and optimize real-world processes with its simulation modeling for complex systems and operations. The discrete-event simulation software provides an intuitive drag-and-drop interface to create 3D simulation models, account for real-world variability, run “what-if” scenarios and perform in-depth analyses.</p>
<p>Developers at <a href="https://www.continental.com/en/" target="_blank" rel="noopener">Continental</a>, a leading German automotive technology company, developed <a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62919/">ContiVerse</a>, a factory planning and manufacturing operations application on OpenUSD and NVIDIA Omniverse. The application helps Continental optimize factory layouts and plan production processes collaboratively, leading to an expected 13% reduction in time to market.<a href="https://www.pegatroncorp.com/"> </a></p>
<p>Partnering with IT consulting and digital services provider <a href="https://www.softserveinc.com/en-us">SoftServe</a>, Continental also developed <a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s61799/">Industrial Co-Pilot</a>, which combines AI-driven insights with immersive visualization to deliver real-time guidance and predictive analytics to engineers. This is expected to reduce maintenance effort and downtime by 10%.</p>
<p><a href="https://www.pegatroncorp.com/" target="_blank" rel="noopener">Pegatron</a>, one of the world’s largest manufacturers of smartphones and consumer electronics, is <a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62553/">developing virtual-factory solutions</a> on OpenUSD to accelerate the development of new factories — as well as to minimize change orders, optimize operations and maximize production-line throughput in existing facilities.</p>
<p><a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62623/">Rockwell Automation</a> is integrating NVIDIA Omniverse Cloud APIs and OpenUSD with its <a href="https://www.demo3d.com/">Emulate3D</a> digital twin software to bring manufacturing teams data interoperability, live collaboration and physically based visualization for designing, building and operating industrial-scale digital twins of production systems.</p>
<p><a href="https://www.siemens.com/global/en.html" target="_blank" rel="noopener">Siemens</a>, a leading technology company for automation, digitalization and sustainability and a member of the Alliance for OpenUSD, is adopting Omniverse Cloud APIs within its Siemens Xcelerator Platform, starting with Teamcenter X, the industry-leading cloud-based product lifecycle management software. This will help teams design, build and test next-generation products, manufacturing processes and factories virtually, before they’re built in the physical world.</p>
<p><a href="https://www.wistron.com/en" target="_blank" rel="noopener">Wistron</a>, a leading global technology service provider and electronics manufacturer, is <a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62600/">digitalizing new and existing factories</a> with OpenUSD. By developing virtual-factory solutions on NVIDIA Omniverse, Wistron enables its factory teams to collaborate remotely to refine layout configurations, optimize surface mount technology and in-circuit testing lines, and transform product-on-dock testing.</p>
<p>With these solutions, Wistron has achieved a 51% boost in worker efficiency and 50% reduction in production process times. Layout optimization and real-time monitoring have decreased defect rates by 40%. And construction time on Wistron’s new <a href="https://www.nvidia.com/en-us/data-center/dgx-platform/">NVIDIA DGX</a> factory was cut in half, from about five months to just two and a half months.</p>
<p><iframe title="Meet a Factory Digital Twin From Wistron" width="500" height="281" src="https://www.youtube.com/embed/OAdqXZGUb70?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>Learn more about developing advanced, generative AI-enabled virtual factory solutions at the <a href="https://www.nvidia.com/en-us/use-cases/ai-for-virtual-factory-solutions/">Virtual Factory Use Case page</a>. Developers can get started with <a href="https://developer.nvidia.com/blog/developing-virtual-factory-solutions-with-openusd-and-nvidia-omniverse/">a reference architecture</a> that provides an overview of components and capabilities to consider when developing virtual-factory solutions.</p>
<p><i>Get started with NVIDIA Omniverse by downloading the standard license </i><a href="https://www.nvidia.com/en-us/omniverse/download/"><i>free</i></a><i>, access </i><a href="https://developer.nvidia.com/usd"><i>OpenUSD</i></a><i> resources, and learn how </i><a href="https://www.nvidia.com/en-us/omniverse/enterprise/"><i>Omniverse Enterprise</i><i> can connect your team</i></a><i>. Stay up to date on </i><a href="https://www.instagram.com/nvidiaomniverse/"><i>Instagram</i></a><i>, </i><a href="https://medium.com/@nvidiaomniverse"><i>Medium</i></a><i> and </i><a href="https://twitter.com/nvidiaomniverse"><i>X</i></a><i>. For more, join the </i><a href="https://www.nvidia.com/en-us/omniverse/community/"><i>Omniverse community</i></a><i> on the </i><a href="https://forums.developer.nvidia.com/c/omniverse/300"><i>forums</i></a><i>, </i><a href="https://discord.com/invite/XWQNJDNuaC"><i>Discord server</i></a><i>, </i><a href="https://www.twitch.tv/nvidiaomniverse"><i>Twitch</i></a><i> and </i><a href="https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA"><i>YouTube</i></a><i> channels. </i></p>
<p><em>Featured visual courtesy of Siemens.</em></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/vfi-blog.jpg"
			type="image/jpeg"
			width="1920"
			height="1020"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/vfi-blog-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[How Virtual Factories Are Making Industrial Digitalization a Reality]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA to Acquire GPU Orchestration Software Provider Run:ai</title>
		<link>https://blogs.nvidia.com/blog/runai/</link>
		
		<dc:creator><![CDATA[Alexis Bjorlin]]></dc:creator>
		<pubDate>Wed, 24 Apr 2024 13:15:08 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Software]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71285</guid>

					<description><![CDATA[To help customers make more efficient use of their AI computing resources, NVIDIA today announced it has entered into a definitive agreement to acquire Run:ai, a Kubernetes-based workload management and orchestration software provider. Customer AI deployments are becoming increasingly complex, with workloads distributed across cloud, edge and on-premises data center infrastructure. Managing and orchestrating generative		<a class="read-more" href="https://blogs.nvidia.com/blog/runai/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>To help customers make more efficient use of their AI computing resources, NVIDIA today announced it has entered into a definitive agreement to acquire Run:ai, a Kubernetes-based workload management and orchestration software provider.</p>
<p>Customer AI deployments are becoming increasingly complex, with workloads distributed across cloud, edge and on-premises data center infrastructure.</p>
<p>Managing and orchestrating generative AI, recommender systems, search engines and other workloads requires sophisticated scheduling to optimize performance at the system level and on the underlying infrastructure.</p>
<p>Run:ai enables enterprise customers to manage and optimize their compute infrastructure, whether on premises, in the cloud or in hybrid environments.</p>
<p>The company has built an open platform on <a href="https://www.nvidia.com/en-us/glossary/kubernetes/">Kubernetes</a>, the orchestration layer for modern AI and cloud infrastructure. It supports all popular Kubernetes variants and integrates with third-party AI tools and frameworks.</p>
<p>Run:ai customers include some of the world’s largest enterprises across multiple industries, which use the Run:ai platform to manage data-center-scale GPU clusters.</p>
<p>“Run:ai has been a close collaborator with NVIDIA since 2020 and we share a passion for helping our customers make the most of their infrastructure,” said Omri Geller, Run:ai cofounder and CEO. “We’re thrilled to join NVIDIA and look forward to continuing our journey together.”</p>
<p>The Run:ai platform provides AI developers and their teams:</p>
<ul>
<li style="font-weight: 400;" aria-level="1">A centralized interface to manage shared compute infrastructure, enabling easier and faster access for complex AI workloads.</li>
<li style="font-weight: 400;" aria-level="1">Functionality to add users, curate them under teams, provide access to cluster resources, control over quotas, priorities and pools, and monitor and report on resource use.</li>
<li style="font-weight: 400;" aria-level="1">The ability to pool GPUs and share computing power — from <a href="https://www.nvidia.com/en-us/technologies/multi-instance-gpu/#:~:text=Multi%2DInstance%20GPU%20(MIG)%20expands%20the%20performance%20and%20value,%2C%20cache%2C%20and%20compute%20cores.">fractions of GPUs</a> to multiple GPUs or multiple nodes of GPUs running on different clusters — for separate tasks.</li>
<li style="font-weight: 400;" aria-level="1">Efficient GPU cluster resource utilization, enabling customers to gain more from their compute investments.</li>
</ul>
<p>NVIDIA will continue to offer Run:ai&#8217;s products under the same business model for the immediate future. And NVIDIA will continue to invest in the Run:ai product roadmap as part of <a href="http://www.nvidia.com/dgx-cloud">NVIDIA DGX Cloud</a>, an AI platform co-engineered with leading clouds for enterprise developers, offering an integrated, full-stack service optimized for generative AI.</p>
<p>NVIDIA DGX and DGX Cloud customers will gain access to Run:ai&#8217;s capabilities for their AI workloads, particularly for large language model deployments. Run:ai&#8217;s solutions are already integrated with <a href="https://www.nvidia.com/en-us/data-center/dgx-platform/">NVIDIA DGX</a>, <a href="https://www.nvidia.com/en-us/data-center/dgx-superpod/">NVIDIA DGX SuperPOD</a>, <a href="https://www.nvidia.com/en-us/data-center/base-command/">NVIDIA Base Command</a>, <a href="https://www.nvidia.com/en-us/gpu-cloud/">NGC</a> containers, and <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> software, among other products.</p>
<p>NVIDIA’s accelerated computing platform and Run:ai&#8217;s platform will continue to support a broad ecosystem of third-party solutions, giving customers choice and flexibility.</p>
<p>Together with Run:ai, NVIDIA will enable customers to have a single fabric that accesses GPU solutions anywhere. Customers can expect to benefit from better GPU utilization, improved management of GPU infrastructure and greater flexibility from the open architecture.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/nvidia-ravioli.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/nvidia-ravioli-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA to Acquire GPU Orchestration Software Provider Run:ai]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Forecasting the Future: AI2’s Christopher Bretherton Discusses Using Machine Learning for Climate Modeling</title>
		<link>https://blogs.nvidia.com/blog/ai2s-christopher-bretherton/</link>
		
		<dc:creator><![CDATA[Kristen Yee]]></dc:creator>
		<pubDate>Wed, 24 Apr 2024 13:00:13 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[The AI Podcast]]></category>
		<category><![CDATA[Climate]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71293</guid>

					<description><![CDATA[Can machine learning help predict extreme weather events and climate change? Christopher Bretherton, senior director of climate modeling at the Allen Institute for Artificial Intelligence, or AI2, explores the technology’s potential to enhance climate modeling with AI Podcast host Noah Kravitz in an episode recorded live at the NVIDIA GTC global AI conference. Bretherton explains		<a class="read-more" href="https://blogs.nvidia.com/blog/ai2s-christopher-bretherton/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Can machine learning help predict extreme weather events and climate change? Christopher Bretherton, senior director of climate modeling at the Allen Institute for Artificial Intelligence, or AI2, explores the technology’s potential to enhance climate modeling with <a href="https://soundcloud.com/theaipodcast">AI Podcast</a> host Noah Kravitz in an episode recorded live at the <a href="https://www.nvidia.com/gtc/">NVIDIA GTC</a> global AI conference. Bretherton explains how machine learning helps overcome the limitations of traditional climate models and underscores the role of localized predictions in empowering communities to prepare for climate-related risks. Through ongoing research and collaboration, Bretherton and his team aim to improve climate modeling and enable society to better mitigate and adapt to the impacts of climate change.</p>
<p>Stay tuned for more episodes recorded live from GTC, and watch the replay of Bretherton’s <a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62359/">GTC session</a> on using machine learning for climate modeling.</p>
<p><iframe src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/1806553098%3Fsecret_token%3Ds-EFTMFbquHt4&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true" width="100%" height="166" frameborder="no" scrolling="no"></iframe></p>
<div style="font-size: 10px; color: #cccccc; line-break: anywhere; word-break: normal; overflow: hidden; white-space: nowrap; text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif; font-weight: 100;"><a style="color: #cccccc; text-decoration: none;" title="The AI Podcast" href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">The AI Podcast</a> · <a style="color: #cccccc; text-decoration: none;" title="AI2’s Christopher Bretherton Discusses Using Machine Learning for Climate Modeling - Ep. XXX" href="https://soundcloud.com/theaipodcast/christopher-bretherton/s-EFTMFbquHt4" target="_blank" rel="noopener">AI2’s Christopher Bretherton Discusses Using Machine Learning for Climate Modeling &#8211; Ep. XXX</a></div>
<h2><b>Time Stamps</b></h2>
<p>2:03: What is climate modeling and how can it prepare us for climate change?</p>
<p>5:28: How can machine learning help enhance climate modeling?</p>
<p>7:21: What were the limitations of traditional climate models?</p>
<p>10:24: How does a climate model work?</p>
<p>12:11: What information can you get from a climate model?</p>
<p>13:26: What are the current climate models telling us about the future?</p>
<p>15:56: How does machine learning help enable localized climate modeling?</p>
<p>18:39: What, if anything, can individuals or small communities do to prepare for what climate change has in store for us?</p>
<p>25:59: How do you measure the accuracy or performance of an emulator that’s doing something like climate modeling out into the future?</p>
<h2><b>You Might Also Like…</b></h2>
<p><a href="https://soundcloud.com/theaipodcast/ai-daniel-castro-itif"><b>ITIF’s Daniel Castro on Energy-Efficient AI and Climate Change &#8211; Ep. 215</b><b><br />
</b><br />
</a>AI-driven change is in the air, as are concerns about the technology’s environmental impact. In this episode of NVIDIA’s AI Podcast, Daniel Castro, vice president of the Information Technology and Innovation Foundation and director of its Center for Data Innovation, speaks with host Noah Kravitz about the motivation behind his AI energy use report, which addresses misconceptions about the technology’s energy consumption.</p>
<p><a href="https://soundcloud.com/theaipodcast/ai-wildfire"><b>DigitalPath’s Ethan Higgins on Using AI to Fight Wildfires &#8211; Ep. 211</b></a></p>
<p>DigitalPath is igniting change in the golden state — using computer vision, generative adversarial networks and a network of thousands of cameras to detect signs of fire in real-time. In the latest episode of NVIDIA’s AI Podcast, host Noah Kravtiz spoke with DigitalPath system architect Ethan Higgins about the company’s role in the ALERTCalifornia initiative, a collaboration between California’s wildfire fighting agency CAL FIRE and the University of California, San Diego.</p>
<p><a href="https://soundcloud.com/theaipodcast/anima-anandkumar"><b>Anima Anandkumar on Using Generative AI to Tackle Global Challenges &#8211; Ep. 203</b></a></p>
<p>Generative AI-based models can not only learn and understand natural languages — they can learn the very language of nature itself, presenting new possibilities for scientific research. On the latest episode of NVIDIA’s AI Podcast, host Noah Kravitz spoke with Anandkumar on generative AI’s potential to make splashes in the scientific community.</p>
<p><a href="https://soundcloud.com/theaipodcast/ai-alex-fielding"><b>How Alex Fielding and Privateer Space Are Taking on Space Debris &#8211; Ep. 196</b></a></p>
<p>In this episode of the NVIDIA AI Podcast, host Noah Kravitz dives into an illuminating conversation with Alex Fielding, co-founder and CEO of Privateer Space. Privateer Space, Fielding’s latest venture, aims to address one of the most daunting challenges facing our world today: space debris.</p>
<h2><b>Subscribe to the AI Podcast</b></h2>
<p>Get the<a href="https://blogs.nvidia.com/ai-podcast/"> AI Podcast</a> through<a href="https://itunes.apple.com/us/podcast/the-ai-podcast/id1186480811?mt=2&amp;adbsc=social_20161220_68874946&amp;adbid=811257941365882882&amp;adbpl=tw&amp;adbpr=61559439"> iTunes</a>,<a href="https://podcasts.google.com/?feed=aHR0cHM6Ly9mZWVkcy5zb3VuZGNsb3VkLmNvbS91c2Vycy9zb3VuZGNsb3VkOnVzZXJzOjI2NDAzNDEzMy9zb3VuZHMucnNz"> Google Podcasts</a>,<a href="https://play.google.com/music/listen?u=0#/ps/I4kyn74qfrsdhrm35mcrf3igxzm"> Google Play</a>, <a href="https://music.amazon.com/podcasts/956857d0-9461-4496-a07e-24be0539ee82/the-ai-podcast">Amazon Music, </a><a href="https://castbox.fm/channel/The-AI-Podcast-id433488?country=us">Castbox</a>, DoggCatcher,<a href="https://overcast.fm/itunes1186480811/the-ai-podcast"> Overcast</a>,<a href="https://player.fm/series/the-ai-podcast"> PlayerFM</a>, Pocket Casts,<a href="http://www.podbay.fm/show/1186480811"> Podbay</a>,<a href="https://www.podbean.com/podcast-detail/cjgnp-4a6e0/The-AI-Podcast"> PodBean</a>, PodCruncher, PodKicker,<a href="https://soundcloud.com/theaipodcast"> Soundcloud</a>,<a href="https://open.spotify.com/show/4TB4pnynaiZ6YHoKmyVN0L"> Spotify</a>,<a href="http://www.stitcher.com/s?fid=130629&amp;refid=stpr"> Stitcher</a> and<a href="https://tunein.com/podcasts/Technology-Podcasts/The-AI-Podcast-p940829/"> TuneIn</a>.</p>
<p>Make the AI Podcast better: Have a few minutes to spare? Fill out<a href="http://survey.podtrac.com/start-survey.aspx?pubid=I5V0tOQFNS8j&amp;ver=short"> this listener survey</a>.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2021/08/ai-podcast-2600x1472_-1-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1159"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2021/08/ai-podcast-2600x1472_-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Forecasting the Future: AI2’s Christopher Bretherton Discusses Using Machine Learning for Climate Modeling]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Rays Up: Decoding AI-Powered DLSS 3.5 Ray Reconstruction</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-ray-reconstruction/</link>
		
		<dc:creator><![CDATA[Henry Lin]]></dc:creator>
		<pubDate>Wed, 24 Apr 2024 13:00:12 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71272</guid>

					<description><![CDATA[DLSS 3.5 with Ray Reconstruction creates higher quality ray-traced images for intensive ray-traced games and apps.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><i>Editor’s note: This post is part of the </i><a href="https://blogs.nvidia.com/blog/tag/ai-decoded/"><i>AI Decoded series</i></a><i>, which demystifies AI by making the technology more accessible, and which showcases new hardware, software, tools and accelerations for RTX PC users.</i></p>
<p>AI continues to raise the bar for PC gaming.</p>
<p><a href="https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-3-5-ray-reconstruction/">DLSS 3.5 with Ray Reconstruction</a> creates higher quality ray-traced images for intensive ray-traced games and apps. This advanced AI-powered neural renderer is a groundbreaking feature that elevates ray-traced image quality for all GeForce RTX GPUs, outclassing traditional hand-tuned denoisers by using an AI network trained by an NVIDIA supercomputer. The result improves lighting effects like reflections, global illumination, and shadows to create a more immersive, realistic gaming experience.</p>
<h2><b>A Ray of Light</b></h2>
<p>Ray tracing is a rendering technique that can realistically simulate the lighting of a scene and its objects by rendering physically accurate reflections, refractions, shadows and indirect lighting. Ray tracing generates computer graphics images by tracing the path of light from the view camera — which determines the view into the scene — through the 2D viewing plane, out into the 3D scene, and back to the light sources. For instance, if rays strike a mirror, reflections are generated.</p>
<figure id="attachment_71276" aria-describedby="caption-attachment-71276" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/ray-tracing-image-1.jpg"><img loading="lazy" decoding="async" class="size-large wp-image-71276" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/ray-tracing-image-1-672x442.jpg" alt="" width="672" height="442" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/ray-tracing-image-1-672x442.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/ray-tracing-image-1-400x263.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/ray-tracing-image-1-768x505.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/ray-tracing-image-1-684x450.jpg 684w, https://blogs.nvidia.com/wp-content/uploads/2024/04/ray-tracing-image-1-327x215.jpg 327w, https://blogs.nvidia.com/wp-content/uploads/2024/04/ray-tracing-image-1-152x100.jpg 152w, https://blogs.nvidia.com/wp-content/uploads/2024/04/ray-tracing-image-1.jpg 1063w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71276" class="wp-caption-text">A visualization of how ray tracing works.</figcaption></figure>
<p>It’s the digital equivalent to real-world objects illuminated by beams of light and the path of the light being followed from the eye of the viewer to the objects that light interacts with. That’s ray tracing.</p>
<p>Simulating light in this manner — shooting rays for every pixel on the screen — is computationally intensive, even for offline renderers that calculate scenes over the course of several minutes or hours. Instead, ray samples fire a handful of rays at various points across the scene for a representative sample of the scene’s lighting, reflectivity and shadowing.</p>
<p>However, there are limitations. The output is a noisy, speckled image with gaps, good enough to ascertain how the scene should look when ray traced. To fill in the missing pixels that weren’t ray traced, hand-tuned denoisers use two different methods, temporally accumulating pixels across multiple frames, and spatially interpolating them to blend neighboring pixels together. Through this process, the noisy raw output is converted into a ray-traced image.</p>
<p>This adds complexity and cost to the development process, and reduces the frame rate in highly ray-traced games where multiple denoisers operate simultaneously for different lighting effects.</p>
<p>DLSS 3.5 Ray Reconstruction introduces an NVIDIA supercomputer-trained, AI-powered neural network that generates higher-quality pixels in between the sampled rays. It recognizes different ray-traced effects to make smarter decisions about using temporal and spatial data, and retains high frequency information for superior-quality upscaling. And it recognizes lighting patterns from its training data, such as that of global illumination or ambient occlusion, and recreates it in-game.</p>
<p><iframe loading="lazy" title="Portal with RTX | DLSS 3.5 with Ray Reconstruction Comparison - Fan Scene" width="500" height="281" src="https://www.youtube.com/embed/Gg_3-X3d3iY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p><i>Portal with RTX</i> is a great example of Ray Reconstruction in action. With DLSS OFF, the denoiser struggles to reconstruct the dynamic shadowing alongside the moving fan.</p>
<p>With DLSS 3.5 and Ray Reconstruction enabled, the denoiser is trained on AI and recognizes certain patterns associated with shadows and keeps the image stable, accumulating accurate pixels while blending neighboring pixels to generate high-quality reflections.</p>
<h2><b>Deep Learning, Deep Gaming</b></h2>
<p>Ray Reconstruction is just one of the AI graphics breakthroughs that multiply performance in DLSS. Super Resolution, the cornerstone of DLSS, samples multiple lower resolution images and uses motion data and feedback from prior frames to reconstruct native-quality images. The result is high image quality without sacrificing game performance.</p>
<p><iframe loading="lazy" title="NVIDIA DLSS 3 | AI-Powered Performance In Your Favorite Games &amp; Apps" width="500" height="281" src="https://www.youtube.com/embed/GKpURmnNMoA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>DLSS 3 introduced Frame Generation, which boosts performance by using AI to analyze data from surrounding frames to predict what the next generated frame should look like. These generated frames are then inserted in between rendered frames. Combining the DLSS-generated frames with DLSS Super Resolution enables DLSS 3 to reconstruct seven-eighths of the displayed pixels with AI, boosting frame rates by up to 4x compared to without DLSS.</p>
<p>Because DLSS Frame Generation is post-processed (applied after the main render) on the GPU, it can boost frame rates even when the game is bottlenecked by the CPU.</p>
<p><i>Generative AI is transforming gaming, videoconferencing and interactive experiences of all kinds. Make sense of what’s new and what’s next by subscribing to the </i><a href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><i>AI Decoded newsletter</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/dlss-ray-reconstruction-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/dlss-ray-reconstruction-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Rays Up: Decoding AI-Powered DLSS 3.5 Ray Reconstruction]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Climate Tech Startups Integrate NVIDIA AI for Sustainability Applications</title>
		<link>https://blogs.nvidia.com/blog/earth-day-2024-climate-tech-ai-startups/</link>
		
		<dc:creator><![CDATA[Tenika Versey Walker]]></dc:creator>
		<pubDate>Mon, 22 Apr 2024 15:00:12 +0000</pubDate>
				<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Agriculture and Food]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Climate]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Inception]]></category>
		<category><![CDATA[Science]]></category>
		<category><![CDATA[Scientific Visualization]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<category><![CDATA[Social Impact]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71220</guid>

					<description><![CDATA[Whether they’re monitoring miniscule insects or delivering insights from satellites in space, NVIDIA-accelerated startups are making every day Earth Day. Sustainable Futures, an initiative within the NVIDIA Inception program for cutting-edge startups, is supporting 750+ companies globally focused on agriculture, carbon capture, clean energy, climate and weather, environmental analysis, green computing, sustainable infrastructure and waste		<a class="read-more" href="https://blogs.nvidia.com/blog/earth-day-2024-climate-tech-ai-startups/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Whether they’re monitoring miniscule insects or delivering insights from satellites in space, NVIDIA-accelerated startups are making every day Earth Day.</p>
<p>Sustainable Futures, an initiative within the <a href="https://www.nvidia.com/startups/?nvid=nv-int-tblg-295718-vt33">NVIDIA Inception</a> program for cutting-edge startups, is supporting 750+ companies globally focused on agriculture, carbon capture, clean energy, climate and weather, environmental analysis, green computing, sustainable infrastructure and waste management.</p>
<p>This Earth Day, discover how five of these sustainability-focused startups are advancing their work with accelerated computing and the <a href="https://www.nvidia.com/en-us/high-performance-computing/earth-2/">NVIDIA Earth-2</a> platform for climate tech.</p>
<p>Earth-2 features a suite of AI models that help simulate, visualize and deliver actionable insights about weather and climate.</p>
<h2><b>Insect Farming Catches the AI Bug</b></h2>
<figure id="attachment_71240" aria-describedby="caption-attachment-71240" style="width: 400px" class="wp-caption alignright"><img loading="lazy" decoding="async" class="wp-image-71240 size-medium" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/Bug-Mars-2-400x400.png" alt="" width="400" height="400" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/Bug-Mars-2-400x400.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Bug-Mars-2-500x500.png 500w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Bug-Mars-2-150x150.png 150w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Bug-Mars-2-768x768.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Bug-Mars-2-450x450.png 450w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Bug-Mars-2-215x215.png 215w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Bug-Mars-2-100x100.png 100w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Bug-Mars-2.png 1024w" sizes="(max-width: 400px) 100vw, 400px" /><figcaption id="caption-attachment-71240" class="wp-caption-text">Image courtesy of Bug Mars</figcaption></figure>
<p>Amid a changing climate, a key component of environmental resilience is food security: the ability to produce and provide enough food to meet the nutrition needs of all people. Edible insects, such as crickets and black soldier flies, are one solution that could reduce humans’ reliance on resource-intensive livestock farming for protein.</p>
<p><a href="https://bugmars.com/" target="_blank" rel="noopener">Bug Mars</a>, a startup based in Ontario, Canada, supports insect protein production with AI tools that monitor variables including temperature, pests and number of insects — and predict issues and recommend actions based on that data. It can help insect farmers increase yield by 30%.</p>
<p>The company uses <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/">NVIDIA Jetson Orin Nano</a> modules to accelerate its work, and <a href="https://www.linkedin.com/feed/update/urn:li:activity:7178512288889368577/" target="_blank" rel="noopener">recently announced</a> it’s using synthetic data and digital twin technology to further advance its AI solutions for insect agriculture.</p>
<h2><b>Seeing the Forest for the Trees</b></h2>
<p>Based in Truckee, Calif., <a href="https://www.vibrantplanet.net" target="_blank" rel="noopener">Vibrant Planet</a> is modeling trillions of trees and other flammable vegetation such as shrublands and grasslands to help land managers, counties and fire districts across North America build wildfire and climate resilience.</p>
<p>NVIDIA hardware and software has helped Vibrant Planet develop <a href="https://blogs.nvidia.com/blog/what-is-a-transformer-model/">transformer models</a> for forest and ecosystem management and AI-enhanced operational planning.</p>
<figure id="attachment_71231" aria-describedby="caption-attachment-71231" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-71231" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/Vibrant-Planet-1-672x311.jpg" alt="Visualization of forest" width="672" height="311" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/Vibrant-Planet-1-672x311.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Vibrant-Planet-1-400x185.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Vibrant-Planet-1-768x356.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Vibrant-Planet-1-1536x712.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Vibrant-Planet-1-842x390.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Vibrant-Planet-1-406x188.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Vibrant-Planet-1-188x87.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Vibrant-Planet-1-1280x593.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Vibrant-Planet-1.jpg 1849w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-71231" class="wp-caption-text">Visualization courtesy of Vibrant Planet</figcaption></figure>
<p>The startup collects and analyzes data from lidar sensors, satellites and aircraft to train AI models that can map vegetation with high precision, estimate canopy height and detect characteristics of forest and vegetation areas such as carbon, water, biodiversity and built infrastructure. Customers can use this data to understand fire and drought hazards, and, with these insights, conduct scenario planning to forecast the effects of potential forest thinning, prescribed fire or other actions.</p>
<h2><b>Delivering Tomorrow’s Forecast</b></h2>
<p><a href="http://tomorrow.io/" target="_blank" rel="noopener">Tomorrow.io</a>, based in Boston, is a leading resilience platform that helps organizations adapt to increasing weather and climate volatility. Powered by next-generation space technology, advanced AI models and proprietary modeling capabilities, the startup enables businesses and governments to proactively mitigate risk, ensure operational resilience and drive critical decision-making.</p>
<figure id="attachment_71234" aria-describedby="caption-attachment-71234" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-71234" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/Resilience-Dashboard-672x420.jpg" alt="screen capture of tomorrow.io dashboard" width="672" height="420" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/Resilience-Dashboard-672x420.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Resilience-Dashboard-400x250.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Resilience-Dashboard-720x450.jpg 720w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Resilience-Dashboard-344x215.jpg 344w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Resilience-Dashboard-160x100.jpg 160w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Resilience-Dashboard.jpg 768w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-71234" class="wp-caption-text">Image courtesy of Tomorrow.io</figcaption></figure>
<p>The startup is developing weather forecasting AI and is launching its own satellites to collect environmental data to further train its models. It’s also conducting experiments using Earth-2 AI forecast models to determine the optimal configurations of satellites to improve weather-forecasting conditions.</p>
<p>One of Tomorrow.io’s projects is an initiative in Kenya with the Bill and Melinda Gates Foundation that provides daily alerts to 6 million farmers with insights around when to water their crops, when to spray pesticides, when to harvest or when to change crops altogether due to changes in the local climate. The team hopes to scale up their user base to 100 million farmers in Africa by 2030.</p>
<h2><b>Winds of Change</b></h2>
<p>Palo Alto, Calif.-based <a href="https://windbornesystems.com" target="_blank" rel="noopener">WindBorne Systems</a> is developing weather-sensing balloons equipped with WeatherMesh, a state-of-the-art AI model for real-time global weather forecasts.</p>
<figure id="attachment_71237" aria-describedby="caption-attachment-71237" style="width: 550px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="wp-image-71237" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/Windborne-1-653x500.jpg" alt="weather balloon against landscape" width="550" height="421" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/Windborne-1-653x500.jpg 653w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Windborne-1-400x306.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Windborne-1-768x588.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Windborne-1-1536x1176.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Windborne-1-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Windborne-1-588x450.jpg 588w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Windborne-1-281x215.jpg 281w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Windborne-1-131x100.jpg 131w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Windborne-1-1280x980.jpg 1280w" sizes="(max-width: 550px) 100vw, 550px" /><figcaption id="caption-attachment-71237" class="wp-caption-text">Image courtesy of WindBorne Systems</figcaption></figure>
<p>WeatherMesh predicts factors including surface temperature, pressure, winds, precipitation and radiation. The model has <a href="https://windbornesystems.com/blog/most-accurate-global-weather-forecasts-with-new-ai-forecast-offering" target="_blank" rel="noopener">set world records for accuracy</a> and is lightweight enough to run on a gaming laptop, unlike traditional models that run on supercomputers.</p>
<p>WindBorne uses NVIDIA GPUs to develop its AI and is an early-access user of Earth-2. The company’s weather balloon development is funded in part by the <a href="https://wpo.noaa.gov/windborne-weather-balloon-reaches-new-heights/" target="_blank" rel="noopener">National Oceanic and Atmospheric Administration’s Weather Program Office</a>.</p>
<h2><b>Taking the Temperature of Global Cities</b></h2>
<p><a href="https://www.fortyguard.com/" target="_blank" rel="noopener">FortyGuard</a>, a startup founded in Abu Dhabi with headquarters in Miami, is developing a system to measure urban heat with AI models that present insights for public health officials, city planners, landscape architects and environmental engineers.</p>
<figure id="attachment_71243" aria-describedby="caption-attachment-71243" style="width: 400px" class="wp-caption alignleft"><img loading="lazy" decoding="async" class="size-medium wp-image-71243" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/FortyGuard-NVIDIA-GTC-Picture-400x300.jpg" alt="" width="400" height="300" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/FortyGuard-NVIDIA-GTC-Picture-400x300.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/FortyGuard-NVIDIA-GTC-Picture-667x500.jpg 667w, https://blogs.nvidia.com/wp-content/uploads/2024/04/FortyGuard-NVIDIA-GTC-Picture-768x576.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/FortyGuard-NVIDIA-GTC-Picture-1536x1152.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/FortyGuard-NVIDIA-GTC-Picture-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/04/FortyGuard-NVIDIA-GTC-Picture-600x450.jpg 600w, https://blogs.nvidia.com/wp-content/uploads/2024/04/FortyGuard-NVIDIA-GTC-Picture-287x215.jpg 287w, https://blogs.nvidia.com/wp-content/uploads/2024/04/FortyGuard-NVIDIA-GTC-Picture-133x100.jpg 133w, https://blogs.nvidia.com/wp-content/uploads/2024/04/FortyGuard-NVIDIA-GTC-Picture-1280x960.jpg 1280w" sizes="(max-width: 400px) 100vw, 400px" /><figcaption id="caption-attachment-71243" class="wp-caption-text">FortyGuard presented in the Expo Hall Theater at NVIDIA GTC.</figcaption></figure>
<p>The company — an early-access user of the Earth-2 platform — aims for its temperature AI models to provide a more granular view into urban heat dynamics, providing data that can help industries and governments shape cooler and more livable cities.</p>
<p>FortyGuard’s technology, offered via application programming interfaces, could integrate with existing enterprise platforms to enable use cases including temperature-based route navigation, predictive enhanced EV performance and property insights.</p>
<p><i></i><i>To learn more about the Sustainable Futures program, watch the “</i><a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-se62606/"><i>AI Nations and Sustainable Futures Day</i></a><i>” session from </i><a href="https://www.nvidia.com/gtc/"><i>NVIDIA GTC</i></a><i>. </i></p>
<p><i>NVIDIA is a member of the <a href="https://www.state.gov/coalition-for-climate-entrepreneurship-cce/" target="_blank" rel="noopener">U.S. Department of State’s Coalition for Climate Entrepreneurship</a>, which aims to address the United Nations’ Sustainable Development Goals using emerging technologies. Learn more in the GTC session, “</i><a href="https://www.nvidia.com/gtc/session-catalog/?search=tenika&amp;search=tenika%2C+tenika&amp;tab.allsessions=1700692987788001F1cG#/session/1696445353484001C0A2"><i>Global Strategies: Startups, Venture Capital, and Climate Change Solutions</i></a><i>.”</i></p>
<p><em>Video at top courtesy of Vibrant Planet.</em></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/forest-still_Vibrant-Planet.jpg"
			type="image/jpeg"
			width="1280"
			height="845"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/forest-still_Vibrant-Planet-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Climate Tech Startups Integrate NVIDIA AI for Sustainability Applications]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Wide Open: NVIDIA Accelerates Inference on Meta Llama 3   </title>
		<link>https://blogs.nvidia.com/blog/meta-llama3-inference-acceleration/</link>
		
		<dc:creator><![CDATA[Ankit Patel]]></dc:creator>
		<pubDate>Thu, 18 Apr 2024 16:30:23 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Cloud Services]]></category>
		<category><![CDATA[Consumer Internet]]></category>
		<category><![CDATA[Hardware]]></category>
		<category><![CDATA[Inference]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[NVIDIA Hopper Architecture]]></category>
		<category><![CDATA[NVIDIA Jetson]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[RTX Mobile Workstations]]></category>
		<category><![CDATA[TensorRT]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71177</guid>

					<description><![CDATA[NVIDIA today announced optimizations across all its platforms to accelerate Meta Llama 3, the latest generation of the large language model (LLM). The open model combined with NVIDIA accelerated computing equips developers, researchers and businesses to innovate responsibly across a wide variety of applications. Trained on NVIDIA AI Meta engineers trained Llama 3 on computer		<a class="read-more" href="https://blogs.nvidia.com/blog/meta-llama3-inference-acceleration/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>NVIDIA today announced optimizations across all its platforms to accelerate <a href="https://ai.meta.com/blog/meta-llama-3/">Meta Llama 3</a>, the latest generation of the large language model (<a href="https://www.nvidia.com/en-us/glossary/large-language-models/">LLM</a>).</p>
<p>The open model combined with NVIDIA <a href="https://blogs.nvidia.com/blog/what-is-accelerated-computing/">accelerated computing</a> equips developers, researchers and businesses to innovate responsibly across a wide variety of applications.</p>
<h2><b>Trained on NVIDIA AI</b></h2>
<p>Meta engineers trained Llama 3 on computer clusters packing 24,576 <a href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPUs</a>, linked with RoCE and <a href="https://www.nvidia.com/en-us/networking/quantum2/">NVIDIA Quantum-2 InfiniBand</a> networks.</p>
<p>To further advance the state of the art in <a href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a>, Meta <a href="https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/">recently described</a> plans to scale its infrastructure to 350,000 H100 GPUs.</p>
<h2><b>Putting Llama 3 to Work</b></h2>
<p>Versions of Llama 3, accelerated on NVIDIA GPUs, are available today for use in the cloud, data center, edge and PC.</p>
<p>From a browser, developers can try Llama 3 at <a href="http://ai.nvidia.com">ai.nvidia.com</a>. It’s packaged as an <a href="https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/">NVIDIA NIM</a> microservice with a standard application programming interface that can be deployed anywhere.</p>
<p>Businesses can fine-tune Llama 3 with their data using <a href="https://www.nvidia.com/en-us/ai-data-science/products/nemo/">NVIDIA NeMo</a>, an open-source framework for LLMs that’s part of the secure, supported <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> platform. Custom models can be optimized for inference with <a href="https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/">NVIDIA TensorRT-LLM</a> and deployed with <a href="https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/">NVIDIA Triton Inference Server</a>.</p>
<h2><b>Taking Llama 3 to Devices and PCs</b></h2>
<p>Llama 3 also runs on <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/">NVIDIA Jetson Orin</a> for robotics and edge computing devices, creating interactive agents like those in the <a href="https://www.jetson-ai-lab.com/">Jetson AI Lab</a>.</p>
<p>What’s more, <a href="https://www.nvidia.com/en-us/geforce/rtx/">NVIDIA RTX</a> and <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/">GeForce RTX</a> GPUs for workstations and PCs speed inference on Llama 3. These systems give developers a target of more than 100 million NVIDIA-accelerated systems worldwide.</p>
<h2><b>Get Optimal Performance with Llama 3</b></h2>
<p>Best practices in deploying an LLM for a chatbot involves a balance of low latency, good reading speed and optimal GPU use to reduce costs.</p>
<p>Such a service needs to deliver tokens — the rough equivalent of words to an LLM — at about twice a user’s reading speed which is about 10 tokens/second.</p>
<p>Applying these metrics, a single <a href="https://www.nvidia.com/en-us/data-center/h200/">NVIDIA H200 Tensor Core GPU</a> generated about 3,000 tokens/second — enough to serve about 300 simultaneous users — in an initial test using the version of Llama 3 with 70 billion parameters.</p>
<p>That means a single <a href="https://www.nvidia.com/en-us/data-center/hgx/">NVIDIA HGX</a> server with eight H200 GPUs could deliver 24,000 tokens/second, further optimizing costs by supporting more than 2,400 users at the same time.</p>
<p>For edge devices, the version of Llama 3 with eight billion parameters generated up to 40 tokens/second on Jetson AGX Orin and 15 tokens/second on Jetson Orin Nano.</p>
<h2><b>Advancing Community Models</b></h2>
<p>An active open-source contributor, NVIDIA is committed to optimizing community software that helps users address their toughest challenges. Open-source models also promote AI transparency and let users broadly share work on AI safety and resilience.</p>
<p>Learn more about how NVIDIA’s AI inference platform, including how NIM, TensorRT-LLM and Triton use state-of-the-art techniques such as <a href="https://developer.nvidia.com/blog/tune-and-deploy-lora-llms-with-nvidia-tensorrt-llm/">low-rank adaptation</a> to accelerate the latest LLMs.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/final-llm-corp-blog-meta-nv-logo-lockup-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/final-llm-corp-blog-meta-nv-logo-lockup-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Wide Open: NVIDIA Accelerates Inference on Meta Llama 3   ]]></media:title>
			<media:description type="html">Image of NVIDIA and Meta logos</media:description>
			</media:content>
			</item>
		<item>
		<title>Up to No Good: ‘No Rest for the Wicked’ Early Access Launches on GeForce NOW</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-no-rest-for-the-wicked/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 18 Apr 2024 13:00:07 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71183</guid>

					<description><![CDATA[It’s time to get a little wicked. Members can now stream No Rest for the Wicked from the cloud. It leads six new games joining the GeForce NOW library of more than 1,500 games. Holy Moly No Rest for the Wicked is the highly anticipated action role-playing game from Moon Studios, developer of the Ori		<a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-no-rest-for-the-wicked/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>It’s time to get a little wicked. Members can now stream <i>No Rest for the Wicked</i> from the cloud.</p>
<p>It leads six new games joining the <a href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a> library of more than 1,500 games.</p>
<h2><b>Holy Moly</b></h2>
<figure id="attachment_71187" aria-describedby="caption-attachment-71187" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/No_Rest_for_the_Wicked-Screenshot-scaled.jpg"><img loading="lazy" decoding="async" class="size-large wp-image-71187" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/No_Rest_for_the_Wicked-Screenshot-672x378.jpg" alt="No Rest For The Wicked on GeForce NOW" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/No_Rest_for_the_Wicked-Screenshot-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/No_Rest_for_the_Wicked-Screenshot-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/No_Rest_for_the_Wicked-Screenshot-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/No_Rest_for_the_Wicked-Screenshot-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/No_Rest_for_the_Wicked-Screenshot-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/04/No_Rest_for_the_Wicked-Screenshot-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/04/No_Rest_for_the_Wicked-Screenshot-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/04/No_Rest_for_the_Wicked-Screenshot-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/04/No_Rest_for_the_Wicked-Screenshot-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71187" class="wp-caption-text">There’s always another fight to be won.</figcaption></figure>
<p><i>No Rest for the Wicked</i> is the highly anticipated action role-playing game from Moon Studios, developer of the <a href="https://blogs.nvidia.com/blog/the-day-before-avatar-ori/"><i>Ori</i> series</a>, and publisher Private Division. Amid a plague-ridden world, step into the boots of a Cerim, a holy warrior on a desperate mission. The Great Pestilence has ravaged the land of Sacra, and a new king reigns. As a colonialist inquisition unfolds, engage in visceral combat, battle plague-infested creatures and uncover the secrets of the continent. Make the character you want with the game’s flexible soft-class system, explore a rich storyline, and prepare for intense boss battles as you build up the town of Sacrament.</p>
<p>Embark on a dark and perilous journey, where no rest awaits the wicked. Rise to the challenge and stream from GeForce RTX 4080 servers with a <a href="http://geforcenow.com">GeForce NOW Ultimate membership</a> for the smoothest gameplay from the cloud. Be among the first to experience early access of the game, without having to wait for downloads.</p>
<h2><b>Shiny New Games</b></h2>
<figure id="attachment_71184" aria-describedby="caption-attachment-71184" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-spotlight-evil-west-tw-li-2048x1024-1.jpg"><img loading="lazy" decoding="async" class="size-large wp-image-71184" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-spotlight-evil-west-tw-li-2048x1024-1-672x336.jpg" alt="Evil West on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-spotlight-evil-west-tw-li-2048x1024-1-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-spotlight-evil-west-tw-li-2048x1024-1-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-spotlight-evil-west-tw-li-2048x1024-1-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-spotlight-evil-west-tw-li-2048x1024-1-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-spotlight-evil-west-tw-li-2048x1024-1-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-spotlight-evil-west-tw-li-2048x1024-1-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-spotlight-evil-west-tw-li-2048x1024-1-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-spotlight-evil-west-tw-li-2048x1024-1-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-spotlight-evil-west-tw-li-2048x1024-1.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71184" class="wp-caption-text">&#8220;Yippie ki-yay, evil doers!&#8221;</figcaption></figure>
<p>Become a Wild West superhero in <i>Evil West</i>, streaming on GeForce NOW this week and part of PC Game Pass. It’s part of six newly supported games this week:</p>
<ul>
<li><i>Kill It With Fire 2 </i>(New release on <a href="https://store.steampowered.com/app/2357000?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 16)</li>
<li><i>The Crew Motorfest </i>(New release on <a href="https://store.steampowered.com/app/2698940?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 18)</li>
<li><i>No Rest for the Wicked </i>(New release on <a href="https://store.steampowered.com/app/1371980?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 18)</li>
<li><i>Evil West </i>(<a href="https://www.xbox.com/games/store/evil-west/9MW581HCJPM6?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Lightyear Frontier </i>(<a href="https://store.steampowered.com/app/1677110?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Tomb Raider I-III Remastered </i>(<a href="https://store.steampowered.com/app/2478970?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
</ul>
<p>Riot Games shared in its 14.8 patch notes that it will soon add its Vanguard security software to <i>League of Legends</i> as part of the publisher’s commitment to remove scripters, bots and bot-leveled accounts from the game and make it more challenging for them to continue. Since Vanguard won’t support virtual machines when it’s added to <i>League of Legends</i>, the game will be put under maintenance and will no longer be playable on GeForce NOW once the 14.9 update goes live globally — currently planned for May 1, 2024. Members can continue to enjoy the game on GeForce NOW until then.</p>
<p>What are you planning to play this weekend? Let us know on <a href="https://www.twitter.com/nvidiagfn">X</a> or in the comments below.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-thursday-4-18-nv-blog-1280x680-no-copy.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/gfn-thursday-4-18-nv-blog-1280x680-no-copy-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Up to No Good: ‘No Rest for the Wicked’ Early Access Launches on GeForce NOW]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Honors Partners of the Year in Europe, Middle East, Africa</title>
		<link>https://blogs.nvidia.com/blog/nvidia-partner-network-awards-emea-2024/</link>
		
		<dc:creator><![CDATA[Kristen Yee]]></dc:creator>
		<pubDate>Thu, 18 Apr 2024 10:00:52 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Education]]></category>
		<category><![CDATA[NVIDIA DGX]]></category>
		<category><![CDATA[Omniverse]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71149</guid>

					<description><![CDATA[NVIDIA today recognized 18 partners in Europe, the Middle East and Africa for their achievements and commitment to driving AI adoption. The recipients were honored at the annual EMEA Partner Day hosted by the NVIDIA Partner Network (NPN). The awards span seven categories that highlight the various ways partners work with NVIDIA to transform the		<a class="read-more" href="https://blogs.nvidia.com/blog/nvidia-partner-network-awards-emea-2024/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>NVIDIA today recognized 18 partners in Europe, the Middle East and Africa for their achievements and commitment to driving AI adoption.</p>
<p>The recipients were honored at the annual EMEA Partner Day hosted by the <a href="https://www.nvidia.com/en-us/about-nvidia/partners/" target="_blank" rel="noopener">NVIDIA Partner Network</a> (NPN). The awards span seven categories that highlight the various ways partners work with NVIDIA to transform the region’s industries with AI.</p>
<p>“This year marks another milestone for NVIDIA and our partners across EMEA as we pioneer technological breakthroughs and unlock new business opportunities using NVIDIA’s full-stack platform,” said Dirk Barfuss, director of EMEA channel at NVIDIA. “These awards celebrate our partners’ dedication and expertise in delivering groundbreaking solutions that drive cost efficiencies, enhance productivity and inspire innovation.”</p>
<p>The 2024 NPN award winners for EMEA are:</p>
<h2><b>Rising Star Awards</b></h2>
<ul>
<li><b>Vesper Technologies</b> received the Rising Star Northern Europe award for its exceptional revenue growth and broad customer base deploying NVIDIA AI solutions in data centers. The company has demonstrated outstanding growth in recent years, augmenting the success of its existing business.</li>
<li><b>AMBER AI &amp; Data Science Solutions GmbH </b>received the Rising Star Central Europe award for its revenue growth of more than 100% across the complete portfolio of NVIDIA technologies. Through extensive collaboration with NVIDIA, the company has become a cornerstone of the NVIDIA partner landscape in Germany.</li>
<li><b>HIPER Global Enterprise Ltd.</b> received the Rising Star Southern Europe &amp; Middle East award for its excellence in serving its broad customer base with NVIDIA compute technologies. Last year, it supported one of the largest customer projects in the region, further accelerating its growth rate.</li>
</ul>
<h2><strong>Star Performer Awards</strong></h2>
<ul>
<li><b>Boston Limited</b> received the Star Performer Northern Europe award for its consistent success in delivering full-stack implementations of NVIDIA technologies for customers across industries. The company over the last year achieved record revenue growth across its business areas.</li>
<li><b>DELTA Computer Products GmbH</b> received the Star Performer Central Europe award for its outstanding sales achievements and strong customer relationships. With a massive technical knowledge base, the company has served as a trusted advisor for customers deploying NVIDIA technologies across industry, higher education and research.</li>
<li><b>COMMit DMCC</b> received the Star Performer Southern Europe &amp; Middle East award for its exceptional execution of strategic and complex solutions built on NVIDIA technologies, which led to record revenues for the United Arab Emirates-based company.</li>
</ul>
<h2><b>Distributor of the Year</b></h2>
<ul>
<li><b>PNY</b> received the Distributor of the Year award for the third consecutive year, underscoring its consistent investment in technology training and commitment to providing NVIDIA accelerated computing platforms and software across markets.</li>
<li><b>TD Synnex</b> received the Networking Distributor of the Year award for the second year in a row, highlighting its massive investments in NVIDIA’s portfolio of technologies —  especially networking — and dedication to delivering technical expertise to customers.</li>
</ul>
<h2><b>Go-to-Market Excellence </b></h2>
<ul>
<li><b>Bynet Data Communications</b> <strong>Ltd.</strong> received the Go-to-Market Excellence award for its collaboration with NVIDIA regional leads to devise and execute effective go-to-market strategies for the Israeli market. This included identifying key opportunities and creating localized marketing campaigns. Its efforts led to great success with the installation of <a href="https://www.nvidia.com/en-us/data-center/dgx-superpod/" target="_blank" rel="noopener">NVIDIA DGX SuperPODs</a> into several new industries in the region.</li>
<li><b>Vesper Technologies</b> was Highly Commended in the Go-to-Market Excellence category for its fully integrated go-to-market strategy around the launch of the <a href="https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/" target="_blank" rel="noopener">NVIDIA GH200 Grace Hopper Superchip</a>. The company successfully deployed a results-driven marketing campaign, demonstrated a commitment to technical training and developed a pre-sales trial and evaluation platform.</li>
<li><b>M Computers s.r.o.</b> was Highly Commended in the Go-to-Market Excellence category for its success and leadership in engaging AI customers in eastern Europe with NVIDIA technologies. The company’s marketing efforts, including speeches at AI events and social media campaigns, helped lead to the first <a href="https://www.nvidia.com/en-us/data-center/dgx-h100/" target="_blank" rel="noopener">NVIDIA DGX H100</a> and <a href="https://www.nvidia.com/en-us/data-center/grace-cpu-superchip/" target="_blank" rel="noopener">NVIDIA Grace CPU Superchip</a> projects in the region.</li>
</ul>
<h2><b>Industry Innovation </b></h2>
<ul>
<li><b>WPP</b> received the Industry Innovation award for its innovative applications of AI and NVIDIA technology in the marketing and advertising sector. The company worked with NVIDIA to build a groundbreaking generative AI-powered content engine, built on the <a href="https://www.nvidia.com/en-us/omniverse/" target="_blank" rel="noopener">NVIDIA Omniverse</a> platform, that enables the creation of brand-consistent content at scale.</li>
<li><b>Ascon Systems</b> was Highly Commended in the Industry Innovation category for its cutting-edge Industrial Metaverse Portal, powered by NVIDIA Omniverse, that helped transform BMW Group’s manufacturing processes with real-time product control and enhanced visualization and interaction.</li>
<li><b>Gcore</b> was Highly Commended in the Industry Innovation category for its creation of the first speech-to-text technology for Luxembourgish, using its fine-tuned LuxemBERT AI model. The technology integrates seamlessly into corporate systems and Luxembourgish messaging platforms, fostering the preservation of the traditionally spoken language, which lacked adequate tools for written communication.</li>
</ul>
<h2><b>Pioneer</b></h2>
<ul>
<li><b>Arrow Electronics &#8211;</b> <b>Intelligent Business Solutions</b> received the Pioneer Award for its work promoting the <a href="https://www.nvidia.com/en-us/edge-computing/products/igx/" target="_blank" rel="noopener">NVIDIA IGX Orin</a> platform for healthcare applications and building strategies to drive adoption of the technology. The company’s innovative approach and support led to the first integration of the NVIDIA IGX Orin Developer Kit with an <a href="https://www.nvidia.com/en-us/design-visualization/rtx-6000/" target="_blank" rel="noopener">NVIDIA RTX 6000 Ada Generation GPU</a> for a robotic surgery platform.</li>
</ul>
<h2><b>Consulting Partner of the Year</b></h2>
<ul>
<li><b>SoftServe</b> received the Consulting Partner of the Year award for its excellence in working with partners to drive the adoption of NVIDIA’s full-stack technologies, helping transform customers’ business with generative AI and NVIDIA Omniverse. Through its SoftServe University corporate learning hub, SoftServe trained its employees, customers and partners to expertly use NVIDIA technology.</li>
<li><b>Deloitte</b> was Highly Commended in the Consulting Partner of the Year category for its focus on building sales and technical skills, efforts to deliver meaningful impact through projects and go-to-market strategy that helped drive enterprise-level AI transformation in the region.</li>
<li><b>Data Monsters</b> was highly commended in the Consulting Partner of the Year category for its development of a virtual assistant with lifelike hearing, speech and animation capabilities using <a href="https://developer.nvidia.com/ace" target="_blank" rel="noopener">NVIDIA Avatar Cloud Engine</a> and <a href="https://www.nvidia.com/en-us/glossary/large-language-models/" target="_blank" rel="noopener">large language models</a>.</li>
</ul>
<p>​​Learn how to <a href="https://www.nvidia.com/en-us/about-nvidia/partners/become-a-partner/" target="_blank" rel="noopener">join NPN, or find a local NPN partner</a>.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/nvidiaheadquarters.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/nvidiaheadquarters-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Honors Partners of the Year in Europe, Middle East, Africa]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Seeing Beyond: Living Optics CEO Robin Wang on Democratizing Hyperspectral Imaging</title>
		<link>https://blogs.nvidia.com/blog/ai-podcast-living-optics/</link>
		
		<dc:creator><![CDATA[Kristen Yee]]></dc:creator>
		<pubDate>Wed, 17 Apr 2024 19:06:52 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[The AI Podcast]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71178</guid>

					<description><![CDATA[Step into the realm of the unseen with Robin Wang, CEO of Living Optics. The startup cofounder discusses the power of hyperspectral imaging with AI Podcast host Noah Kravitz in an episode recorded live at the NVIDIA GTC global AI conference. Living Optics’ hyperspectral imaging camera, which can capture visual data across 96 colors, reveals		<a class="read-more" href="https://blogs.nvidia.com/blog/ai-podcast-living-optics/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Step into the realm of the unseen with Robin Wang, CEO of Living Optics. The startup cofounder discusses the power of hyperspectral imaging with <a href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">AI Podcast</a> host Noah Kravitz in an episode recorded live at the <a href="https://www.nvidia.com/gtc/" target="_blank" rel="noopener">NVIDIA GTC</a> global AI conference. Living Optics’ <a href="http://demo.livingoptics.com">hyperspectral imaging camera</a>, which can capture visual data across 96 colors, reveals details invisible to the human eye. Potential applications are as diverse as monitoring plant health to detecting cracks in bridges. The startup aims to empower users across industries to gain new insights from richer, more informative datasets fueled by hyperspectral imaging technology.</p>
<p>Living Optics is a member of the <a href="https://www.nvidia.com/startups/?nvid=nv-int-tblg-295718-vt33" target="_blank" rel="noopener">NVIDIA Inception</a> program for cutting-edge startups.</p>
<p>Stay tuned for more episodes recorded live from GTC.</p>
<p><iframe loading="lazy" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/1802251500%3Fsecret_token%3Ds-oBFbqeFsrkO&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true" width="100%" height="166" frameborder="no" scrolling="no"></iframe></p>
<div style="font-size: 10px; color: #cccccc; line-break: anywhere; word-break: normal; overflow: hidden; white-space: nowrap; text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif; font-weight: 100;"><a style="color: #cccccc; text-decoration: none;" title="The AI Podcast" href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">The AI Podcast</a> · <a style="color: #cccccc; text-decoration: none;" title="Living Optics CEO Robin Wang on Democratizing Hyperspectral Imaging" href="https://soundcloud.com/theaipodcast/robin-wang/s-oBFbqeFsrkO" target="_blank" rel="noopener">Living Optics CEO Robin Wang on Democratizing Hyperspectral Imaging</a></div>
<h2><b>Time Stamps</b></h2>
<p>1:05: What is hyperspectral imaging?</p>
<p>1:45: The Living Optics camera’s ability to capture 96 colors</p>
<p>3:36: Where is hyperspectral imaging being used, and why is it so important?</p>
<p>7:19: How are hyperspectral images represented and accessed by the user?</p>
<p>9:34: Other use cases of hyperspectral imaging</p>
<p>13:07: What’s unique about Living Optics’ hyperspectral imaging camera?</p>
<p>18:36: Breakthroughs, challenges during the technology’s development</p>
<p>23:27: What’s next for Living Optics and hyperspectral imaging?</p>
<h2><b>You Might Also Like…</b></h2>
<p><a href="https://soundcloud.com/theaipodcast/gtc24-cornel-amariei-inception" target="_blank" rel="noopener"><b>Dotlumen CEO Cornel Amariei on Assisstive Technology for the Visually Impaired &#8211; Ep. 217</b><b><br />
</b><br />
</a>Dotlumen is illuminating a new technology to help people with visual impairments navigate the world. In this episode of NVIDIA’s AI Podcast, recorded live at the NVIDIA GTC global AI conference, host Noah Kravitz spoke with the Romanian startup’s founder and CEO, Cornel Amariei, about developing its flagship Dotlumen Glasses.</p>
<p><a href="https://soundcloud.com/theaipodcast/ai-wildfire" target="_blank" rel="noopener"><b>DigitalPath’s Ethan Higgins on Using AI to Fight Wildfires &#8211; Ep. 211</b></a></p>
<p>DigitalPath is igniting change in the golden state — using computer vision, generative adversarial networks and a network of thousands of cameras to detect signs of fire in real time. In the latest episode of NVIDIA’s AI Podcast, host Noah Kravtiz spoke with DigitalPath system architect Ethan Higgins about the company’s role in the ALERTCalifornia initiative, a collaboration between California’s wildfire fighting agency CAL FIRE and the University of California, San Diego.</p>
<p><a href="https://soundcloud.com/theaipodcast/mosaicml-naveen-rao" target="_blank" rel="noopener"><b>MosaicML’s Naveen Rao on Making Custom LLMs More Accessible &#8211; Ep. 199</b></a></p>
<p>Startup MosaicML is on a mission to help the AI community enhance prediction accuracy, decrease costs, and save time by providing tools for easy training and deployment of large AI models. In this episode of NVIDIA&#8217;s AI Podcast, host Noah Kravitz speaks with MosaicML CEO and co-founder Naveen Rao about how the company aims to democratize access to large language models.</p>
<p><a href="https://soundcloud.com/theaipodcast/peter-ma-ai-technosignatures" target="_blank" rel="noopener"><b>Peter Ma on Using AI to Find Promising Signals for Alien Life &#8211; Ep. 191</b></a></p>
<p>In this episode of the NVIDIA AI Podcast, host Noah Kravitz interviews Ma, an undergraduate student at the University of Toronto, about how he developed an AI algorithm that outperformed traditional methods in the search for extraterrestrial intelligence.</p>
<h2><b>Subscribe to the AI Podcast</b></h2>
<p>Get the <a href="https://blogs.nvidia.com/ai-podcast/" target="_blank" rel="noopener">AI Podcast</a> through <a href="https://itunes.apple.com/us/podcast/the-ai-podcast/id1186480811?mt=2&amp;adbsc=social_20161220_68874946&amp;adbid=811257941365882882&amp;adbpl=tw&amp;adbpr=61559439" target="_blank" rel="noopener">iTunes</a>, <a href="https://podcasts.google.com/?feed=aHR0cHM6Ly9mZWVkcy5zb3VuZGNsb3VkLmNvbS91c2Vycy9zb3VuZGNsb3VkOnVzZXJzOjI2NDAzNDEzMy9zb3VuZHMucnNz" target="_blank" rel="noopener">Google Podcasts</a>, <a href="https://play.google.com/music/listen?u=0#/ps/I4kyn74qfrsdhrm35mcrf3igxzm" target="_blank" rel="noopener">Google Play</a>, <a href="https://music.amazon.com/podcasts/956857d0-9461-4496-a07e-24be0539ee82/the-ai-podcast" target="_blank" rel="noopener">Amazon Music</a>, <a href="https://castbox.fm/channel/The-AI-Podcast-id433488?country=us" target="_blank" rel="noopener">Castbox</a>, DoggCatcher, <a href="https://overcast.fm/itunes1186480811/the-ai-podcast" target="_blank" rel="noopener">Overcast</a>, <a href="https://player.fm/series/the-ai-podcast" target="_blank" rel="noopener">PlayerFM</a>, Pocket Casts,<a href="http://www.podbay.fm/show/1186480811" target="_blank" rel="noopener"> Podbay</a>, <a href="https://www.podbean.com/podcast-detail/cjgnp-4a6e0/The-AI-Podcast" target="_blank" rel="noopener">PodBean</a>, PodCruncher, PodKicker, <a href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">Soundcloud</a>, <a href="https://open.spotify.com/show/4TB4pnynaiZ6YHoKmyVN0L" target="_blank" rel="noopener">Spotify</a>, <a href="http://www.stitcher.com/s?fid=130629&amp;refid=stpr" target="_blank" rel="noopener">Stitcher</a> and <a href="https://tunein.com/podcasts/Technology-Podcasts/The-AI-Podcast-p940829/" target="_blank" rel="noopener">TuneIn</a>.</p>
<p>Make the AI Podcast better: Have a few minutes to spare? Fill out <a href="http://survey.podtrac.com/start-survey.aspx?pubid=I5V0tOQFNS8j&amp;ver=short" target="_blank" rel="noopener">this listener survey</a>.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2018/05/ai-podcast.jpg"
			type="image/jpeg"
			width="1400"
			height="931"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2018/05/ai-podcast-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Seeing Beyond: Living Optics CEO Robin Wang on Democratizing Hyperspectral Imaging]]></media:title>
			<media:description type="html">NVIDIA AI Podcast</media:description>
			</media:content>
			</item>
		<item>
		<title>Moving Pictures: Transform Images Into 3D Scenes With NVIDIA Instant NeRF</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-instant-nerf/</link>
		
		<dc:creator><![CDATA[Jesse Clayton]]></dc:creator>
		<pubDate>Wed, 17 Apr 2024 13:00:42 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71152</guid>

					<description><![CDATA[NVIDIA RTX technology-powered AI helps turn such an imagination into reality. Using Instant NeRF, creatives are transforming collections of still images into digital 3D scenes in just seconds.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><em>Editor’s note: This post is part of the </em><a href="https://blogs.nvidia.com/blog/tag/ai-decoded/"><em>AI Decoded series</em></a><em>, which demystifies AI by making the technology more accessible, and which showcases new hardware, software, tools and accelerations for RTX PC users.</em></p>
<p>Imagine a gorgeous vista, like a cliff along the water’s edge. Even as a 2D image, the scene would be beautiful and inviting. Now imagine exploring that same view in 3D – without needing to be there.</p>
<p><a href="https://www.nvidia.com/en-us/geforce/rtx/">NVIDIA RTX</a> technology-powered AI helps turn such an imagination into reality. Using <a href="https://blogs.nvidia.com/blog/instant-nerf-research-3d-ai/">Instant NeRF</a>, creatives are <a href="https://developer.nvidia.com/blog/how-nerfs-helped-me-re-imagine-the-world/">transforming collections of still images into digital 3D scene</a>s in just seconds.</p>
<h2><strong>Simply Radiant</strong></h2>
<p>A NeRF, or neural radiance field, is an AI model that takes 2D images representing a scene as input and interpolates between them to render a complete 3D scene. The model operates as a neural network — a model that replicates how the brain is organized and is often used for tasks that require pattern recognition.</p>
<p>Using spatial location and volumetric rendering, a NeRF uses the camera pose from the images to render a 3D iteration of the scene. Traditionally, these models are computationally intensive, requiring large amounts of rendering power and time.</p>
<p>A recent NVIDIA AI research project changed that.</p>
<p><a href="https://developer.nvidia.com/blog/getting-started-with-nvidia-instant-nerfs/">Instant NeRF</a> takes NeRFs to the next level, using AI-accelerated inverse rendering to approximate how light behaves in the real world. It enables researchers to construct a 3D scene from 2D images taken at different angles. Scenes can now be generated in seconds, and the longer the NeRF model is trained, the more detailed the resulting 3D renders.</p>
<p>NVIDIA researchers released four neural graphics primitives, or <a href="https://blogs.nvidia.com/blog/what-is-a-pretrained-ai-model/">pretrained</a> datasets, as part of the <a href="https://nvlabs.github.io/instant-ngp/">Instant-NGP</a> training toolset at the SIGGRAPH computer graphics conference in 2022. The tools let anyone create NeRFs with their own data. The researchers won a <a href="https://blogs.nvidia.com/blog/instant-nerf-creators-siggraph/">best paper award</a> for the work, and <em>TIME Magazine</em> named Instant NeRF a <a href="https://developer.nvidia.com/blog/time-magazine-names-nvidia-instant-nerf-a-best-invention-of-2022/">best invention of 2022</a>.</p>
<p>In addition to speeding rendering for NeRFs, Instant NeRF makes the entire image reconstruction process accessible using NVIDIA RTX and GeForce RTX desktop and laptop GPUs. While the time it takes to render a scene depends on factors like dataset size and the mix of image and video source content, the AI training doesn’t require server-grade or cloud-based hardware.</p>
<p>NVIDIA RTX workstations and GeForce RTX PCs are ideally suited to meet the computational demands of rendering NeRFs. NVIDIA RTX and GeForce RTX GPUs feature Tensor Cores, dedicated AI hardware accelerators that provide the horsepower to run generative AI locally.</p>
<h2><strong>Ready, Set, Go</strong></h2>
<p>Get started with Instant NeRF to learn about radiance fields and experience imagery in a new way.</p>
<p>Developers and tech enthusiasts can download the source-code base to compile. Nontechnical users can download the Windows installers for <a href="https://github.com/NVlabs/instant-ngp">Instant-NGP</a><a href="https://github.com/NVlabs/instant-ngp"> software, available on GitHub</a>.</p>
<p>While the installer is available for a wide range of RTX GPUs, the program performs best on the latest-architecture <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/">GeForce RTX 40 Series</a> and <a href="https://www.nvidia.com/en-us/technologies/ada-architecture/">NVIDIA RTX Ada Generation</a> GPUs.</p>
<p><iframe loading="lazy" title="Updated: Making a NeRF movie with NVIDIA&#039;s Instant NGP" width="500" height="281" src="https://www.youtube.com/embed/3TWxO1PftMc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>The <a href="https://developer.nvidia.com/blog/getting-started-with-nvidia-instant-nerfs/">“Getting Started With Instant NeRF”</a> guide walks users through the process, including loading one of the primitives, such as “NeRF Fox,” to get a sense of what’s possible. Detailed instructions and video walkthroughs — like the one above — demonstrate how to create NeRFs with custom data, including tips for capturing good input imagery and compiling codebases (if built from source). The guide also covers using the Instant NeRF graphical user interface, optimizing scene parameters and creating an animation from the scene.</p>
<p>The NeRF community also offers many tips and tricks to help users get started. For example, check out the livestream below and this technical <a href="https://developer.nvidia.com/blog/getting-started-with-nvidia-instant-nerfs/">blog post</a>.</p>
<p><iframe loading="lazy" title="Hands on With Nvidia Instant NeRFs" width="500" height="281" src="https://www.youtube.com/embed/z3-fjYzd0BA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><strong>Show and Tell</strong></h2>
<p>Digital artists are composing beautiful scenes and telling fresh stories with NVIDIA Instant NeRF. The <a href="https://www.nvidia.com/en-us/research/ai-art-gallery/instant-nerf/">Instant NeRF gallery</a> showcases some of the most innovative and thought-provoking examples, viewable as video clips in any web browser.</p>
<p>Here are a few:</p>
<ul>
<li><strong>“Through the Looking Glass” by Karen X. Cheng and James Perlman </strong>A pianist practices her song, part of her daily routine, though there’s nothing mundane about what happens next. The viewer peers into the mirror, a virtual world that can be observed but not traversed; it’s unreachable by normal means. Then, crossing the threshold, it’s revealed that this mirror is in fact a window into an inverted reality that can be explored from within. Which one is real?</li>
</ul>
<div style="width: 1280px;" class="wp-video"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video class="wp-video-shortcode" id="video-71152-1" width="1280" height="680" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-ai-decoded-karenx-v9-1280x680-1.mp4?_=1" /><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-ai-decoded-karenx-v9-1280x680-1.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-ai-decoded-karenx-v9-1280x680-1.mp4</a></video></div>
<p>&nbsp;</p>
<ul>
<li><strong>“Meditation” by Franc Lucent </strong>As soon as they walked into one of many rooms in Nico Santucci’s estate, Lucent knew they needed to turn it into a NeRF. Playing with the dynamic range and reflections in the pond, it presented the artist with an unknown exploration. They were pleased with the softness of the light and the way the NeRF elevates the room into what looks like something out of a dream — the perfect place to meditate. A NeRF can freeze a moment in a way that’s more immersive than a photo or video.</li>
</ul>
<div style="width: 1280px;" class="wp-video"><video class="wp-video-shortcode" id="video-71152-2" width="1280" height="680" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-ai-decoded-franc-lucent-meditation-1280x680-1.mp4?_=2" /><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-ai-decoded-franc-lucent-meditation-1280x680-1.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-ai-decoded-franc-lucent-meditation-1280x680-1.mp4</a></video></div>
<p>&nbsp;</p>
<ul>
<li><strong>“Zeus” by Hugues Bruyère </strong>These rendered 3D scenes with Instant NeRF use the data Bruyère previously captured for traditional photogrammetry using mirrorless digital cameras, smartphones, 360-degree cameras and drones. Instant NeRF gives him a powerful tool to help preserve and share cultural artifacts through online libraries, museums, virtual-reality experiences and heritage-conservation projects. This NeRF was trained using a dataset of photos taken with an iPhone at the Royal Ontario Museum.</li>
</ul>
<div style="width: 1280px;" class="wp-video"><video class="wp-video-shortcode" id="video-71152-3" width="1280" height="680" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-ai-decoded-hugues-zeus-1280x680-1.mp4?_=3" /><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-ai-decoded-hugues-zeus-1280x680-1.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-ai-decoded-hugues-zeus-1280x680-1.mp4</a></video></div>
<p>&nbsp;</p>
<h2><strong>From Image to Video to Reality</strong></h2>
<p>Transforming images into a 3D scene with AI is cool. Stepping into that 3D creation is next level.</p>
<p>Thanks to a recent Instant NeRF update, users can render their scenes from static images and virtually step inside the environments, moving freely within the 3D space. In virtual-reality (VR) environments, users can feel complete immersion into new worlds, all within their headsets.</p>
<p>The potential benefits are nearly endless.</p>
<p>For example, a realtor can create and share a 3D model of a property, offering virtual tours at new levels. Retailers can showcase products in an online shop, powered by a collection of images and AI running on RTX GPUs. These AI models power creativity and are helping drive the accessibility of 3D immersive experiences across other industries.</p>
<p>Instant NeRF comes with the capability to clean up scenes easily in VR, making the creation of high-quality NeRFs more intuitive than ever. Learn more about <a href="https://developer.nvidia.com/blog/turn-2d-images-into-immersive-3d-scenes-with-nvidia-instant-nerf-in-vr/">navigating Instant NeRF spaces in VR</a>.</p>
<p>Download Instant-NGP to get started, and share your creations on social media with the hashtag #InstantNeRF.</p>
<p><em>Generative AI is transforming gaming, videoconferencing and interactive experiences of all kinds. Make sense of what’s new and what’s next by subscribing to the </em><a href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><em>AI Decoded newsletter</em></a><em>.</em></p>
]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-ai-decoded-karenx-v9-1280x680-1.mp4" length="1719192" type="video/mp4" />
<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-ai-decoded-franc-lucent-meditation-1280x680-1.mp4" length="1717035" type="video/mp4" />
<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-ai-decoded-hugues-zeus-1280x680-1.mp4" length="1738082" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/NeRF-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/NeRF-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Moving Pictures: Transform Images Into 3D Scenes With NVIDIA Instant NeRF]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>New NVIDIA RTX A400 and A1000 GPUs Enhance AI-Powered Design and Productivity Workflows</title>
		<link>https://blogs.nvidia.com/blog/ampere-rtx-a400-a1000-ai/</link>
		
		<dc:creator><![CDATA[Stacy Ozorio]]></dc:creator>
		<pubDate>Tue, 16 Apr 2024 16:00:41 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[Rendering]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71084</guid>

					<description><![CDATA[AI integration across design and productivity applications is becoming the new standard, fueling demand for advanced computing performance. This means professionals and creatives will need to tap into increased compute power, regardless of the scale, complexity or scope of their projects. To meet this growing need, NVIDIA is expanding its RTX professional graphics offerings with		<a class="read-more" href="https://blogs.nvidia.com/blog/ampere-rtx-a400-a1000-ai/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>AI integration across design and productivity applications is becoming the new standard, fueling demand for advanced computing performance. This means professionals and creatives will need to tap into increased compute power, regardless of the scale, complexity or scope of their projects.</p>
<p>To meet this growing need, NVIDIA is expanding its <a href="https://www.nvidia.com/en-us/design-visualization/desktop-graphics/">RTX professional graphics</a> offerings with two new <a href="https://www.nvidia.com/en-us/design-visualization/ampere-architecture/">NVIDIA Ampere architecture</a>-based GPUs for desktops: the NVIDIA RTX A400 and NVIDIA RTX A1000.</p>
<p>They expand access to AI and ray-tracing technology, equipping professionals with the tools they need to transform their daily workflows.</p>
<h2><b>A New Era of Creativity, Performance and Efficiency</b></h2>
<p>The RTX A400 GPU introduces accelerated ray tracing and AI to the RTX 400 series GPUs. With 24 Tensor Cores for AI processing, it surpasses traditional CPU-based solutions, enabling professionals to run cutting-edge AI applications, such as intelligent chatbots and copilots, directly on their desktops.</p>
<p>The GPU delivers real-time ray tracing so creators can build vivid, physically accurate 3D renders that push the boundaries of creativity and realism.</p>
<p>The A400 also includes four display outputs, a first for its series. This makes it ideal for high-density display environments, which are critical for industries like financial services, command and control, retail, and transportation.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A400.jpg"><img loading="lazy" decoding="async" class="aligncenter size-large wp-image-71213" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A400-672x378.jpg" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A400-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A400-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A400-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A400-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A400-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A400-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A400-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A400-1280x720.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A400.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>The NVIDIA RTX A1000 GPU brings Tensor Cores and RT Cores to the RTX 1000 series GPUs for the first time, unlocking accelerated AI and ray-tracing performance for creatives and professionals.</p>
<p>With 72 Tensor Cores, the A1000 offers a tremendous upgrade over the previous generation, delivering over 3x faster generative AI processing for tools like Stable Diffusion. In addition, its 18 RT Cores speed graphics and rendering tasks by up to 3x, accelerating professional workflows such as 2D and 3D computer-aided design (CAD), product and architectural design, and 4K video editing.</p>
<p>The A1000 also excels in video processing, handling up to 38% more encode streams and offering 2x faster decode performance over the previous generation.</p>
<p>With a sleek, single-slot design and consuming just 50W, the A400 and A1000 GPUs bring impressive features to compact, energy-efficient workstations.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A1000-Copy-2.jpg"><img loading="lazy" decoding="async" class="aligncenter size-large wp-image-71171" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A1000-Copy-2-672x357.jpg" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A1000-Copy-2-672x357.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A1000-Copy-2-400x213.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A1000-Copy-2-768x408.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A1000-Copy-2-842x447.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A1000-Copy-2-406x215.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A1000-Copy-2-188x100.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A1000-Copy-2.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>&nbsp;</p>
<h2><b>Expanding the Reach of RTX</b></h2>
<p>These new GPUs empower users with cutting-edge AI, graphics and compute capabilities to boost productivity and unlock creative possibilities. Advanced workflows involving ray-traced renders and AI are now within reach, allowing professionals to push the boundaries of their work and achieve stunning levels of realism.</p>
<p>Industrial planners can use ‌these new powerful and energy-efficient computing solutions for <a href="https://blogs.nvidia.com/blog/what-is-edge-computing/">edge</a> deployments. Creators can boost editing and rendering speeds to produce richer visual content. Architects and engineers can seamlessly transition ideas from 3D CAD concepts into tangible designs. Teams working in smart spaces can use the GPUs for real-time data processing, AI-enhanced security and digital signage management in space-constrained settings. And healthcare professionals can achieve quicker, more precise medical imaging analyses.</p>
<p>Financial professionals have always used expansive, high-resolution visual workspaces for more effective trading, analysis and data management. With the RTX A400 GPU supporting up to four 4K displays natively, financial services users can now achieve a high display density with fewer GPUs, streamlining their setups and reducing costs.</p>
<h2><b>Next-Generation Features and Accelerated Performance </b></h2>
<p>The NVIDIA RTX A400 and A1000 GPUs are equipped with features designed to supercharge everyday workflows, including:</p>
<ul>
<li><b>Second-generation RT Cores:</b> Real-time ray tracing, photorealistic, physically based rendering and visualization for all professional workflows, including architectural drafting, 3D design and content creation, where accurate lighting and shadow simulations can greatly enhance the quality of work.</li>
<li><b>Third-generation Tensor Cores: </b>Accelerates AI-augmented tools and applications such as generative AI, image rendering denoising and deep learning super sampling to improve image generation speed and quality.</li>
<li><b>Ampere architecture-based CUDA cores: </b>Up to 2x the single-precision floating point throughput of the previous generation for significant speedups in graphics and compute workloads<b>.</b></li>
<li><b>4GB or 8GB of GPU memory: </b>4GB of GPU memory with the A400 GPU and 8GB with the A1000 GPU accommodate a range of professional needs, from basic graphic design and photo editing to more demanding 3D modeling with textures or high-resolution editing and data analyses. The GPUs also feature increased memory bandwidth over the previous generation for quicker data processing and smoother handling of larger datasets and scenes.</li>
<li><b>Encode and decode engines: </b>With seventh-generation encode (<a href="https://developer.nvidia.com/video-codec-sdk">NVENC</a>) and fifth-generation decode (NVDEC) engines, the GPUs offer efficient video processing to support high-resolution video editing, streaming and playback with ultra-low latency. Inclusion of AV1 decode enables higher efficiency and smoother playback of more video formats.</li>
</ul>
<h2><b>Availability</b><b> </b></h2>
<p>The NVIDIA RTX A1000 GPU is now available through global distribution partners such as PNY and Ryoyo Electric. The RTX A400 GPU is expected to be available from channel partners starting in May, with anticipated availability from manufacturers in the summer.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A1000.jpg"
			type="image/jpeg"
			width="2048"
			height="1152"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX-A1000-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[New NVIDIA RTX A400 and A1000 GPUs Enhance AI-Powered Design and Productivity Workflows]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>To Cut a Long Story Short: Video Editors Benefit From DaVinci Resolve’s New AI Features Powered by RTX</title>
		<link>https://blogs.nvidia.com/blog/studio-davinci-resolve-sketchup-driver/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Tue, 16 Apr 2024 13:00:54 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[In the NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA Studio Driver]]></category>
		<category><![CDATA[Rendering]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71090</guid>

					<description><![CDATA[Blackmagic Design’s DaVinci Resolve released version 19 adding the IntelliTrack AI point tracker and UltraNR AI-powered features to further streamline video editing workflows.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><i>Editor’s note: This post is part of our </i><a href="https://blogs.nvidia.com/blog/tag/in-the-nvidia-studio/"><i>In the NVIDIA Studio</i></a><i> series, which celebrates featured artists, offers creative tips and tricks, and demonstrates how </i><a href="https://www.nvidia.com/en-us/studio/"><i>NVIDIA Studio</i></a><i> technology improves creative workflows. We’re also deep diving on new </i><a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/"><i>GeForce RTX 40 Series GPU</i></a><i> features, technologies and resources, and how they dramatically accelerate content creation.</i></p>
<p>Video editors have more to look forward to than just April showers.</p>
<p>Blackmagic Design’s DaVinci Resolve released version 19, adding the IntelliTrack AI point tracker and UltraNR AI-powered features to further streamline video editing workflows.</p>
<p>The NAB 2024 trade show is bringing together thousands of content professionals from all corners of the broadcast, media and entertainment industries, with video editors and livestreamers seeking ways to improve their creative workflows with <a href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX</a> technology.</p>
<p>The recently launched Design app SketchUp 2024 introduced a new graphics engine that uses DirectX 12, which renders scenes 2.5x faster than the previous engine.</p>
<p>April also brings the latest NVIDIA Studio Driver, which optimizes the latest creative app updates, available for download today.</p>
<p>And this week’s featured <i>In the NVIDIA Studio </i>artist Rakesh created his captivating 3D scene <i>The Rooted Vault</i> using RTX acceleration.</p>
<h2><b>Video Editor’s DaVinci Code</b></h2>
<p>DaVinci Resolve is a powerful video editing package with color correction, visual effects, motion graphics and audio post-production all in one software tool. Its elegant, modern interface is easy to learn for new users, while offering powerful capabilities for professionals.</p>
<p>Two new AI features make video editing even more efficient: the IntelliTrack AI point tracker for object tracking, stabilization and audio panning, and UltraNR, which uses AI for spatial noise reduction — doing so 3x faster on the GeForce RTX 4090 vs. the Mac M2 Ultra.</p>
<p>All DaVinci Resolve AI effects are accelerated on RTX GPUs by <a href="https://developer.nvidia.com/tensorrt">NVIDIA TensorRT</a>, boosting AI performance by up to 2x. The update also includes acceleration for Beauty, Edge Detect and Watercolor effects, doubling performance on NVIDIA GPUs.</p>
<p>For more information, check out the DaVinci Resolve <a href="https://www.blackmagicdesign.com/products/davinciresolve">website</a>.</p>
<h2><b>SketchUp Steps Up</b></h2>
<p>SketchUp 2024 is a professional-grade 3D design software toolkit for designing buildings and landscapes, commonly used by designers and architects.</p>
<p>The new app, already receiving positive reviews, introduced a robust graphics engine that uses DirectX 12, which increases frames per second (FPS) by a factor of 2.5x over the previous engine. Navigating and orbiting complex models feels considerably lighter and faster with quicker, more predictable performance.</p>
<p>In testing, the scene below runs 4.5x faster FPS using the NVIDIA RTX 4090 vs. the Mac M2 Ultra and other competitors.</p>
<figure id="attachment_71119" aria-describedby="caption-attachment-71119" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1.png"><img loading="lazy" decoding="async" class="size-large wp-image-71119" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71119" class="wp-caption-text">2.5x faster FPS with the GeForce RTX 4090 GPU. Image courtesy of Trimble SketchUp.</figcaption></figure>
<p>SketchUp 2024 also unlocks import and export functionality for <a href="https://www.nvidia.com/en-us/omniverse/usd/">OpenUSD</a> files to efficiently manage the interoperability of complex 3D scenes and animations across numerous 3D apps.</p>
<p>Get the <a href="https://blog.sketchup.com/article/unlock-enhanced-visuals-inside-sketchup-2024-2">full release details</a>.</p>
<h2><b>Art Rooted in Nature</b></h2>
<p>Rakesh’s passion for 3D modeling and animation stemmed from his love for gaming and storytelling.</p>
<p>“My goal is to inspire audiences and take them to new realms by showcasing the power of immersive storytelling, captivating visuals and the idea of creating worlds and characters that evoke emotions,” said Rakesh.</p>
<p>His scene <i>The Rooted Vault</i> aims to convey the beauty of the natural world, transporting viewers to a serene setting filled with the soothing melodies of nature.</p>
<div style="width: 1280px;" class="wp-video"><video class="wp-video-shortcode" id="video-71090-4" width="1280" height="734" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-breakdown-video-1280w.mp4?_=4" /><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-breakdown-video-1280w.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-breakdown-video-1280w.mp4</a></video></div>
<p>&nbsp;</p>
<p>Rakesh began by gathering reference material.</p>
<figure id="attachment_71103" aria-describedby="caption-attachment-71103" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w.png"><img loading="lazy" decoding="async" class="size-large wp-image-71103" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71103" class="wp-caption-text">There’s reference sheets … and then there’s reference sheets.</figcaption></figure>
<p>He then used Autodesk Maya to block out the basic structure and piece together the house as a series of modules. GPU-accelerated viewport graphics ensured fast, interactive 3D modeling and animations.</p>
<p>Next, Rakesh used ZBrush to sculpt high-resolution details into the modular assets.</p>
<figure id="attachment_71115" aria-describedby="caption-attachment-71115" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w.png"><img loading="lazy" decoding="async" class="size-large wp-image-71115" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71115" class="wp-caption-text">Fine details applied in ZBrush.</figcaption></figure>
<div class="simplePullQuote right"><p>“I chose an NVIDIA RTX GPU-powered system for real-time ray tracing to achieve lifelike visuals, reliable performance for smoother workflows, faster render times and industry-standard software compatibility.” — Rakesh</p>
</div>
<p>He used the ZBrush decimation tool alongside Unreal Engine’s Nanite workflow to efficiently create most of the modular building props.</p>
<p>Traditional poly-modeling workflows for the walls enabled vertex blending shaders for seamless texture transitions.</p>
<p>Textures were created with Adobe Substance 3D Painter. Rakesh’s RTX GPU used RTX-accelerated light and ambient occlusion to bake and optimize assets in mere seconds.</p>
<p>Rakesh moved the project to Unreal Engine 5, where near-final finishing touches such as lighting, shadows and visual effects were applied.</p>
<figure id="attachment_71112" aria-describedby="caption-attachment-71112" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w.png"><img loading="lazy" decoding="async" class="size-large wp-image-71112" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71112" class="wp-caption-text">Textures applied in Adobe Substance 3D Painter.</figcaption></figure>
<p>GPU acceleration played a crucial role in real-time rendering, allowing him to instantly see and adjust the scene.</p>
<figure id="attachment_71125" aria-describedby="caption-attachment-71125" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1.png"><img loading="lazy" decoding="async" class="size-large wp-image-71125" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71125" class="wp-caption-text">Adobe Premiere Pro has a vast selection of GPU-accelerated features.</figcaption></figure>
<p>Rakesh then moved to Blackmagic Design’s DaVinci Resolve to color grade the scene for the desired mood and aesthetic, before he began final editing in Premiere Pro, adding transitions and audio.</p>
<p>“While the initial concept required significant revisions, the final result demonstrates the iterative nature of artistic creation — all inspired by my mentors, friends and family, who were always there to support me,” Rakesh said.</p>
<figure id="attachment_71122" aria-describedby="caption-attachment-71122" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1.png"><img loading="lazy" decoding="async" class="size-large wp-image-71122" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-672x218.png" alt="" width="672" height="218" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-672x218.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-400x130.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-768x249.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-842x273.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-406x132.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-188x61.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71122" class="wp-caption-text">3D artist Rakesh Kumar.</figcaption></figure>
<p>Check out Rakesh’s latest work on <a href="https://www.instagram.com/rakesh__art/">Instagram</a>.</p>
<p><i>Follow NVIDIA Studio on </i><a href="https://www.instagram.com/nvidiastudio/"><i>Instagram</i></a><i>, </i><a href="https://twitter.com/NVIDIAStudio"><i>X</i></a><i> and </i><a href="https://www.facebook.com/NVIDIAStudio/"><i>Facebook</i></a><i>. Access tutorials on the </i><a href="https://www.youtube.com/channel/UCDeQdW6Lt6nhq3mLM4oLGWw"><i>Studio YouTube channel</i></a><i> and get updates directly in your inbox by subscribing to the </i><a href="https://www.nvidia.com/en-us/studio/?nvmid=subscribe-creators-mail-icon"><i>Studio newsletter</i></a><i>.</i><b> </b></p>
]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-breakdown-video-1280w.mp4" length="1833156" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/roots-nv-blog-header-preview-1280x680-2.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/roots-nv-blog-header-preview-1280x680-2-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[To Cut a Long Story Short: Video Editors Benefit From DaVinci Resolve’s New AI Features Powered by RTX]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>AI Is Tech’s ‘Greatest Contribution to Social Elevation,’ NVIDIA CEO Tells Oregon State Students</title>
		<link>https://blogs.nvidia.com/blog/oregon-state-higher-ed/</link>
		
		<dc:creator><![CDATA[Brian Caulfield]]></dc:creator>
		<pubDate>Mon, 15 Apr 2024 13:00:59 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Education]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71074</guid>

					<description><![CDATA[Amid a flurry of investments in higher education, NVIDIA founder Jensen Huang highlights potential for AI to serve as a “collaborator” as alma mater breaks ground on $213 million research complex. ]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>AI promises to bring the full benefits of the digital revolution to billions across the globe, NVIDIA CEO Jensen Huang said Friday during a conversation with Oregon State University President Jayathi Murthy.</p>
<p>“I believe that artificial intelligence is the technology industry’s single greatest contribution to social elevation, to lift all of the people that have historically been left behind,” Huang told more than 2,000 faculty, students and staff gathered for his conversation with Murthy.</p>
<p>The talk was the highlight of a forum marking the groundbreaking for a new research building that will be named for Huang and his wife, Lori, both Oregon State alumni.</p>
<p>The facility positions Oregon State as a leader not just in the semiconductor industry but also at the intersection of high performance computing and a growing number of fields.</p>
<figure id="attachment_71079" aria-describedby="caption-attachment-71079" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="wp-image-71079 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-672x448.png" alt="" width="672" height="448" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-672x448.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-400x267.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-768x512.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-1536x1024.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-675x450.png 675w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-323x215.png 323w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-150x100.png 150w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-1280x853.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking.png 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-71079" class="wp-caption-text">Friday’s event at Oregon State University followed the groundbreaking for the Jen-Hsun Huang and Lori Mills Huang Collaborative Innovation Complex. Image courtesy of Oregon State University.</figcaption></figure>
<p>Those innovations have world-changing implications.</p>
<p>Huang said those who know a programming language such as C++ typically have greater opportunities.</p>
<p>“Because programming is so hard, the number of people who have benefitted from this, putting it to use for their economic prosperity, has been limited,” Huang said.</p>
<p>AI unlocks that and more.</p>
<p>“So you essentially have a collaborator with you at all times, essentially have a tutor at all times, and so I think the ability for AI to elevate all of the people left behind is quite extraordinary,” he added.</p>
<p>Huang’s appearance in Corvallis, Oregon, capped off a week of announcements underscoring NVIDIA’s commitment to preparing the future workforce with advanced AI, data science and high performance computing training.</p>
<p>On Tuesday, NVIDIA announced that it would participate in a <a href="https://blogs.nvidia.com/blog/partnership-universities-teach-ai-skills/">$110 million partnership between Japan and the United States</a>, which would include funding for university research.</p>
<p>On Wednesday, <a href="https://coe.gatech.edu/news/2024/04/georgia-tech-unveils-new-ai-makerspace-collaboration-nvidia">Georgia Tech announced a new NVIDIA-powered supercomputer</a> that will help prepare undergraduate students to solve complex challenges with AI and HPC.</p>
<p>And later this month, NVIDIA founder Chris Malachowsky will be inducted into the Hall of Fame for the University of Florida’s Department of Electrical &amp; Computer Engineering, following the November inauguration of the university’s $150 million Malachowsky Hall for Data Science &amp; Information Technology.</p>
<h2>Educating Future Leaders for ‘New Industrial Revolution’</h2>
<p>NVIDIA has been investing in universities for decades, providing computing resources, advanced training curricula, donations and other support.</p>
<p>These contributions enable students and professors to access the high performance computing necessary for groundbreaking results at a key moment in the history of the industry.</p>
<p>“We’re at the beginning of a new industrial revolution, and the reason why I say that is because an industrial revolution produces something new that was impossible to produce in the past,” Huang said.</p>
<p>“And in this new world, you can apply electricity, and what’s going to come out of it is a whole bunch of floating-point numbers. We call them tokens, and those tokens are essentially artificial intelligence,” Huang said.</p>
<p>“And so this industrial revolution is going to be manufacturing intelligence at a very large scale,” Huang said.</p>
<h2>OSU Breaks Ground on $213 Million Research Complex</h2>
<p>Friday’s event in Oregon highlighted the Huangs’ commitment to education and reflected the couple’s deep personal ties to Oregon State, where the two met.</p>
<p>The conversation with Murthy followed the groundbreaking for the Jen-Hsun Huang and Lori Mills Huang Collaborative Innovation Complex, which took place Friday morning on the Corvallis campus.</p>
<p>When it opens in 2026, the 150,000-square-foot, $213 million complex — supported by a $50 million gift from the Huangs — will increase Oregon State’s support for the semiconductor and technology industry in Oregon and beyond.</p>
<p>Harnessing one of the nation’s most powerful NVIDIA supercomputers, the complex will bring together faculty and students to solve critical challenges facing the world in areas such as climate science, clean energy and water resources.</p>
<p>Huang sees the center — and AI — as helping put the benefits of computing at the service of people doing work across a broad range of disciplines.</p>
<p>Oregon State is one of the world’s premier schools in forestry, Huang said, adding that “let’s just face it, it’s very unlikely that somebody who was in forestry, it’s not impossible, but C++ is probably not your thing,” Huang said.</p>
<p>Thanks to ChatGPT, you can “now use a computer to apply it to your field of science and apply this computing technology to revolutionize your work.”</p>
<p>That makes learning how to think — and how to collaborate — more important than ever, Huang said. It’s “no different than if I gave you a partner to collaborate with you to solve problems,” Huang said.</p>
<p>“You still need to know how to collaborate, how to prompt, how to frame a problem, how to refine the solution, how to iterate on it and how to change your mind.”</p>
<h2>NVIDIA Joins $110 Million Partnership to Help Universities Teach AI Skills</h2>
<p>The groundbreaking at Oregon State is just one of several announcements highlighting NVIDIA’s global commitment to advancing the global technology industry.</p>
<p>Last week, the Biden Administration announced a new $110 million AI partnership between Japan and the United States, including an initiative to fund research through collaboration between the University of Washington and the University of Tsukuba.</p>
<p>As part of this, NVIDIA is committing $25 million to a collaboration with Amazon to bring the latest technologies to the University of Washington, in Seattle, and the University of Tsukuba, northeast of Tokyo.</p>
<h2>Georgia Tech Unveils New AI Makerspace in Collaboration With NVIDIA</h2>
<p>And on Wednesday, Georgia Tech’s College of Engineering established an AI supercomputing hub dedicated to teaching students.</p>
<p>The AI Makerspace was launched in collaboration with NVIDIA. College leaders call it a “digital sandbox” for students to understand and use AI. Initially focusing on undergraduates, the AI Makerspace aims to democratize access to computing resources typically reserved for researchers or technology companies.</p>
<p>Students will access the cluster online as part of their coursework. The Makerspace will also better position students after graduation as they work with AI professionals and help shape future applications.</p>
<h2>‘Beginning of a New World’</h2>
<p>To be sure, AI has limits, Huang explained. “It’s no different than when you work with teammates or lab partners; you’re guiding each other along because you know each other’s weaknesses and strengths,” he said.</p>
<p>However, Huang said now is a fantastic time to get an education and prepare for a career.</p>
<p>“This is the beginning of a new world and this is the best of times to go to school — the whole world is changing, right? New technology and new capabilities, new instruments and new ways to learn,” Huang said.</p>
<p><em>Images courtesy of Oregon State University.</em></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/oregon-state.jpg"
			type="image/jpeg"
			width="2048"
			height="1215"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/oregon-state-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[AI Is Tech’s ‘Greatest Contribution to Social Elevation,’ NVIDIA CEO Tells Oregon State Students]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Bethesda’s ‘Fallout’ Titles Join GeForce NOW</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-fallout-series/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 11 Apr 2024 13:00:16 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71056</guid>

					<description><![CDATA[Welcome to the wasteland, Vault Dwellers. Bethesda’s Fallout 4 and Fallout 76 are bringing post-nuclear adventures to the cloud. These highly acclaimed action role-playing games lead 10 new titles joining GeForce NOW this week. Announced as coming to GeForce NOW at CES, Honkai: Star Rail is targeting a release this quarter. Stay tuned for future		<a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-fallout-series/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Welcome to the wasteland, Vault Dwellers. Bethesda’s <i>Fallout 4</i> and <i>Fallout 76</i> are bringing post-nuclear adventures to the cloud.</p>
<p>These highly acclaimed action role-playing games lead 10 new titles joining <a href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a> this week.</p>
<p>Announced as <a href="https://blogs.nvidia.com/blog/ces-2024-geforce-now-activision-blizzard-day-passes-g-sync/">coming to GeForce NOW at CES</a>, <i>Honkai: Star Rail</i> is targeting a release this quarter. Stay tuned for future updates.</p>
<h2><b>Vault Into the Cloud</b></h2>
<p>Adventurers needed, whether for mapping the irradiated wasteland or shaping the fate of humanity.</p>
<figure id="attachment_71060" aria-describedby="caption-attachment-71060" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-71060" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-672x378.jpg" alt="Fallout 4 on GeForce NOW" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-1280x720.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4.jpg 1920w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-71060" class="wp-caption-text"><em>Don’t let Dogmeat venture out alone.</em></figcaption></figure>
<p>Embark on a journey through ruins of the post-apocalyptic Commonwealth in <i>Fallout 4</i>. As the sole survivor of Vault 111, navigate a world destroyed by nuclear war, make choices to reshape the wasteland and rebuild society one settlement at a time. With a vast, open world, dynamic crafting systems and a gripping storyline, the game offers an immersive single-player experience that challenges dwellers to emerge as beacons of hope for humanity’s remnants.</p>
<figure id="attachment_71063" aria-describedby="caption-attachment-71063" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-71063" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-672x378.jpg" alt="Fallout 76 on GeForce NOW" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-71063" class="wp-caption-text"><em>Dust off your Pip-Boy and stream &#8216;Fallout 76&#8217; from the cloud.</em></figcaption></figure>
<p>Plus, in <i>Fallout 76</i>, head back to the early days of post-nuclear Appalachia and experience the <i>Fallout</i> universe’s largest, most dynamic world. Encounter unique challenges, build portable player homes called C.A.M.P.s, and cooperate or compete with other survivors in the mountainous lands in West Virginia.</p>
<p>Join the proud ranks of Vault survivors in the cloud today and stream these titles, including Creation Club content for <i>Fallout 4</i>, across devices. With longer gaming sessions and faster access to servers, GeForce NOW members can play anywhere, anytime, and at up to 4K resolution, streaming with an <a href="http://geforcenow.com">Ultimate membership</a>. The games come just in time for those tuning into the <i>Fallout </i>series TV adaptation, released today, for a <i>Fallout</i>-filled week.</p>
<h2><b>Go Big or Go Home</b></h2>
<figure id="attachment_71066" aria-describedby="caption-attachment-71066" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-71066" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-672x336.jpg" alt="Gigantic: Rampage Edition on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-71066" class="wp-caption-text"><em>Larger than life MOBA now streaming on GeForce NOW.</em></figcaption></figure>
<p><i>Gigantic: Rampage Edition </i>promises big fun with epic 5v5 matches, crossplay support, an exciting roster of heroes and more. Rush to the cloud to jump into the latest game from Arc Games and team with four other players to control objectives and take down the opposing team’s mighty Guardian. Think fast, be bold and go gigantic!</p>
<p>Look forward to these new games this week:</p>
<ul>
<li><i>Gigantic: Rampage Edition </i>(New release on <a href="https://store.steampowered.com/app/1924490?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 9)</li>
<li><i>Inkbound 1.0</i> (New release, on <a href="https://store.steampowered.com/app/1062810/Inkbound/">Steam</a>, April 9)</li>
<li><i>Broken Roads </i>(New release on <a href="https://store.steampowered.com/app/1403440?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 10)</li>
<li><i>Infection Free Zone </i>(New release on <a href="https://store.steampowered.com/app/1465460?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 11)</li>
<li><i>Shadow of the Tomb Raider: Definitive Edition </i>(New release on <a href="https://www.xbox.com/games/store/shadow-of-the-tomb-raider-definitive-edition/BNQQ3WVBNZCQ?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass, April 11)</li>
<li><i>Backpack Battles </i>(<a href="https://store.steampowered.com/app/2427700?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Fallout 4 </i>(<a href="https://store.steampowered.com/app/377160?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Fallout 76 </i>(<a href="https://store.steampowered.com/app/1151340?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a href="https://www.xbox.com/games/store/fallout-76-pc/9nkgnmnk3k3z?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Ghostrunner </i>(<a href="https://www.epicgames.com/store/p/ghostrunner?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>, free April 11-18)</li>
<li><i>Terra Invicta</i> (<a href="https://www.xbox.com/games/store/terra-invicta-game-preview/9pb0glgxqz83?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
</ul>
<p>What are you planning to play this weekend? Let us know on <a href="https://www.twitter.com/nvidiagfn">X</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">can&#39;t help 𝙛𝙖𝙡𝙡ing into the cloud</p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f329.png" alt="🌩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a href="https://twitter.com/NVIDIAGFN/status/1778090566644801863?ref_src=twsrc%5Etfw">April 10, 2024</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-April_11.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-April_11-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Bethesda’s ‘Fallout’ Titles Join GeForce NOW]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Combating Corruption With Data: Cleanlab and Berkeley Research Group on Using AI-Powered Investigative Analytics  </title>
		<link>https://blogs.nvidia.com/blog/cleanlab-podcast/</link>
		
		<dc:creator><![CDATA[Brian Caulfield]]></dc:creator>
		<pubDate>Wed, 10 Apr 2024 13:00:49 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71052</guid>

					<description><![CDATA[Talk about scrubbing data. Curtis Northcutt, cofounder and CEO of Cleanlab, and Steven Gawthorpe, senior data scientist at Berkeley Research Group, speak about Cleanlab’s groundbreaking approach to data curation with Noah Kravitz, host of NVIDIA’s AI Podcast, in an episode recorded live at the NVIDIA GTC global AI conference. The startup’s tools enhance data reliability		<a class="read-more" href="https://blogs.nvidia.com/blog/cleanlab-podcast/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Talk about scrubbing data. Curtis Northcutt, cofounder and CEO of Cleanlab, and Steven Gawthorpe, senior data scientist at Berkeley Research Group, speak about Cleanlab’s groundbreaking approach to data curation with Noah Kravitz, host of NVIDIA’s <a href="https://soundcloud.com/theaipodcast">AI Podcast</a>, in an episode recorded live at the <a href="https://www.nvidia.com/gtc/">NVIDIA GTC</a> global AI conference. The startup’s tools enhance data reliability and trustworthiness through sophisticated error identification and correction algorithms. Northcutt and Gawthorpe provide insights into how AI-powered data analytics can help combat economic crimes and corruption and discuss the intersection of AI, data science and ethical governance in fostering a more just society.</p>
<p>Cleanlab is a member of the <a href="https://www.nvidia.com/startups/?nvid=nv-int-tblg-295718-vt33">NVIDIA Inception</a> program for cutting-edge startups.</p>
<p>Stay tuned for more episodes recorded live from GTC.</p>
<p><iframe loading="lazy" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/1797413893%3Fsecret_token%3Ds-bfbkVACWPn4&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true" width="100%" height="166" frameborder="no" scrolling="no"></iframe></p>
<div style="font-size: 10px; color: #cccccc; line-break: anywhere; word-break: normal; overflow: hidden; white-space: nowrap; text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif; font-weight: 100;"><a style="color: #cccccc; text-decoration: none;" title="The AI Podcast" href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">The AI Podcast</a> · <a style="color: #cccccc; text-decoration: none;" title="Cleanlab's Curtis Northcutt and Berkeley Research Group's Steven Gawthorpe on AI for Fighting Crime" href="https://soundcloud.com/theaipodcast/cleanlabs-curtis-northcutt-and-berkeley-research-groups-steven-gawthorpe-on-ai-for-fighting-crime-ep-218/s-bfbkVACWPn4" target="_blank" rel="noopener">Cleanlab&#8217;s Curtis Northcutt and Berkeley Research Group&#8217;s Steven Gawthorpe on AI for Fighting Crime</a></div>
<h2><b>Time Stamps</b></h2>
<p>1:05: Northcutt on Cleanlab’s inception and mission<br />
2:41: What Cleanlab offers its customers<br />
4:24: The human element in Cleanlab’s data verification<br />
8:57: Gawthorpe on the core functions, aims of the Berkeley Research Group<br />
10:42: Gawthorpe’s approach to data collection and analysis in fraud investigations<br />
16:38: Cleanlab’s one-click solution for generating machine learning models<br />
18:30: The evolution of machine learning and its impact on data analytics<br />
20:07: Future directions in data-driven crimefighting</p>
<h2><b>You Might Also Like…</b></h2>
<p><a href="https://soundcloud.com/theaipodcast/legal"><b>The Case for Generative AI in the Legal Field &#8211; Ep. 210</b><b><br />
</b><br />
</a>Thomson Reuters, the global content and technology company, is transforming the legal industry with generative AI. In the latest episode of NVIDIA’s AI Podcast, host Noah Kravitz spoke with Thomson Reuters’ Chief Product Officer David Wong about its potential — and implications.</p>
<p><a href="https://soundcloud.com/theaipodcast/making-machines-mindful"><b>Making Machines Mindful: NYU Professor Talks Responsible AI &#8211; Ep. 205</b></a></p>
<p>Artificial intelligence is now a household term. Responsible AI is hot on its heels. Julia Stoyanovich, associate professor of computer science and engineering at NYU and director of the university’s Center for Responsible AI, wants to make the terms “AI” and “responsible AI” synonymous.</p>
<p><a href="https://soundcloud.com/theaipodcast/mlcommons"><b>MLCommons’ David Kanter, NVIDIA’s Daniel Galvez on Publicly Accessible Datasets &#8211; Ep. 167</b></a></p>
<p>On this episode of NVIDIA’s AI Podcast, host Noah Kravitz spoke with David Kanter, founder and executive director of MLCommons, and NVIDIA senior AI developer technology engineer Daniel Galvez, about the democratization of access to speech technology and how ML Commons is helping advance the research and development of machine learning for everyone.</p>
<p><a href="https://soundcloud.com/theaipodcast/ai-migel-tissera"><b>Metaspectral’s Migel Tissera on AI-Based Data Management &#8211; Ep. 155</b></a></p>
<p>Cofounder and CTO of Metaspectral speaks with‌ ‌‌NVIDIA‌ ‌AI‌ ‌Podcast‌‌ ‌host‌ ‌Noah‌ ‌Kravitz‌ ‌about‌ ‌how‌ ‌Metaspectral’s‌ ‌technologies‌ ‌help‌ ‌space‌ ‌explorers‌ ‌make‌ ‌quicker‌ ‌and‌ ‌better‌ ‌use‌ ‌of‌ ‌the‌ ‌massive‌ ‌amounts‌ ‌of‌ ‌image‌ ‌data‌ ‌they‌ ‌collect‌ ‌out‌ ‌in‌ ‌the‌ ‌cosmos.‌ ‌</p>
<h2><b>Subscribe to the AI Podcast</b></h2>
<p>Get the<a href="https://blogs.nvidia.com/ai-podcast/"> AI Podcast</a> through<a href="https://itunes.apple.com/us/podcast/the-ai-podcast/id1186480811?mt=2&amp;adbsc=social_20161220_68874946&amp;adbid=811257941365882882&amp;adbpl=tw&amp;adbpr=61559439"> iTunes</a>,<a href="https://podcasts.google.com/?feed=aHR0cHM6Ly9mZWVkcy5zb3VuZGNsb3VkLmNvbS91c2Vycy9zb3VuZGNsb3VkOnVzZXJzOjI2NDAzNDEzMy9zb3VuZHMucnNz"> Google Podcasts</a>,<a href="https://play.google.com/music/listen?u=0#/ps/I4kyn74qfrsdhrm35mcrf3igxzm"> Google Play</a>, <a href="https://music.amazon.com/podcasts/956857d0-9461-4496-a07e-24be0539ee82/the-ai-podcast">Amazon Music, </a><a href="https://castbox.fm/channel/The-AI-Podcast-id433488?country=us">Castbox</a>, DoggCatcher,<a href="https://overcast.fm/itunes1186480811/the-ai-podcast"> Overcast</a>,<a href="https://player.fm/series/the-ai-podcast"> PlayerFM</a>, Pocket Casts,<a href="http://www.podbay.fm/show/1186480811"> Podbay</a>,<a href="https://www.podbean.com/podcast-detail/cjgnp-4a6e0/The-AI-Podcast"> PodBean</a>, PodCruncher, PodKicker,<a href="https://soundcloud.com/theaipodcast"> Soundcloud</a>,<a href="https://open.spotify.com/show/4TB4pnynaiZ6YHoKmyVN0L"> Spotify</a>,<a href="http://www.stitcher.com/s?fid=130629&amp;refid=stpr"> Stitcher</a> and<a href="https://tunein.com/podcasts/Technology-Podcasts/The-AI-Podcast-p940829/"> TuneIn</a>.</p>
<p>Make the AI Podcast better: Have a few minutes to spare? Fill out<a href="http://survey.podtrac.com/start-survey.aspx?pubid=I5V0tOQFNS8j&amp;ver=short"> this listener survey</a>.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2021/08/ai-podcast-2600x1472_-1-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1159"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2021/08/ai-podcast-2600x1472_-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Combating Corruption With Data: Cleanlab and Berkeley Research Group on Using AI-Powered Investigative Analytics  ]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>The Building Blocks of AI: Decoding the Role and Significance of Foundation Models</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-foundation-models/</link>
		
		<dc:creator><![CDATA[Jesse Clayton]]></dc:creator>
		<pubDate>Wed, 10 Apr 2024 13:00:47 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71030</guid>

					<description><![CDATA[Skyscrapers start with strong foundations. The same goes for apps powered by AI.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><i>Editor’s note: This post is part of the </i><a href="https://blogs.nvidia.com/blog/tag/ai-decoded/"><i>AI Decoded series</i></a><i>, which demystifies AI by making the technology more accessible, and which showcases new hardware, software, tools and accelerations for RTX PC users.</i></p>
<p>Skyscrapers start with strong foundations. The same goes for apps powered by AI.</p>
<p>A <a href="https://blogs.nvidia.com/blog/what-are-foundation-models/">foundation model</a> is an AI neural network trained on immense amounts of raw data, generally with <a href="https://blogs.nvidia.com/blog/supervised-unsupervised-learning/">unsupervised learning</a>.</p>
<p>It’s a type of artificial intelligence model trained to understand and generate human-like language. Imagine giving a computer a huge library of books to read and learn from, so it can understand the context and meaning behind words and sentences, just like a human does.</p>
<figure id="attachment_71041" aria-describedby="caption-attachment-71041" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model.jpg"><img loading="lazy" decoding="async" class="wp-image-71041 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-672x457.jpg" alt="" width="672" height="457" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-672x457.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-400x272.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-768x522.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-662x450.jpg 662w, https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-316x215.jpg 316w, https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-147x100.jpg 147w, https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model.jpg 1054w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71041" class="wp-caption-text">Foundation models.</figcaption></figure>
<p>A foundation model’s deep knowledge base and ability to communicate in natural language make it useful for a broad range of applications, including text generation and summarization, copilot production and computer code analysis, image and video creation, and audio transcription and speech synthesis.</p>
<p>ChatGPT, one of the most notable generative AI applications, is a chatbot built with OpenAI’s GPT foundation model. Now in its fourth version, GPT-4 is a large multimodal model that can ingest text or images and generate text or image responses.</p>
<p>Online apps built on foundation models typically access the models from a data center. But many of these models, and the applications they power, can now run locally on PCs and workstations with <a href="https://www.nvidia.com/en-us/geforce/">NVIDIA GeForce</a> and <a href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX</a> GPUs.</p>
<h2><strong>Foundation Model Uses</strong></h2>
<p>Foundation models can perform a variety of functions, including:</p>
<ul>
<li>Language processing: understanding and generating text</li>
<li>Code generation: analyzing and debugging computer code in many programming languages</li>
<li>Visual processing: analyzing and generating images</li>
<li>Speech: generating text to speech and transcribing speech to text</li>
</ul>
<p>They can be used as is or with further refinement. Rather than training an entirely new AI model for each generative AI application — a costly and time-consuming endeavor — users commonly fine-tune foundation models for specialized use cases.</p>
<p>Pretrained foundation models are remarkably capable, thanks to prompts and data-retrieval techniques like <a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">retrieval-augmented generation</a>, or RAG. Foundation models also excel at <a href="https://blogs.nvidia.com/blog/what-is-transfer-learning/">transfer learning</a>, which means they can be trained to perform a second task related to their original purpose.</p>
<p>For example, a general-purpose large language model (LLM) designed to converse with humans can be further trained to act as a customer service chatbot capable of answering inquiries using a corporate knowledge base.</p>
<p>Enterprises across industries are fine-tuning foundation models to get the best performance from their AI applications.</p>
<h2><strong>Types of Foundation Models</strong></h2>
<p>More than 100 foundation models are in use — a number that continues to grow. LLMs and image generators are the two most popular types of foundation models. And many of them are free for anyone to try — on any hardware — in the <a href="https://build.nvidia.com/explore/discover">NVIDIA API Catalog</a>.</p>
<p>LLMs are models that understand natural language and can respond to queries. Google’s <a href="https://build.nvidia.com/google/gemma-7b">Gemma</a> is one example; it excels at text comprehension, transformation and code generation. When asked about the astronomer Cornelius Gemma, it shared that his “contributions to celestial navigation and astronomy significantly impacted scientific progress.” It also provided information on his key achievements, legacy and other facts.</p>
<p>Extending the collaboration <a href="https://blogs.nvidia.com/blog/google-gemma-llm-rtx-ai-pc/">of the Gemma models</a>, accelerated with the NVIDIA TensorRT-LLM on RTX GPUs, <a href="https://developers.googleblog.com/2024/04/gemma-family-expands.html">Google’s CodeGemma</a> brings powerful yet lightweight coding capabilities to the community. CodeGemma models are available as 7B and 2B pretrained variants that specialize in code completion and code generation tasks.</p>
<p>MistralAI’s <a href="https://build.nvidia.com/mistralai/mistral-7b-instruct-v2">Mistral</a> LLM can follow instructions, complete requests and generate creative text. In fact, it helped brainstorm the headline for this blog, including the requirement that it use a variation of the series’ name “AI Decoded,” and it assisted in writing the definition of a foundation model.</p>
<figure id="attachment_71034" aria-describedby="caption-attachment-71034" style="width: 589px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/Hello-world-indeed.png"><img loading="lazy" decoding="async" class="wp-image-71034 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/Hello-world-indeed.png" alt="" width="589" height="275" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/Hello-world-indeed.png 589w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Hello-world-indeed-400x187.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Hello-world-indeed-406x190.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Hello-world-indeed-188x88.png 188w" sizes="(max-width: 589px) 100vw, 589px" /></a><figcaption id="caption-attachment-71034" class="wp-caption-text">Hello, world, indeed.</figcaption></figure>
<p>Meta’s <a href="https://build.nvidia.com/meta/llama2-70b">Llama 2</a> is a cutting-edge LLM that generates text and code in response to prompts.</p>
<p>Mistral and Llama 2 are available in the <a href="https://www.nvidia.com/en-ph/ai-on-rtx/chat-with-rtx-generative-ai/">NVIDIA ChatRTX</a> tech demo, running on RTX PCs and workstations. ChatRTX lets users personalize these foundation models by connecting them to personal content — such as documents, doctors’ notes and other data — through RAG. It’s accelerated by <a href="https://blogs.nvidia.com/blog/ai-decoded-tensorrt-stable-diffusion-automatic1111">TensorRT-LLM</a> for quick, contextually relevant answers. And because it runs locally, results are fast and secure.</p>
<p>Image generators like StabilityAI’s <a href="https://build.nvidia.com/stabilityai/stable-diffusion-xl">Stable Diffusion XL</a> and <a href="https://build.nvidia.com/stabilityai/sdxl-turbo">SDXL Turbo</a> let users generate images and stunning, realistic visuals. StabilityAI’s video generator, <a href="https://build.nvidia.com/stabilityai/stable-video-diffusion">Stable Video Diffusion</a>, uses a generative diffusion model to synthesize video sequences with a single image as a conditioning frame.</p>
<p>Multimodal foundation models can simultaneously process more than one type of data — such as text and images — to generate more sophisticated outputs.</p>
<p>A multimodal model that works with both text and images could let users upload an image and ask questions about it. These types of models are quickly working their way into real-world applications like customer service, where they can serve as faster, more user-friendly versions of traditional manuals.</p>
<figure id="attachment_71044" aria-describedby="caption-attachment-71044" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/image.png"><img loading="lazy" decoding="async" class="size-large wp-image-71044" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/image-672x334.png" alt="" width="672" height="334" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/image-672x334.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-400x199.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-768x382.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-1536x764.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-842x419.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-406x202.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-188x94.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-1280x637.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image.png 1783w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71044" class="wp-caption-text">Many foundation models are free to try — on any hardware — in the NVIDIA API Catalog.</figcaption></figure>
<p><a href="https://build.nvidia.com/microsoft/microsoft-kosmos-2">Kosmos 2</a> is Microsoft’s groundbreaking multimodal model designed to understand and reason about visual elements in images.</p>
<h2><strong>Think Globally, Run AI Models Locally </strong></h2>
<p>GeForce RTX and NVIDIA RTX GPUs can run foundation models locally.</p>
<p>The results are fast and secure. Rather than relying on cloud-based services, users can harness apps like ChatRTX to process sensitive data on their local PC without sharing the data with a third party or needing an internet connection.</p>
<p>Users can choose from a rapidly growing catalog of open foundation models to download and run on their own hardware. This lowers costs compared with using cloud-based apps and APIs, and it eliminates latency and network connectivity issues. <i>Generative AI is transforming gaming, videoconferencing and interactive experiences of all kinds. Make sense of what’s new and what’s next by subscribing to the </i><a href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><i>AI Decoded newsletter</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-models-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-models-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[The Building Blocks of AI: Decoding the Role and Significance of Foundation Models]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Joins $110 Million Partnership to Help Universities Teach AI Skills</title>
		<link>https://blogs.nvidia.com/blog/partnership-universities-teach-ai-skills/</link>
		
		<dc:creator><![CDATA[Ruth Berry]]></dc:creator>
		<pubDate>Tue, 09 Apr 2024 19:01:49 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[HPC Stories]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71006</guid>

					<description><![CDATA[The Biden Administration has announced a new $110 million AI partnership between Japan and the United States that includes an initiative to fund research through a collaboration between the University of Washington and the University of Tsukuba. NVIDIA is committing $25 million in a collaboration with Amazon that aims to bring the latest technologies to		<a class="read-more" href="https://blogs.nvidia.com/blog/partnership-universities-teach-ai-skills/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>The Biden Administration has announced a new $110 million AI partnership between Japan and the United States that includes an initiative to fund research through a collaboration between the University of Washington and the University of Tsukuba.</p>
<p>NVIDIA is committing $25 million in a collaboration with Amazon that aims to bring the latest technologies to the University of Washington, in Seattle, and the University of Tsukuba, which is northeast of Tokyo.</p>
<p>Universities around the world are preparing students for crucial AI skills by providing access to the high performance computing capabilities of supercomputing.</p>
<p>“This collaboration between the University of Washington, University of Tsukuba, Amazon, and NVIDIA will help provide the research and workforce training for our regions’ tech sectors to keep up with the profound impacts AI is having across every sector of our economy,” said Jay Inslee, governor of Washington State.</p>
<h2><b>Creating AI Opportunities for Students</b></h2>
<p>NVIDIA has been investing in universities for decades computing resources, advanced training curriculums, donations, and other support to provide students and professors with access to high performance computing (HPC) for groundbreaking research results.</p>
<p>NVIDIA founder and CEO Jensen Huang and his wife, Lori Huang, <a href="https://blogs.nvidia.com/blog/ai-supercomputer-oregon-state/">donated $50 million</a> to their alma mater Oregon State University — where they met and earned engineering degrees —  to help build one of the world’s fastest supercomputers in a facility bearing their names. This computing center will help students research, develop and apply AI across Oregon State’s top-ranked programs in agriculture, computer sciences, climate science, forestry, oceanography, robotics, water resources, materials sciences and more.</p>
<p>The University of Florida <a href="https://blogs.nvidia.com/blog/starship-for-mind-uf-malachowsky-hall/">recently unveiled</a> Malachowsky Hall, which was made possible with a $50 million donation from NVIDIA co-founder Chris Malachowsky. This new building along with a previous donation of an AI supercomputer is enabling the University of Florida to offer world class AI training and research opportunities.</p>
<h2><b>Strengthening US-Japan AI Research Collaboration</b></h2>
<p>The U.S.-Japan HPC alliance will advance AI research and development and support the two nations’ global leadership in cutting-edge technology.</p>
<p>The University of Washington and Tsukuba University initiative will support research in critical areas where AI can drive impactful change, such as robotics, healthcare, climate change and atmospheric science, among others.</p>
<p>In addition to the university partnership,  NVIDIA recently announced a collaboration with Japan’s National Institute of Advanced Industrial Science and Technology (AIST) on AI and quantum technology.</p>
<h2><b>Addressing Worldwide AI Talent Shortage</b></h2>
<p>Demand for key AI skills is creating a talent shortage worldwide. Some <a href="https://www.ox.ac.uk/news/2023-10-09-expert-comment-ai-demand-booming-right-skills-and-technology-glue-guys">experts calculate </a>there has been a fivefold increase in demand for these skills as a percentage of total U.S. jobs. Universities around the world are looking for ways to prepare students with new skills for the workforce, and corporate-university partnerships are a key tool to help bridge the gap.</p>
<p>NVIDIA <a href="https://blogs.nvidia.com/blog/generative-ai-professional-certification/">unveiled at GTC 2024</a> new professional certifications in generative AI to help enable the next generation of developers to obtain technical credibility in this important domain.</p>
<p><i>Learn more about NVIDIA generative AI courses </i><a href="https://www.nvidia.com/en-us/training/instructor-led-workshops/generative-ai-with-diffusion-models/"><i>here</i></a><i> and </i><a href="https://www.nvidia.com/en-us/learn/certification/generative-ai-llm-associate/"><i>here</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/University-of-Washington.jpg"
			type="image/jpeg"
			width="1280"
			height="678"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/University-of-Washington-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Joins $110 Million Partnership to Help Universities Teach AI Skills]]></media:title>
			<media:description type="html">Image of University of Washington</media:description>
			</media:content>
			</item>
		<item>
		<title>Broadcasting Breakthroughs: NVIDIA Holoscan for Media, Available Now, Transforms Live Media With Easy AI Integration</title>
		<link>https://blogs.nvidia.com/blog/holoscan-live-media-ai-solutions/</link>
		
		<dc:creator><![CDATA[Sepi Motamedi]]></dc:creator>
		<pubDate>Tue, 09 Apr 2024 15:00:11 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Graphics]]></category>
		<category><![CDATA[Video Streaming]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70983</guid>

					<description><![CDATA[Whether delivering live sports programming, streaming services, network broadcasts or content on social platforms, media companies face a daunting landscape. Viewers are increasingly opting for interactive and personalized content. Virtual reality and augmented reality continue their drive into the mainstream. New video compression standards are challenging traditional computing infrastructure. And AI is having an impact		<a class="read-more" href="https://blogs.nvidia.com/blog/holoscan-live-media-ai-solutions/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Whether delivering live sports programming, streaming services, network broadcasts or content on social platforms, media companies face a daunting landscape.</p>
<p>Viewers are increasingly opting for interactive and personalized content. Virtual reality and augmented reality continue their drive into the mainstream. New video compression standards are challenging traditional computing infrastructure. And AI is having an impact across the board.</p>
<p>In a situation this dynamic, media companies will benefit most from AI-enabled media solutions that flexibly align with their changing development and delivery needs.</p>
<p>NVIDIA Holoscan for Media, available now, is a software-defined platform that enables developers to easily build live media applications, supercharge them with AI and then deploy them across media platforms.</p>
<h2><b>A New Approach to Media Application Development </b></h2>
<p>Holoscan for Media offers a new approach to development in live media. It simplifies application development by providing an internet protocol (IP)-based, cloud-native architecture that isn’t constrained by dedicated hardware, environments or locations. Instead, it integrates open-source and ubiquitous technologies and streamlines application delivery to customers, all while optimizing costs.</p>
<p>Traditional application development for the live media market relies on dedicated hardware. Because software is tied to that hardware, developers are constrained when it comes to innovating or upgrading applications.</p>
<p>Each deployment type, whether on premises or in the cloud, requires its own build, making development costly and inefficient. Beyond designing an application’s user interface and core functionalities, developers have to build out additional infrastructure services, further eating into research and development budgets.</p>
<p>The most significant challenge is incorporating AI, due to the complexity of building an AI software stack. This prevents many applications in pilot programs from moving to production.</p>
<p>Holoscan for Media eases the integration of AI into application development due to its underlying architecture, which enables software-defined video to be deployed on the same software stack as AI applications, including generative AI-based tools. This benefits vendors and research and development departments looking to incorporate AI apps into live video.</p>
<p>Since the platform is cloud-native, the same architecture can run independent of location, whether in the cloud, on premises or at the edge. Additionally, it’s not tied to a specific device, field-programmable gate array or appliance.</p>
<p>The Holoscan for Media architecture includes services like authentication, logging and security, as well as features that help broadcasters migrate to IP-based technologies, including the SMPTE ST 2110 transport protocol, the precision time protocol for timing and synchronization, and the NMOS controller and registry for dynamic device management.</p>
<h2><b>A Growing Ecosystem of Partners</b></h2>
<p>Beamr, Comprimato, Lawo, Media.Monks, Pebble, RED Digital Cinema, Sony Corporation and Telestream are among the early adopters already transforming live media with Holoscan for Media.</p>
<p>“We use Holoscan for Media as the core infrastructure for our broadcast and media workflow, granting us powerful scale to deliver interest-based content across a wide range of channels and platforms,” said Lewis Smithingham, senior vice president of innovation special operations at Media.Monks, a provider of software-defined production workflows.</p>
<p>“By compartmentalizing applications and making them interoperable, Holoscan for Media allows for easy adoption of new innovations from many different companies in one platform,” said Jeff Goodman, vice president of product management at RED Digital Cinema, a manufacturer of professional digital cinema cameras. “It takes much of the integration complexity out of the equation and will significantly increase the pace of innovation. We are very excited to be a part of it.”</p>
<p>“We believe NVIDIA Holoscan for Media is one of the paths forward to enabling the development of next-generation products and services for the industry, allowing the scaling of GPU power as needed,” said Masakazu Murata, senior general manager of media solutions business at Sony Corporation. “Our M2L-X software switcher prototype running on Holoscan for Media demonstrates how customers can run Sony’s solutions on GPU clusters.”</p>
<p>“Telestream is committed to transforming the media landscape, enhancing efficiency and content experiences without sacrificing quality or user-friendliness,” said Charlie Dunn, senior vice president and general manager at Telestream, a provider of digital media software and solutions. “We’ve seamlessly integrated the Holoscan for Media platform into our INSPECT IP video monitoring solution to achieve a clear and efficient avenue for ST 2110 compliance.”</p>
<h2><b>Experience Holoscan for Media at NAB Show</b></h2>
<p>These partners will showcase how they’re using NVIDIA Holoscan for Media at NAB Show, an event for the broadcast, media and entertainment industry, taking place April 13-17 in Las Vegas.</p>
<p>Explore development on Holoscan for Media and discover applications running on the platform at the <a href="https://nab24.mapyourshow.com/8_0/floorplan/?hallID=SL&amp;selectedBooth=SL8065">Dell Technologies booth</a>. Learn more about <a href="https://www.nvidia.com/en-us/events/nab/">NVIDIA’s presence at NAB Show</a>, including details on sessions and demos on generative AI, software-defined broadcast and immersive graphics.</p>
<p><i>Apply for access to </i><a href="https://developer.nvidia.com/holoscan-for-media"><i>NVIDIA Holoscan for Media</i></a><i> today.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/Holoscan-Corp-Blog.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/Holoscan-Corp-Blog-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Broadcasting Breakthroughs: NVIDIA Holoscan for Media, Available Now, Transforms Live Media With Easy AI Integration]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
	</channel>
</rss>
