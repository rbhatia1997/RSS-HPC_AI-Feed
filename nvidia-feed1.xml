<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:media="http://search.yahoo.com/mrss/">

<channel>
	<title>NVIDIA Blog</title>
	<atom:link href="https://blogs.nvidia.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.nvidia.com/</link>
	<description></description>
	<lastBuildDate>Wed, 25 Oct 2023 14:59:46 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.3.1</generator>
	<item>
		<title>Next-Gen Neural Networks: NVIDIA Research Announces Array of AI Advancements at NeurIPS</title>
		<link>https://blogs.nvidia.com/blog/2023/10/25/neurips-ai-research/</link>
		
		<dc:creator><![CDATA[Isha Salian]]></dc:creator>
		<pubDate>Wed, 25 Oct 2023 13:00:09 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Research]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Climate]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Healthcare and Life Sciences]]></category>
		<category><![CDATA[NVIDIA Research]]></category>
		<category><![CDATA[Robotics]]></category>
		<category><![CDATA[Science]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67715</guid>

					<description><![CDATA[NVIDIA researchers are collaborating with academic centers worldwide to advance generative AI, robotics and the natural sciences — and more than a dozen of these projects will be shared at NeurIPS, one of the world’s top AI conferences. Set for Dec. 10-16 in New Orleans, NeurIPS brings together experts in generative AI, machine learning, computer <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/25/neurips-ai-research/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>NVIDIA researchers are collaborating with academic centers worldwide to advance <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">generative AI</a>, robotics and the natural sciences — and more than a dozen of these projects will be shared at <a href="https://neurips.cc/" target="_blank" rel="noopener">NeurIPS</a>, one of the world’s top AI conferences.</p>
<p>Set for Dec. 10-16 in New Orleans, NeurIPS brings together experts in generative AI, machine learning, computer vision and more. Among the innovations <a href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a> will present are new techniques for transforming text to images, photos to 3D avatars, and specialized robots into multi-talented machines.</p>
<p>“NVIDIA Research continues to drive progress across the field — including generative AI models that transform text to images or speech, autonomous AI agents that learn new tasks faster, and neural networks that calculate complex physics,” said Jan Kautz, vice president of learning and perception research at NVIDIA. “These projects, often done in collaboration with leading minds in academia, will help accelerate developers of virtual worlds, simulations and autonomous machines.”</p>
<h2><b>Picture This: Improving Text-to-Image Diffusion Models</b></h2>
<p>Diffusion models have become the most popular type of generative AI models to turn text into realistic imagery. NVIDIA researchers have collaborated with universities on multiple projects advancing diffusion models that will be presented at NeurIPS.</p>
<ul style="font-weight: 300;">
<li>A paper accepted as an oral presentation focuses on improving generative AI models’ ability to <a href="https://neurips.cc/virtual/2023/oral/73870" target="_blank" rel="noopener">understand the link between modifier words and main entities</a> in text prompts. While existing text-to-image models asked to depict a yellow tomato and a red lemon may incorrectly generate images of yellow lemons and red tomatoes, the new model analyzes the syntax of a user’s prompt, encouraging a bond between an entity and its modifiers to deliver a more faithful visual depiction of the prompt.</li>
<li>SceneScape, a new framework using <a href="https://neurips.cc/virtual/2023/poster/71859" target="_blank" rel="noopener">diffusion models to create long videos of 3D scenes from text prompts</a>, will be presented as a poster. The project combines a text-to-image model with a depth prediction model that helps the videos maintain plausible-looking scenes with consistency between the frames — generating videos of art museums, haunted houses and ice castles (pictured above).</li>
<li>Another poster describes work that improves how text-to-image models <a href="https://neurips.cc/virtual/2023/poster/70922" target="_blank" rel="noopener">generate concepts rarely seen in training data</a>. Attempts to generate such images usually result in low-quality visuals that aren’t an exact match to the user’s prompt. The new method uses a small set of example images that help the model identify good seeds — random number sequences that guide the AI to generate images from the specified rare classes.</li>
<li>A third poster shows how a text-to-image diffusion model can <a href="https://neurips.cc/virtual/2023/poster/70648" target="_blank" rel="noopener">use the text description of an incomplete point cloud</a> to generate missing parts and create a complete 3D model of the object. This could help complete point cloud data collected by lidar scanners and other depth sensors for robotics and autonomous vehicle AI applications. Collected imagery is often incomplete because objects are scanned from a specific angle — for example, a lidar sensor mounted to a vehicle would only scan one side of each building as the car drives down a street.</li>
</ul>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/Point-Cloud-Completion.jpg"><img decoding="async" fetchpriority="high" class="aligncenter size-large wp-image-67733" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/Point-Cloud-Completion-672x222.jpg" alt="" width="672" height="222" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/Point-Cloud-Completion-672x222.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Point-Cloud-Completion-400x132.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Point-Cloud-Completion-768x253.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Point-Cloud-Completion-1536x506.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Point-Cloud-Completion-842x278.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Point-Cloud-Completion-406x134.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Point-Cloud-Completion-188x62.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Point-Cloud-Completion-1280x422.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Point-Cloud-Completion.jpg 1999w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<h2><b>Character Development: Advancements in AI Avatars</b></h2>
<p>AI avatars combine multiple generative AI models to create and animate virtual characters, produce text and convert it to speech. Two NVIDIA posters at NeurIPS present new ways to make these tasks more efficient.</p>
<ul style="font-weight: 300;">
<li>A poster describes a new method to <a href="https://neurips.cc/virtual/2023/poster/72615" target="_blank" rel="noopener">turn a single portrait image into a 3D head avatar</a> while capturing details including hairstyles and accessories. Unlike current methods that require multiple images and a time-consuming optimization process, this model achieves high-fidelity 3D reconstruction without additional optimization during inference. The avatars can be animated either with blendshapes, which are 3D mesh representations used to represent different facial expressions, or with a reference video clip where a person’s facial expressions and motion are applied to the avatar.</li>
<li>Another poster by NVIDIA researchers and university collaborators advances zero-shot text-to-speech synthesis with P-Flow, a generative AI model that can <a href="https://pflow-demo.github.io/projects/pflow/" target="_blank" rel="noopener">rapidly synthesize high-quality personalized speech</a> given a three-second reference prompt. P-Flow features better pronunciation, human likeness and speaker similarity compared to recent state-of-the-art counterparts. The model can near-instantly convert text to speech on a single <a href="https://www.nvidia.com/en-us/data-center/a100/">NVIDIA A100 Tensor Core GPU</a>.</li>
</ul>
<h2><b>Research Breakthroughs in Reinforcement Learning, Robotics</b></h2>
<p>In the fields of reinforcement learning and robotics, NVIDIA researchers will present two posters highlighting innovations that improve the generalizability of AI across different tasks and environments.</p>
<ul style="font-weight: 300;">
<li>The first proposes a <a href="https://neurips.cc/virtual/2023/poster/72040" target="_blank" rel="noopener">framework for developing reinforcement learning algorithms</a> that can adapt to new tasks while avoiding the common pitfalls of gradient bias and data inefficiency. The researchers showed that their method — which features a novel meta-algorithm that can create a robust version of <i>any</i> meta-reinforcement learning model — performed well on multiple benchmark tasks.</li>
<li>Another by an NVIDIA researcher and university collaborators tackles the challenge of <a href="https://neurips.cc/virtual/2023/poster/71709" target="_blank" rel="noopener">object manipulation in robotics</a>. Prior AI models that help robotic hands pick up and interact with objects can handle specific shapes but struggle with objects unseen in the training data. The researchers introduce a new framework that estimates how objects across different categories are geometrically alike — such as drawers and pot lids that have similar handles — enabling the model to more quickly generalize to new shapes.</li>
</ul>
<h2><b>Supercharging Science: AI-Accelerated Physics, Climate, Healthcare</b></h2>
<p>NVIDIA researchers at NeurIPS will also present papers across the natural sciences — covering physics simulations, climate models and AI for healthcare.</p>
<ul style="font-weight: 300;">
<li>To accelerate <a href="https://neurips.cc/virtual/2023/poster/72670" target="_blank" rel="noopener">computational fluid dynamics for large-scale 3D simulations</a>, a team of NVIDIA researchers proposed a neural operator architecture that combines accuracy and computational efficiency to estimate the pressure field around vehicles — the first deep learning-based computational fluid dynamics method on an industry-standard, large-scale automotive benchmark. The method achieved 100,000x acceleration on a single <a href="https://www.nvidia.com/en-us/data-center/tensor-cores/">NVIDIA Tensor Core GPU</a> compared to another GPU-based solver, while reducing the error rate. Researchers can incorporate the model into their own applications using the open-source <a href="https://github.com/neuraloperator/neuraloperator" target="_blank" rel="noopener">neuraloperator library</a>.</li>
</ul>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/Computational-Fluid-Dynamics-1.jpg"><img decoding="async" class="aligncenter size-large wp-image-67740" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/Computational-Fluid-Dynamics-1-672x390.jpg" alt="" width="672" height="390" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/Computational-Fluid-Dynamics-1-672x390.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Computational-Fluid-Dynamics-1-400x232.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Computational-Fluid-Dynamics-1-768x446.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Computational-Fluid-Dynamics-1-775x450.jpg 775w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Computational-Fluid-Dynamics-1-370x215.jpg 370w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Computational-Fluid-Dynamics-1-172x100.jpg 172w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Computational-Fluid-Dynamics-1.jpg 1171w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>&nbsp;</p>
<ul>
<li>A consortium of climate scientists and machine learning researchers from universities, national labs, research institutes, Allen AI and NVIDIA collaborated on <a href="https://neurips.cc/virtual/2023/poster/73569" target="_blank" rel="noopener">ClimSim</a>, a massive dataset for physics and machine learning-based climate research that will be shared in an oral presentation at NeurIPS. The dataset covers the globe over multiple years at high resolution — and machine learning emulators built using that data can be plugged into existing operational climate simulators to improve their fidelity, accuracy and precision. This can help scientists produce better predictions of storms and other extreme events.</li>
<li>NVIDIA Research interns are presenting a poster introducing an AI algorithm that provides personalized <a href="https://neurips.cc/virtual/2023/poster/71940" target="_blank" rel="noopener">predictions of the effects of medicine dosage</a> on patients. Using real-world data, the researchers tested the model’s predictions of blood coagulation for patients given different dosages of a treatment. They also analyzed the new algorithm’s predictions of the antibiotic vancomycin levels in patients who received the medication — and found that prediction accuracy significantly improved compared to prior methods.</li>
</ul>
<p><a href="https://www.nvidia.com/en-us/research/"><i>NVIDIA Research</i></a><i> comprises hundreds of scientists and engineers worldwide, with teams focused on topics including AI, computer graphics, computer vision, self-driving cars and robotics. </i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/Scenescape-screengrab-1.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/Scenescape-screengrab-1-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Next-Gen Neural Networks: NVIDIA Research Announces Array of AI Advancements at NeurIPS]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>On Razer’s Edge: VFX Star Surfaced Studio Creates Stunning Sci-Fi World This Week ‘In The NVIDIA Studio’</title>
		<link>https://blogs.nvidia.com/blog/2023/10/24/surfaced-studio-adobe-premiere-pro-after-effects-blender-unreal/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Tue, 24 Oct 2023 13:00:42 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[In the NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA Studio]]></category>
		<category><![CDATA[Rendering]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67668</guid>

					<description><![CDATA[Visual effects artist Surfaced Studio returns to 'In the NVIDIA Studio' to share his real-world VFX project, created on a brand new Razer Blade 16 Mercury Edition laptop powered by GeForce RTX 4080 graphics. ]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Visual effects artist Surfaced Studio returns to <i>In the NVIDIA Studio</i> to share his real-world VFX project, created on a brand new <a href="https://www.razer.com/gaming-laptops/razer-blade-16/RZ09-0483TEM3-R3U1">Razer Blade 16 Mercury Edition</a> laptop powered by <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4080/">GeForce RTX 4080</a> graphics.</p>
<p><iframe title="Razer Blade 16 Mercury Edition - Real World VFX Test &amp; Review" width="500" height="281" src="https://www.youtube.com/embed/d6AR3XnwtIs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Surfaced Studio creates photorealistic, digitally generated imagery that seamlessly integrates visual effects into short films, television and console gaming.</p>
<p>He found inspiration for a recent sci-fi project by experimenting with 3D transitions: using a laptop screen as a gateway between worlds, like the portals from <i>Dr. Strange</i> or the transitions from <i>The Matrix</i>.</p>
<h2><b>Break the Rules and Become a Hero</b></h2>
<p>Surfaced Studio aimed to create an immersive experience with his latest project.</p>
<p>“I wanted to get my audience to feel surprised getting ‘sucked into’ the 3D world,” he explained.</p>
<p>Surfaced Studio began with a simple script, alongside sketches of brainstormed ideas and played out shots. “This usually helps me think through how I’d pull each effect off and whether they’re actually possible,” he said.</p>
<p>From there, he shot video and imported the footage into Adobe Premiere Pro for a rough test edit. Then, Surfaced Studio selected the most suitable clips for use.</p>
<p>He cleaned up the footage in Adobe After Effects, stabilizing shots with the Warp Stabilizer tool and removing distracting background elements with the Mocha Pro tool. Both effects were accelerated by his GeForce RTX 4080 Laptop GPU.</p>
<p>After, he created a high-contrast version of the shot for 3D motion tracking in Blender.</p>
<figure id="attachment_67675" aria-describedby="caption-attachment-67675" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-3d-motion-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67675" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-3d-motion-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-3d-motion-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-3d-motion-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-3d-motion-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-3d-motion-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-3d-motion-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-3d-motion-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-3d-motion-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67675" class="wp-caption-text">3D motion tracking in Blender.</figcaption></figure>
<p>Motion tracking is used to apply tracking data to 3D objects. “This was pretty tricky, as it’s a 16-second gimbal shot with fast moving sections and a decent camera blur,” said Surfaced Studio. “It took me a good few days to get a decent track and fix issues with manual keyframes and ‘patches’ between different sections.”</p>
<figure id="attachment_67682" aria-describedby="caption-attachment-67682" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-shooting-footage-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67682" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-shooting-footage-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-shooting-footage-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-shooting-footage-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-shooting-footage-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-shooting-footage-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-shooting-footage-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-shooting-footage-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-shooting-footage-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67682" class="wp-caption-text">A gimbal shot uses sensors and motors to stabilize and support the camera.</figcaption></figure>
<p>Surfaced Studio exported footage of the animated camera into a 3D FBX file to use in Unreal Engine and set it up in the <a href="https://www.unrealengine.com/marketplace/en-US/product/high-city-sp#">Cyberpunk High City pack</a>, which contains a modular constructor for creating highly detailed sci-fi city streets, alleys and blocks.</p>
<p><iframe loading="lazy" title="High City (Unreal 5)" width="500" height="281" src="https://www.youtube.com/embed/9ra8bt4sEcQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>“I’m not much of a 3D artist so using [the Cyberpunk High City pack] was the best option to complete the project on this side of the century,” the artist said. He then made modifications to the cityscape, reducing flickering lights and adding buildings, custom fog and Razer and NVIDIA Studio banners. He even added a billboard with an ad encouraging kindness to cats. “It’s so off to the side of most shots I doubt anyone actually noticed,” noted a satisfied Surfaced Studio.</p>
<figure id="attachment_67685" aria-describedby="caption-attachment-67685" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-razer-blade-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67685" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-razer-blade-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-razer-blade-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-razer-blade-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-razer-blade-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-razer-blade-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-razer-blade-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-razer-blade-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-razer-blade-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67685" class="wp-caption-text">A PSA from Surfaced Studio: be nice to cats.</figcaption></figure>
<p>Learning 3D effects can seem overwhelming due to the vast knowledge needed across multiple apps and district workflows. But Surfaced Studio stresses the simple importance of first understanding workflow hierarchies — and how one feeds into another — as an approachable entry point to choosing a specialty suited to a creator’s unique passion and natural talent.</p>
<p>Surfaced Studio was able to seamlessly run his scene in Unreal Engine full 4K resolution — with all textures and materials loading at maximum graphical fidelity — thanks to the GeForce RTX 4080 Laptop GPU in his Razer Blade 16. The graphics card also contains <a href="https://www.nvidia.com/en-us/geforce/news/dlss-3-5-available-september-21/#:~:text=DLSS%203.5%20is%20available%20for,NVIDIA%20DLSS%203.5%20announcement%20article.">NVIDIA DLSS</a> capabilities to increase viewport interactivity by using AI to upscale frames rendered at lower resolution while retaining high-fidelity detail.</p>
<figure id="attachment_67688" aria-describedby="caption-attachment-67688" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-unreal-engine-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67688" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-unreal-engine-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-unreal-engine-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-unreal-engine-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-unreal-engine-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-unreal-engine-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-unreal-engine-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-unreal-engine-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-unreal-engine-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67688" class="wp-caption-text">Moving virtual objects in Unreal Engine.</figcaption></figure>
<p>Surfaced Studio then took the FBX file with the exported camera tracking data into Unreal Engine, matching his ‘3D camera’ with the real-world one used to film the laptop with. “This was the crucial step in creating the ‘look-through’ effect I wanted,” he said.</p>
<p>Once satisfied with the look, Surfaced Studio exported all sequences from Unreal Engine as multilayer EXR files — including a Z-depth pass, a grayscale value range to create a depth-of-field effect — to separate visual elements from the 3D footage.</p>
<figure id="attachment_67691" aria-describedby="caption-attachment-67691" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-z-depth-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67691" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-z-depth-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-z-depth-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-z-depth-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-z-depth-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-z-depth-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-z-depth-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-z-depth-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-z-depth-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67691" class="wp-caption-text">Composite work in Adobe After Effects.</figcaption></figure>
<p>Surfaced Studio went back to After Effects for the final composites. He added distortion effects and some glow for the transition from the physical screen to the 3D world.</p>
<figure id="attachment_67694" aria-describedby="caption-attachment-67694" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-mocha-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67694" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-mocha-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-mocha-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-mocha-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-mocha-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-mocha-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-mocha-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-mocha-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-mocha-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67694" class="wp-caption-text">Cleaning up screen tracking in Adobe After Effects.</figcaption></figure>
<p>Then, Surfaced Studio again used the Z-depth pass to extract the 3D cars and overlay them onto the real footage.</p>
<figure id="attachment_67697" aria-describedby="caption-attachment-67697" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-enter-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67697" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-enter-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-enter-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-enter-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-enter-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-enter-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-enter-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-enter-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-ae-enter-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67697" class="wp-caption-text">Composite work in Adobe After Effects.</figcaption></figure>
<p>He exported the final project into Premiere Pro and added sound effects, music and a few color correction edits.</p>
<figure id="attachment_67700" aria-describedby="caption-attachment-67700" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-premiere-pro-final-edit-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67700" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-premiere-pro-final-edit-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-premiere-pro-final-edit-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-premiere-pro-final-edit-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-premiere-pro-final-edit-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-premiere-pro-final-edit-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-premiere-pro-final-edit-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-premiere-pro-final-edit-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-premiere-pro-final-edit-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67700" class="wp-caption-text">Final edits in Adobe Premiere Pro.</figcaption></figure>
<p>With GeForce RTX 4080 dual encoders, Surface Studio nearly halved Adobe Premiere Pro video decoding and encoding export times. Surfaced Studio has been using NVIDIA GPUs for over a decade, citing their widespread integration with commonly used tools.</p>
<p>“NVIDIA has simply done a better job than its competitors to reach out to and integrate with other companies that create creative apps,” said Surfaced Studio. “CUDA and RTX are widespread technologies that you find in most popular creative apps to accelerate workflows.”</p>
<p>When he’s not working on VFX projects, Surfaced Studio also uses his laptop to game. The Razer Blade 16 has the first dual-mode mini-LED display with two native resolutions: UHD+ at 120Hz — suited for VFX workflows — and FHD at 240Hz — ideal for gamers (or creators who like gaming).</p>
<figure id="attachment_67706" aria-describedby="caption-attachment-67706" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-studio-workspace-1-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67706" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-studio-workspace-1-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-studio-workspace-1-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-studio-workspace-1-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-studio-workspace-1-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-studio-workspace-1-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-studio-workspace-1-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-studio-workspace-1-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-surfaced-studio-wk80-studio-workspace-1-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67706" class="wp-caption-text">Powerful, elegant, beautiful: the Razer Blade 16 Mercury Edition.</figcaption></figure>
<p>For a limited time, gamers and creators can <a href="https://www.razer.com/campaigns/nvidia-game-bundle">get the critically acclaimed game <i>Alan Wake 2</i></a> with the purchase of the Razer Blade 16 powered by GeForce RTX 40 Series graphics cards.</p>
<p><iframe loading="lazy" title="Alan Wake 2 | 4K NVIDIA DLSS 3.5 World Premiere" width="500" height="281" src="https://www.youtube.com/embed/HwGbQwoMCxM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Surfaced Studio’s VFX tutorials are <a href="https://www.youtube.com/surfacedstudio">available on YouTube</a>, where he covers filmmaking, VFX and 3D techniques using Adobe After Effects, Blender, Photoshop, Premiere Pro and other apps.</p>
<figure id="attachment_67703" aria-describedby="caption-attachment-67703" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-surfaced-studio-wk80-artist-feature-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67703" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-surfaced-studio-wk80-artist-feature-1280w-672x282.png" alt="" width="672" height="282" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-surfaced-studio-wk80-artist-feature-1280w-672x282.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-surfaced-studio-wk80-artist-feature-1280w-400x168.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-surfaced-studio-wk80-artist-feature-1280w-768x323.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-surfaced-studio-wk80-artist-feature-1280w-842x354.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-surfaced-studio-wk80-artist-feature-1280w-406x171.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-surfaced-studio-wk80-artist-feature-1280w-188x79.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-surfaced-studio-wk80-artist-feature-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67703" class="wp-caption-text">VFX artist Surfaced Studio.</figcaption></figure>
<h2><b>Join the #SeasonalArtChallenge</b></h2>
<p>Don’t forget to join the #SeasonalArtChallenge by submitting spooky Halloween-inspired art in October and harvest- and fall-themed pieces in November.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">Welcome to our community <a href="https://twitter.com/hashtag/SeasonalArtChallenge?src=hash&amp;ref_src=twsrc%5Etfw">#SeasonalArtChallenge</a>! <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f383.png" alt="🎃" class="wp-smiley" style="height: 1em; max-height: 1em;" /><img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f342.png" alt="🍂" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>We want to see your spooky/Halloween-themed art like this piece from <a href="https://twitter.com/michal_pruski?ref_src=twsrc%5Etfw">@michal_pruski</a> (IG), this October!</p>
<p>Share your art with <a href="https://twitter.com/hashtag/SeasonalArtChallenge?src=hash&amp;ref_src=twsrc%5Etfw">#SeasonalArtChallenge</a> for a chance to be featured on the Studio or <a href="https://twitter.com/nvidiaomniverse?ref_src=twsrc%5Etfw">@NVIDIAOmniverse</a> channels! <a href="https://t.co/J2tqIXoahP">pic.twitter.com/J2tqIXoahP</a></p>
<p>&mdash; NVIDIA Studio (@NVIDIAStudio) <a href="https://twitter.com/NVIDIAStudio/status/1708980054456144287?ref_src=twsrc%5Etfw">October 2, 2023</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<p><i>Follow NVIDIA Studio on </i><a href="https://www.instagram.com/nvidiastudio/"><i>Instagram</i></a><i>, </i><a href="https://twitter.com/NVIDIAStudio"><i>Twitter</i></a><i> and </i><a href="https://www.facebook.com/NVIDIAStudio/"><i>Facebook</i></a><i>. Access tutorials on the </i><a href="https://www.youtube.com/channel/UCDeQdW6Lt6nhq3mLM4oLGWw"><i>Studio YouTube channel</i></a><i> and get updates directly in your inbox by subscribing to the </i><a href="https://www.nvidia.com/en-us/studio/?nvmid=subscribe-creators-mail-icon"><i>Studio newsletter</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/surfaced-studio-nv-blog-header-preview-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/surfaced-studio-nv-blog-header-preview-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[On Razer’s Edge: VFX Star Surfaced Studio Creates Stunning Sci-Fi World This Week ‘In The NVIDIA Studio’]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Street View to the Rescue: Deep Learning Paves the Way to Safer Buildings</title>
		<link>https://blogs.nvidia.com/blog/2023/10/23/street-view-image-deep-learning-research-urban-building/</link>
		
		<dc:creator><![CDATA[Kristen Yee]]></dc:creator>
		<pubDate>Mon, 23 Oct 2023 20:30:39 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Research]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Education]]></category>
		<category><![CDATA[GPU]]></category>
		<category><![CDATA[Inference]]></category>
		<category><![CDATA[Public Sector]]></category>
		<category><![CDATA[Social Impact]]></category>
		<category><![CDATA[Supercomputing]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67650</guid>

					<description><![CDATA[Images such as those in Google Street View are taking on a new purpose in the hands of University of Florida Assistant Professor of Artificial Intelligence Chaofeng Wang. He’s using them, along with deep learning, in a research project to automate the evaluation of urban buildings. The project aims to help governments mitigate natural disaster <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/23/street-view-image-deep-learning-research-urban-building/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Images such as those in Google Street View are taking on a new purpose in the hands of University of Florida Assistant Professor of Artificial Intelligence Chaofeng Wang.</p>
<p>He’s using them, along with deep learning, in a research project to automate the evaluation of urban buildings. The project aims to help governments mitigate natural disaster damage by providing the information needed for decision-makers to bolster building structures or perform post-disaster recovery.</p>
<p>After a natural disaster such as an earthquake, local governments send teams to check and evaluate building conditions. Manually done, it can take up to months to go through the full stock of a city.</p>
<p>Wang’s project uses AI to accelerate the evaluation process — cutting the time needed to a few hours. The AI model is trained using images sourced from Google Street View and local governments to assign scores to buildings based on <a href="https://www.fema.gov/emergency-managers/risk-management/earthquake/training/fema-p-154" target="_blank" rel="noopener">Federal Emergency Management Agency (FEMA) P-154 standards</a>, which provide assessment guidelines based on factors like wall material, structure type, building age and more. Wang also collaborated with the <a href="https://www.worldbank.org/en/topic/disasterriskmanagement/brief/global-program-for-resilient-housing" target="_blank" rel="noopener">World Bank Global Program for Resilient Housing</a> to collect images and perform annotations, which were used to improve the model.</p>
<p>The collected images are placed in a data repository. The AI model reads the repository and performs inference on the images — a process accelerated by <a href="https://www.nvidia.com/en-us/data-center/dgx-a100/" target="_blank" rel="noopener">NVIDIA DGX A100</a> systems.</p>
<p>“Without NVIDIA GPUs, we wouldn’t have been able to do this,” Wang said. “They significantly accelerate the process, ensuring timely results.”</p>
<p>Wang used the DGX A100 nodes in the <a href="https://blogs.nvidia.com/blog/2020/07/21/university-of-florida-nvidia-ai-supercomputer/" target="_blank" rel="noopener">University of Florida’s supercomputer, HiPerGator</a>. HiPerGator is one of the world’s fastest AI supercomputers in academia, delivering 700 petaflops of AI performance, and was built with the support of NVIDIA founder and UF alumnus Chris Malachowsky and hardware, software, training and services from NVIDIA.</p>
<p>The AI model’s output is compiled into a database that feeds into a web portal, which shows information — including the safety assessment score, building type and even roof or wall material — in a map-based format.</p>
<p><img decoding="async" loading="lazy" class="alignnone size-full wp-image-67651" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/streetviewscreenshot.png" alt="" width="1757" height="787" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/streetviewscreenshot.png 1757w, https://blogs.nvidia.com/wp-content/uploads/2023/10/streetviewscreenshot-400x179.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/streetviewscreenshot-672x301.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/streetviewscreenshot-768x344.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/streetviewscreenshot-1536x688.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/streetviewscreenshot-842x377.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/streetviewscreenshot-406x182.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/streetviewscreenshot-188x84.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/streetviewscreenshot-1280x573.png 1280w" sizes="(max-width: 1757px) 100vw, 1757px" /></p>
<p>Wang’s work was funded by the <a href="https://www.nvidia.com/en-us/industries/higher-education-research/applied-research-program/" target="_blank" rel="noopener">NVIDIA Applied Research Accelerator Program</a>, which supports research projects that have the potential to make a real-world impact through the deployment of NVIDIA-accelerated applications adopted by commercial and government organizations.</p>
<h2><strong>A Helping Eye</strong></h2>
<p>Wang says that the portal can serve different needs depending on the use case. To prepare for a natural disaster, a government can use predictions solely from street view images.</p>
<p>“Those are static images — one example is Google Street View images, which get updated every several years,” he said. “But that’s good enough for collecting information and getting a general understanding about certain statistics.”</p>
<p>But for rural areas or developing regions, where such images aren’t available or not frequently updated, governments can collect the images themselves. Powered by NVIDIA GPUs, the timely delivery of building assessments can help accelerate analyses.</p>
<p>Wang also suggests that with enough refinement, his research could also create ripples for the urban planning and insurance industries.</p>
<p>The project is currently being tested by a few local governments in Mexico and is garnering interest in some African, Asian and South American countries. At its current state, it can achieve over 85% accuracy in its assessment scores, per ‌FEMA P-154 standards.</p>
<h2><b>Survey of the Land</b></h2>
<p>One challenge Wang cites is the variation in urban landscapes in different countries. Different regions have their own cultural and architectural styles. Not trained on a large or diverse enough pool of images, the AI model could be thrown off by factors like paint color when performing wall material analysis. Another challenge is urban density variation.</p>
<p>“It is a very general limitation of current AI technology,” Wang said. “In order to be useful, it requires enough training data to represent the distribution of the real world, so we’re putting efforts into the data collection process to solve the generalization issue.”</p>
<p>To overcome this challenge, Wang aims to train and test the model for more cities. So far, he’s tested about eight cities in different countries.</p>
<p>“We need to generate more detailed and high-quality annotations to train the model with,” he said. “That is the way we can improve the model in the future so that it can be used more widely.”</p>
<p>Wang’s goal is to get the project to a point where it can be deployed as a service for more general industry use.</p>
<p>“We are creating application programming interfaces that can estimate and analyze buildings and households to allow seamless integration with other products,” he said. “We are also building a user-friendly application that all government agencies and organizations can use.”</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/streetview3.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/streetview3-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Street View to the Rescue: Deep Learning Paves the Way to Safer Buildings]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>For the World to See: Nonprofit Deploys GPU-Powered Simulators to Train Providers in Sight-Saving Surgery</title>
		<link>https://blogs.nvidia.com/blog/2023/10/20/helpmesee-nonprofit-training-simulator-for-cataract-surgery/</link>
		
		<dc:creator><![CDATA[Isha Salian]]></dc:creator>
		<pubDate>Fri, 20 Oct 2023 13:00:50 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Education]]></category>
		<category><![CDATA[Embedded Computing]]></category>
		<category><![CDATA[Healthcare and Life Sciences]]></category>
		<category><![CDATA[Rendering]]></category>
		<category><![CDATA[Social Impact]]></category>
		<category><![CDATA[Virtual Reality]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67485</guid>

					<description><![CDATA[GPU-powered surgical-simulation devices are helping train more than 2,000 doctors a year in lower-income countries to treat cataract blindness, the world’s leading cause of blindness, thanks to the nonprofit HelpMeSee. While cataract surgery has a success rate of around 99%, many patients in low- and middle-income countries lack access to the common procedure due to <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/20/helpmesee-nonprofit-training-simulator-for-cataract-surgery/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>GPU-powered surgical-simulation devices are helping train more than 2,000 doctors a year in lower-income countries to treat cataract blindness, the world’s <a href="https://academic.oup.com/inthealth/article/14/Supplement_1/i68/6563812" target="_blank" rel="noopener">leading cause of blindness</a>, thanks to the nonprofit HelpMeSee.</p>
<p>While cataract surgery has a success rate of <a href="https://okko.com.au/what-is-the-cataract-surgery-success-rate/#:~:text=The%20success%20rate%20for%20cataract,Sloan%2C%20F.%2C%202021." target="_blank" rel="noopener">around 99%</a>, many patients in low- and middle-income countries lack access to the common procedure due to a severe shortage of ophthalmologists. An estimated 90% of the <a href="https://academic.oup.com/inthealth/article/14/Supplement_1/i68/6563812" target="_blank" rel="noopener">100 million people</a> affected by cataract-related visual impairment or blindness are in these locations.</p>
<p>By training more healthcare providers — including those without a specialty in ophthalmology — to treat cataracts, <a href="https://helpmesee.org/" target="_blank" rel="noopener">HelpMeSee</a> improves the quality of life for patients such as a mother of two young children in Bhiwandi, near Mumbai, India, who was blinded by cataracts in both eyes.</p>
<p>“After the surgery, her vision improved dramatically and she was able to take up a job, changing the course of her entire family,” said Dr. Chetan Ahiwalay, chief instructor and subject-matter expert for HelpMeSee in India. “She and her husband are now happily raising their kids and leading a healthy life. These are the things that keep us going as doctors.”</p>
<p>HelpMeSee’s simulator devices use <a href="https://www.nvidia.com/en-us/design-visualization/rtx/">NVIDIA RTX GPUs</a> to render high-quality visuals, providing a more realistic training environment for doctors to hone their surgical skills. To further improve the trainee experience, NVIDIA experts are working with the HelpMeSee team to improve rendering performance, increase visual realism and augment the simulator with next-generation technologies such as real-time ray tracing and AI.</p>
<h2><b>Tackling Treatable Blindness With Accessible Training</b></h2>
<p>High-income countries have <a href="https://bmcpublichealth.biomedcentral.com/articles/10.1186/s12889-022-14491-0" target="_blank" rel="noopener">18x more ophthalmologists</a> per million residents than low-income countries. That coverage gap, which is far wider still in certain countries, makes it harder for those in thinly resourced areas to receive treatment for avoidable blindness.</p>
<p>HelpMeSee’s devices can train doctors on multiple eye procedures using immersive tools inspired by flight simulators used in aviation. The team trains doctors in countries including India, China, Madagascar, Mexico and the U.S., and rolls out multilingual training each year for new procedures.</p>
<p>The eye surgery simulator offers realistic 3D visuals, haptic feedback, performance scores and the opportunity to attempt a step of the procedure multiple times until the trainee achieves proficiency. Qualified instructors like Dr. Ahiwalay travel to rural and urban areas to deliver the training through structured courses — and help surgeons transition from the simulators to live surgeries.</p>
<figure id="attachment_67489" aria-describedby="caption-attachment-67489" style="width: 533px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/IMO-First-MSTC-Training_23-1-scaled.jpeg"><img decoding="async" loading="lazy" class="size-large wp-image-67489" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/IMO-First-MSTC-Training_23-1-533x500.jpeg" alt="Doctors training to perform cataract surgery" width="533" height="500" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/IMO-First-MSTC-Training_23-1-533x500.jpeg 533w, https://blogs.nvidia.com/wp-content/uploads/2023/10/IMO-First-MSTC-Training_23-1-400x375.jpeg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/IMO-First-MSTC-Training_23-1-768x720.jpeg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/IMO-First-MSTC-Training_23-1-1536x1441.jpeg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/IMO-First-MSTC-Training_23-1-scaled.jpeg 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/IMO-First-MSTC-Training_23-1-480x450.jpeg 480w, https://blogs.nvidia.com/wp-content/uploads/2023/10/IMO-First-MSTC-Training_23-1-229x215.jpeg 229w, https://blogs.nvidia.com/wp-content/uploads/2023/10/IMO-First-MSTC-Training_23-1-107x100.jpeg 107w, https://blogs.nvidia.com/wp-content/uploads/2023/10/IMO-First-MSTC-Training_23-1-1280x1201.jpeg 1280w" sizes="(max-width: 533px) 100vw, 533px" /></a><figcaption id="caption-attachment-67489" class="wp-caption-text">During a training session, doctors learn to perform manual small-incision cataract surgery.</figcaption></figure>
<p>“We’re lowering the barrier for healthcare practitioners to learn these specific skills that can have a profound impact on patients,” said Dr. Bonnie An Henderson, CEO of HelpMeSee, which is based in New York. “Simulation-based training will improve surgical skills while keeping patients safe.”</p>
<h2><b>Looking Ahead to AI, Advanced Rendering </b></h2>
<p>HelpMeSee works with <a href="https://surgicalscience.com/" target="_blank" rel="noopener">Surgical Science</a>, a supplier of medical virtual-reality simulators, based in Gothenburg, Sweden, to develop the 3D models and real-time rendering for its devices. Other collaborators — Strasbourg, France-based <a href="https://www.insimo.com/" target="_blank" rel="noopener">InSimo</a> and Pune, India-based <a href="https://services.harman.com/" target="_blank" rel="noopener">Harman Connected Services</a> — develop the physics-based simulations and user interface, respectively.  <a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/delivering_the_nucleus.png"><img decoding="async" loading="lazy" class="alignright wp-image-67492" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/delivering_the_nucleus-400x400.png" alt="" width="250" height="250" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/delivering_the_nucleus-400x400.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/delivering_the_nucleus-500x500.png 500w, https://blogs.nvidia.com/wp-content/uploads/2023/10/delivering_the_nucleus-150x150.png 150w, https://blogs.nvidia.com/wp-content/uploads/2023/10/delivering_the_nucleus-768x768.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/delivering_the_nucleus-450x450.png 450w, https://blogs.nvidia.com/wp-content/uploads/2023/10/delivering_the_nucleus-215x215.png 215w, https://blogs.nvidia.com/wp-content/uploads/2023/10/delivering_the_nucleus-100x100.png 100w, https://blogs.nvidia.com/wp-content/uploads/2023/10/delivering_the_nucleus.png 1024w" sizes="(max-width: 250px) 100vw, 250px" /></a></p>
<p>“Since there are many crucial visual cues during eye surgery, the simulation requires high fidelity,” said Sebastian Ullrich, senior manager of software development at Surgical Science, who has worked with HelpMeSee for years. “To render a realistic 3D representation of the human eye, we use custom shader materials with high-resolution textures to represent various anatomical components, mimic optical properties such as refraction, use order-independent transparency sorting and employ volume rendering.”</p>
<p>NVIDIA RTX GPUs support 3D volume rendering, stereoscopic rendering and depth sorting algorithms that provide a realistic visual experience for HelpMeSee’s trainees. Working with NVIDIA, the team is investigating AI models that could provide trainees with a real-time analysis of the practice procedure and offer recommendations for improvement.</p>
<p>Watch a demo of HelpMeSee’s <a href="https://www.youtube.com/watch?v=Y2vHBo5jpD8" target="_blank" rel="noopener">cataract surgery training simulation</a>.</p>
<p><a href="https://www.nvidia.com/en-us/industries/healthcare-life-sciences/healthcare-news-sign-up/"><i>Subscribe to NVIDIA healthcare news</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/Dr.-Samuel-Kwizera.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/Dr.-Samuel-Kwizera-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[For the World to See: Nonprofit Deploys GPU-Powered Simulators to Train Providers in Sight-Saving Surgery]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Eureka! NVIDIA Research Breakthrough Puts New Spin on Robot Learning</title>
		<link>https://blogs.nvidia.com/blog/2023/10/20/eureka-robotics-research/</link>
		
		<dc:creator><![CDATA[Angie Lee]]></dc:creator>
		<pubDate>Fri, 20 Oct 2023 13:00:24 +0000</pubDate>
				<category><![CDATA[Autonomous Machines]]></category>
		<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Research]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Isaac]]></category>
		<category><![CDATA[NVIDIA Research]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Robotics]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67624</guid>

					<description><![CDATA[A new AI agent developed by NVIDIA Research that can teach robots complex skills has trained a robotic hand to perform rapid pen-spinning tricks — for the first time as well as a human can. The stunning prestidigitation, showcased in the video above, is one of nearly 30 tasks that robots have learned to expertly <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/20/eureka-robotics-research/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>A new AI agent developed by <a href="https://www.nvidia.com/en-us/research/" target="_blank" rel="noopener">NVIDIA Research</a> that can teach robots complex skills has trained a robotic hand to perform rapid pen-spinning tricks — for the first time as well as a human can.</p>
<p>The stunning prestidigitation, showcased in the video above, is one of nearly 30 tasks that robots have learned to expertly accomplish thanks to Eureka, which autonomously writes reward algorithms to train bots.</p>
<p>Eureka has also taught robots to open drawers and cabinets, toss and catch balls, and manipulate scissors, among other tasks.</p>
<p>The Eureka research, <a href="https://eureka-research.github.io/" target="_blank" rel="noopener">published today</a>, includes a paper and the project’s AI algorithms, which developers can experiment with using <a href="https://developer.nvidia.com/isaac-gym" target="_blank" rel="noopener">NVIDIA Isaac Gym</a>, a physics simulation reference application for <a href="https://blogs.nvidia.com/blog/2018/08/02/supervised-unsupervised-learning/" target="_blank" rel="noopener">reinforcement learning</a> research. Isaac Gym is built on <a href="https://developer.nvidia.com/omniverse" target="_blank" rel="noopener">NVIDIA Omniverse</a>, a development platform for building 3D tools and applications based on the OpenUSD framework. Eureka itself is powered by the GPT-4 <a href="https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/" target="_blank" rel="noopener">large language model</a>.</p>
<p>“Reinforcement learning has enabled impressive wins over the last decade, yet many challenges still exist, such as reward design, which remains a trial-and-error process,” said Anima Anandkumar, senior director of AI research at NVIDIA and an author of the Eureka paper. “Eureka is a first step toward developing new algorithms that integrate generative and reinforcement learning methods to solve hard tasks.”</p>
<p><iframe loading="lazy" title="Eureka! Extreme Robot Dexterity with LLMs | NVIDIA Research Paper" width="500" height="281" src="https://www.youtube.com/embed/sDFAWnrCqKc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<h2><b>AI Trains Robots</b></h2>
<p>Eureka-generated reward programs — which enable trial-and-error learning for robots — outperform expert human-written ones on more than 80% of tasks, according to the paper. This leads to an average performance improvement of more than 50% for the bots.</p>
<div style="width: 1920px;" class="wp-video"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video class="wp-video-shortcode" id="video-67624-1" width="1920" height="1080" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/franka_cabinet.mp4?_=1" /><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/franka_cabinet.mp4">https://blogs.nvidia.com/wp-content/uploads/2023/10/franka_cabinet.mp4</a></video></div>
<p><i>Robot arm taught by Eureka to open a drawer.</i></p>
<p>The AI agent taps the GPT-4 LLM and <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/" target="_blank" rel="noopener">generative AI</a> to write software code that rewards robots for reinforcement learning. It doesn’t require task-specific prompting or predefined reward templates — and readily incorporates human feedback to modify its rewards for results more accurately aligned with a developer’s vision.</p>
<p>Using GPU-accelerated simulation in Isaac Gym, Eureka can quickly evaluate the quality of large batches of reward candidates for more efficient training.</p>
<p>Eureka then constructs a summary of the key stats from the training results and instructs the LLM to improve its generation of reward functions. In this way, the AI is self-improving. It’s taught all kinds of robots — quadruped, bipedal, quadrotor, dexterous hands, cobot arms and others — to accomplish all kinds of tasks.</p>
<p>The research paper provides in-depth evaluations of 20 Eureka-trained tasks, based on open-source dexterity benchmarks that require robotic hands to demonstrate a wide range of complex manipulation skills.</p>
<p>The results from nine Isaac Gym environments are showcased in visualizations generated using NVIDIA Omniverse.</p>
<div style="width: 1920px;" class="wp-video"><video class="wp-video-shortcode" id="video-67624-2" width="1920" height="1080" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/humanoid.mp4?_=2" /><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/humanoid.mp4">https://blogs.nvidia.com/wp-content/uploads/2023/10/humanoid.mp4</a></video></div>
<p><i>Humanoid robot learns a running gait via Eureka.</i></p>
<p>“Eureka is a unique combination of large language models and NVIDIA GPU-accelerated simulation technologies,” said Linxi “Jim” Fan, senior research scientist at NVIDIA, who’s one of the project’s contributors. “We believe that Eureka will enable dexterous robot control and provide a new way to produce physically realistic animations for artists.”</p>
<p>It’s breakthrough work bound to get developers’ minds spinning with possibilities, adding to recent NVIDIA Research advancements like <a href="https://blogs.nvidia.com/blog/2023/10/04/ai-jim-fan/" target="_blank" rel="noopener">Voyager</a>, an AI agent built with GPT-4 that can autonomously play <i>Minecraft</i>.</p>
<p>NVIDIA Research comprises hundreds of scientists and engineers worldwide, with teams focused on topics including AI, computer graphics, computer vision, self-driving cars and robotics.</p>
<p><i>Learn more about </i><a href="https://eureka-research.github.io/" target="_blank" rel="noopener"><i>Eureka</i></a><i> and </i><a href="https://www.nvidia.com/en-us/research/" target="_blank" rel="noopener"><i>NVIDIA Research</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2023/10/franka_cabinet.mp4" length="275129" type="video/mp4" />
<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2023/10/humanoid.mp4" length="2901442" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/eureka-featured-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/eureka-featured-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Eureka! NVIDIA Research Breakthrough Puts New Spin on Robot Learning]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Next-Level Computing: NVIDIA and AMD Deliver Powerful Workstations to Accelerate AI, Rendering and Simulation</title>
		<link>https://blogs.nvidia.com/blog/2023/10/19/ai-workstations/</link>
		
		<dc:creator><![CDATA[Stacy Ozorio]]></dc:creator>
		<pubDate>Thu, 19 Oct 2023 19:30:07 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[Rendering]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67603</guid>

					<description><![CDATA[To enable professionals worldwide to build and run AI applications right from their desktops, NVIDIA and AMD are powering a new line of workstations equipped with NVIDIA RTX Ada Generation GPUs and AMD Ryzen Threadripper PRO 7000 WX-Series CPUs. Bringing together the highest levels of AI computing, rendering and simulation capabilities, these new platforms enable <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/19/ai-workstations/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>To enable professionals worldwide to build and run AI applications right from their desktops, NVIDIA and AMD are powering a new line of <a href="https://www.nvidia.com/en-us/design-visualization/workstations/latest/">workstations</a> equipped with <a href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/" target="_blank" rel="noopener">NVIDIA RTX Ada Generation GPUs</a> and <a href="https://ir.amd.com/news-events/press-releases/detail/1162/amd-introduces-new-amd-ryzen-threadripper-7000-series" target="_blank" rel="noopener">AMD Ryzen Threadripper PRO 7000 WX-Series CPUs</a>.</p>
<p>Bringing together the highest levels of AI computing, rendering and simulation capabilities, these new platforms enable professionals to efficiently tackle the most resource-intensive, large-scale AI workflows locally.</p>
<h2><b>Bringing AI Innovation to the Desktop</b></h2>
<p>Advanced AI tasks typically require data-center-level performance. Training a <a href="https://www.nvidia.com/en-us/glossary/data-science/large-language-models/" target="_blank" rel="noopener">large language model</a> with a trillion parameters, for example, takes thousands of GPUs running for weeks, though research is underway to reduce model size and enable model training on smaller systems while still maintaining high levels of AI model accuracy.</p>
<p>The new NVIDIA RTX GPU and AMD CPU-powered AI workstations provide the power and performance required for training such smaller models, as well as local fine-tuning, and helping to offload data center and cloud resources for AI development tasks. The devices let users select single- or multi-GPU configurations as required for their workloads.</p>
<p>Smaller trained AI models also provide the opportunity to use workstations for local inferencing. RTX GPU and AMD CPU-powered workstations can be configured to run these smaller AI models for inference serving for small workgroups or departments.</p>
<p>With up to 48GB of memory in a single NVIDIA RTX GPU, these workstations offer a cost-effective way to reduce compute load on data centers. And when professionals do need to scale training and deployment from these workstations to data centers or the cloud, the <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/" target="_blank" rel="noopener">NVIDIA AI Enterprise</a> software platform enables seamless portability of workflows and toolchains.</p>
<p>RTX GPU and AMD CPU-powered workstations also enable cutting-edge visual workflows. With accelerated computing power, the new workstations enable highly interactive content creation, industrial digitalization, and advanced simulation and design.</p>
<h2><b>Unmatched Power, Performance and Flexibility</b></h2>
<p>AMD Ryzen Threadripper PRO 7000 WX-Series processors provide the CPU platform for the next generation of demanding workloads. The processors deliver a significant increase in core count — up to 96 cores per CPU — and industry-leading maximum memory bandwidth in a single socket.</p>
<p>Combining them with the latest NVIDIA RTX Ada Generation GPUs brings unmatched power and performance in a workstation. The GPUs enable up to 2x the performance in ray tracing, AI processing, graphics rendering and computational tasks compared to the previous generation.</p>
<p>Ada Generation GPUs options include the RTX 4000 SFF, RTX 4000, RTX 4500, RTX 5000 and RTX 6000. They’re built on the <a href="https://www.nvidia.com/en-us/geforce/ada-lovelace-architecture/" target="_blank" rel="noopener">NVIDIA Ada Lovelace architecture</a> and feature up to 142 third-generation RT Cores, 568 fourth-generation Tensor Cores and 18,176 latest-generation CUDA cores.</p>
<p>From architecture and manufacturing to media and entertainment and healthcare, professionals across industries will be able to use the new workstations to tackle challenging AI computing workloads — along with 3D rendering, product visualization, simulation and scientific computing tasks.</p>
<h2><b>Availability</b></h2>
<p><a href="https://www.nvidia.com/en-us/design-visualization/workstations/latest/">New workstations</a> powered by NVIDIA RTX Ada Generation GPUs and the latest AMD Threadripper Pro processors will be available starting next month from BOXX and HP, with other system integrators offering them soon.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/proviz-intel-nv-workstation-kv-2972327-edit-r2.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/proviz-intel-nv-workstation-kv-2972327-edit-r2-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Next-Level Computing: NVIDIA and AMD Deliver Powerful Workstations to Accelerate AI, Rendering and Simulation]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA AI Now Available in Oracle Cloud Marketplace</title>
		<link>https://blogs.nvidia.com/blog/2023/10/19/nvidia-ai-now-available-in-oracle-cloud-marketplace/</link>
		
		<dc:creator><![CDATA[Charlie Boyle]]></dc:creator>
		<pubDate>Thu, 19 Oct 2023 19:00:07 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[NVIDIA DGX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67595</guid>

					<description><![CDATA[Training generative AI models just got easier. NVIDIA DGX Cloud AI supercomputing platform and NVIDIA AI Enterprise software are now available in Oracle Cloud Marketplace, making it possible for Oracle Cloud Infrastructure customers to access high-performance accelerated computing and software to run secure, stable and supported production AI in just a few clicks. The addition <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/19/nvidia-ai-now-available-in-oracle-cloud-marketplace/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Training generative AI models just got easier.</p>
<p><a href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a> AI supercomputing platform and <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> software are <a href="https://www.oracle.com/news/announcement/oracle-continues-ai-momentum-with-nvidia-ai-enterprise-and-dgx-cloud-availability-in-the-oracle-cloud-marketplace-2023-10-19/">now available in Oracle Cloud Marketplace</a>, making it possible for Oracle Cloud Infrastructure customers to access high-performance accelerated computing and software to run secure, stable and supported production AI in just a few clicks.</p>
<p>The addition — an industry first — brings new capabilities for end-to-end development and deployment on Oracle Cloud. Enterprises can get started from the Oracle Cloud Marketplace to train models on DGX Cloud, and then deploy their applications on OCI with NVIDIA AI Enterprise.</p>
<h2><b>Oracle Cloud and NVIDIA Lift Industries Into Era of AI</b></h2>
<p>Thousands of enterprises around the world rely on OCI to power the applications that drive their businesses. Its customers include leaders across industries such as healthcare, scientific research, financial services, telecommunications and more.</p>
<p>Oracle Cloud Marketplace is a catalog of solutions that offers customers flexible consumption models and simple billing. Its addition of DGX Cloud and NVIDIA AI Enterprise lets OCI customers use their existing cloud credits to integrate NVIDIA’s leading AI supercomputing platform and software into their development and deployment pipelines.</p>
<p>With DGX Cloud, OCI customers can train models for generative AI applications like intelligent chatbots, search, summarization and content generation.</p>
<p>The University at Albany, in upstate New York, recently launched its <a href="https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.albany.edu%2Fai-plus&amp;data=05%7C01%7Cacourtney%40nvidia.com%7Ca0e8ba71696546347ee408dbd00ebf82%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638332534011886277%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&amp;sdata=TTPp%2BLF%2B%2FopKSMJB76gzSDH%2FUYQhSIptBj4WUTYnPEo%3D&amp;reserved=0">AI Plus initiative</a>, which is integrating teaching and learning about AI across the university’s research and academic enterprise, in fields such as cybersecurity, weather prediction, health data analytics, drug discovery and next-generation semiconductor design. It will also foster collaborations across the humanities, social sciences, public policy and public health. The university is using DGX Cloud AI supercomputing instances on OCI as it builds out an on-premises supercomputer.</p>
<p>“We’re accelerating our mission to infuse AI into virtually every academic and research disciplines,” said Thenkurussi (Kesh) Kesavadas, vice president for research and economic development at UAlbany. “We will drive advances in healthcare, security and economic competitiveness, while equipping students for roles in the evolving job market.”</p>
<p>NVIDIA AI Enterprise brings the software layer of the NVIDIA AI platform to OCI. It includes <a href="https://developer.nvidia.com/nemo">NVIDIA NeMo</a> frameworks for building LLMs, <a href="https://www.nvidia.com/en-us/deep-learning-ai/software/rapids/">NVIDIA RAPIDS</a> for data science and <a href="https://developer.nvidia.com/tensorrt-llm-early-access">NVIDIA TensorRT-LLM</a> and <a href="https://developer.nvidia.com/triton-inference-server">NVIDIA Triton Inference Server</a> for supercharging production AI. NVIDIA software for cybersecurity, computer vision, speech AI and more is also included. Enterprise-grade support, security and stability ensure a smooth transition of AI projects from pilot to production.</p>
<figure id="attachment_65649" aria-describedby="caption-attachment-65649" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/07/dgx-cloud-key-visual.jpg"><img decoding="async" loading="lazy" class="wp-image-65649 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2023/07/dgx-cloud-key-visual-672x357.jpg" alt="NVIDIA DGX Cloud generative AI training" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/07/dgx-cloud-key-visual-672x357.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/07/dgx-cloud-key-visual-400x213.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/07/dgx-cloud-key-visual-768x408.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/07/dgx-cloud-key-visual-842x447.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/07/dgx-cloud-key-visual-406x215.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/07/dgx-cloud-key-visual-188x100.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/07/dgx-cloud-key-visual.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-65649" class="wp-caption-text">NVIDIA DGX Cloud provides enterprises immediate access to AI supercomputing platform and software hosted by their preferred cloud provider.</figcaption></figure>
<h2><b>AI Supercomputing Platform Hosted by OCI</b></h2>
<p>NVIDIA DGX Cloud provides enterprises immediate access to an AI supercomputing platform and software.</p>
<p>Hosted by OCI, DGX Cloud provides enterprises with access to multi-node training on NVIDIA GPUs, paired with NVIDIA AI software, for training advanced models for generative AI and other groundbreaking applications.</p>
<p>Each DGX Cloud instance consists of eight <a href="https://www.nvidia.com/en-us/data-center/tensor-cores/">NVIDIA Tensor Core GPUs</a> interconnected with network fabric, purpose-built for multi-node training. This high-performance computing architecture also includes industry-leading AI development software and offers direct access to NVIDIA AI expertise so businesses can train LLMs faster.</p>
<p>OCI customers access DGX Cloud using <a href="https://www.nvidia.com/en-us/data-center/base-command-platform/">NVIDIA Base Command Platform</a>, which gives developers access to an AI supercomputer through a web browser. By providing a single-pane view of the customer’s AI infrastructure, Base Command Platform simplifies the management of multinode clusters.</p>
<figure id="attachment_61238" aria-describedby="caption-attachment-61238" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2022/12/nvaie-promo-3-0.jpg"><img decoding="async" loading="lazy" class="size-large wp-image-61238" src="https://blogs.nvidia.com/wp-content/uploads/2022/12/nvaie-promo-3-0-672x336.jpg" alt="NVIDIA AI Enterprise software" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2022/12/nvaie-promo-3-0-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2022/12/nvaie-promo-3-0-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2022/12/nvaie-promo-3-0-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2022/12/nvaie-promo-3-0-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2022/12/nvaie-promo-3-0-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2022/12/nvaie-promo-3-0-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2022/12/nvaie-promo-3-0-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2022/12/nvaie-promo-3-0-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2022/12/nvaie-promo-3-0.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-61238" class="wp-caption-text">NVIDIA AI Enterprise software powers secure, stable and supported production AI and data science.</figcaption></figure>
<h2><b>Software for Secure, Stable and Supported Production AI</b></h2>
<p>NVIDIA AI Enterprise enables rapid development and deployment of AI and data science.</p>
<p>With NVIDIA AI Enterprise on Oracle Cloud Marketplace, enterprises can efficiently build an application once and deploy it on OCI and their on-prem infrastructure, making a multi- or hybrid-cloud strategy cost-effective and easy to adopt. Since NVIDIA AI Enterprise is also included in NVIDIA DGX Cloud, customers can streamline the transition from training on DGX Cloud to deploying their AI application into production with NVIDIA AI Enterprise on OCI, since the AI software runtime is consistent across the environments.</p>
<p>Qualified customers can purchase NVIDIA AI Enterprise and NVIDIA DGX Cloud with their existing Oracle Universal Credits.</p>
<p>Visit <a href="https://cloudmarketplace.oracle.com/marketplace/en_US/adf.task-flow?tabName=O&amp;adf.tfDoc=%2FWEB-INF%2Ftaskflow%2Fadhtf.xml&amp;application_id=155314141&amp;adf.tfId=adhtf">NVIDIA AI Enterprise</a> and <a href="https://cloudmarketplace.oracle.com/marketplace/en_US/adf.task-flow?tabName=O&amp;adf.tfDoc=%2FWEB-INF%2Ftaskflow%2Fadhtf.xml&amp;application_id=154619827&amp;adf.tfId=adhtf">NVIDIA DGX Cloud</a> on the Oracle Cloud Marketplace to get started today.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-OCI-logos.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-OCI-logos-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA AI Now Available in Oracle Cloud Marketplace]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Coming in Clutch: Stream ‘Counter-Strike 2’ From the Cloud for Highest Frame Rates</title>
		<link>https://blogs.nvidia.com/blog/2023/10/19/geforce-now-thursday-oct-19/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 19 Oct 2023 13:00:22 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67578</guid>

					<description><![CDATA[Rush to the cloud — stream Counter-Strike 2 on GeForce NOW for the highest frame rates. Members can play through the newest chapter of Valve’s elite, competitive, first-person shooter from the cloud. It’s all part of an action-packed GFN Thursday, with 22 more games joining the cloud gaming platform’s library, including Hot Wheels Unleashed 2 <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/19/geforce-now-thursday-oct-19/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Rush to the cloud — stream <i>Counter-Strike 2 </i>on <a href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a> for the highest frame rates. Members can play through the newest chapter of Valve’s elite, competitive, first-person shooter from the cloud.</p>
<p>It’s all part of an action-packed GFN Thursday, with 22 more games joining the cloud gaming platform’s library, including <i>Hot Wheels Unleashed 2 &#8211; Turbocharged</i>.</p>
<h2><b>“Rush B! Rush B!”</b></h2>
<p><iframe loading="lazy" title="Counter-Strike 2: Responsive Smokes" width="500" height="281" src="https://www.youtube.com/embed/_y9MpNcAitQ?start=1&#038;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>&nbsp;</p>
<p><i>Counter-Strike 2 </i>is the long-awaited upgrade to one of the most recognizable competitive first-person shooters in the world.</p>
<p>Building on the legacy of <i>Counter-Strike: Global Offensive</i>, the latest iteration brings the action to Valve’s long-anticipated Source 2 video game engine, promising enhanced graphical fidelity with a physically based rendering system for more realistic textures and materials, dynamic lighting, reflections and more.</p>
<p>Smoke grenades are now dynamic volumetric objects that can interact with their surroundings by reacting to lighting and other environmental effects. And smoke particles work with the unified lighting system, allowing for more realistic light and color.</p>
<p>Even better: <a href="https://www.nvidia.com/en-us/geforce-now/memberships/">GeForce NOW Ultimate members</a> can take full advantage of <a href="https://www.nvidia.com/en-us/geforce/technologies/reflex/">NVIDIA Reflex</a> for ultra-low-latency gameplay streaming from the cloud. Rush the objective with the squad on <i>Counter-Strike 2</i>’s remastered maps at up to 240 frames per second — a first for cloud gaming. Upgrade today for the Ultimate <i>Counter-Strike</i> experience.</p>
<h2><b>Vroom, Vroom!</b></h2>
<figure id="attachment_67584" aria-describedby="caption-attachment-67584" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-67584" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Hot_wheels_Unleashed_2_Turbocharged-672x336.jpg" alt="" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Hot_wheels_Unleashed_2_Turbocharged-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Hot_wheels_Unleashed_2_Turbocharged-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Hot_wheels_Unleashed_2_Turbocharged-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Hot_wheels_Unleashed_2_Turbocharged-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Hot_wheels_Unleashed_2_Turbocharged-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Hot_wheels_Unleashed_2_Turbocharged-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Hot_wheels_Unleashed_2_Turbocharged-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Hot_wheels_Unleashed_2_Turbocharged-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Hot_wheels_Unleashed_2_Turbocharged.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-67584" class="wp-caption-text"><em>We’re going turbo.</em></figcaption></figure>
<p>There’s more action around every turn of the GeForce NOW library. Put the pedal to the metal in <i>Hot Wheels Unleashed 2 &#8211; Turbocharged, </i>one of 22 newly supported games joining this week:</p>
<ul>
<li><i>Wizard With a Gun </i>(New release on <a href="https://store.steampowered.com/app/1150530?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 17)</li>
<li><i>Alaskan Road Truckers </i>(New release on <a href="https://store.steampowered.com/app/849100?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 18)</li>
<li><i>Hellboy: Web of Wyrd </i>(New release on <a href="https://store.steampowered.com/app/2160480?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 18)</li>
<li><i>AirportSim </i>(New release on <a href="https://store.steampowered.com/app/1715280?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 19)</li>
<li><i>Eternal Threads </i>(New release on <a href="https://www.epicgames.com/store/p/eternal-threads-197169?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>, Oct. 19)</li>
<li><i>Hot Wheels Unleashed 2 &#8211; Turbocharged </i>(New release on <a href="https://store.steampowered.com/app/2051120?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 19)</li>
<li><i>Laika Aged Through Blood </i>(New release on <a href="https://store.steampowered.com/app/1796220?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 19)</li>
<li><i>Battle Chasers: Nightwar </i>(<a href="https://www.xbox.com/en-us/games/store/battle-chasers-nightwar/9nt8xr7d5l00">Xbox</a>, available on Microsoft Store)</li>
<li><i>Black Skylands </i>(<a href="https://www.xbox.com/games/store/black-skylands/9N9TTNP04TTK?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Blair Witch </i>(<a href="https://www.xbox.com/en-US/games/store/blair-witch/9ng7lg421v0q">Xbox</a>, available on Microsoft Store)</li>
<li><i>Chicory: A Colorful Tale </i>(<a href="https://www.xbox.com/games/store/chicory-a-colorful-tale/9PFGQGC0XWLV?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass)</li>
<li><i>Dead by Daylight</i> (<a href="https://www.xbox.com/games/store/dead-by-daylight-windows/9NMS4SFNBGBH?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass)</li>
<li><i>Dune: Spice Wars </i>(<a href="https://www.xbox.com/games/store/dune-spice-wars-game-preview/9N46JZZNGS3P?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass)</li>
<li><i>Everspace 2 </i>(<a href="https://www.xbox.com/games/store/everspace-2/9PFX7F33KVG8?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass)</li>
<li><i>EXAPUNKS</i> (<a href="https://www.xbox.com/games/store/exapunks/9P87731ZDLG0?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass)</li>
<li><i>Gungrave G.O.R.E </i>(<a href="https://www.xbox.com/games/store/gungrave-gore/9P87CLMPXSN6?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass)</li>
<li><i>Railway Empire 2 </i>(<a href="https://www.xbox.com/games/store/railway-empire-2-win/9NDXLPW4TTF2?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass)</li>
<li><i>Techtonica </i>(<a href="https://www.xbox.com/games/store/techtonica-game-preview/9nr1rvdk9t78?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass)</li>
<li><i>Teenage Mutant Ninja Turtles: Shredder&#8217;s Revenge </i>(<a href="https://www.xbox.com/games/store/teenage-mutant-ninja-turtles-shredders-revenge/9NS3673HVH41?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass)</li>
<li><i>Torchlight III </i>(<a href="https://www.xbox.com/games/store/torchlight-iii/9N944BP7SMCG?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass)</li>
<li><i>Trine 5: A Clockwork Conspiracy </i>(<a href="https://www.epicgames.com/store/p/trine-5?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
<li><i>Vampire Survivors </i>(<a href="https://www.xbox.com/games/store/vampire-survivors/9PD5BM2Z8C4L?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
</ul>
<p>What are you planning to play this weekend? Let us know on <a href="https://www.twitter.com/nvidiagfn">Twitter</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">What&#39;s a game you played this year that has become a fave? <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f3ae.png" alt="🎮" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f329.png" alt="🌩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a href="https://twitter.com/NVIDIAGFN/status/1714672689145925898?ref_src=twsrc%5Etfw">October 18, 2023</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/gfn-thursday-10-19-nv-blog-1280x680-no-copy.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/gfn-thursday-10-19-nv-blog-1280x680-no-copy-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Coming in Clutch: Stream ‘Counter-Strike 2’ From the Cloud for Highest Frame Rates]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Expands Robotics Platform to Meet the Rise of Generative AI</title>
		<link>https://blogs.nvidia.com/blog/2023/10/18/metropolis-jetson-isaac-robotics-edge-ai-developers/</link>
		
		<dc:creator><![CDATA[Amit Goel]]></dc:creator>
		<pubDate>Wed, 18 Oct 2023 14:00:48 +0000</pubDate>
				<category><![CDATA[Autonomous Machines]]></category>
		<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Metropolis]]></category>
		<category><![CDATA[NVIDIA Isaac Sim]]></category>
		<category><![CDATA[NVIDIA Jetson]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67421</guid>

					<description><![CDATA[Powerful generative AI models and cloud-native APIs and microservices are coming to the edge. Generative AI is bringing the power of transformer models and large language models to virtually every industry. That reach now includes areas that touch edge, robotics and logistics systems: defect detection, real-time asset tracking, autonomous planning and navigation, human-robot interactions and <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/18/metropolis-jetson-isaac-robotics-edge-ai-developers/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Powerful generative AI models and cloud-native APIs and microservices are coming to the edge.</p>
<p><a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">Generative AI</a> is bringing the power of transformer models and large language models to virtually every industry. That reach now includes areas that touch edge, robotics and logistics systems: defect detection, real-time asset tracking, autonomous planning and navigation, human-robot interactions and more.</p>
<p>NVIDIA today announced major expansions to two frameworks on the <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/">NVIDIA Jetson platform</a> for edge AI and robotics: the NVIDIA <a href="https://developer.nvidia.com/isaac-ros">Isaac ROS</a> robotics framework has entered general availability, and the NVIDIA Metropolis expansion on Jetson is coming next.</p>
<p>To accelerate AI application development and deployments at the edge, NVIDIA has also created a <a href="https://www.jetson-ai-lab.com">Jetson Generative AI Lab</a> for developers to use with the latest open-source generative AI models.</p>
<p>More than 1.2 million developers and over 10,000 customers have chosen NVIDIA AI and the Jetson platform, including Amazon Web Services, Cisco, John Deere, Medtronic, Pepsico and Siemens.</p>
<p>With the rapidly evolving AI landscape addressing increasingly complicated scenarios, developers are being challenged by longer development cycles to build AI applications for the edge. Reprogramming robots and AI systems on the fly to meet changing environments, manufacturing lines and automation needs of customers is time-consuming and requires expert skills.</p>
<p>Generative AI offers zero-shot learning — the ability for a model to recognize things specifically unseen before in training — with a natural language interface to simplify the development, deployment and management of AI at the edge.</p>
<h2><b>Transforming the AI Landscape</b></h2>
<p>Generative AI dramatically improves ease of use by understanding human language prompts to make model changes. Those AI models are more flexible in detecting, segmenting, tracking, searching and even reprogramming — and  help outperform traditional <a href="https://blogs.nvidia.com/blog/2018/09/05/whats-the-difference-between-a-cnn-and-an-rnn/">convolutional neural network</a>-based models.</p>
<p>Generative AI is expected to add $10.5 billion in revenue for manufacturing operations worldwide by 2033, according to ABI Research.</p>
<p>“Generative AI will significantly accelerate deployments of AI at the edge with better generalization, ease of use and higher accuracy than previously possible,” said Deepu Talla, vice president of embedded and edge computing at NVIDIA. “This largest-ever software expansion of our Metropolis and Isaac frameworks on Jetson, combined with the power of transformer models and generative AI, addresses this need.”</p>
<p><iframe loading="lazy" title="Generative AI Models at the Edge Powered by NVIDIA Jetson Orin" width="500" height="281" src="https://www.youtube.com/embed/BAMOw7qlVXw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p><b>Developing With Generative AI at the Edge</b></p>
<p>The Jetson Generative AI Lab provides developers access to optimized tools and tutorials for deploying open-source LLMs, diffusion models to generate stunning interactive images, vision language models (VLMs) and vision transformers (ViTs) that combine vision AI and natural language processing to provide comprehensive understanding of the scene.</p>
<p>Developers can also use the <a href="https://developer.nvidia.com/tao-toolkit">NVIDIA TAO Toolkit</a> to create efficient and accurate AI models for edge applications. TAO provides a low-code interface to fine-tune and optimize vision AI models, including ViT and vision foundational models. They can also customize and fine-tune foundational models like NVIDIA NV-DINOv2 or public models like OpenCLIP to create highly accurate vision AI models with very little data. TAO additionally now includes VisualChangeNet, a new transformer-based model for defect inspection.</p>
<h2><b>Harnessing New Metropolis and Isaac Frameworks</b></h2>
<p><a href="https://developer.nvidia.com/metropolis">NVIDIA Metropolis</a> makes it easier and more cost-effective for enterprises to embrace world-class, vision AI-enabled solutions to improve critical operational efficiency and safety problems. The platform brings a collection of powerful application programming interfaces and microservices for developers to quickly develop complex vision-based applications.</p>
<p><img decoding="async" loading="lazy" class="wp-image-67550 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-400x225.jpg" alt="" width="673" height="379" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-1280x720.jpg 1280w" sizes="(max-width: 673px) 100vw, 673px" /></p>
<p>More than 1,000 companies, including BMW Group, Pepsico, Kroger, Tyson Foods, Infosys and Siemens, are using NVIDIA Metropolis developer tools to solve Internet of Things, sensor processing and operational challenges with vision AI — and the rate of adoption is quickening. The tools have now been downloaded over 1 million times by those looking to build vision AI applications.</p>
<p>To help developers quickly build and deploy scalable vision AI applications, an expanded set of Metropolis APIs and microservices on NVIDIA Jetson will be available by year’s end.</p>
<p><iframe loading="lazy" title="Build Complex Vision AI Applications Faster with NVIDIA Metropolis APIs and Microservices" width="500" height="281" src="https://www.youtube.com/embed/ifaMK8mVrHk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Hundreds of customers use the NVIDIA Isaac platform to develop high-performance robotics solutions across diverse domains, including agriculture, warehouse automation, last-mile delivery and service robotics, among others.</p>
<p>At ROSCon 2023, NVIDIA <a href="https://developer.nvidia.com/blog/accelerate-ai-enabled-robotics-with-advanced-simulation-and-perception-tools-in-nvidia-isaac-platform/">announced major improvements</a> to perception and simulation capabilities with new releases of Isaac ROS and Isaac Sim software. Built on the widely adopted open-source Robot Operating System (ROS), Isaac ROS brings perception to automation, giving eyes and ears to the things that move. By harnessing the power of GPU-accelerated GEMs, including visual odometry, depth perception, 3D scene reconstruction, localization and planning, robotics developers gain the tools needed to swiftly engineer robotic solutions tailored for a diverse range of applications.</p>
<p><img decoding="async" loading="lazy" class="wp-image-67547 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-400x225.jpg" alt="" width="690" height="388" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-1280x720.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support.jpg 1920w" sizes="(max-width: 690px) 100vw, 690px" /></p>
<p>With the general availability of the latest <a href="https://developer.nvidia.com/isaac-ros">Isaac ROS 2.0</a> release, developers can now create and bring high-performance robotics solutions to market with Jetson.</p>
<p>“ROS continues to grow and evolve to provide open-source software for the whole robotics community,” said Geoff Biggs, CTO of the Open Source Robotics Foundation. “NVIDIA’s new prebuilt ROS 2 packages, launched with this release, will accelerate that growth by making ROS 2 readily available to the vast NVIDIA Jetson developer community.”</p>
<h2><b>Delivering New Reference AI Workflows</b></h2>
<p>Developing a production-ready AI solution entails optimizing the development and training of AI models tailored to specific use cases, implementing robust security features on the platform, orchestrating the application, managing fleets, establishing seamless edge-to-cloud communication and more.</p>
<p>NVIDIA announced a curated collection of AI reference workflows based on Metropolis and Isaac frameworks that enable developers to quickly adopt the entire workflow or selectively integrate individual components, resulting in substantial reductions in both development time and cost. The three distinct AI workflows include: Network Video Recording, Automatic Optical Inspection and Autonomous Mobile Robot.</p>
<p>“NVIDIA Jetson, with its broad and diverse user base and partner ecosystem, has helped drive a revolution in robotics and AI at the edge,” said Jim McGregor, principal analyst at Tirias Research. “As application requirements become increasingly complex, we need a foundational shift to platforms that simplify and accelerate the creation of edge deployments. This significant software expansion by NVIDIA gives developers access to new multi-sensor models and generative AI capabilities.”</p>
<h2><b>More Coming on the Horizon </b></h2>
<p>NVIDIA announced a collection of system services which are fundamental capabilities that every developer requires when building edge AI solutions. These services will simplify integration into workflows and spare developer the arduous task of building them from the ground up.</p>
<p>The new NVIDIA JetPack 6, expected to be available by year’s end, will empower AI developers to stay at the cutting edge of computing without the need for a full Jetson Linux upgrade, substantially expediting development timelines and liberating them from Jetson Linux dependencies. JetPack 6 will also use the collaborative efforts with Linux distribution partners to expand the horizon of Linux-based distribution choices, including Canonical’s Optimized and Certified Ubuntu, Wind River Linux, Concurrent Real-Time Redhawk Linux and various Yocto-based distributions.</p>
<h2><b>Partner Ecosystem Benefits From Platform Expansion</b></h2>
<p>The <a href="https://developer.nvidia.com/embedded/ecosystem">Jetson partner ecosystem</a> provides a wide range of support, from hardware, AI software and application design services to sensors, connectivity and developer tools. These <a href="https://www.nvidia.com/en-us/about-nvidia/partners/">NVIDIA Partner Network</a> innovators play a vital role in providing the building blocks and sub-systems for many products sold on the market.</p>
<p>The latest release allows Jetson partners to accelerate their time to market and expand their customer base by adopting AI with increased performance and capabilities.</p>
<p>Independent software vendor partners will also be able to expand their offerings for Jetson.</p>
<p><img decoding="async" loading="lazy" class="wp-image-67544 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-400x225.jpg" alt="" width="588" height="331" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-1280x720.jpg 1280w" sizes="(max-width: 588px) 100vw, 588px" /></p>
<p><i>Join us Tuesday, Nov. 7, at 9 a.m. PT for the </i><a href="https://info.nvidia.com/jetson-gen-ai-webinar.html"><i>Bringing Generative AI to Life with NVIDIA Jetson</i></a><i> webinar, where technical experts will dive deeper into the news announced here, including accelerated APIs and quantization methods for deploying LLMs and VLMs on Jetson, optimizing vision transformers with TensorRT, and more.</i></p>
<p><i>Sign up for NVIDIA Metropolis early access </i><a href="https://developer.nvidia.com/metropolis/notify-me"><i>here</i></a><i>. </i></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/MetropolisJetson2.png"
			type="image/png"
			width="1600"
			height="867"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/MetropolisJetson2-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Expands Robotics Platform to Meet the Rise of Generative AI]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Making Machines Mindful: NYU Professor Talks Responsible AI</title>
		<link>https://blogs.nvidia.com/blog/2023/10/18/julia-stoyanovich-responsible-ai/</link>
		
		<dc:creator><![CDATA[Kristen Yee]]></dc:creator>
		<pubDate>Wed, 18 Oct 2023 13:00:49 +0000</pubDate>
				<category><![CDATA[The AI Podcast]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Trustworthy AI]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67565</guid>

					<description><![CDATA[Artificial intelligence is now a household term. Responsible AI is hot on its heels. Julia Stoyanovich, associate professor of computer science and engineering at NYU and director of the university’s Center for Responsible AI, wants to make the terms “AI” and “responsible AI” synonymous. In the latest episode of the NVIDIA AI Podcast, host Noah <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/18/julia-stoyanovich-responsible-ai/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Artificial intelligence is now a household term. Responsible AI is hot on its heels.</p>
<p>Julia Stoyanovich, associate professor of computer science and engineering at NYU and director of the university’s <a href="https://engineering.nyu.edu/research-innovation/centers/center-responsible-ai" target="_blank" rel="noopener">Center for Responsible AI</a>, wants to make the terms “AI” and “responsible AI” synonymous.</p>
<p>In the latest episode of the NVIDIA <a href="https://blogs.nvidia.com/ai-podcast/" target="_blank" rel="noopener">AI Podcast</a>, host Noah Kravitz ‌spoke with Stoyanovich about responsible AI, her advocacy efforts and how people can help.</p>
<p>Stoyanovich started her work at the Center for Responsible AI with basic research. She soon realized that what was needed were better guardrails, not just more algorithms.</p>
<p>As AI’s potential has grown, along with the ethical concerns surrounding its use, Stoyanovich clarifies that the “responsibility” lies with people, not AI.</p>
<p>“The responsibility refers to people taking responsibility for the decisions that we make individually and collectively about whether to build an AI system and how to build, test, deploy and keep it in check,” she said.</p>
<p>AI ethics is a related concern, used to refer to “the embedding of moral values and principles into the design, development and use of the AI,” she added.</p>
<p>Lawmakers have taken notice. For example, New York recently implemented a law that makes job candidate screening more transparent.</p>
<p>According to Stoyanovich, “the law is not perfect,” but “we can only learn how to regulate something if we try regulating” and converse openly with the “people at the table being impacted.”</p>
<p>Stoyanovich wants two things: for people to recognize that AI can’t predict human choices and that AI systems be transparent and accountable, carrying a “nutritional label.”</p>
<p>That process should include considerations on who is using AI tools, how they’re used to make decisions and who is subjected to those decisions, she said.</p>
<p>Stoyanovich urges people to “start demanding actions and explanations to understand” how AI is used at local, state and federal levels.</p>
<p>“We need to teach ourselves to help others learn about what AI is and why we should care,” she said. “So please get involved in how we govern ourselves, because we live in a democracy. We have to step up.”</p>
<p><iframe loading="lazy" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/1643145072%3Fsecret_token%3Ds-R0r3BAvxa09&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true" width="100%" height="166" frameborder="no" scrolling="no"></iframe></p>
<div style="font-size: 10px; color: #cccccc; line-break: anywhere; word-break: normal; overflow: hidden; white-space: nowrap; text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif; font-weight: 100;"><a style="color: #cccccc; text-decoration: none;" title="The AI Podcast" href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">The AI Podcast</a> · <a style="color: #cccccc; text-decoration: none;" title="Making Machines Mindful: NYU Professor Talks Responsible AI - Ep. 205" href="https://soundcloud.com/theaipodcast/making-machines-mindful/s-R0r3BAvxa09" target="_blank" rel="noopener">Making Machines Mindful: NYU Professor Talks Responsible AI &#8211; Ep. 205</a></div>
<h2>You Might Also Like</h2>
<p><a href="https://soundcloud.com/theaipodcast/jules-anh-tuan-nguyen-explains-how-ai-lets-amputee-control-prosthetic-hand-video-games-ep-149">Jules Anh Tuan Nguyen Explains How AI Lets Amputee Control Prosthetic Hand, Video Games<br />
</a>A postdoctoral researcher at the University of Minnesota discusses his efforts to allow amputees to control their prosthetic limb — right down to the finger motions — with their minds.</p>
<p><a href="https://soundcloud.com/theaipodcast/ai-overjet">Overjet’s Ai Wardah Inam on Bringing AI to Dentistry<br />
</a>Overjet, a member of <a href="https://www.nvidia.com/en-us/startups/">NVIDIA Inception</a>, is moving fast to bring AI to dentists’ offices. Dr. Wardah Inam, CEO of the company, discusses using AI to improve patient care.</p>
<p><a href="https://soundcloud.com/theaipodcast/ai-luis-voloch">Immunai CTO and Co-Founder Luis Voloch on Using Deep Learning to Develop New Drugs<br />
</a>Luis Voloch talks about tackling the challenges of the immune system with a machine learning and data science mindset.</p>
<h2>Subscribe to the AI Podcast: Now Available on Amazon Music</h2>
<p><a href="https://music.amazon.com/podcasts/956857d0-9461-4496-a07e-24be0539ee82/the-ai-podcast">The AI Podcast is now available through Amazon Music</a>.</p>
<p>In addition, get the <a href="https://blogs.nvidia.com/ai-podcast/">AI Podcast</a> through <a href="https://itunes.apple.com/us/podcast/the-ai-podcast/id1186480811?mt=2&amp;adbsc=social_20161220_68874946&amp;adbid=811257941365882882&amp;adbpl=tw&amp;adbpr=61559439">iTunes</a>, <a href="https://podcasts.google.com/?feed=aHR0cHM6Ly9mZWVkcy5zb3VuZGNsb3VkLmNvbS91c2Vycy9zb3VuZGNsb3VkOnVzZXJzOjI2NDAzNDEzMy9zb3VuZHMucnNz">Google Podcasts</a>, <a href="https://play.google.com/music/listen?u=0#/ps/I4kyn74qfrsdhrm35mcrf3igxzm">Google Play</a>, <a href="https://castbox.fm/channel/The-AI-Podcast-id433488?country=us">Castbox</a>, DoggCatcher, <a href="https://overcast.fm/itunes1186480811/the-ai-podcast">Overcast</a>, <a href="https://player.fm/series/the-ai-podcast">PlayerFM</a>, Pocket Casts, <a href="http://www.podbay.fm/show/1186480811">Podbay</a>, <a href="https://www.podbean.com/podcast-detail/cjgnp-4a6e0/The-AI-Podcast">PodBean</a>, PodCruncher, PodKicker, <a href="https://soundcloud.com/theaipodcast">Soundcloud</a>, <a href="https://open.spotify.com/show/4TB4pnynaiZ6YHoKmyVN0L">Spotify</a>, <a href="http://www.stitcher.com/s?fid=130629&amp;refid=stpr">Stitcher</a> and <a href="https://tunein.com/podcasts/Technology-Podcasts/The-AI-Podcast-p940829/">TuneIn</a>.</p>
<p>Make the AI Podcast better. Have a few minutes to spare? Fill out <a href="http://survey.podtrac.com/start-survey.aspx?pubid=I5V0tOQFNS8j&amp;ver=short">this listener survey</a>.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2018/05/ai-podcast.jpg"
			type="image/jpeg"
			width="1400"
			height="931"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2018/05/ai-podcast-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Making Machines Mindful: NYU Professor Talks Responsible AI]]></media:title>
			<media:description type="html">NVIDIA AI Podcast</media:description>
			</media:content>
			</item>
		<item>
		<title>Into the Omniverse: Marmoset Brings Breakthroughs in Rendering, Extends OpenUSD Support to Enhance 3D Art Production</title>
		<link>https://blogs.nvidia.com/blog/2023/10/18/marmoset-extends-openusd-support/</link>
		
		<dc:creator><![CDATA[Pooya Ghobadpour]]></dc:creator>
		<pubDate>Wed, 18 Oct 2023 13:00:39 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[Into the Omniverse]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Universal Scene Description]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67520</guid>

					<description><![CDATA[Real-time rendering, animation and texture baking are essential workflows for 3D art production. Using the Marmoset Toolbag software, 3D artists can enhance their creative workflows and build complex 3D models without disruptions to productivity. ]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p><i>Editor’s note: This post is part of </i><a href="https://www.nvidia.com/en-us/omniverse/news/" target="_blank" rel="noopener"><i>Into the Omniverse</i></a><i>, a series focused on how artists and developers from startups to enterprises can transform their workflows using the latest advances in </i><a href="https://www.nvidia.com/en-us/omniverse/usd/" target="_blank" rel="noopener"><i>OpenUSD</i></a><i> and </i><a href="https://www.nvidia.com/en-us/omniverse/usd/" target="_blank" rel="noopener"><i>NVIDIA Omniverse</i></a><i>.</i></p>
<p>Real-time rendering, animation and texture baking are essential workflows for 3D art production. Using the Marmoset Toolbag software, 3D artists can enhance their creative workflows and build complex 3D models without disruptions to productivity.</p>
<p>The latest release of Marmoset Toolbag, version 4.06, brings increased support for <a href="https://www.nvidia.com/en-us/omniverse/usd/" target="_blank" rel="noopener">Universal Scene Description</a>, aka OpenUSD, enabling seamless compatibility with <a href="https://www.nvidia.com/en-us/omniverse/" target="_blank" rel="noopener">NVIDIA Omniverse</a>, a development platform for connecting and building OpenUSD-based tools and applications.</p>
<p>3D creators and technical artists using Marmoset can now enjoy improved interoperability, accelerated rendering, real-time visualization and efficient performance —  redefining the possibilities of their creative workflows.</p>
<h2><b>Enhancing Cross-Platform Creativity With OpenUSD</b></h2>
<p>Creators are taking their workflows to the next level with OpenUSD.</p>
<p>Berlin-based Armin Halač works as a principal animator at <a href="https://www.wooga.com/" target="_blank" rel="noopener">Wooga</a>, a mobile games development studio known for projects like <a href="https://play.google.com/store/apps/details?id=net.wooga.junes_journey_hidden_object_mystery_game&amp;hl=en&amp;gl=US" target="_blank" rel="noopener"><i>June’s Journey</i></a> and <a href="https://play.google.com/store/apps/details?id=com.netflix.NGP.GhostDetective&amp;hl=en_US" target="_blank" rel="noopener"><i>Ghost Detective</i></a>. The nature of his job means Halač is no stranger to 3D workflows — he gets hands-on with animation and character rigging.</p>
<p>For texturing and producing high-quality renders, Marmoset is Halač’s go-to tool, providing a user-friendly interface and powerful features to simplify his workflow. Recently, Halač used Marmoset to create the captivating cover image for his book, <a href="https://www.routledge.com/A-Complete-Guide-to-Character-Rigging-for-Games-Using-Blender/Halac/p/book/9781032203003" target="_blank" rel="noopener"><i>A Complete Guide to Character Rigging for Games Using Blender</i></a><i>.</i></p>
<p>Using the added support for USD, Halač can seamlessly send 3D assets from Blender to Marmoset, creating new possibilities for collaboration and improved visuals.</p>
<p style="text-align: center;"><img decoding="async" loading="lazy" class="wp-image-67521 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1.png" alt="" width="933" height="525" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1.png 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-1536x864.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-1280x720.png 1280w" sizes="(max-width: 933px) 100vw, 933px" /><i>The cover image of Halač’s book.</i></p>
<p>Nkoro Anselem Ire, a.k.a <a href="https://www.youtube.com/@askNK/videos" target="_blank" rel="noopener">askNK</a>, is a popular YouTube creator as well as a media and visual arts professor at a couple of universities who is also seeing workflow benefits from increased USD support.</p>
<p>As a 3D content creator, he uses Marmoset Toolbag for the majority of his <a href="https://www.adobe.com/products/substance3d/discover/pbr.html#:~:text=Physically%20based%20rendering%20(PBR)%2C,light%20interacts%20with%20material%20properties." target="_blank" rel="noopener">PBR</a> workflow — from texture baking and lighting to animation and rendering. Now, with USD, askNK is enjoying newfound levels of creative flexibility as the framework allows him to “collaborate with individuals or team members a lot easier because they can now pick up and drop off processes while working on the same file.”</p>
<p><iframe loading="lazy" title="Mamoverse - Into the Omniverse with Marmoset Toolbag!" width="500" height="281" src="https://www.youtube.com/embed/xLKyE6wXHcs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Halač and askNK recently joined an <a href="https://www.youtube.com/live/8UsOTa8rTgg?si=yUI8kUSWxROufxsf" target="_blank" rel="noopener">NVIDIA-hosted livestream</a> where community members and the Omniverse team explored the benefits of a Marmoset- and Omniverse-boosted workflow.</p>
<p><iframe loading="lazy" title="Learning Marmoset Toolbag for Enhanced 3D Workflows" width="500" height="281" src="https://www.youtube.com/embed/8UsOTa8rTgg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p><a href="https://www.linkedin.com/in/metalbyte/" target="_blank" rel="noopener">Daniel Bauer</a> is another creator experiencing the benefits of Marmoset, OpenUSD and Omniverse. A <a href="https://www.solidworks.com/" target="_blank" rel="noopener">SolidWorks</a> mechanical engineer with over 10 years of experience, Bauer works frequently in CAD software environments, where it’s typical to assign different materials to various scene components. The variance can often lead to shading errors and incorrect geometry representation, but using USD, Bauer can avoid errors by easily importing versions of his scene from Blender to Marmoset Toolbag to <a href="https://www.nvidia.com/en-us/omniverse/apps/usd-composer/" target="_blank" rel="noopener">Omniverse USD Composer</a>.</p>
<p style="text-align: center;"><img decoding="async" loading="lazy" class="size-full wp-image-67524 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/GRIPPER_-1.gif" alt="" width="600" height="382" /><i>A Kuka Scara robot simulation with 10 parallel small grippers for sorting and handling pens.</i></p>
<p>Additionally, 3D artists <a href="https://blogs.nvidia.com/blog/2023/04/19/studio-omniverse-usd-composer-animation/" target="_blank" rel="noopener">Gianluca Squillace and Pasquale Scionti</a> are harnessing the collaborative power of Omniverse, Marmoset and OpenUSD to transform their workflows from a convoluted series of exports and imports to a streamlined, real-time, interconnected process.</p>
<p>Squillace crafted a captivating 3D character with Pixologic ZBrush, Autodesk Maya, Adobe Substance 3D Painter and Marmoset Toolbag — aggregating the data from the various tools in Omniverse. With USD, he seamlessly integrated his animations and made real-time adjustments without the need for constant file exports.</p>
<p>Simultaneously, Scionti constructed a stunning glacial environment using Autodesk 3ds Max, Adobe Substance 3D Painter, Quixel and Unreal Engine, uniting the various pieces from his tools in Omniverse. His work showcased the potential of Omniverse to foster real-time collaboration as he was able to seamlessly integrate Squillace’s character into his snowy world.</p>
<p><img decoding="async" loading="lazy" class=" wp-image-67530 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w.jpg" alt="" width="804" height="452" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w-178x100.jpg 178w" sizes="(max-width: 804px) 100vw, 804px" /></p>
<h2><b>Advancing Interoperability and Real-Time Rendering</b></h2>
<p><a href="https://marmoset.co/posts/toolbag-4-06-available-now/" target="_blank" rel="noopener">Marmoset Toolbag 4.06</a> provides significant improvements to interoperability and image fidelity for artists working across platforms and applications. This is achieved through updates to Marmoset’s OpenUSD support, allowing for seamless compatibility and connection with the Omniverse ecosystem.</p>
<p>The improved USD import and export capabilities enhance interoperability with popular content creation apps and creative toolkits like Autodesk Maya and Autodesk 3ds Max, SideFX Houdini and Unreal Engine.</p>
<p>Additionally, Marmoset Toolbag 4.06 brings additional updates, including:</p>
<ul>
<li><b>RTX-accelerated rendering and baking: </b>Toolbag’s ray-traced renderer and texture baker are accelerated by NVIDIA RTX GPUs, providing up to a 2x improvement in render times and a 4x improvement in bake times.</li>
<li><b>Real-time denoising with OptiX:</b> With NVIDIA RTX devices, creators can enjoy a smooth and interactive ray-tracing experience, enabling real-time navigation of the active viewport without visual artifacts or performance disruptions.</li>
<li><b>High DPI performance with DLSS image upscaling:</b> The viewport now renders at a reduced resolution and uses AI-based technology to upscale images, improving performance while minimizing image-quality reductions.</li>
</ul>
<p>Download Toolbag 4.06 directly from Marmoset to explore USD support and RTX-accelerated production tools. New users are eligible for a full-featured, <a href="https://marmoset.co/toolbag/#dltoolbag" target="_blank" rel="noopener">30-day free trial license</a>.</p>
<h2><b>Get Plugged Into the Omniverse </b></h2>
<p>Learn from industry experts on how OpenUSD is enabling custom 3D pipelines, easing 3D tool development and delivering interoperability between 3D applications in sessions from SIGGRAPH 2023, now available <a href="https://www.youtube.com/playlist?list=PL3jK4xNnlCVevpoiQ8YR-kYz5h0_9GTD9" target="_blank" rel="noopener">on demand</a>.</p>
<p>Anyone can build their own <a href="https://developer.nvidia.com/omniverse" target="_blank" rel="noopener">Omniverse extension or Connector</a> to enhance their 3D workflows and tools. Explore the Omniverse ecosystem’s <a href="https://www.nvidia.com/en-us/omniverse/ecosystem/" target="_blank" rel="noopener">growing catalog</a> of connections, extensions, foundation applications and third-party tools.</p>
<p>For more <a href="https://developer.nvidia.com/usd" target="_blank" rel="noopener">resources on OpenUSD</a>, explore the <a href="https://forum.aousd.org/" target="_blank" rel="noopener">Alliance for OpenUSD forum</a> or visit the <a href="https://aousd.org/" target="_blank" rel="noopener">AOUSD website</a>.</p>
<p>Share your Marmoset Toolbag and Omniverse work as part of the latest community challenge, <a href="https://forums.developer.nvidia.com/t/the-new-community-challenge-is-here-we-are-kicking-off-the-seasonalartchallenge/268346" target="_blank" rel="noopener">#SeasonalArtChallenge</a>. Use the hashtag to submit a spooky or festive scene for a chance to be featured on the @NVIDIAStudio and @NVIDIAOmniverse social channels.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">Show us your spookiest 3D scenes in the new <a href="https://twitter.com/hashtag/SeasonalArtChallenge?src=hash&amp;ref_src=twsrc%5Etfw">#SeasonalArtChallenge</a>. </p>
<p>Get in the spooky spirit and share your scenes with us and <a href="https://twitter.com/NVIDIAStudio?ref_src=twsrc%5Etfw">@NVIDIAStudio</a> for a chance to be featured on our channels. </p>
<p><img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f383.png" alt="🎃" class="wp-smiley" style="height: 1em; max-height: 1em;" /> courtesy of <a href="https://twitter.com/TanjaLanggner?ref_src=twsrc%5Etfw">@TanjaLanggner</a></p>
<p>&mdash; NVIDIA Omniverse (@nvidiaomniverse) <a href="https://twitter.com/nvidiaomniverse/status/1711874129249685564?ref_src=twsrc%5Etfw">October 10, 2023</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<p><i>Get started with NVIDIA Omniverse by downloading the standard license </i><a href="https://www.nvidia.com/en-us/omniverse/download/" target="_blank" rel="noopener"><i>free</i></a><i>, or learn how </i><a href="https://www.nvidia.com/en-us/omniverse/enterprise/" target="_blank" rel="noopener"><i>Omniverse Enterprise</i><i> can connect your team</i></a><i>. </i></p>
<p><i>Developers can check out these </i><a href="https://developer.nvidia.com/omniverse/get-started/" target="_blank" rel="noopener"><i>Omniverse resources</i></a><i> to begin building on the platform. </i></p>
<p><i>Stay up to date on the platform by subscribing to the </i><a href="https://nvda.ws/3u5KPv1" target="_blank" rel="noopener"><i>newsletter</i></a><i> and following NVIDIA Omniverse on </i><a href="https://www.instagram.com/nvidiaomniverse/"><i>Instagram</i></a><i>, </i><a href="https://www.linkedin.com/showcase/nvidia-omniverse"><i>LinkedIn</i></a><i>, </i><a href="https://medium.com/@nvidiaomniverse"><i>Medium</i></a><i>, </i><a href="https://www.threads.net/@nvidiaomniverse"><i>Threads</i></a><i> and </i><a href="https://twitter.com/nvidiaomniverse"><i>Twitter</i></a><i>.</i></p>
<p><i>For more, check out our </i><a href="https://forums.developer.nvidia.com/c/omniverse/300"><i>forums</i></a><i>, </i><a href="https://discord.com/invite/XWQNJDNuaC"><i>Discord server</i></a><i>, </i><a href="https://www.twitch.tv/nvidiaomniverse"><i>Twitch</i></a><i> and </i><a href="https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA"><i>YouTube</i></a><i> channels.</i></p>
<p><i>Featured image courtesy of Armin Halač, Christian Nauck and Masuquddin Ahmed.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/nv-ov-ito-1280x680_Marmoset.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/nv-ov-ito-1280x680_Marmoset-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Into the Omniverse: Marmoset Brings Breakthroughs in Rendering, Extends OpenUSD Support to Enhance 3D Art Production]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Foxconn and NVIDIA Amp Up Electric Vehicle Innovation</title>
		<link>https://blogs.nvidia.com/blog/2023/10/17/foxconn-nvidia-electric-vehicle/</link>
		
		<dc:creator><![CDATA[Danny Shapiro]]></dc:creator>
		<pubDate>Wed, 18 Oct 2023 03:00:44 +0000</pubDate>
				<category><![CDATA[Driving]]></category>
		<category><![CDATA[NVIDIA DRIVE]]></category>
		<category><![CDATA[Transportation]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67555</guid>

					<description><![CDATA[NVIDIA founder and CEO Jensen Huang joined Hon Hai (Foxconn) Chairman and CEO Young Liu to unveil the latest in their ongoing partnership to develop the next wave of intelligent electric vehicle (EV) platforms for the global automotive market. This latest move, announced today at the fourth annual Hon Hai Tech Day in Taiwan, will <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/17/foxconn-nvidia-electric-vehicle/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>NVIDIA founder and CEO Jensen Huang joined Hon Hai (Foxconn) Chairman and CEO Young Liu to unveil the latest in their <a href="https://nvidianews.nvidia.com/news/nvidia-partners-with-foxconn-to-build-factories-and-systemsfor-the-ai-industrial-revolution" target="_blank" rel="noopener">ongoing partnership</a> to develop the next wave of intelligent electric vehicle (EV) platforms for the global automotive market.</p>
<p>This latest move, announced today at the fourth annual Hon Hai Tech Day in Taiwan, will help Foxconn realize its EV vision with a range of NVIDIA DRIVE solutions — including NVIDIA DRIVE Orin today and its successor, DRIVE Thor, down the road.</p>
<p>In addition, Foxconn will be a contract manufacturer of highly automated and autonomous, AI-rich EVs featuring the upcoming <a href="https://blogs.nvidia.com/blog/2022/03/22/drive-hyperion-9-thor/" target="_blank" rel="noopener">NVIDIA DRIVE Hyperion 9</a> platform, which includes DRIVE Thor and a state-of-the-art sensor architecture.</p>
<h2><strong>Next-Gen EVs With Extraordinary Performance  </strong></h2>
<p>The computational requirements for highly automated and fully self-driving vehicles are enormous. NVIDIA offers the most advanced, highest-performing AI car computers for the transportation industry, with DRIVE Orin selected for use by more than 25 global automakers.</p>
<p>Already a tier-one manufacturer of DRIVE Orin-powered electronic control units (ECUs), Foxconn will also manufacture ECUs featuring DRIVE Thor, once available.</p>
<p>The upcoming DRIVE Thor superchip harnesses advanced AI capabilities first deployed in NVIDIA Grace CPUs and Hopper and Ada Lovelace architecture-based GPUs — and is expected to deliver a staggering 2,000 teraflops of high-performance compute to enable functionally safe and secure intelligent driving.</p>
<figure id="attachment_67556" aria-describedby="caption-attachment-67556" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="wp-image-67556 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-1536x864.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-1280x720.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia.png 1600w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-67556" class="wp-caption-text">Next-generation NVIDIA DRIVE Thor.</figcaption></figure>
<h2><strong>Heightened Senses</strong></h2>
<p>Unveiled at GTC last year, DRIVE Hyperion 9 is the latest evolution of NVIDIA’s modular development platform and reference architecture for automated and autonomous vehicles. Set to be powered by DRIVE Thor, it will integrate a qualified sensor architecture for level 3 urban and level 4 highway driving scenarios.</p>
<p>With a diverse and redundant array of high-resolution camera, radar, lidar and ultrasonic sensors, DRIVE Hyperion can process an extraordinary amount of safety-critical data to enable vehicles to deftly navigate their surroundings.</p>
<p>Another advantage of DRIVE Hyperion is its compatibility across generations, as it retains the same compute form factor and NVIDIA DriveWorks application programming interfaces, enabling a seamless transition from DRIVE Orin to DRIVE Thor and beyond.</p>
<p>Plus, DRIVE Hyperion can help speed development times and lower costs for electronics manufacturers like Foxconn, since the sensors available on the platform have cleared NVIDIA’s rigorous qualification processes.</p>
<p>The shift to software-defined vehicles with a centralized electronic architecture will drive the need for high-performance, energy-efficient computing solutions such as DRIVE Thor. By coupling it with the DRIVE Hyperion sensor architecture, Foxconn and its automotive customers will be better equipped to realize a new era of safe and intelligent EVs.</p>
<p>Since its inception, Hon Hai Tech Day has served as a launch pad for Foxconn to showcase its latest endeavors in contract design and manufacturing services and new technologies. These accomplishments span the EV sector and extend to the broader consumer electronics industry.</p>
<p>Catch more on Liu and Huang’s <a href="https://www.youtube.com/watch?v=HmT3MAU09tg&amp;ab_channel=%E9%B4%BB%E6%B5%B7" target="_blank" rel="noopener">fireside chat at Hon Hai Tech Day</a>.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnhhtd-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnhhtd-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Foxconn and NVIDIA Amp Up Electric Vehicle Innovation]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Striking Performance: Large Language Models up to 4x Faster on RTX With TensorRT-LLM for Windows</title>
		<link>https://blogs.nvidia.com/blog/2023/10/17/tensorrt-llm-windows-stable-diffusion-rtx/</link>
		
		<dc:creator><![CDATA[Jesse Clayton]]></dc:creator>
		<pubDate>Tue, 17 Oct 2023 13:00:42 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[TensorRT]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67466</guid>

					<description><![CDATA[GeForce RTX and NVIDIA RTX GPUs, which are packed with dedicated AI processors called Tensor Cores, are bringing the power of generative AI natively to more than 100 million Windows PCs and workstations.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p><a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/" target="_blank" rel="noopener">Generative AI</a> is one of the most important trends in the history of personal computing, bringing advancements to gaming, creativity, video, productivity, development and more.</p>
<p>And <a href="https://www.nvidia.com/en-us/geforce/rtx/">GeForce RTX</a> and NVIDIA RTX GPUs, which are packed with dedicated AI processors called Tensor Cores, are bringing the power of generative AI natively to more than 100 million Windows PCs and workstations.</p>
<p>Today, generative AI on PC is getting up to 4x faster via <a href="https://developer.nvidia.com/tensorrt">TensorRT-LLM</a> for Windows, an open-source library that accelerates inference performance for the latest AI large language models, like Llama 2 and Code Llama. This follows the announcement of TensorRT-LLM for <a href="https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/">data centers</a> last month.</p>
<p>NVIDIA has also released tools to help developers accelerate their LLMs, including scripts that optimize custom models with TensorRT-LLM, TensorRT-optimized open-source models and a developer reference project that showcases both the speed and quality of LLM responses.</p>
<p>TensorRT acceleration is now available for Stable Diffusion in the popular Web UI by Automatic1111 distribution. It speeds up the generative AI diffusion model by up to 2x over the previous fastest implementation.</p>
<p>Plus, <a href="https://blogs.nvidia.com/blog/2023/02/28/rtx-video-super-resolution/">RTX Video Super Resolution</a> (VSR) version 1.5 is available as part of today’s <a href="https://www.nvidia.com/en-us/geforce/news/game-ready-driver-dlss-3-naraka-vermintide-rtx-vsr">Game Ready Driver</a> release — and will be available in the next <a href="https://www.nvidia.com/en-us/studio/resources/">NVIDIA Studio Driver</a>, releasing early next month.</p>
<h2><b>Supercharging LLMs With TensorRT</b></h2>
<p>LLMs are fueling productivity — engaging in chat, summarizing documents and web content, drafting emails and blogs — and are at the core of new pipelines of AI and other software that can automatically analyze data and generate a vast array of content.</p>
<p>TensorRT-LLM, a library for accelerating LLM inference, gives developers and end users the benefit of LLMs that can now operate up to 4x faster on RTX-powered Windows PCs.</p>
<p>At higher batch sizes, this acceleration significantly improves the experience for more sophisticated LLM use — like writing and coding assistants that output multiple, unique auto-complete results at once. The result is accelerated performance and improved quality that lets users select the best of the bunch.</p>
<p>TensorRT-LLM acceleration is also beneficial when integrating LLM capabilities with other technology, such as in retrieval-augmented generation (RAG), where an LLM is paired with a vector library or vector database. RAG enables the LLM to deliver responses based on a specific dataset, like user emails or articles on a website, to provide more targeted answers.</p>
<p>To show this in practical terms, when the question “How does NVIDIA ACE generate emotional responses?” was asked of the LLaMa 2 base model, it returned an unhelpful response.</p>
<figure id="attachment_67501" aria-describedby="caption-attachment-67501" style="width: 1858px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE.png"><img decoding="async" loading="lazy" class="size-full wp-image-67501" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE.png" alt="" width="1858" height="919" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE.png 1858w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-400x198.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-672x332.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-768x380.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-1536x760.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-842x416.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-406x201.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-188x93.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-1280x633.png 1280w" sizes="(max-width: 1858px) 100vw, 1858px" /></a><figcaption id="caption-attachment-67501" class="wp-caption-text">Better responses, faster.</figcaption></figure>
<p>Conversely, using RAG with recent <a href="https://www.nvidia.com/en-us/geforce/news/">GeForce news articles</a> loaded into a vector library and connected to the same Llama 2 model not only returned the correct answer — using NeMo SteerLM — but did so much quicker with TensorRT-LLM acceleration. This combination of speed and proficiency gives users smarter solutions.</p>
<p>TensorRT-LLM will soon be available to download from the <a href="https://developer.nvidia.com/">NVIDIA Developer</a> website. TensorRT-optimized open source models and the RAG demo with GeForce news as a sample project are available at <a href="https://catalog.ngc.nvidia.com/">ngc.nvidia.com</a> and <a href="https://github.com/nvidia">GitHub.com/NVIDIA</a>.</p>
<h2><b>Automatic Acceleration</b></h2>
<p>Diffusion models, like Stable Diffusion, are used to imagine and create stunning, novel works of art. Image generation is an iterative process that can take hundreds of cycles to achieve the perfect output. When done on an underpowered computer, this iteration can add up to hours of wait time.</p>
<p>TensorRT is designed to accelerate AI models through layer fusion, precision calibration, kernel auto-tuning and other capabilities that significantly boost inference efficiency and speed. This makes it indispensable for real-time applications and resource-intensive tasks.</p>
<p>And now, <a href="https://developer.nvidia.com/blog/unlock-faster-image-generation-in-stable-diffusion-web-ui-with-nvidia-tensorrt/">TensorRT doubles the speed of Stable Diffusion</a>.</p>
<p>Compatible with the most popular distribution, WebUI from Automatic1111, Stable Diffusion with TensorRT acceleration helps users iterate faster and spend less time waiting on the computer, delivering a final image sooner. On a GeForce RTX 4090, it runs 7x faster than the top implementation on Macs with an Apple M2 Ultra. The extension is <a href="https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT">available for download</a> today.</p>
<p>The <a href="https://github.com/NVIDIA/TensorRT/tree/release/8.6/demo/Diffusion">TensorRT demo of a Stable Diffusion pipeline</a> provides developers with a reference implementation on how to prepare diffusion models and accelerate them using TensorRT. This is the starting point for developers interested in turbocharging a diffusion pipeline and bringing lightning-fast inferencing to applications.</p>
<h2><b>Video That’s Super</b></h2>
<p>AI is improving everyday PC experiences for all users. Streaming video — from nearly any source, like YouTube, Twitch, Prime Video, Disney+ and countless others — is among the most popular activities on a PC. Thanks to AI and RTX, it’s getting another update in image quality.</p>
<p><a href="https://blogs.nvidia.com/blog/2023/02/28/rtx-video-super-resolution/">RTX VSR</a> is a breakthrough in AI pixel processing that improves the quality of streamed video content by reducing or eliminating artifacts caused by video compression. It also sharpens edges and details.</p>
<p><iframe loading="lazy" title="AI-Enhanced Video: NVIDIA RTX Video Super Resolution Update 1.5" width="500" height="281" src="https://www.youtube.com/embed/VkKsamTPk7g?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Available now, RTX VSR version 1.5 further improves visual quality with updated models, de-artifacts content played in its native resolution and adds support for RTX GPUs based on the NVIDIA Turing architecture — both professional RTX and GeForce RTX 20 Series GPUs.</p>
<p>Retraining the VSR AI model helped it learn to accurately identify the difference between subtle details and compression artifacts. As a result, AI-enhanced images more accurately preserve details during the upscaling process. Finer details are more visible, and the overall image looks sharper and crisper.</p>
<figure id="attachment_67470" aria-describedby="caption-attachment-67470" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1.png"><img decoding="async" loading="lazy" class="size-large wp-image-67470" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-672x236.png" alt="" width="672" height="236" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-672x236.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-400x140.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-768x269.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-1536x539.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1.png 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-842x295.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-406x142.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-188x66.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-1280x449.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67470" class="wp-caption-text">RTX Video Super Resolution v1.5 improves detail and sharpness.</figcaption></figure>
<p>New with version 1.5 is the ability to de-artifact video played at the display’s native resolution. The original release only enhanced video when it was being upscaled. Now, for example, 1080p video streamed to a 1080p resolution display will look smoother as heavy artifacts are reduced.</p>
<figure id="attachment_67473" aria-describedby="caption-attachment-67473" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR.png"><img decoding="async" loading="lazy" class="size-large wp-image-67473" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-672x377.png" alt="" width="672" height="377" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-672x377.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-400x224.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-768x431.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-1536x861.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-802x450.png 802w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-383x215.png 383w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-1280x718.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR.png 1999w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67473" class="wp-caption-text">RTX VSR now de-artifacts video played at its native resolution.</figcaption></figure>
<p>RTX VSR 1.5 is available today for all RTX users in the latest Game Ready Driver. It will be available in the upcoming NVIDIA Studio Driver, scheduled for early next month.</p>
<p>RTX VSR is among the NVIDIA software, tools, libraries and SDKs — like those mentioned above, plus DLSS, Omniverse, AI Workbench and others — that have helped bring over 400 AI-enabled apps and games to consumers.</p>
<p>The AI era is upon us. And RTX is supercharging at every step in its evolution.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-ai-announcemenet-blog-kv-oct2023-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-ai-announcemenet-blog-kv-oct2023-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Striking Performance: Large Language Models up to 4x Faster on RTX With TensorRT-LLM for Windows]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA RTX Video Super Resolution Update Enhances Video Quality, Detail Preservation and Expands to GeForce RTX 20 Series GPUs</title>
		<link>https://blogs.nvidia.com/blog/2023/10/17/rtx-video-super-resolution-ai-obs-broadcast/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Tue, 17 Oct 2023 13:00:34 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[In the NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[NVIDIA Studio]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67476</guid>

					<description><![CDATA[NVIDIA today announced an update to RTX Video Super Resolution (VSR) that delivers greater overall graphical fidelity with preserved details, upscaling for native videos and support for GeForce RTX 20 Series GPUs. ]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>NVIDIA today announced an update to <a href="https://blogs.nvidia.com/blog/2023/02/28/rtx-video-super-resolution/">RTX Video Super Resolution</a> (VSR) that delivers greater overall graphical fidelity with preserved details, upscaling for native videos and support for GeForce RTX 20 Series desktop and laptop GPUs.</p>
<p><iframe loading="lazy" title="AI-Enhanced Video: NVIDIA RTX Video Super Resolution Update 1.5" width="500" height="281" src="https://www.youtube.com/embed/VkKsamTPk7g?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>For AI assists from RTX VSR and more — from enhanced creativity and productivity to blisteringly fast gaming — check out the <a href="https://www.nvidia.com/en-us/ai-on-rtx/">RTX for AI page</a>.</p>
<p>Plus, this week <i>In the NVIDIA Studio</i>, Twitch personality Runebee shares her inspiration, streaming tips and how she uses AI and RTX GPU acceleration.</p>
<p><iframe loading="lazy" title="The Best of Highlights 1-100" width="500" height="281" src="https://www.youtube.com/embed/Gs8gTICgAGM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>And don’t forget to join the #SeasonalArtChallenge by submitting spooky Halloween-themed art in October and harvest- and fall-themed pieces in November. For inspiration, check out the hauntingly adorable work of artists like <a href="https://www.instagram.com/iryna.blender3d/">iryna.blender3d</a> on <a href="https://twitter.com/NVIDIAStudio/status/1711411202809577569">Twitter</a>.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">The <a href="https://twitter.com/hashtag/SeasonalArtChallenge?src=hash&amp;ref_src=twsrc%5Etfw">#SeasonalArtChallenge</a> continues on with an incredible render from iryna.blender3d (IG). <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f383.png" alt="🎃" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>Share your spooky/Halloween-themed art with <a href="https://twitter.com/hashtag/SeasonalArtChallenge?src=hash&amp;ref_src=twsrc%5Etfw">#SeasonalArtChallenge</a> for a chance to be featured on the Studio or <a href="https://twitter.com/nvidiaomniverse?ref_src=twsrc%5Etfw">@NVIDIAOmniverse</a> channels! <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f64c.png" alt="🙌" class="wp-smiley" style="height: 1em; max-height: 1em;" /> <a href="https://t.co/lOxcYNhBJl">pic.twitter.com/lOxcYNhBJl</a></p>
<p>&mdash; NVIDIA Studio (@NVIDIAStudio) <a href="https://twitter.com/NVIDIAStudio/status/1711411202809577569?ref_src=twsrc%5Etfw">October 9, 2023</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<h2><b>The Super RTX VSR Update 1.5</b></h2>
<p>RTX VSR’s AI model has been retrained to more accurately identify the difference between subtle details and compression artifacts to better preserve image details during the upscaling process. Finer details are more visible, and the overall image looks sharper and crisper than before.</p>
<figure id="attachment_67470" aria-describedby="caption-attachment-67470" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1.png"><img decoding="async" loading="lazy" class="size-large wp-image-67470" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-672x236.png" alt="" width="672" height="236" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-672x236.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-400x140.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-768x269.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-1536x539.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1.png 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-842x295.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-406x142.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-188x66.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-1280x449.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67470" class="wp-caption-text">RTX VSR v1.5 improves detail and sharpness.</figcaption></figure>
<p>RTX VSR version 1.5 will also de-artifact videos played at their native resolution — prior, only upscaled video could be enhanced. Providing a leap in graphical fidelity for laptop owners with 1080p screens, the updated RTX VSR makes 1080p resolution, which is popular for content and displays, look smoother at its native resolution, even with heavy artifacts.</p>
<figure id="attachment_67473" aria-describedby="caption-attachment-67473" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR.png"><img decoding="async" loading="lazy" class="size-large wp-image-67473" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-672x377.png" alt="" width="672" height="377" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-672x377.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-400x224.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-768x431.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-1536x861.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-802x450.png 802w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-383x215.png 383w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-1280x718.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR.png 1999w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67473" class="wp-caption-text">RTX VSR now de-artifacts video played at native resolution.</figcaption></figure>
<p>And with expanded RTX VSR support, owners of GeForce RTX 20 Series GPUs can benefit from the same AI-enhanced video as those using RTX 30 and 40 Series GPUs.</p>
<p>RTX VSR 1.5 is available as part of the latest <a href="https://www.nvidia.com/en-us/geforce/game-ready-drivers/">Game Ready Driver</a>, available for download today. Content creators downloading <a href="https://www.nvidia.com/en-us/studio/">NVIDIA Studio Drivers</a> — designed to enhance features, reduce repetitiveness and dramatically accelerate creative workflows — can install the driver with RTX VSR releasing in early November.</p>
<h2><b>Runebee-lievable Streaming</b></h2>
<p>Runebee has been livestreaming for over 10 years, providing a space for viewers to hang out and talk about games, movies or whatever else is going on in life. Over the years, she’s realized how common a desire for escapism is.</p>
<p>“Things aren’t always sunshine and rainbows, so it’s nice to have some company that can help take your mind off things,” said Runebee.</p>
<p><iframe loading="lazy" title="Playable Mercs Wesker in Separate Ways! (With Trainer) - Resident Evil 4 Remake" width="500" height="281" src="https://www.youtube.com/embed/I3LUccjySJY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Runebee has amassed over 100K followers on Twitch, YouTube and Instagram, crediting her success to thorough preparation of her setup. Her technology-forward approach ensures efficiency and reliability — allowing her focus to be on performance.</p>
<p>“There’s a lot of planning involved in streaming, but at the end of the day, hitting the ‘start streaming’ button is the most important step, and NVIDIA GPU-acceleration is a massive factor in allowing it to go as smoothly as it does,” said Runebee.</p>
<div class="simplePullQuote right"><p>“I never thought I’d have this smooth of a stream just by upgrading to a GeForce RTX 40 Series GPU.” &#8211; Runebee</p>
</div>
<p>OBS is Runbee’s preferred open-source software for video recording and livestreaming on Twitch. For maximum efficiency, Runebee deploys her <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4080/">GeForce RTX 4080 RTX GPU</a>, taking advantage of the eighth-generation NVIDIA encoder, <a href="https://developer.nvidia.com/video-codec-sdk">NVENC</a>, to independently encode video, which frees up the graphics card to focus on livestreaming.</p>
<p>“Streaming games and running OBS used to kill my CPU, and NVENC has taken so much stress off,” said Runebee. “I was hardly even able to stream PC games until I switched to NVENC.”</p>
<p>For livestreamers, RTX 40 Series GPUs can offer support for real-time AV1 hardware encoding, providing a 40% efficiency boost compared to H.264 and delivering higher quality than competing GPUs.</p>
<p><iframe loading="lazy" title="Unlock Higher Quality Live Streams with AV1 Support on GeForce RTX 40 Series GPUs" width="500" height="281" src="https://www.youtube.com/embed/kxznHq8be8I?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<div class="simplePullQuote right"><p>“As I started building more PCs with NVIDIA GPUs, I never had a reason to switch!” &#8211; Runebee</p>
</div>
<p>Runebee can export recordings of her livestreams with Adobe Premiere Pro in half the normally required time thanks to GeForce RTX 40 Series dual encoders working together, dividing the work evenly to double output.</p>
<p>They’re capable of recording up to 8K, 60 frames per second content in real time via <a href="https://www.nvidia.com/en-us/geforce/geforce-experience/">GeForce Experience</a> and OBS Studio.</p>
<p><iframe loading="lazy" title="Faster Video Editing with GeForce RTX 40 Series GPUs &amp; DaVinci Resolve" width="500" height="281" src="https://www.youtube.com/embed/DhBFuU8Gnik?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Always looking to improve her livestreaming process, Runebee plans on experimenting with the <a href="https://www.nvidia.com/en-us/geforce/broadcasting/broadcast-app/">NVIDIA Broadcast app</a>, which transforms any room into a home studio by upgrading standard webcams, microphones and speakers into premium smart devices using the power of AI.</p>
<p><iframe loading="lazy" title="NVIDIA Broadcast 1.4 Update Featuring Eye Contact" width="500" height="281" src="https://www.youtube.com/embed/nR-vP_7XFHE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Runebee encourages those interested in livestreaming to at least give their potential passion project a shot. “It’s a great way to meet tons of new friends, become more articulate at describing the things you love — be it games or movies — and cultivate a community to share your passions with,” she said.</p>
<figure id="attachment_67477" aria-describedby="caption-attachment-67477" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67477" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67477" class="wp-caption-text">Twitch livestreamer Runebee’s setup.</figcaption></figure>
<p>Follow Runebee on <a href="https://www.twitch.tv/runebee">Twitch</a>.</p>
<p><i>Follow NVIDIA Studio on </i><a href="https://www.instagram.com/nvidiastudio/"><i>Instagram</i></a><i>, </i><a href="https://twitter.com/NVIDIAStudio"><i>Twitter</i></a><i> and </i><a href="https://www.facebook.com/NVIDIAStudio/"><i>Facebook</i></a><i>. Access tutorials on the </i><a href="https://www.youtube.com/channel/UCDeQdW6Lt6nhq3mLM4oLGWw"><i>Studio YouTube channel</i></a><i> and get updates directly in your inbox by subscribing to the </i><a href="https://www.nvidia.com/en-us/studio/?nvmid=subscribe-creators-mail-icon"><i>Studio newsletter</i></a><i>.</i> <i>See </i><a href="https://www.nvidia.com/en-us/about-nvidia/legal-info/"><i>notice </i></a><i>regarding software product information.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/runebee-nv-blog-header-preview-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/runebee-nv-blog-header-preview-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA RTX Video Super Resolution Update Enhances Video Quality, Detail Preservation and Expands to GeForce RTX 20 Series GPUs]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>From Skylines to Streetscapes: How SHoP Architects Brings Innovative Designs to Life</title>
		<link>https://blogs.nvidia.com/blog/2023/10/13/rtx-ambassador-mengyi-fan/</link>
		
		<dc:creator><![CDATA[JJ Kim]]></dc:creator>
		<pubDate>Fri, 13 Oct 2023 16:00:17 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[AEC]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67432</guid>

					<description><![CDATA[At SHoP Architects, a New York City-based architectural firm, Mengyi Fan and her team aim to inspire industry professionals to create visual masterpieces by incorporating emerging technologies. Fan, the director of visualization at SHoP, has expertise that spans the fields of architectural visualization and design. She takes a definitive, novel and enduring approach to designing <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/13/rtx-ambassador-mengyi-fan/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>At <a href="https://www.shoparc.com/">SHoP Architects</a>, a New York City-based architectural firm, Mengyi Fan and her team aim to inspire industry professionals to create visual masterpieces by incorporating emerging technologies.</p>
<p>Fan, the director of visualization at SHoP, has expertise that spans the fields of architectural visualization and design. She takes a definitive, novel and enduring approach to designing and planning architecture for city skylines and streetscapes.</p>
<p>Fan and her team work on various architecture visualization projects, from still renderings to real-time walkthroughs. They use multiple creative applications throughout the course of their projects, including Adobe Photoshop, Autodesk 3ds Max, Autodesk Revit and Epic Games&#8217; Unreal Engine. SHoP also collaborates directly with architects at project kickoff, providing images and animations that facilitate quicker decision-making during the design process.</p>
<p>The team consistently integrates new technologies that allow them to explore untapped innovation opportunities, as well as boost research and development. Fan often incorporates real-time and traditional rendering, <a href="https://blogs.nvidia.com/blog/2022/05/20/what-is-extended-reality/">extended reality</a> and AI into her creative workflows.</p>
<p>To capture all the details that bring the designs together, SHoP uses <a href="https://www.nvidia.com/en-us/design-visualization/rtx-a5500/">NVIDIA RTX A5500</a>. Fan is also part of the <a href="https://www.nvidia.com/en-us/design-visualization/community/rtx-ambassador/">NVIDIA RTX Ambassador Program</a>, which is designed to amplify the work of professionals from diverse industries who are using RTX technology. Equipped with the latest capabilities of RTX, Fan hopes to continue pushing boundaries in real-time visualization, AI and <a href="https://blogs.nvidia.com/blog/2021/12/14/what-is-a-digital-twin/">digital twin</a> applications.</p>
<figure id="attachment_67433" aria-describedby="caption-attachment-67433" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4.png"><img decoding="async" loading="lazy" class="size-large wp-image-67433" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67433" class="wp-caption-text">All images courtesy of SHoP Architects.</figcaption></figure>
<h2><b>Redefining Creative Experiences </b></h2>
<p>3D models play a critical role as the single source of truth, which is why SHoP designers need advanced technology to help them create detailed models and visualizations without creativity or productivity slowdowns.</p>
<p>Previously, the team used CPU-based offerings, which limited the scope of work and research and development they could take on. But with RTX, ‌designers can create and communicate complex designs while continuously collaborating with others.</p>
<p>By tapping into RTX A5500, Fan can prioritize efficiency and high rendering quality without worrying about compute power limitations.</p>
<p>“NVIDIA’s professional RTX GPUs are currently known as the industry standard for graphics cards solutions,” said Fan. “RTX provides us with the performance and power needed to do all the above without worrying about hardware constraints.”</p>
<p>The advanced features of the RTX GPUs allow SHoP designers to explore new ways of representation.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02.jpg"><img decoding="async" loading="lazy" class="aligncenter size-large wp-image-67436" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-672x259.jpg" alt="" width="672" height="259" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-672x259.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-400x154.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-768x296.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-1536x591.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-842x324.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-406x156.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-188x72.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-1280x493.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>SHoP Architects’ projects have grown in scale, location and diversity, and Fan and her team are constantly learning and adapting from each project, drawing inspiration from diverse areas such as automotive, aviation, film and gaming.</p>
<p>Fan views RTX-powered tools as a means of opening up diverse approaches and solutions to be more widely adopted within the industry. And as an NVIDIA RTX Ambassador, she aims to push past technological boundaries by connecting with like-minded designers and creatives.</p>
<p>See more of Fan’s work below. Discover how <a href="https://www.nvidia.com/en-us/design-visualization/rtx/">NVIDIA RTX</a> can help enhance architectural workflows and learn more about the <a href="https://www.nvidia.com/en-us/design-visualization/community/rtx-ambassador/">NVIDIA RTX Ambassador Program</a>.</p>
<div style="width: 1280px;" class="wp-video"><video class="wp-video-shortcode" id="video-67432-3" width="1280" height="720" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_Botswana-Innovation-Hub_lite.mp4?_=3" /><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_Botswana-Innovation-Hub_lite.mp4">https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_Botswana-Innovation-Hub_lite.mp4</a></video></div>
</div>]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_Botswana-Innovation-Hub_lite.mp4" length="9616749" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_1.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_1-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[From Skylines to Streetscapes: How SHoP Architects Brings Innovative Designs to Life]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>UK Tech Festival Showcases Startups Using AI for Creative Industries</title>
		<link>https://blogs.nvidia.com/blog/2023/10/12/ai-for-creative-industries-uk-startups/</link>
		
		<dc:creator><![CDATA[Jamie Allan]]></dc:creator>
		<pubDate>Thu, 12 Oct 2023 19:58:03 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Game Development]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Inception]]></category>
		<category><![CDATA[NVIDIA in Europe]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[Virtual Reality]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67426</guid>

					<description><![CDATA[At one of the U.K.’s largest technology festivals, top enterprises and startups are this week highlighting their latest innovations, hosting workshops and celebrating the growing tech ecosystem based in the country’s southwest. The Bristol Technology Festival today showcased the work of nine startups that recently participated in a challenge hosted by Digital Catapult — the <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/12/ai-for-creative-industries-uk-startups/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>At one of the U.K.’s largest technology festivals, top enterprises and startups are this week highlighting their latest innovations, hosting workshops and celebrating the growing tech ecosystem based in the country’s southwest.</p>
<p>The Bristol Technology Festival today showcased the work of nine startups that recently participated in a challenge hosted by Digital Catapult — the U.K. authority on advanced digital technology — in collaboration with NVIDIA.</p>
<p>The challenge, which ran for four months, supported companies in developing a prototype or extending an innovation that could transform experiences using reality capture, real-time collaboration and creation, or cross-platform content delivery.</p>
<p>It’s part of MyWorld, an initiative for pioneering creative technology focused on the western U.K.</p>
<p>Each selected startup was given £50,000 to help develop projects that foster the advancement of <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/" target="_blank" rel="noopener">generative AI</a>, digital twins and other groundbreaking technologies for use in creative industries.</p>
<h2><b>Lux Aeterna Explores Generative AI for Visual Effects</b></h2>
<p>Emmy Award-winning independent visual effects studio <a href="https://www.myworld-creates.com/blogs/lux-aeterna-investigating-generative-ai-tools-for-vfx/" target="_blank" rel="noopener">Lux Aeterna</a> — which is using gen AI and neural networks for VFX production — deployed its funds to develop a generative AI-powered text-to-image toolkit for creating maps, or 2D images used to represent aspects of a scene, object or effect.</p>
<p>At the Bristol Technology Festival, Lux Aeterna demonstrated this technology, powered by <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/" target="_blank" rel="noopener">NVIDIA RTX 40 Series GPUs</a>, with a focus on its ability to generate parallax occlusion maps, a method of creating the effect of depth for 3D textured surfaces.</p>
<p>“Our goal is to tackle the unique VFX challenges with bespoke AI-assisted solutions, and to put these tools of the future into the hands of our talented artists,” said James Pollock, creative technologist at Lux Aeterna. “NVIDIA’s insightful feedback on our work as a part of the MyWorld challenge has been invaluable in informing our strategy toward innovation in this rapidly changing space.”</p>
<h2><b>Meaning Machine Brings AI to Game Characters, Dialogue</b></h2>
<p><a href="https://www.myworld-creates.com/blogs/why-game-developers-hate-ai/" target="_blank" rel="noopener">Meaning Machine</a>, a studio pioneering gameplay that uses natural language AI, used its funds from the challenge to develop a generative AI system for in-game characters and dialogue. Its Game Consciousness technology enables in-game characters to accurately talk about their world, in real time, so that every line of dialogue reflects the game developer’s creative vision.</p>
<p>Meaning Machine’s demo at today’s showcase invited attendees to experience its interrogation game, “Dead Meat,” in which players must chat with an in-game character — a murder suspect — with the aim of manipulating them into giving a confession.</p>
<p>A member of the <a href="https://www.nvidia.com/en-us/startups/" target="_blank" rel="noopener">NVIDIA Inception</a> program for cutting-edge startups, Meaning Machine powers its generative AI technology for game development using the <a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/" target="_blank" rel="noopener">NVIDIA NeMo</a> framework for building, customizing and deploying <a href="https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/" target="_blank" rel="noopener">large language models</a>.</p>
<p>“NVIDIA NeMo enables us to deliver scalable model tuning and inference,” said Ben Ackland, cofounder and chief technology officer at Meaning Machine. “We see potential for Game Consciousness to transform blockbuster games — delivering next-gen characters that feel at home in bigger, deeper, more complex virtual worlds — and our collaboration with NVIDIA will help us make this a reality sooner.”</p>
<h2><b>More Startups Showcase AI for Creative Industries</b></h2>
<p>Additional challenge participants that hosted demos today at the Bristol Technology Festival include:</p>
<ul>
<li style="font-weight: 300;" aria-level="1"><a href="https://www.myworld-creates.com/blogs/from-puppets-to-pixels/" target="_blank" rel="noopener">Black Laboratory</a>, an NVIDIA Inception member demonstrating a live puppet-performance capture system, puppix, that can seamlessly transfer the physicality of puppets to digital characters.</li>
<li style="font-weight: 300;" aria-level="1"><a href="https://www.myworld-creates.com/blogs/impress-launchpad-influencer-marketing-for-indie-games/" target="_blank" rel="noopener">IMPRESS</a>, which is developing an AI-powered launchpad for self-publishing indie video games. It offers data-driven market research for game development, marketing campaign support, press engagement tools and more.</li>
<li style="font-weight: 300;" aria-level="1"><a href="https://www.myworld-creates.com/blogs/otto-a-realtime-generative-visual-media-solution-for-the-performing-arts/" target="_blank" rel="noopener">Larkhall</a>, which is expanding Otto, its AI system that generates live, reactive visuals based on musical performances, as well as automatic, expressive captioning for speech-based performances.</li>
<li style="font-weight: 300;" aria-level="1"><a href="https://myworld-creates.com/blogs/unlocking-the-future-of-cinematography-software-based-control-for-agito-systems/" target="_blank" rel="noopener">Motion Impossible</a>, which is building a software platform for centralized control of its AGITO systems — free-roaming, modular, camera dolly systems for filmmaking.</li>
<li style="font-weight: 300;" aria-level="1"><a href="https://www.myworld-creates.com/blogs/future-places-toolkit-using-ar-and-vr-to-design-future-urban-environments/" target="_blank" rel="noopener">Zubr and Uninvited Guests</a>, two companies collaborating on the development of <a href="https://blogs.nvidia.com/blog/2022/05/20/what-is-extended-reality/" target="_blank" rel="noopener">augmented- and virtual-reality</a> tools for designing futuristic urban environments.</li>
</ul>
<p>“NVIDIA’s involvement in the MyWorld challenge, led by Digital Catapult, has created extraordinary value for the participating teams,” said Sarah Addezio, senior innovation partner and MyWorld program lead at Digital Catapult. “We’ve seen the benefit of our cohort having access to industry-leading technical and business-development expertise, elevating their projects in ways that would not have been possible otherwise.”</p>
<p><i>Learn more about </i><a href="https://www.nvidia.com/en-us/startups/" target="_blank" rel="noopener"><i>NVIDIA Inception</i></a><i> and </i><a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/" target="_blank" rel="noopener"><i>NVIDIA generative AI technologies</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/bristol-tech-fest-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/bristol-tech-fest-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[UK Tech Festival Showcases Startups Using AI for Creative Industries]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Get in Gear: ‘Forza Motorsport’ Races Onto GeForce NOW</title>
		<link>https://blogs.nvidia.com/blog/2023/10/12/geforce-now-thursday-oct-12/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 12 Oct 2023 13:00:10 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67408</guid>

					<description><![CDATA[Put the pedal to the metal this GFN Thursday as Forza Motorsport leads 23 new games in the cloud. Plus, Acer’s Predator Connect 6E is the newest addition to the GeForce NOW Recommended program, with easy cloud gaming quality-of-service (QoS) settings built in to give Ultimate members the best streaming experience. No Breaks, No Limits, <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/12/geforce-now-thursday-oct-12/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Put the pedal to the metal this GFN Thursday as <i>Forza Motorsport</i> leads 23 new games in the cloud.</p>
<p>Plus, Acer’s Predator Connect 6E is the newest addition to the <a href="https://www.nvidia.com/en-us/geforce-now/recommended/">GeForce NOW Recommended program</a>, with easy cloud gaming quality-of-service (QoS) settings built in to give <a href="http://geforcenow.com/memberships">Ultimate members</a> the best streaming experience.</p>
<h2><b>No Breaks, No Limits, No Downloads</b></h2>
<p><iframe loading="lazy" title="Forza Motorsport - Official Launch Trailer" width="500" height="281" src="https://www.youtube.com/embed/yJumrR_bbg0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Take the pole position thanks to the cloud. Turn 10 Studios’ <i>Forza Motorsport</i> joins the <a href="https://www.nvidia.com/en-us/geforce-now/games/">GeForce NOW library</a> this week.</p>
<p>The realistic racing sim features over 500 realistically rendered cars across 20 dynamic and world-famous tracks, each with dynamic time-of-day, weather and driving conditions, so no two laps will ever be the same. Unlock more than 800 performance upgrades and outbuild the competition, either online or against new, highly competitive AI racers in the single-player Builders Cup Career Mode.</p>
<p>Stream every turn at GeForce quality on nearly any device and max out image quality thanks to the cloud. Ultimate members can get in gear at up to 4K resolution andat up to 120 frames per second for the most realistic driving experience.</p>
<h2><b>Need for Speed</b></h2>
<figure id="attachment_67416" aria-describedby="caption-attachment-67416" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-67416" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-672x363.jpg" alt="Acer Predator Connect W6 router for GeForce NOW" width="672" height="363" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-672x363.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-400x216.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-768x414.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-834x450.jpg 834w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-398x215.jpg 398w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-185x100.jpg 185w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router.jpg 1260w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-67416" class="wp-caption-text"><em>Better together.</em></figcaption></figure>
<p>Say hello to the newest addition to the GeForce NOW Recommended program.</p>
<p>GeForce NOW members have access to the best cloud streaming experience, and Acer’s newly released <a href="https://www.acer.com/us-en/predator/networking/wi-fi/predator-connect-w6-wifi-6e-router">Predator Connect W6 wireless router</a> is built to support it, providing the ultrafast, stable gaming environment needed for 4K cloud streaming.</p>
<p>NVIDIA and Acer have collaborated to create a best-in-class streaming experience, creating a special QoS option in the Predator Connect that prioritizes cloud gaming network traffic for maximized speed. The software underwent six months of rigorous testing, ensuring it can consistently deliver the high-performance offerings of a GeForce NOW Ultimate membership, including 4K 120 fps gaming with ultra-low latency.</p>
<p>The Predator Connect W6 also includes tri-band network support with the latest wireless technologies, like WiFi 6E. Pair it with a GeForce NOW Ultimate membership for an unrivaled cloud gaming experience.</p>
<h2><b>Play On</b></h2>
<figure id="attachment_67413" aria-describedby="caption-attachment-67413" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-67413" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-672x336.jpg" alt="Star Trek Infinite on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-67413" class="wp-caption-text"><em>Live long and prosper in the cloud.</em></figcaption></figure>
<p>Get the weekend started with the new weekly games list:</p>
<ul>
<li><i>Forza Motorsport </i>(New release on <a href="https://store.steampowered.com/app/2440510?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a href="https://www.xbox.com/games/store/forza-motorsport-standard-edition/9PLKVSWR299F?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass, Oct. 12)</li>
<li><i>From Space</i> (New release on <a href="https://www.xbox.com/games/store/from-space/9PLK75782446?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass, Oct. 12)</li>
<li><i>Hotel: A Resort Simulator </i>(New release on <a href="https://store.steampowered.com/app/1389840?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 12)</li>
<li><i>Saltsea Chronicles </i>(New release on <a href="https://store.steampowered.com/app/1419620?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 12)</li>
<li><i>Star Trek: Infinite </i>(New release on <a href="https://store.steampowered.com/app/1622900?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 12)</li>
<li><i>Tribe: Primitive Builder</i> (New release on <a href="https://store.steampowered.com/app/1059900?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 12)</li>
<li><i>Lords of the Fallen </i>(New release on <a href="https://store.steampowered.com/app/1501750?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a href="https://www.epicgames.com/store/p/lords-of-the-fallen-2-46fdd6?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>, Oct. 13)</li>
<li><i>Bad North </i>(<a href="https://www.xbox.com/games/store/bad-north-jotunn-edition/9N4T8VCMQVDT?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Call of the Sea </i>(<a href="https://www.xbox.com/games/store/call-of-the-sea/9NNG78K7N91K?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store</li>
<li><i>For The King </i>(<a href="https://www.xbox.com/games/store/for-the-king/9NS1CD1V4BKW?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Golf With Your Friends</i> (<a href="https://www.xbox.com/games/store/golf-with-your-friends-windows-version/9MVK5W0HMRP7?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Metro Simulator 2 </i>(<a href="https://store.steampowered.com/app/1787480?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Moonbreaker </i>(<a href="https://store.steampowered.com/app/845890?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Narita Boy </i>(<a href="https://www.xbox.com/games/store/narita-boy/9NT3FGQC1DFR?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Rubber Bandits </i>(<a href="https://www.xbox.com/games/store//rubber-bandits/9PL36RW9ZTPW?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Sifu </i>(<a href="https://www.xbox.com/games/store/sifu/9P7PF6ZP3958?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Star Renegades</i> (<a href="https://www.xbox.com/games/store/star-renegades/9PK5S1QKV10D?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Streets of Rogue</i> (<a href="https://www.xbox.com/games/store/streets-of-rogue/9NKRBSZXQ2HM?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Supraland </i>(<a href="https://www.xbox.com/games/store/supraland/9P75CZFXMS7N?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Supraland Six Inches Under </i>(<a href="https://www.epicgames.com/store/p/supraland-six-inches-under-dd0220?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
<li><i>The Surge </i>(<a href="https://www.xbox.com/games/store/the-surge-windows-10-version/9NKLXF5DLBKT?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Tiny Football</i> (<a href="https://store.steampowered.com/app/1887010?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Yes, Your Grace</i> (<a href="https://www.xbox.com/games/store/yes-your-grace/9NX14Q0QZD1T?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
</ul>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">It&#39;s time to head to the cloud. What gear are you bringing? <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f914.png" alt="🤔" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f329.png" alt="🌩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a href="https://twitter.com/NVIDIAGFN/status/1712135972844958026?ref_src=twsrc%5Etfw">October 11, 2023</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<p>What are you planning to play this weekend? Let us know on <a href="https://www.twitter.com/nvidiagfn">Twitter</a> or in the comments below.</p>
<p>&nbsp;</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/gfn-thursday-10-12-nv-blog-1280x680-no-cta.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/gfn-thursday-10-12-nv-blog-1280x680-no-cta-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Get in Gear: ‘Forza Motorsport’ Races Onto GeForce NOW]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Take the Wheel: NVIDIA NeMo SteerLM Lets Companies Customize a Model’s Responses During Inference</title>
		<link>https://blogs.nvidia.com/blog/2023/10/11/customize-ai-models-steerlm/</link>
		
		<dc:creator><![CDATA[Annamalai Chockalingam]]></dc:creator>
		<pubDate>Wed, 11 Oct 2023 14:30:17 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Inference]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[NVIDIA Research]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Trustworthy AI]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67289</guid>

					<description><![CDATA[Developers have a new AI-powered steering wheel to help them hug the road while they drive powerful large language models (LLMs) to their desired locations. NVIDIA NeMo SteerLM lets companies define knobs to dial in a model’s responses as it’s running in production, a process called inference. Unlike current methods for customizing an LLM, it <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/11/customize-ai-models-steerlm/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Developers have a new AI-powered steering wheel to help them hug the road while they drive powerful large language models (<a href="https://www.nvidia.com/en-us/glossary/data-science/large-language-models/#:~:text=Large%20language%20models%20(LLMs)%20are,content%20using%20very%20large%20datasets.">LLMs</a>) to their desired locations.</p>
<p>NVIDIA NeMo SteerLM lets companies define knobs to dial in a model’s responses as it’s running in production, a process called <a href="https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/">inference</a>. Unlike current methods for customizing an LLM, it lets a single training run create one model that can serve dozens or even hundreds of use cases, saving time and money.</p>
<p>NVIDIA researchers created SteerLM to teach AI models what users care about, like road signs to follow in their particular use cases or markets. These user-defined attributes can gauge nearly anything — for example, the degree of helpfulness or humor in the model’s responses.</p>
<h2><b>One Model, Many Uses</b></h2>
<p>The result is a new level of flexibility.</p>
<p>With SteerLM, users define all the attributes they want and embed them in a single model. Then they can choose the combination they need for a given use case while the model is running.</p>
<p>For example, a custom model can now be tuned during inference to the unique needs of, say, an accounting, sales or engineering department or a vertical market.</p>
<p>The method also enables a continuous improvement cycle. Responses from a custom model can serve as data for a future training run that dials the model into new levels of usefulness.</p>
<h2><b>Saving Time and Money</b></h2>
<p>To date, fitting a <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">generative AI</a> model to the needs of a specific application has been the equivalent of rebuilding an engine’s transmission. Developers had to painstakingly label datasets, write lots of new code, adjust the hyperparameters under the hood of the neural network and retrain the model several times.</p>
<p>SteerLM replaces those complex, time-consuming processes with three simple steps:</p>
<ul>
<li style="font-weight: 400;" aria-level="1">Using a basic set of prompts, responses and desired attributes, customize an AI model that predicts how those attributes will perform.</li>
<li style="font-weight: 400;" aria-level="1">Automatically generating a dataset using this model.</li>
<li style="font-weight: 400;" aria-level="1">Training the model with the dataset using standard supervised fine-tuning techniques.</li>
</ul>
<h2><b>Many Enterprise Use Cases</b></h2>
<p>Developers can adapt SteerLM to nearly any enterprise use case that requires generating text.</p>
<p>With SteerLM, a company might produce a single chatbot it can tailor in real time to customers’ changing attitudes, demographics or circumstances in the many vertical markets or geographies it serves.</p>
<p>SteerLM also enables a single LLM to act as a flexible writing co-pilot for an entire corporation.</p>
<p>For example, lawyers can modify their model during inference to adopt a formal style for their legal communications. Or marketing staff can dial in a more conversational style for their audience.</p>
<h2><b>Game On With SteerLM</b></h2>
<p>To show the potential of SteerLM, NVIDIA demonstrated it on one of its classic applications — gaming (see the video below).</p>
<p>Today, some games pack dozens of non-playable characters — characters that the player can’t control — which mechanically repeat prerecorded text, regardless of the user or situation.</p>
<p>SteerLM makes these characters come alive, responding with more personality and emotion to players’ prompts. It’s a tool game developers can use to unlock unique new experiences for every player.</p>
<p><iframe loading="lazy" title="NVIDIA ACE Enhanced with Dynamic Responses for Virtual Characters" width="500" height="281" src="https://www.youtube.com/embed/lf0z8Z3OQvM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<h2><b>The Genesis of SteerLM</b></h2>
<p>The concept behind the new method arrived unexpectedly.</p>
<p>“I woke up early one morning with this idea, so I jumped up and wrote it down,” recalled Yi Dong, an applied research scientist at NVIDIA who initiated the work on SteerLM.</p>
<p>While building a prototype, he realized a popular model-conditioning technique could also be part of the method. Once all the pieces came together and his experiment worked, the team helped articulate the method in four simple steps.</p>
<p>It’s the latest advance in model customization, a hot area in AI research.</p>
<p>“It’s a challenging field, a kind of holy grail for making AI more closely reflect a human perspective — and I love a new challenge,” said the researcher, who earned a Ph.D. in computational neuroscience at Johns Hopkins University, then worked on machine learning algorithms in finance before joining NVIDIA.</p>
<h2><b>Get Hands on the Wheel</b></h2>
<p>SteerLM is available as open-source software for developers to try out today. They can also get <a href="https://huggingface.co/nvidia/SteerLM-llama2-13B">details</a> on how to experiment with a Llama-2-13b model customized using the SteerLM method.</p>
<p>For users who want full enterprise security and support, SteerLM will be integrated into <a href="https://developer.nvidia.com/nemo">NVIDIA NeMo</a>, a rich framework for building, customizing and deploying large generative AI models.</p>
<p>The SteerLM method works on all models supported on NeMo, including popular community-built pretrained LLMs such as Llama-2 and BLOOM.</p>
<p>Read a <a href="https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/">technical blog</a> to learn more about SteerLM.</p>
<p><i>See </i><a href="https://www.nvidia.com/en-us/about-nvidia/legal-info/"><i>notice</i></a><i> regarding software product information.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/SteerLM-KV-x1280-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1089"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/SteerLM-KV-x1280-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Take the Wheel: NVIDIA NeMo SteerLM Lets Companies Customize a Model’s Responses During Inference]]></media:title>
			<media:description type="html">Image for NVIDIA NeMo SteerLM</media:description>
			</media:content>
			</item>
	</channel>
</rss>
