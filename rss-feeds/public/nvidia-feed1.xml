<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:media="http://search.yahoo.com/mrss/">

<channel>
	<title>NVIDIA Blog</title>
	<atom:link href="https://blogs.nvidia.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.nvidia.com/</link>
	<description></description>
	<lastBuildDate>Wed, 18 Oct 2023 14:45:34 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.3.1</generator>
	<item>
		<title>NVIDIA Expands Robotics Platform to Meet the Rise of Generative AI</title>
		<link>https://blogs.nvidia.com/blog/2023/10/18/metropolis-jetson-isaac-robotics-edge-ai-developers/</link>
		
		<dc:creator><![CDATA[Amit Goel]]></dc:creator>
		<pubDate>Wed, 18 Oct 2023 14:00:48 +0000</pubDate>
				<category><![CDATA[Autonomous Machines]]></category>
		<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Metropolis]]></category>
		<category><![CDATA[NVIDIA Isaac Sim]]></category>
		<category><![CDATA[NVIDIA Jetson]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67421</guid>

					<description><![CDATA[Powerful generative AI models and cloud-native APIs and microservices are coming to the edge. Generative AI is bringing the power of transformer models and large language models to virtually every industry. That reach now includes areas that touch edge, robotics and logistics systems: defect detection, real-time asset tracking, autonomous planning and navigation, human-robot interactions and <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/18/metropolis-jetson-isaac-robotics-edge-ai-developers/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Powerful generative AI models and cloud-native APIs and microservices are coming to the edge.</p>
<p><a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">Generative AI</a> is bringing the power of transformer models and large language models to virtually every industry. That reach now includes areas that touch edge, robotics and logistics systems: defect detection, real-time asset tracking, autonomous planning and navigation, human-robot interactions and more.</p>
<p>NVIDIA today announced major expansions to two frameworks on the <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/">NVIDIA Jetson platform</a> for edge AI and robotics: the NVIDIA <a href="https://developer.nvidia.com/isaac-ros">Isaac ROS</a> robotics framework has entered general availability, and the NVIDIA Metropolis expansion on Jetson is coming next.</p>
<p>To accelerate AI application development and deployments at the edge, NVIDIA has also created a <a href="https://www.jetson-ai-lab.com">Jetson Generative AI Lab</a> for developers to use with the latest open-source generative AI models.</p>
<p>More than 1.2 million developers and over 10,000 customers have chosen NVIDIA AI and the Jetson platform, including Amazon Web Services, Cisco, John Deere, Medtronic, Pepsico and Siemens.</p>
<p>With the rapidly evolving AI landscape addressing increasingly complicated scenarios, developers are being challenged by longer development cycles to build AI applications for the edge. Reprogramming robots and AI systems on the fly to meet changing environments, manufacturing lines and automation needs of customers is time-consuming and requires expert skills.</p>
<p>Generative AI offers zero-shot learning — the ability for a model to recognize things specifically unseen before in training — with a natural language interface to simplify the development, deployment and management of AI at the edge.</p>
<h2><b>Transforming the AI Landscape</b></h2>
<p>Generative AI dramatically improves ease of use by understanding human language prompts to make model changes. Those AI models are more flexible in detecting, segmenting, tracking, searching and even reprogramming — and  help outperform traditional <a href="https://blogs.nvidia.com/blog/2018/09/05/whats-the-difference-between-a-cnn-and-an-rnn/">convolutional neural network</a>-based models.</p>
<p>Generative AI is expected to add $10.5 billion in revenue for manufacturing operations worldwide by 2033, according to ABI Research.</p>
<p>“Generative AI will significantly accelerate deployments of AI at the edge with better generalization, ease of use and higher accuracy than previously possible,” said Deepu Talla, vice president of embedded and edge computing at NVIDIA. “This largest-ever software expansion of our Metropolis and Isaac frameworks on Jetson, combined with the power of transformer models and generative AI, addresses this need.”</p>
<p><iframe title="Bring Powerful Generative AI Models to the Edge | Powered by Jetson Orin" width="500" height="281" src="https://www.youtube.com/embed/BAMOw7qlVXw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p><b>Developing With Generative AI at the Edge</b></p>
<p>The Jetson Generative AI Lab provides developers access to optimized tools and tutorials for deploying open-source LLMs, diffusion models to generate stunning interactive images, vision language models (VLMs) and vision transformers (ViTs) that combine vision AI and natural language processing to provide comprehensive understanding of the scene.</p>
<p>Developers can also use the <a href="https://developer.nvidia.com/tao-toolkit">NVIDIA TAO Toolkit</a> to create efficient and accurate AI models for edge applications. TAO provides a low-code interface to fine-tune and optimize vision AI models, including ViT and vision foundational models. They can also customize and fine-tune foundational models like NVIDIA NV-DINOv2 or public models like OpenCLIP to create highly accurate vision AI models with very little data. TAO additionally now includes VisualChangeNet, a new transformer-based model for defect inspection.</p>
<h2><b>Harnessing New Metropolis and Isaac Frameworks</b></h2>
<p><a href="https://developer.nvidia.com/metropolis">NVIDIA Metropolis</a> makes it easier and more cost-effective for enterprises to embrace world-class, vision AI-enabled solutions to improve critical operational efficiency and safety problems. The platform brings a collection of powerful application programming interfaces and microservices for developers to quickly develop complex vision-based applications.</p>
<p><img decoding="async" fetchpriority="high" class="wp-image-67550 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-400x225.jpg" alt="" width="673" height="379" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-metropolis-vision-ai-2560x1440-1-1280x720.jpg 1280w" sizes="(max-width: 673px) 100vw, 673px" /></p>
<p>More than 1,000 companies, including BMW Group, Pepsico, Kroger, Tyson Foods, Infosys and Siemens, are using NVIDIA Metropolis developer tools to solve Internet of Things, sensor processing and operational challenges with vision AI — and the rate of adoption is quickening. The tools have now been downloaded over 1 million times by those looking to build vision AI applications.</p>
<p>To help developers quickly build and deploy scalable vision AI applications, an expanded set of Metropolis APIs and microservices on NVIDIA Jetson will be available by year’s end.</p>
<p><iframe title="Build Complex Vision AI Applications Faster with NVIDIA Metropolis APIs and Microservices" width="500" height="281" src="https://www.youtube.com/embed/ifaMK8mVrHk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Hundreds of customers use the NVIDIA Isaac platform to develop high-performance robotics solutions across diverse domains, including agriculture, warehouse automation, last-mile delivery and service robotics, among others.</p>
<p>At ROSCon 2023, NVIDIA <a href="https://developer.nvidia.com/blog/accelerate-ai-enabled-robotics-with-advanced-simulation-and-perception-tools-in-nvidia-isaac-platform/">announced major improvements</a> to perception and simulation capabilities with new releases of Isaac ROS and Isaac Sim software. Built on the widely adopted open-source Robot Operating System (ROS), Isaac ROS brings perception to automation, giving eyes and ears to the things that move. By harnessing the power of GPU-accelerated GEMs, including visual odometry, depth perception, 3D scene reconstruction, localization and planning, robotics developers gain the tools needed to swiftly engineer robotic solutions tailored for a diverse range of applications.</p>
<p><img decoding="async" loading="lazy" class="wp-image-67547 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-400x225.jpg" alt="" width="690" height="388" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support-1280x720.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Isaac-Sim-Improves-RTX-Lidar-and-Sensor-Support.jpg 1920w" sizes="(max-width: 690px) 100vw, 690px" /></p>
<p>Isaac ROS has reached production-ready status with the latest <a href="https://developer.nvidia.com/isaac-ros">Isaac ROS 2.0</a> release, enabling developers to create and bring high-performance robotics solutions to market with Jetson.</p>
<p>“ROS continues to grow and evolve to provide open-source software for the whole robotics community,” said Geoff Biggs, CTO of the Open Source Robotics Foundation. “NVIDIA’s new prebuilt ROS 2 packages, launched with this release, will accelerate that growth by making ROS 2 readily available to the vast NVIDIA Jetson developer community.”</p>
<h2><b>Delivering New Reference AI Workflows</b></h2>
<p>Developing a production-ready AI solution entails optimizing the development and training of AI models tailored to specific use cases, implementing robust security features on the platform, orchestrating the application, managing fleets, establishing seamless edge-to-cloud communication and more.</p>
<p>NVIDIA announced a curated collection of AI reference workflows based on Metropolis and Isaac frameworks that enable developers to quickly adopt the entire workflow or selectively integrate individual components, resulting in substantial reductions in both development time and cost. The three distinct AI workflows include: Network Video Recording, Automatic Optical Inspection and Autonomous Mobile Robot.</p>
<p>“NVIDIA Jetson, with its broad and diverse user base and partner ecosystem, has helped drive a revolution in robotics and AI at the edge,” said Jim McGregor, principal analyst at Tirias Research. “As application requirements become increasingly complex, we need a foundational shift to platforms that simplify and accelerate the creation of edge deployments. This significant software expansion by NVIDIA gives developers access to new multi-sensor models and generative AI capabilities.”</p>
<h2><b>More Coming on the Horizon </b></h2>
<p>NVIDIA announced a collection of system services which are fundamental capabilities that every developer requires when building edge AI solutions. These services will simplify integration into workflows and spare developer the arduous task of building them from the ground up.</p>
<p>The new NVIDIA JetPack 6, expected to be available by year’s end, will empower AI developers to stay at the cutting edge of computing without the need for a full Jetson Linux upgrade, substantially expediting development timelines and liberating them from Jetson Linux dependencies. JetPack 6 will also use the collaborative efforts with Linux distribution partners to expand the horizon of Linux-based distribution choices, including Canonical’s Optimized and Certified Ubuntu, Wind River Linux, Concurrent Real’s Redhawk Linux and various Yocto-based distributions.</p>
<h2><b>Partner Ecosystem Benefits From Platform Expansion</b></h2>
<p>The <a href="https://developer.nvidia.com/embedded/ecosystem">Jetson partner ecosystem</a> provides a wide range of support, from hardware, AI software and application design services to sensors, connectivity and developer tools. These <a href="https://www.nvidia.com/en-us/about-nvidia/partners/">NVIDIA Partner Network</a> innovators play a vital role in providing the building blocks and sub-systems for many products sold on the market.</p>
<p>The latest release allows Jetson partners to accelerate their time to market and expand their customer base by adopting AI with increased performance and capabilities.</p>
<p>Independent software vendor partners will also be able to expand their offerings for Jetson.</p>
<p><img decoding="async" loading="lazy" class="wp-image-67544 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-400x225.jpg" alt="" width="588" height="331" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/NVIDIA-JetPack-6-Largest-Ever-Software-Update-1280x720.jpg 1280w" sizes="(max-width: 588px) 100vw, 588px" /></p>
<p><i>Join us Tuesday, Nov. 7, at 9 a.m. PT for the </i><a href="https://info.nvidia.com/jetson-gen-ai-webinar.html"><i>Bringing Generative AI to Life with NVIDIA Jetson</i></a><i> webinar, where technical experts will dive deeper into the news announced here, including accelerated APIs and quantization methods for deploying LLMs and VLMs on Jetson, optimizing vision transformers with TensorRT, and more.</i></p>
<p><i>Sign up for NVIDIA Metropolis early access </i><a href="https://developer.nvidia.com/metropolis/notify-me"><i>here</i></a><i>. </i></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/MetropolisJetson2.png"
			type="image/png"
			width="1600"
			height="867"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/MetropolisJetson2-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Expands Robotics Platform to Meet the Rise of Generative AI]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Into the Omniverse: Marmoset Brings Breakthroughs in Rendering, Extends OpenUSD Support to Enhance 3D Art Production</title>
		<link>https://blogs.nvidia.com/blog/2023/10/18/marmoset-extends-openusd-support/</link>
		
		<dc:creator><![CDATA[Pooya Ghobadpour]]></dc:creator>
		<pubDate>Wed, 18 Oct 2023 13:00:39 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[Into the Omniverse]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Universal Scene Description]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67520</guid>

					<description><![CDATA[Real-time rendering, animation and texture baking are essential workflows for 3D art production. Using the Marmoset Toolbag software, 3D artists can enhance their creative workflows and build complex 3D models without disruptions to productivity. ]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p><i>Editor’s note: This post is part of </i><a href="https://www.nvidia.com/en-us/omniverse/news/" target="_blank" rel="noopener"><i>Into the Omniverse</i></a><i>, a series focused on how artists and developers from startups to enterprises can transform their workflows using the latest advances in </i><a href="https://www.nvidia.com/en-us/omniverse/usd/" target="_blank" rel="noopener"><i>OpenUSD</i></a><i> and </i><a href="https://www.nvidia.com/en-us/omniverse/usd/" target="_blank" rel="noopener"><i>NVIDIA Omniverse</i></a><i>.</i></p>
<p>Real-time rendering, animation and texture baking are essential workflows for 3D art production. Using the Marmoset Toolbag software, 3D artists can enhance their creative workflows and build complex 3D models without disruptions to productivity.</p>
<p>The latest release of Marmoset Toolbag, version 4.06, brings increased support for <a href="https://www.nvidia.com/en-us/omniverse/usd/" target="_blank" rel="noopener">Universal Scene Description</a>, aka OpenUSD, enabling seamless compatibility with <a href="https://www.nvidia.com/en-us/omniverse/" target="_blank" rel="noopener">NVIDIA Omniverse</a>, a development platform for connecting and building OpenUSD-based tools and applications.</p>
<p>3D creators and technical artists using Marmoset can now enjoy improved interoperability, accelerated rendering, real-time visualization and efficient performance —  redefining the possibilities of their creative workflows.</p>
<h2><b>Enhancing Cross-Platform Creativity With OpenUSD</b></h2>
<p>Creators are taking their workflows to the next level with OpenUSD.</p>
<p>Berlin-based Armin Halač works as a principal animator at <a href="https://www.wooga.com/" target="_blank" rel="noopener">Wooga</a>, a mobile games development studio known for projects like <a href="https://play.google.com/store/apps/details?id=net.wooga.junes_journey_hidden_object_mystery_game&amp;hl=en&amp;gl=US" target="_blank" rel="noopener"><i>June’s Journey</i></a> and <a href="https://play.google.com/store/apps/details?id=com.netflix.NGP.GhostDetective&amp;hl=en_US" target="_blank" rel="noopener"><i>Ghost Detective</i></a>. The nature of his job means Halač is no stranger to 3D workflows — he gets hands-on with animation and character rigging.</p>
<p>For texturing and producing high-quality renders, Marmoset is Halač’s go-to tool, providing a user-friendly interface and powerful features to simplify his workflow. Recently, Halač used Marmoset to create the captivating cover image for his book, <a href="https://www.routledge.com/A-Complete-Guide-to-Character-Rigging-for-Games-Using-Blender/Halac/p/book/9781032203003" target="_blank" rel="noopener"><i>A Complete Guide to Character Rigging for Games Using Blender</i></a><i>.</i></p>
<p>Using the added support for USD, Halač can seamlessly send 3D assets from Blender to Marmoset, creating new possibilities for collaboration and improved visuals.</p>
<p style="text-align: center"><img decoding="async" loading="lazy" class="wp-image-67521 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1.png" alt="" width="933" height="525" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1.png 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-1536x864.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/cover_image_4k-1-1280x720.png 1280w" sizes="(max-width: 933px) 100vw, 933px" /><i>The cover image of Halač’s book.</i></p>
<p>Nkoro Anselem Ire, a.k.a <a href="https://www.youtube.com/@askNK/videos" target="_blank" rel="noopener">askNK</a>, is a popular YouTube creator as well as a media and visual arts professor at a couple of universities who is also seeing workflow benefits from increased USD support.</p>
<p>As a 3D content creator, he uses Marmoset Toolbag for the majority of his <a href="https://www.adobe.com/products/substance3d/discover/pbr.html#:~:text=Physically%20based%20rendering%20(PBR)%2C,light%20interacts%20with%20material%20properties." target="_blank" rel="noopener">PBR</a> workflow — from texture baking and lighting to animation and rendering. Now, with USD, askNK is enjoying newfound levels of creative flexibility as the framework allows him to “collaborate with individuals or team members a lot easier because they can now pick up and drop off processes while working on the same file.”</p>
<p><iframe loading="lazy" title="Mamoverse - Into the Omniverse with Marmoset Toolbag!" width="500" height="281" src="https://www.youtube.com/embed/xLKyE6wXHcs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Halač and askNK recently joined an <a href="https://www.youtube.com/live/8UsOTa8rTgg?si=yUI8kUSWxROufxsf" target="_blank" rel="noopener">NVIDIA-hosted livestream</a> where community members and the Omniverse team explored the benefits of a Marmoset- and Omniverse-boosted workflow.</p>
<p><iframe loading="lazy" title="Learning Marmoset Toolbag for Enhanced 3D Workflows" width="500" height="281" src="https://www.youtube.com/embed/8UsOTa8rTgg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Daniel Bauer is another creator experiencing the benefits of Marmoset, OpenUSD and Omniverse. A <a href="https://www.solidworks.com/" target="_blank" rel="noopener">SolidWorks</a> mechanical engineer with over 10 years of experience, Bauer works frequently in CAD software environments, where it’s typical to assign different materials to various scene components. The variance can often lead to shading errors and incorrect geometry representation, but using USD, Bauer can avoid errors by easily importing versions of his scene from Blender to Marmoset Toolbag to <a href="https://www.nvidia.com/en-us/omniverse/apps/usd-composer/" target="_blank" rel="noopener">Omniverse USD Composer</a>.</p>
<p style="text-align: center"><img decoding="async" loading="lazy" class="size-full wp-image-67524 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/GRIPPER_-1.gif" alt="" width="600" height="382" /><i>A Kuka Scara robot simulation with 10 parallel small grippers for sorting and handling pens.</i></p>
<p>Additionally, 3D artists <a href="https://blogs.nvidia.com/blog/2023/04/19/studio-omniverse-usd-composer-animation/" target="_blank" rel="noopener">Gianluca Squillace and Pasquale Scionti</a> are harnessing the collaborative power of Omniverse, Marmoset and OpenUSD to transform their workflows from a convoluted series of exports and imports to a streamlined, real-time, interconnected process.</p>
<p>Squillace crafted a captivating 3D character with Pixologic ZBrush, Autodesk Maya, Adobe Substance 3D Painter and Marmoset Toolbag — aggregating the data from the various tools in Omniverse. With USD, he seamlessly integrated his animations and made real-time adjustments without the need for constant file exports.</p>
<p>Simultaneously, Scionti constructed a stunning glacial environment using Autodesk 3ds Max, Adobe Substance 3D Painter, Quixel and Unreal Engine, uniting the various pieces from his tools in Omniverse. His work showcased the potential of Omniverse to foster real-time collaboration as he was able to seamlessly integrate Squillace’s character into his snowy world.</p>
<p><img decoding="async" loading="lazy" class=" wp-image-67530 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w.jpg" alt="" width="804" height="452" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-itns-pasquale-gianluca-wk53-scene-004-1280w-178x100.jpg 178w" sizes="(max-width: 804px) 100vw, 804px" /></p>
<h2><b>Advancing Interoperability and Real-Time Rendering</b></h2>
<p><a href="https://marmoset.co/posts/toolbag-4-06-available-now/" target="_blank" rel="noopener">Marmoset Toolbag 4.06</a> provides significant improvements to interoperability and image fidelity for artists working across platforms and applications. This is achieved through updates to Marmoset’s OpenUSD support, allowing for seamless compatibility and connection with the Omniverse ecosystem.</p>
<p>The improved USD import and export capabilities enhance interoperability with popular content creation apps and creative toolkits like Autodesk Maya and Autodesk 3ds Max, SideFX Houdini and Unreal Engine.</p>
<p>Additionally, Marmoset Toolbag 4.06 brings additional updates, including:</p>
<ul>
<li><b>RTX-accelerated rendering and baking: </b>Toolbag’s ray-traced renderer and texture baker are accelerated by NVIDIA RTX GPUs, providing up to a 2x improvement in render times and a 4x improvement in bake times.</li>
<li><b>Real-time denoising with OptiX:</b> With NVIDIA RTX devices, creators can enjoy a smooth and interactive ray-tracing experience, enabling real-time navigation of the active viewport without visual artifacts or performance disruptions.</li>
<li><b>High DPI performance with DLSS image upscaling:</b> The viewport now renders at a reduced resolution and uses AI-based technology to upscale images, improving performance while minimizing image-quality reductions.</li>
</ul>
<p>Download Toolbag 4.06 directly from Marmoset to explore USD support and RTX-accelerated production tools. New users are eligible for a full-featured, <a href="https://marmoset.co/toolbag/#dltoolbag" target="_blank" rel="noopener">30-day free trial license</a>.</p>
<h2><b>Get Plugged Into the Omniverse </b></h2>
<p>Learn from industry experts on how OpenUSD is enabling custom 3D pipelines, easing 3D tool development and delivering interoperability between 3D applications in sessions from SIGGRAPH 2023, now available <a href="https://www.youtube.com/playlist?list=PL3jK4xNnlCVevpoiQ8YR-kYz5h0_9GTD9" target="_blank" rel="noopener">on demand</a>.</p>
<p>Anyone can build their own <a href="https://developer.nvidia.com/omniverse" target="_blank" rel="noopener">Omniverse extension or Connector</a> to enhance their 3D workflows and tools. Explore the Omniverse ecosystem’s <a href="https://www.nvidia.com/en-us/omniverse/ecosystem/" target="_blank" rel="noopener">growing catalog</a> of connections, extensions, foundation applications and third-party tools.</p>
<p>For more <a href="https://developer.nvidia.com/usd" target="_blank" rel="noopener">resources on OpenUSD</a>, explore the <a href="https://forum.aousd.org/" target="_blank" rel="noopener">Alliance for OpenUSD forum</a> or visit the <a href="https://aousd.org/" target="_blank" rel="noopener">AOUSD website</a>.</p>
<p>Share your Marmoset Toolbag and Omniverse work as part of the latest community challenge, <a href="https://forums.developer.nvidia.com/t/the-new-community-challenge-is-here-we-are-kicking-off-the-seasonalartchallenge/268346" target="_blank" rel="noopener">#SeasonalArtChallenge</a>. Use the hashtag to submit a spooky or festive scene for a chance to be featured on the @NVIDIAStudio and @NVIDIAOmniverse social channels.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">Show us your spookiest 3D scenes in the new <a href="https://twitter.com/hashtag/SeasonalArtChallenge?src=hash&amp;ref_src=twsrc%5Etfw">#SeasonalArtChallenge</a>. </p>
<p>Get in the spooky spirit and share your scenes with us and <a href="https://twitter.com/NVIDIAStudio?ref_src=twsrc%5Etfw">@NVIDIAStudio</a> for a chance to be featured on our channels. </p>
<p><img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f383.png" alt="🎃" class="wp-smiley" style="height: 1em; max-height: 1em;" /> courtesy of <a href="https://twitter.com/TanjaLanggner?ref_src=twsrc%5Etfw">@TanjaLanggner</a></p>
<p>&mdash; NVIDIA Omniverse (@nvidiaomniverse) <a href="https://twitter.com/nvidiaomniverse/status/1711874129249685564?ref_src=twsrc%5Etfw">October 10, 2023</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<p><i>Get started with NVIDIA Omniverse by downloading the standard license </i><a href="https://www.nvidia.com/en-us/omniverse/download/" target="_blank" rel="noopener"><i>free</i></a><i>, or learn how </i><a href="https://www.nvidia.com/en-us/omniverse/enterprise/" target="_blank" rel="noopener"><i>Omniverse Enterprise</i><i> can connect your team</i></a><i>. </i></p>
<p><i>Developers can check out these </i><a href="https://developer.nvidia.com/omniverse/get-started/" target="_blank" rel="noopener"><i>Omniverse resources</i></a><i> to begin building on the platform. </i></p>
<p><i>Stay up to date on the platform by subscribing to the </i><a href="https://nvda.ws/3u5KPv1" target="_blank" rel="noopener"><i>newsletter</i></a><i> and following NVIDIA Omniverse on </i><a href="https://www.instagram.com/nvidiaomniverse/"><i>Instagram</i></a><i>, </i><a href="https://www.linkedin.com/showcase/nvidia-omniverse"><i>LinkedIn</i></a><i>, </i><a href="https://medium.com/@nvidiaomniverse"><i>Medium</i></a><i>, </i><a href="https://www.threads.net/@nvidiaomniverse"><i>Threads</i></a><i> and </i><a href="https://twitter.com/nvidiaomniverse"><i>Twitter</i></a><i>.</i></p>
<p><i>For more, check out our </i><a href="https://forums.developer.nvidia.com/c/omniverse/300"><i>forums</i></a><i>, </i><a href="https://discord.com/invite/XWQNJDNuaC"><i>Discord server</i></a><i>, </i><a href="https://www.twitch.tv/nvidiaomniverse"><i>Twitch</i></a><i> and </i><a href="https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA"><i>YouTube</i></a><i> channels.</i></p>
<p><i>Featured image courtesy of Armin Halač, Christian Nauck and Masuquddin Ahmed.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/nv-ov-ito-1280x680_Marmoset.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/nv-ov-ito-1280x680_Marmoset-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Into the Omniverse: Marmoset Brings Breakthroughs in Rendering, Extends OpenUSD Support to Enhance 3D Art Production]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Foxconn and NVIDIA Amp Up Electric Vehicle Innovation</title>
		<link>https://blogs.nvidia.com/blog/2023/10/17/foxconn-nvidia-electric-vehicle/</link>
		
		<dc:creator><![CDATA[Danny Shapiro]]></dc:creator>
		<pubDate>Wed, 18 Oct 2023 03:00:44 +0000</pubDate>
				<category><![CDATA[Driving]]></category>
		<category><![CDATA[NVIDIA DRIVE]]></category>
		<category><![CDATA[Transportation]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67555</guid>

					<description><![CDATA[NVIDIA founder and CEO Jensen Huang joined Hon Hai (Foxconn) Chairman and CEO Young Liu to unveil the latest in their ongoing partnership to develop the next wave of intelligent electric vehicle (EV) platforms for the global automotive market. This latest move, announced today at the fourth annual Hon Hai Tech Day in Taiwan, will <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/17/foxconn-nvidia-electric-vehicle/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>NVIDIA founder and CEO Jensen Huang joined Hon Hai (Foxconn) Chairman and CEO Young Liu to unveil the latest in their <a href="https://nvidianews.nvidia.com/news/nvidia-partners-with-foxconn-to-build-factories-and-systemsfor-the-ai-industrial-revolution" target="_blank" rel="noopener">ongoing partnership</a> to develop the next wave of intelligent electric vehicle (EV) platforms for the global automotive market.</p>
<p>This latest move, announced today at the fourth annual Hon Hai Tech Day in Taiwan, will help Foxconn realize its EV vision with a range of NVIDIA DRIVE solutions — including NVIDIA DRIVE Orin today and its successor, DRIVE Thor, down the road.</p>
<p>In addition, Foxconn will be a contract manufacturer of highly automated and autonomous, AI-rich EVs featuring the upcoming <a href="https://blogs.nvidia.com/blog/2022/03/22/drive-hyperion-9-thor/" target="_blank" rel="noopener">NVIDIA DRIVE Hyperion 9</a> platform, which includes DRIVE Thor and a state-of-the-art sensor architecture.</p>
<h2><strong>Next-Gen EVs With Extraordinary Performance  </strong></h2>
<p>The computational requirements for highly automated and fully self-driving vehicles are enormous. NVIDIA offers the most advanced, highest-performing AI car computers for the transportation industry, with DRIVE Orin selected for use by more than 25 global automakers.</p>
<p>Already a tier-one manufacturer of DRIVE Orin-powered electronic control units (ECUs), Foxconn will also manufacture ECUs featuring DRIVE Thor, once available.</p>
<p>The upcoming DRIVE Thor superchip harnesses advanced AI capabilities first deployed in NVIDIA Grace CPUs and Hopper and Ada Lovelace architecture-based GPUs — and is expected to deliver a staggering 2,000 teraflops of high-performance compute to enable functionally safe and secure intelligent driving.</p>
<figure id="attachment_67556" aria-describedby="caption-attachment-67556" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="wp-image-67556 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-1536x864.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia-1280x720.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnnvidia.png 1600w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-67556" class="wp-caption-text">Next-generation NVIDIA DRIVE Thor.</figcaption></figure>
<h2><strong>Heightened Senses</strong></h2>
<p>Unveiled at GTC last year, DRIVE Hyperion 9 is the latest evolution of NVIDIA’s modular development platform and reference architecture for automated and autonomous vehicles. Set to be powered by DRIVE Thor, it will integrate a qualified sensor architecture for level 3 urban and level 4 highway driving scenarios.</p>
<p>With a diverse and redundant array of high-resolution camera, radar, lidar and ultrasonic sensors, DRIVE Hyperion can process an extraordinary amount of safety-critical data to enable vehicles to deftly navigate their surroundings.</p>
<p>Another advantage of DRIVE Hyperion is its compatibility across generations, as it retains the same compute form factor and NVIDIA DriveWorks application programming interfaces, enabling a seamless transition from DRIVE Orin to DRIVE Thor and beyond.</p>
<p>Plus, DRIVE Hyperion can help speed development times and lower costs for electronics manufacturers like Foxconn, since the sensors available on the platform have cleared NVIDIA’s rigorous qualification processes.</p>
<p>The shift to software-defined vehicles with a centralized electronic architecture will drive the need for high-performance, energy-efficient computing solutions such as DRIVE Thor. By coupling it with the DRIVE Hyperion sensor architecture, Foxconn and its automotive customers will be better equipped to realize a new era of safe and intelligent EVs.</p>
<p>Since its inception, Hon Hai Tech Day has served as a launch pad for Foxconn to showcase its latest endeavors in contract design and manufacturing services and new technologies. These accomplishments span the EV sector and extend to the broader consumer electronics industry.</p>
<p>Catch more on Liu and Huang’s <a href="https://www.youtube.com/watch?v=HmT3MAU09tg&amp;ab_channel=%E9%B4%BB%E6%B5%B7" target="_blank" rel="noopener">fireside chat at Hon Hai Tech Day</a>.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnhhtd-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/foxconnhhtd-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Foxconn and NVIDIA Amp Up Electric Vehicle Innovation]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Striking Performance: Large Language Models up to 4x Faster on RTX With TensorRT-LLM for Windows</title>
		<link>https://blogs.nvidia.com/blog/2023/10/17/tensorrt-llm-windows-stable-diffusion-rtx/</link>
		
		<dc:creator><![CDATA[Jesse Clayton]]></dc:creator>
		<pubDate>Tue, 17 Oct 2023 13:00:42 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[TensorRT]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67466</guid>

					<description><![CDATA[GeForce RTX and NVIDIA RTX GPUs, which are packed with dedicated AI processors called Tensor Cores, are bringing the power of generative AI natively to more than 100 million Windows PCs and workstations.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p><a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/" target="_blank" rel="noopener">Generative AI</a> is one of the most important trends in the history of personal computing, bringing advancements to gaming, creativity, video, productivity, development and more.</p>
<p>And <a href="https://www.nvidia.com/en-us/geforce/rtx/">GeForce RTX</a> and NVIDIA RTX GPUs, which are packed with dedicated AI processors called Tensor Cores, are bringing the power of generative AI natively to more than 100 million Windows PCs and workstations.</p>
<p>Today, generative AI on PC is getting up to 4x faster via <a href="https://developer.nvidia.com/tensorrt">TensorRT-LLM</a> for Windows, an open-source library that accelerates inference performance for the latest AI large language models, like Llama 2 and Code Llama. This follows the announcement of TensorRT-LLM for <a href="https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/">data centers</a> last month.</p>
<p>NVIDIA has also released tools to help developers accelerate their LLMs, including scripts that optimize custom models with TensorRT-LLM, TensorRT-optimized open-source models and a developer reference project that showcases both the speed and quality of LLM responses.</p>
<p>TensorRT acceleration is now available for Stable Diffusion in the popular Web UI by Automatic1111 distribution. It speeds up the generative AI diffusion model by up to 2x over the previous fastest implementation.</p>
<p>Plus, <a href="https://blogs.nvidia.com/blog/2023/02/28/rtx-video-super-resolution/">RTX Video Super Resolution</a> (VSR) version 1.5 is available as part of today’s <a href="https://www.nvidia.com/en-us/geforce/news/game-ready-driver-dlss-3-naraka-vermintide-rtx-vsr">Game Ready Driver</a> release — and will be available in the next <a href="https://www.nvidia.com/en-us/studio/resources/">NVIDIA Studio Driver</a>, releasing early next month.</p>
<h2><b>Supercharging LLMs With TensorRT</b></h2>
<p>LLMs are fueling productivity — engaging in chat, summarizing documents and web content, drafting emails and blogs — and are at the core of new pipelines of AI and other software that can automatically analyze data and generate a vast array of content.</p>
<p>TensorRT-LLM, a library for accelerating LLM inference, gives developers and end users the benefit of LLMs that can now operate up to 4x faster on RTX-powered Windows PCs.</p>
<p>At higher batch sizes, this acceleration significantly improves the experience for more sophisticated LLM use — like writing and coding assistants that output multiple, unique auto-complete results at once. The result is accelerated performance and improved quality that lets users select the best of the bunch.</p>
<p>TensorRT-LLM acceleration is also beneficial when integrating LLM capabilities with other technology, such as in retrieval-augmented generation (RAG), where an LLM is paired with a vector library or vector database. RAG enables the LLM to deliver responses based on a specific dataset, like user emails or articles on a website, to provide more targeted answers.</p>
<p>To show this in practical terms, when the question “How does NVIDIA ACE generate emotional responses?” was asked of the LLaMa 2 base model, it returned an unhelpful response.</p>
<figure id="attachment_67501" aria-describedby="caption-attachment-67501" style="width: 1858px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE.png"><img decoding="async" loading="lazy" class="size-full wp-image-67501" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE.png" alt="" width="1858" height="919" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE.png 1858w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-400x198.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-672x332.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-768x380.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-1536x760.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-842x416.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-406x201.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-188x93.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/ChatWithGeForceNewsACE-1280x633.png 1280w" sizes="(max-width: 1858px) 100vw, 1858px" /></a><figcaption id="caption-attachment-67501" class="wp-caption-text">Better responses, faster.</figcaption></figure>
<p>Conversely, using RAG with recent <a href="https://www.nvidia.com/en-us/geforce/news/">GeForce news articles</a> loaded into a vector library and connected to the same Llama 2 model not only returned the correct answer — using NeMo SteerLM — but did so much quicker with TensorRT-LLM acceleration. This combination of speed and proficiency gives users smarter solutions.</p>
<p>TensorRT-LLM will soon be available to download from the <a href="https://developer.nvidia.com/">NVIDIA Developer</a> website. TensorRT-optimized open source models and the RAG demo with GeForce news as a sample project are available at <a href="https://catalog.ngc.nvidia.com/">ngc.nvidia.com</a> and <a href="https://github.com/nvidia">GitHub.com/NVIDIA</a>.</p>
<h2><b>Automatic Acceleration</b></h2>
<p>Diffusion models, like Stable Diffusion, are used to imagine and create stunning, novel works of art. Image generation is an iterative process that can take hundreds of cycles to achieve the perfect output. When done on an underpowered computer, this iteration can add up to hours of wait time.</p>
<p>TensorRT is designed to accelerate AI models through layer fusion, precision calibration, kernel auto-tuning and other capabilities that significantly boost inference efficiency and speed. This makes it indispensable for real-time applications and resource-intensive tasks.</p>
<p>And now, <a href="https://developer.nvidia.com/blog/unlock-faster-image-generation-in-stable-diffusion-web-ui-with-nvidia-tensorrt/">TensorRT doubles the speed of Stable Diffusion</a>.</p>
<p>Compatible with the most popular distribution, WebUI from Automatic1111, Stable Diffusion with TensorRT acceleration helps users iterate faster and spend less time waiting on the computer, delivering a final image sooner. On a GeForce RTX 4090, it runs 7x faster than the top implementation on Macs with an Apple M2 Ultra. The extension is <a href="https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT">available for download</a> today.</p>
<p>The <a href="https://github.com/NVIDIA/TensorRT/tree/release/8.6/demo/Diffusion">TensorRT demo of a Stable Diffusion pipeline</a> provides developers with a reference implementation on how to prepare diffusion models and accelerate them using TensorRT. This is the starting point for developers interested in turbocharging a diffusion pipeline and bringing lightning-fast inferencing to applications.</p>
<h2><b>Video That’s Super</b></h2>
<p>AI is improving everyday PC experiences for all users. Streaming video — from nearly any source, like YouTube, Twitch, Prime Video, Disney+ and countless others — is among the most popular activities on a PC. Thanks to AI and RTX, it’s getting another update in image quality.</p>
<p><a href="https://blogs.nvidia.com/blog/2023/02/28/rtx-video-super-resolution/">RTX VSR</a> is a breakthrough in AI pixel processing that improves the quality of streamed video content by reducing or eliminating artifacts caused by video compression. It also sharpens edges and details.</p>
<p><iframe loading="lazy" title="AI-Enhanced Video: NVIDIA RTX Video Super Resolution Update 1.5" width="500" height="281" src="https://www.youtube.com/embed/VkKsamTPk7g?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Available now, RTX VSR version 1.5 further improves visual quality with updated models, de-artifacts content played in its native resolution and adds support for RTX GPUs based on the NVIDIA Turing architecture — both professional RTX and GeForce RTX 20 Series GPUs.</p>
<p>Retraining the VSR AI model helped it learn to accurately identify the difference between subtle details and compression artifacts. As a result, AI-enhanced images more accurately preserve details during the upscaling process. Finer details are more visible, and the overall image looks sharper and crisper.</p>
<figure id="attachment_67470" aria-describedby="caption-attachment-67470" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1.png"><img decoding="async" loading="lazy" class="size-large wp-image-67470" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-672x236.png" alt="" width="672" height="236" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-672x236.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-400x140.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-768x269.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-1536x539.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1.png 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-842x295.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-406x142.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-188x66.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-1280x449.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67470" class="wp-caption-text">RTX Video Super Resolution v1.5 improves detail and sharpness.</figcaption></figure>
<p>New with version 1.5 is the ability to de-artifact video played at the display’s native resolution. The original release only enhanced video when it was being upscaled. Now, for example, 1080p video streamed to a 1080p resolution display will look smoother as heavy artifacts are reduced.</p>
<figure id="attachment_67473" aria-describedby="caption-attachment-67473" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR.png"><img decoding="async" loading="lazy" class="size-large wp-image-67473" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-672x377.png" alt="" width="672" height="377" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-672x377.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-400x224.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-768x431.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-1536x861.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-802x450.png 802w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-383x215.png 383w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-1280x718.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR.png 1999w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67473" class="wp-caption-text">RTX VSR now de-artifacts video played at its native resolution.</figcaption></figure>
<p>RTX VSR 1.5 is available today for all RTX users in the latest Game Ready Driver. It will be available in the upcoming NVIDIA Studio Driver, scheduled for early next month.</p>
<p>RTX VSR is among the NVIDIA software, tools, libraries and SDKs — like those mentioned above, plus DLSS, Omniverse, AI Workbench and others — that have helped bring over 400 AI-enabled apps and games to consumers.</p>
<p>The AI era is upon us. And RTX is supercharging at every step in its evolution.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-ai-announcemenet-blog-kv-oct2023-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-ai-announcemenet-blog-kv-oct2023-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Striking Performance: Large Language Models up to 4x Faster on RTX With TensorRT-LLM for Windows]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA RTX Video Super Resolution Update Enhances Video Quality, Detail Preservation and Expands to GeForce RTX 20 Series GPUs</title>
		<link>https://blogs.nvidia.com/blog/2023/10/17/rtx-video-super-resolution-ai-obs-broadcast/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Tue, 17 Oct 2023 13:00:34 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[In the NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[NVIDIA Studio]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67476</guid>

					<description><![CDATA[NVIDIA today announced an update to RTX Video Super Resolution (VSR) that delivers greater overall graphical fidelity with preserved details, upscaling for native videos and support for GeForce RTX 20 Series GPUs. ]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>NVIDIA today announced an update to <a href="https://blogs.nvidia.com/blog/2023/02/28/rtx-video-super-resolution/">RTX Video Super Resolution</a> (VSR) that delivers greater overall graphical fidelity with preserved details, upscaling for native videos and support for GeForce RTX 20 Series desktop and laptop GPUs.</p>
<p><iframe loading="lazy" title="AI-Enhanced Video: NVIDIA RTX Video Super Resolution Update 1.5" width="500" height="281" src="https://www.youtube.com/embed/VkKsamTPk7g?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>For AI assists from RTX VSR and more — from enhanced creativity and productivity to blisteringly fast gaming — check out the <a href="https://www.nvidia.com/en-us/ai-on-rtx/">RTX for AI page</a>.</p>
<p>Plus, this week <i>In the NVIDIA Studio</i>, Twitch personality Runebee shares her inspiration, streaming tips and how she uses AI and RTX GPU acceleration.</p>
<p><iframe loading="lazy" title="The Best of Highlights 1-100" width="500" height="281" src="https://www.youtube.com/embed/Gs8gTICgAGM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>And don’t forget to join the #SeasonalArtChallenge by submitting spooky Halloween-themed art in October and harvest- and fall-themed pieces in November. For inspiration, check out the hauntingly adorable work of artists like <a href="https://www.instagram.com/iryna.blender3d/">iryna.blender3d</a> on <a href="https://twitter.com/NVIDIAStudio/status/1711411202809577569">Twitter</a>.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">The <a href="https://twitter.com/hashtag/SeasonalArtChallenge?src=hash&amp;ref_src=twsrc%5Etfw">#SeasonalArtChallenge</a> continues on with an incredible render from iryna.blender3d (IG). <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f383.png" alt="🎃" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>Share your spooky/Halloween-themed art with <a href="https://twitter.com/hashtag/SeasonalArtChallenge?src=hash&amp;ref_src=twsrc%5Etfw">#SeasonalArtChallenge</a> for a chance to be featured on the Studio or <a href="https://twitter.com/nvidiaomniverse?ref_src=twsrc%5Etfw">@NVIDIAOmniverse</a> channels! <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f64c.png" alt="🙌" class="wp-smiley" style="height: 1em; max-height: 1em;" /> <a href="https://t.co/lOxcYNhBJl">pic.twitter.com/lOxcYNhBJl</a></p>
<p>&mdash; NVIDIA Studio (@NVIDIAStudio) <a href="https://twitter.com/NVIDIAStudio/status/1711411202809577569?ref_src=twsrc%5Etfw">October 9, 2023</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<h2><b>The Super RTX VSR Update 1.5</b></h2>
<p>RTX VSR’s AI model has been retrained to more accurately identify the difference between subtle details and compression artifacts to better preserve image details during the upscaling process. Finer details are more visible, and the overall image looks sharper and crisper than before.</p>
<figure id="attachment_67470" aria-describedby="caption-attachment-67470" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1.png"><img decoding="async" loading="lazy" class="size-large wp-image-67470" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-672x236.png" alt="" width="672" height="236" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-672x236.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-400x140.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-768x269.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-1536x539.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1.png 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-842x295.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-406x142.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-188x66.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/VSR-v1.5-sbsbs-1-1280x449.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67470" class="wp-caption-text">RTX VSR v1.5 improves detail and sharpness.</figcaption></figure>
<p>RTX VSR version 1.5 will also de-artifact videos played at their native resolution — prior, only upscaled video could be enhanced. Providing a leap in graphical fidelity for laptop owners with 1080p screens, the updated RTX VSR makes 1080p resolution, which is popular for content and displays, look smoother at its native resolution, even with heavy artifacts.</p>
<figure id="attachment_67473" aria-describedby="caption-attachment-67473" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR.png"><img decoding="async" loading="lazy" class="size-large wp-image-67473" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-672x377.png" alt="" width="672" height="377" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-672x377.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-400x224.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-768x431.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-1536x861.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-802x450.png 802w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-383x215.png 383w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR-1280x718.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/RTX-VSR.png 1999w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67473" class="wp-caption-text">RTX VSR now de-artifacts video played at native resolution.</figcaption></figure>
<p>And with expanded RTX VSR support, owners of GeForce RTX 20 Series GPUs can benefit from the same AI-enhanced video as those using RTX 30 and 40 Series GPUs.</p>
<p>RTX VSR 1.5 is available as part of the latest <a href="https://www.nvidia.com/en-us/geforce/game-ready-drivers/">Game Ready Driver</a>, available for download today. Content creators downloading <a href="https://www.nvidia.com/en-us/studio/">NVIDIA Studio Drivers</a> — designed to enhance features, reduce repetitiveness and dramatically accelerate creative workflows — can install the driver with RTX VSR releasing in early November.</p>
<h2><b>Runebee-lievable Streaming</b></h2>
<p>Runebee has been livestreaming for over 10 years, providing a space for viewers to hang out and talk about games, movies or whatever else is going on in life. Over the years, she’s realized how common a desire for escapism is.</p>
<p>“Things aren’t always sunshine and rainbows, so it’s nice to have some company that can help take your mind off things,” said Runebee.</p>
<p><iframe loading="lazy" title="Playable Mercs Wesker in Separate Ways! (With Trainer) - Resident Evil 4 Remake" width="500" height="281" src="https://www.youtube.com/embed/I3LUccjySJY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Runebee has amassed over 100K followers on Twitch, YouTube and Instagram, crediting her success to thorough preparation of her setup. Her technology-forward approach ensures efficiency and reliability — allowing her focus to be on performance.</p>
<p>“There’s a lot of planning involved in streaming, but at the end of the day, hitting the ‘start streaming’ button is the most important step, and NVIDIA GPU-acceleration is a massive factor in allowing it to go as smoothly as it does,” said Runebee.</p>
<div class="simplePullQuote right"><p>“I never thought I’d have this smooth of a stream just by upgrading to a GeForce RTX 40 Series GPU.” &#8211; Runebee</p>
</div>
<p>OBS is Runbee’s preferred open-source software for video recording and livestreaming on Twitch. For maximum efficiency, Runebee deploys her <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4080/">GeForce RTX 4080 RTX GPU</a>, taking advantage of the eighth-generation NVIDIA encoder, <a href="https://developer.nvidia.com/video-codec-sdk">NVENC</a>, to independently encode video, which frees up the graphics card to focus on livestreaming.</p>
<p>“Streaming games and running OBS used to kill my CPU, and NVENC has taken so much stress off,” said Runebee. “I was hardly even able to stream PC games until I switched to NVENC.”</p>
<p>For livestreamers, RTX 40 Series GPUs can offer support for real-time AV1 hardware encoding, providing a 40% efficiency boost compared to H.264 and delivering higher quality than competing GPUs.</p>
<p><iframe loading="lazy" title="Unlock Higher Quality Live Streams with AV1 Support on GeForce RTX 40 Series GPUs" width="500" height="281" src="https://www.youtube.com/embed/kxznHq8be8I?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<div class="simplePullQuote right"><p>“As I started building more PCs with NVIDIA GPUs, I never had a reason to switch!” &#8211; Runebee</p>
</div>
<p>Runebee can export recordings of her livestreams with Adobe Premiere Pro in half the normally required time thanks to GeForce RTX 40 Series dual encoders working together, dividing the work evenly to double output.</p>
<p>They’re capable of recording up to 8K, 60 frames per second content in real time via <a href="https://www.nvidia.com/en-us/geforce/geforce-experience/">GeForce Experience</a> and OBS Studio.</p>
<p><iframe loading="lazy" title="Faster Video Editing with GeForce RTX 40 Series GPUs &amp; DaVinci Resolve" width="500" height="281" src="https://www.youtube.com/embed/DhBFuU8Gnik?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Always looking to improve her livestreaming process, Runebee plans on experimenting with the <a href="https://www.nvidia.com/en-us/geforce/broadcasting/broadcast-app/">NVIDIA Broadcast app</a>, which transforms any room into a home studio by upgrading standard webcams, microphones and speakers into premium smart devices using the power of AI.</p>
<p><iframe loading="lazy" title="NVIDIA Broadcast 1.4 Update Featuring Eye Contact" width="500" height="281" src="https://www.youtube.com/embed/nR-vP_7XFHE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Runebee encourages those interested in livestreaming to at least give their potential passion project a shot. “It’s a great way to meet tons of new friends, become more articulate at describing the things you love — be it games or movies — and cultivate a community to share your passions with,” she said.</p>
<figure id="attachment_67477" aria-describedby="caption-attachment-67477" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67477" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-runebee-wk79-runebee-setup-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67477" class="wp-caption-text">Twitch livestreamer Runebee’s setup.</figcaption></figure>
<p>Follow Runebee on <a href="https://www.twitch.tv/runebee">Twitch</a>.</p>
<p><i>Follow NVIDIA Studio on </i><a href="https://www.instagram.com/nvidiastudio/"><i>Instagram</i></a><i>, </i><a href="https://twitter.com/NVIDIAStudio"><i>Twitter</i></a><i> and </i><a href="https://www.facebook.com/NVIDIAStudio/"><i>Facebook</i></a><i>. Access tutorials on the </i><a href="https://www.youtube.com/channel/UCDeQdW6Lt6nhq3mLM4oLGWw"><i>Studio YouTube channel</i></a><i> and get updates directly in your inbox by subscribing to the </i><a href="https://www.nvidia.com/en-us/studio/?nvmid=subscribe-creators-mail-icon"><i>Studio newsletter</i></a><i>.</i> <i>See </i><a href="https://www.nvidia.com/en-us/about-nvidia/legal-info/"><i>notice </i></a><i>regarding software product information.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/runebee-nv-blog-header-preview-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/runebee-nv-blog-header-preview-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA RTX Video Super Resolution Update Enhances Video Quality, Detail Preservation and Expands to GeForce RTX 20 Series GPUs]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>From Skylines to Streetscapes: How SHoP Architects Brings Innovative Designs to Life</title>
		<link>https://blogs.nvidia.com/blog/2023/10/13/rtx-ambassador-mengyi-fan/</link>
		
		<dc:creator><![CDATA[JJ Kim]]></dc:creator>
		<pubDate>Fri, 13 Oct 2023 16:00:17 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[AEC]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67432</guid>

					<description><![CDATA[At SHoP Architects, a New York City-based architectural firm, Mengyi Fan and her team aim to inspire industry professionals to create visual masterpieces by incorporating emerging technologies. Fan, the director of visualization at SHoP, has expertise that spans the fields of architectural visualization and design. She takes a definitive, novel and enduring approach to designing <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/13/rtx-ambassador-mengyi-fan/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>At <a href="https://www.shoparc.com/">SHoP Architects</a>, a New York City-based architectural firm, Mengyi Fan and her team aim to inspire industry professionals to create visual masterpieces by incorporating emerging technologies.</p>
<p>Fan, the director of visualization at SHoP, has expertise that spans the fields of architectural visualization and design. She takes a definitive, novel and enduring approach to designing and planning architecture for city skylines and streetscapes.</p>
<p>Fan and her team work on various architecture visualization projects, from still renderings to real-time walkthroughs. They use multiple creative applications throughout the course of their projects, including Adobe Photoshop, Autodesk 3ds Max, Autodesk Revit and Epic Games&#8217; Unreal Engine. SHoP also collaborates directly with architects at project kickoff, providing images and animations that facilitate quicker decision-making during the design process.</p>
<p>The team consistently integrates new technologies that allow them to explore untapped innovation opportunities, as well as boost research and development. Fan often incorporates real-time and traditional rendering, <a href="https://blogs.nvidia.com/blog/2022/05/20/what-is-extended-reality/">extended reality</a> and AI into her creative workflows.</p>
<p>To capture all the details that bring the designs together, SHoP uses <a href="https://www.nvidia.com/en-us/design-visualization/rtx-a5500/">NVIDIA RTX A5500</a>. Fan is also part of the <a href="https://www.nvidia.com/en-us/design-visualization/community/rtx-ambassador/">NVIDIA RTX Ambassador Program</a>, which is designed to amplify the work of professionals from diverse industries who are using RTX technology. Equipped with the latest capabilities of RTX, Fan hopes to continue pushing boundaries in real-time visualization, AI and <a href="https://blogs.nvidia.com/blog/2021/12/14/what-is-a-digital-twin/">digital twin</a> applications.</p>
<figure id="attachment_67433" aria-describedby="caption-attachment-67433" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4.png"><img decoding="async" loading="lazy" class="size-large wp-image-67433" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_4.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67433" class="wp-caption-text">All images courtesy of SHoP Architects.</figcaption></figure>
<h2><b>Redefining Creative Experiences </b></h2>
<p>3D models play a critical role as the single source of truth, which is why SHoP designers need advanced technology to help them create detailed models and visualizations without creativity or productivity slowdowns.</p>
<p>Previously, the team used CPU-based offerings, which limited the scope of work and research and development they could take on. But with RTX, ‌designers can create and communicate complex designs while continuously collaborating with others.</p>
<p>By tapping into RTX A5500, Fan can prioritize efficiency and high rendering quality without worrying about compute power limitations.</p>
<p>“NVIDIA’s professional RTX GPUs are currently known as the industry standard for graphics cards solutions,” said Fan. “RTX provides us with the performance and power needed to do all the above without worrying about hardware constraints.”</p>
<p>The advanced features of the RTX GPUs allow SHoP designers to explore new ways of representation.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02.jpg"><img decoding="async" loading="lazy" class="aligncenter size-large wp-image-67436" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-672x259.jpg" alt="" width="672" height="259" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-672x259.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-400x154.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-768x296.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-1536x591.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-842x324.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-406x156.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-188x72.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02-1280x493.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_immersive_02.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>SHoP Architects’ projects have grown in scale, location and diversity, and Fan and her team are constantly learning and adapting from each project, drawing inspiration from diverse areas such as automotive, aviation, film and gaming.</p>
<p>Fan views RTX-powered tools as a means of opening up diverse approaches and solutions to be more widely adopted within the industry. And as an NVIDIA RTX Ambassador, she aims to push past technological boundaries by connecting with like-minded designers and creatives.</p>
<p>See more of Fan’s work below. Discover how <a href="https://www.nvidia.com/en-us/design-visualization/rtx/">NVIDIA RTX</a> can help enhance architectural workflows and learn more about the <a href="https://www.nvidia.com/en-us/design-visualization/community/rtx-ambassador/">NVIDIA RTX Ambassador Program</a>.</p>
<div style="width: 1280px;" class="wp-video"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video class="wp-video-shortcode" id="video-67432-1" width="1280" height="720" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_Botswana-Innovation-Hub_lite.mp4?_=1" /><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_Botswana-Innovation-Hub_lite.mp4">https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_Botswana-Innovation-Hub_lite.mp4</a></video></div>
</div>]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2023/10/SHoP_Botswana-Innovation-Hub_lite.mp4" length="9616749" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_1.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/Mengyi-Fan-copy_1-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[From Skylines to Streetscapes: How SHoP Architects Brings Innovative Designs to Life]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>UK Tech Festival Showcases Startups Using AI for Creative Industries</title>
		<link>https://blogs.nvidia.com/blog/2023/10/12/ai-for-creative-industries-uk-startups/</link>
		
		<dc:creator><![CDATA[Jamie Allan]]></dc:creator>
		<pubDate>Thu, 12 Oct 2023 19:58:03 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Game Development]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Inception]]></category>
		<category><![CDATA[NVIDIA in Europe]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[Virtual Reality]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67426</guid>

					<description><![CDATA[At one of the U.K.’s largest technology festivals, top enterprises and startups are this week highlighting their latest innovations, hosting workshops and celebrating the growing tech ecosystem based in the country’s southwest. The Bristol Technology Festival today showcased the work of nine startups that recently participated in a challenge hosted by Digital Catapult — the <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/12/ai-for-creative-industries-uk-startups/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>At one of the U.K.’s largest technology festivals, top enterprises and startups are this week highlighting their latest innovations, hosting workshops and celebrating the growing tech ecosystem based in the country’s southwest.</p>
<p>The Bristol Technology Festival today showcased the work of nine startups that recently participated in a challenge hosted by Digital Catapult — the U.K. authority on advanced digital technology — in collaboration with NVIDIA.</p>
<p>The challenge, which ran for four months, supported companies in developing a prototype or extending an innovation that could transform experiences using reality capture, real-time collaboration and creation, or cross-platform content delivery.</p>
<p>It’s part of MyWorld, an initiative for pioneering creative technology focused on the western U.K.</p>
<p>Each selected startup was given £50,000 to help develop projects that foster the advancement of <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/" target="_blank" rel="noopener">generative AI</a>, digital twins and other groundbreaking technologies for use in creative industries.</p>
<h2><b>Lux Aeterna Explores Generative AI for Visual Effects</b></h2>
<p>Emmy Award-winning independent visual effects studio <a href="https://www.myworld-creates.com/blogs/lux-aeterna-investigating-generative-ai-tools-for-vfx/" target="_blank" rel="noopener">Lux Aeterna</a> — which is using gen AI and neural networks for VFX production — deployed its funds to develop a generative AI-powered text-to-image toolkit for creating maps, or 2D images used to represent aspects of a scene, object or effect.</p>
<p>At the Bristol Technology Festival, Lux Aeterna demonstrated this technology, powered by <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/" target="_blank" rel="noopener">NVIDIA RTX 40 Series GPUs</a>, with a focus on its ability to generate parallax occlusion maps, a method of creating the effect of depth for 3D textured surfaces.</p>
<p>“Our goal is to tackle the unique VFX challenges with bespoke AI-assisted solutions, and to put these tools of the future into the hands of our talented artists,” said James Pollock, creative technologist at Lux Aeterna. “NVIDIA’s insightful feedback on our work as a part of the MyWorld challenge has been invaluable in informing our strategy toward innovation in this rapidly changing space.”</p>
<h2><b>Meaning Machine Brings AI to Game Characters, Dialogue</b></h2>
<p><a href="https://www.myworld-creates.com/blogs/why-game-developers-hate-ai/" target="_blank" rel="noopener">Meaning Machine</a>, a studio pioneering gameplay that uses natural language AI, used its funds from the challenge to develop a generative AI system for in-game characters and dialogue. Its Game Consciousness technology enables in-game characters to accurately talk about their world, in real time, so that every line of dialogue reflects the game developer’s creative vision.</p>
<p>Meaning Machine’s demo at today’s showcase invited attendees to experience its interrogation game, “Dead Meat,” in which players must chat with an in-game character — a murder suspect — with the aim of manipulating them into giving a confession.</p>
<p>A member of the <a href="https://www.nvidia.com/en-us/startups/" target="_blank" rel="noopener">NVIDIA Inception</a> program for cutting-edge startups, Meaning Machine powers its generative AI technology for game development using the <a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/" target="_blank" rel="noopener">NVIDIA NeMo</a> framework for building, customizing and deploying <a href="https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/" target="_blank" rel="noopener">large language models</a>.</p>
<p>“NVIDIA NeMo enables us to deliver scalable model tuning and inference,” said Ben Ackland, cofounder and chief technology officer at Meaning Machine. “We see potential for Game Consciousness to transform blockbuster games — delivering next-gen characters that feel at home in bigger, deeper, more complex virtual worlds — and our collaboration with NVIDIA will help us make this a reality sooner.”</p>
<h2><b>More Startups Showcase AI for Creative Industries</b></h2>
<p>Additional challenge participants that hosted demos today at the Bristol Technology Festival include:</p>
<ul>
<li style="font-weight: 300;" aria-level="1"><a href="https://www.myworld-creates.com/blogs/from-puppets-to-pixels/" target="_blank" rel="noopener">Black Laboratory</a>, an NVIDIA Inception member demonstrating a live puppet-performance capture system, puppix, that can seamlessly transfer the physicality of puppets to digital characters.</li>
<li style="font-weight: 300;" aria-level="1"><a href="https://www.myworld-creates.com/blogs/impress-launchpad-influencer-marketing-for-indie-games/" target="_blank" rel="noopener">IMPRESS</a>, which is developing an AI-powered launchpad for self-publishing indie video games. It offers data-driven market research for game development, marketing campaign support, press engagement tools and more.</li>
<li style="font-weight: 300;" aria-level="1"><a href="https://www.myworld-creates.com/blogs/otto-a-realtime-generative-visual-media-solution-for-the-performing-arts/" target="_blank" rel="noopener">Larkhall</a>, which is expanding Otto, its AI system that generates live, reactive visuals based on musical performances, as well as automatic, expressive captioning for speech-based performances.</li>
<li style="font-weight: 300;" aria-level="1"><a href="https://myworld-creates.com/blogs/unlocking-the-future-of-cinematography-software-based-control-for-agito-systems/" target="_blank" rel="noopener">Motion Impossible</a>, which is building a software platform for centralized control of its AGITO systems — free-roaming, modular, camera dolly systems for filmmaking.</li>
<li style="font-weight: 300;" aria-level="1"><a href="https://www.myworld-creates.com/blogs/future-places-toolkit-using-ar-and-vr-to-design-future-urban-environments/" target="_blank" rel="noopener">Zubr and Uninvited Guests</a>, two companies collaborating on the development of <a href="https://blogs.nvidia.com/blog/2022/05/20/what-is-extended-reality/" target="_blank" rel="noopener">augmented- and virtual-reality</a> tools for designing futuristic urban environments.</li>
</ul>
<p>“NVIDIA’s involvement in the MyWorld challenge, led by Digital Catapult, has created extraordinary value for the participating teams,” said Sarah Addezio, senior innovation partner and MyWorld program lead at Digital Catapult. “We’ve seen the benefit of our cohort having access to industry-leading technical and business-development expertise, elevating their projects in ways that would not have been possible otherwise.”</p>
<p><i>Learn more about </i><a href="https://www.nvidia.com/en-us/startups/" target="_blank" rel="noopener"><i>NVIDIA Inception</i></a><i> and </i><a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/" target="_blank" rel="noopener"><i>NVIDIA generative AI technologies</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/bristol-tech-fest-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/bristol-tech-fest-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[UK Tech Festival Showcases Startups Using AI for Creative Industries]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Get in Gear: ‘Forza Motorsport’ Races Onto GeForce NOW</title>
		<link>https://blogs.nvidia.com/blog/2023/10/12/geforce-now-thursday-oct-12/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 12 Oct 2023 13:00:10 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67408</guid>

					<description><![CDATA[Put the pedal to the metal this GFN Thursday as Forza Motorsport leads 23 new games in the cloud. Plus, Acer’s Predator Connect 6E is the newest addition to the GeForce NOW Recommended program, with easy cloud gaming quality-of-service (QoS) settings built in to give Ultimate members the best streaming experience. No Breaks, No Limits, <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/12/geforce-now-thursday-oct-12/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Put the pedal to the metal this GFN Thursday as <i>Forza Motorsport</i> leads 23 new games in the cloud.</p>
<p>Plus, Acer’s Predator Connect 6E is the newest addition to the <a href="https://www.nvidia.com/en-us/geforce-now/recommended/">GeForce NOW Recommended program</a>, with easy cloud gaming quality-of-service (QoS) settings built in to give <a href="http://geforcenow.com/memberships">Ultimate members</a> the best streaming experience.</p>
<h2><b>No Breaks, No Limits, No Downloads</b></h2>
<p><iframe loading="lazy" title="Forza Motorsport - Official Launch Trailer" width="500" height="281" src="https://www.youtube.com/embed/yJumrR_bbg0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Take the pole position thanks to the cloud. Turn 10 Studios’ <i>Forza Motorsport</i> joins the <a href="https://www.nvidia.com/en-us/geforce-now/games/">GeForce NOW library</a> this week.</p>
<p>The realistic racing sim features over 500 realistically rendered cars across 20 dynamic and world-famous tracks, each with dynamic time-of-day, weather and driving conditions, so no two laps will ever be the same. Unlock more than 800 performance upgrades and outbuild the competition, either online or against new, highly competitive AI racers in the single-player Builders Cup Career Mode.</p>
<p>Stream every turn at GeForce quality on nearly any device and max out image quality thanks to the cloud. Ultimate members can get in gear at up to 4K resolution andat up to 120 frames per second for the most realistic driving experience.</p>
<h2><b>Need for Speed</b></h2>
<figure id="attachment_67416" aria-describedby="caption-attachment-67416" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-67416" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-672x363.jpg" alt="Acer Predator Connect W6 router for GeForce NOW" width="672" height="363" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-672x363.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-400x216.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-768x414.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-834x450.jpg 834w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-398x215.jpg 398w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router-185x100.jpg 185w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Acer_Predator_W6_Router.jpg 1260w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-67416" class="wp-caption-text"><em>Better together.</em></figcaption></figure>
<p>Say hello to the newest addition to the GeForce NOW Recommended program.</p>
<p>GeForce NOW members have access to the best cloud streaming experience, and Acer’s newly released <a href="https://www.acer.com/us-en/predator/networking/wi-fi/predator-connect-w6-wifi-6e-router">Predator Connect W6 wireless router</a> is built to support it, providing the ultrafast, stable gaming environment needed for 4K cloud streaming.</p>
<p>NVIDIA and Acer have collaborated to create a best-in-class streaming experience, creating a special QoS option in the Predator Connect that prioritizes cloud gaming network traffic for maximized speed. The software underwent six months of rigorous testing, ensuring it can consistently deliver the high-performance offerings of a GeForce NOW Ultimate membership, including 4K 120 fps gaming with ultra-low latency.</p>
<p>The Predator Connect W6 also includes tri-band network support with the latest wireless technologies, like WiFi 6E. Pair it with a GeForce NOW Ultimate membership for an unrivaled cloud gaming experience.</p>
<h2><b>Play On</b></h2>
<figure id="attachment_67413" aria-describedby="caption-attachment-67413" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-67413" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-672x336.jpg" alt="Star Trek Infinite on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Star_Trek_Infinte.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-67413" class="wp-caption-text"><em>Live long and prosper in the cloud.</em></figcaption></figure>
<p>Get the weekend started with the new weekly games list:</p>
<ul>
<li><i>Forza Motorsport </i>(New release on <a href="https://store.steampowered.com/app/2440510?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a href="https://www.xbox.com/games/store/forza-motorsport-standard-edition/9PLKVSWR299F?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass, Oct. 12)</li>
<li><i>From Space</i> (New release on <a href="https://www.xbox.com/games/store/from-space/9PLK75782446?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass, Oct. 12)</li>
<li><i>Hotel: A Resort Simulator </i>(New release on <a href="https://store.steampowered.com/app/1389840?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 12)</li>
<li><i>Saltsea Chronicles </i>(New release on <a href="https://store.steampowered.com/app/1419620?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 12)</li>
<li><i>Star Trek: Infinite </i>(New release on <a href="https://store.steampowered.com/app/1622900?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 12)</li>
<li><i>Tribe: Primitive Builder</i> (New release on <a href="https://store.steampowered.com/app/1059900?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 12)</li>
<li><i>Lords of the Fallen </i>(New release on <a href="https://store.steampowered.com/app/1501750?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a href="https://www.epicgames.com/store/p/lords-of-the-fallen-2-46fdd6?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>, Oct. 13)</li>
<li><i>Bad North </i>(<a href="https://www.xbox.com/games/store/bad-north-jotunn-edition/9N4T8VCMQVDT?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Call of the Sea </i>(<a href="https://www.xbox.com/games/store/call-of-the-sea/9NNG78K7N91K?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store</li>
<li><i>For The King </i>(<a href="https://www.xbox.com/games/store/for-the-king/9NS1CD1V4BKW?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Golf With Your Friends</i> (<a href="https://www.xbox.com/games/store/golf-with-your-friends-windows-version/9MVK5W0HMRP7?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Metro Simulator 2 </i>(<a href="https://store.steampowered.com/app/1787480?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Moonbreaker </i>(<a href="https://store.steampowered.com/app/845890?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Narita Boy </i>(<a href="https://www.xbox.com/games/store/narita-boy/9NT3FGQC1DFR?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Rubber Bandits </i>(<a href="https://www.xbox.com/games/store//rubber-bandits/9PL36RW9ZTPW?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Sifu </i>(<a href="https://www.xbox.com/games/store/sifu/9P7PF6ZP3958?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Star Renegades</i> (<a href="https://www.xbox.com/games/store/star-renegades/9PK5S1QKV10D?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Streets of Rogue</i> (<a href="https://www.xbox.com/games/store/streets-of-rogue/9NKRBSZXQ2HM?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Supraland </i>(<a href="https://www.xbox.com/games/store/supraland/9P75CZFXMS7N?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Supraland Six Inches Under </i>(<a href="https://www.epicgames.com/store/p/supraland-six-inches-under-dd0220?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
<li><i>The Surge </i>(<a href="https://www.xbox.com/games/store/the-surge-windows-10-version/9NKLXF5DLBKT?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Tiny Football</i> (<a href="https://store.steampowered.com/app/1887010?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Yes, Your Grace</i> (<a href="https://www.xbox.com/games/store/yes-your-grace/9NX14Q0QZD1T?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
</ul>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">It&#39;s time to head to the cloud. What gear are you bringing? <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f914.png" alt="🤔" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f329.png" alt="🌩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a href="https://twitter.com/NVIDIAGFN/status/1712135972844958026?ref_src=twsrc%5Etfw">October 11, 2023</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<p>What are you planning to play this weekend? Let us know on <a href="https://www.twitter.com/nvidiagfn">Twitter</a> or in the comments below.</p>
<p>&nbsp;</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/gfn-thursday-10-12-nv-blog-1280x680-no-cta.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/gfn-thursday-10-12-nv-blog-1280x680-no-cta-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Get in Gear: ‘Forza Motorsport’ Races Onto GeForce NOW]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Take the Wheel: NVIDIA NeMo SteerLM Lets Companies Customize a Model’s Responses During Inference</title>
		<link>https://blogs.nvidia.com/blog/2023/10/11/customize-ai-models-steerlm/</link>
		
		<dc:creator><![CDATA[Annamalai Chockalingam]]></dc:creator>
		<pubDate>Wed, 11 Oct 2023 14:30:17 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Inference]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[NVIDIA Research]]></category>
		<category><![CDATA[Open Source]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67289</guid>

					<description><![CDATA[Developers have a new AI-powered steering wheel to help them hug the road while they drive powerful large language models (LLMs) to their desired locations. NVIDIA NeMo SteerLM lets companies define knobs to dial in a model’s responses as it’s running in production, a process called inference. Unlike current methods for customizing an LLM, it <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/11/customize-ai-models-steerlm/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Developers have a new AI-powered steering wheel to help them hug the road while they drive powerful large language models (<a href="https://www.nvidia.com/en-us/glossary/data-science/large-language-models/#:~:text=Large%20language%20models%20(LLMs)%20are,content%20using%20very%20large%20datasets.">LLMs</a>) to their desired locations.</p>
<p>NVIDIA NeMo SteerLM lets companies define knobs to dial in a model’s responses as it’s running in production, a process called <a href="https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/">inference</a>. Unlike current methods for customizing an LLM, it lets a single training run create one model that can serve dozens or even hundreds of use cases, saving time and money.</p>
<p>NVIDIA researchers created SteerLM to teach AI models what users care about, like road signs to follow in their particular use cases or markets. These user-defined attributes can gauge nearly anything — for example, the degree of helpfulness or humor in the model’s responses.</p>
<h2><b>One Model, Many Uses</b></h2>
<p>The result is a new level of flexibility.</p>
<p>With SteerLM, users define all the attributes they want and embed them in a single model. Then they can choose the combination they need for a given use case while the model is running.</p>
<p>For example, a custom model can now be tuned during inference to the unique needs of, say, an accounting, sales or engineering department or a vertical market.</p>
<p>The method also enables a continuous improvement cycle. Responses from a custom model can serve as data for a future training run that dials the model into new levels of usefulness.</p>
<h2><b>Saving Time and Money</b></h2>
<p>To date, fitting a <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">generative AI</a> model to the needs of a specific application has been the equivalent of rebuilding an engine’s transmission. Developers had to painstakingly label datasets, write lots of new code, adjust the hyperparameters under the hood of the neural network and retrain the model several times.</p>
<p>SteerLM replaces those complex, time-consuming processes with three simple steps:</p>
<ul>
<li style="font-weight: 400;" aria-level="1">Using a basic set of prompts, responses and desired attributes, customize an AI model that predicts how those attributes will perform.</li>
<li style="font-weight: 400;" aria-level="1">Automatically generating a dataset using this model.</li>
<li style="font-weight: 400;" aria-level="1">Training the model with the dataset using standard supervised fine-tuning techniques.</li>
</ul>
<h2><b>Many Enterprise Use Cases</b></h2>
<p>Developers can adapt SteerLM to nearly any enterprise use case that requires generating text.</p>
<p>With SteerLM, a company might produce a single chatbot it can tailor in real time to customers’ changing attitudes, demographics or circumstances in the many vertical markets or geographies it serves.</p>
<p>SteerLM also enables a single LLM to act as a flexible writing co-pilot for an entire corporation.</p>
<p>For example, lawyers can modify their model during inference to adopt a formal style for their legal communications. Or marketing staff can dial in a more conversational style for their audience.</p>
<h2><b>Game On With SteerLM</b></h2>
<p>To show the potential of SteerLM, NVIDIA demonstrated it on one of its classic applications — gaming (see the video below).</p>
<p>Today, some games pack dozens of non-playable characters — characters that the player can’t control — which mechanically repeat prerecorded text, regardless of the user or situation.</p>
<p>SteerLM makes these characters come alive, responding with more personality and emotion to players’ prompts. It’s a tool game developers can use to unlock unique new experiences for every player.</p>
<p><iframe loading="lazy" title="NVIDIA ACE Enhanced with Dynamic Responses for Virtual Characters" width="500" height="281" src="https://www.youtube.com/embed/lf0z8Z3OQvM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<h2><b>The Genesis of SteerLM</b></h2>
<p>The concept behind the new method arrived unexpectedly.</p>
<p>“I woke up early one morning with this idea, so I jumped up and wrote it down,” recalled Yi Dong, an applied research scientist at NVIDIA who initiated the work on SteerLM.</p>
<p>While building a prototype, he realized a popular model-conditioning technique could also be part of the method. Once all the pieces came together and his experiment worked, the team helped articulate the method in four simple steps.</p>
<p>It’s the latest advance in model customization, a hot area in AI research.</p>
<p>“It’s a challenging field, a kind of holy grail for making AI more closely reflect a human perspective — and I love a new challenge,” said the researcher, who earned a Ph.D. in computational neuroscience at Johns Hopkins University, then worked on machine learning algorithms in finance before joining NVIDIA.</p>
<h2><b>Get Hands on the Wheel</b></h2>
<p>SteerLM is available as open-source software for developers to try out today. They can also get <a href="https://huggingface.co/nvidia/SteerLM-llama2-13B">details</a> on how to experiment with a Llama-2-13b model customized using the SteerLM method.</p>
<p>For users who want full enterprise security and support, SteerLM will be integrated into <a href="https://developer.nvidia.com/nemo">NVIDIA NeMo</a>, a rich framework for building, customizing and deploying large generative AI models.</p>
<p>The SteerLM method works on all models supported on NeMo, including popular community-built pretrained LLMs such as Llama-2 and BLOOM.</p>
<p>Read a <a href="https://developer.nvidia.com/blog/announcing-steerlm-a-simple-and-practical-technique-to-customize-llms-during-inference/">technical blog</a> to learn more about SteerLM.</p>
<p><i>See </i><a href="https://www.nvidia.com/en-us/about-nvidia/legal-info/"><i>notice</i></a><i> regarding software product information.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/SteerLM-KV-x1280-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1089"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/SteerLM-KV-x1280-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Take the Wheel: NVIDIA NeMo SteerLM Lets Companies Customize a Model’s Responses During Inference]]></media:title>
			<media:description type="html">Image for NVIDIA NeMo SteerLM</media:description>
			</media:content>
			</item>
		<item>
		<title>MAXimum AI Performance: Latest Adobe Updates Accelerated by NVIDIA GPUs Improve Workflows for Millions of Creatives</title>
		<link>https://blogs.nvidia.com/blog/2023/10/10/adobe-max-firefly-creative-cloud-substance-3d/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Tue, 10 Oct 2023 16:00:26 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[In the NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA Studio Driver]]></category>
		<category><![CDATA[Rendering]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67366</guid>

					<description><![CDATA[Generative AI is helping creatives across many industries bring ideas to life at unprecedented speed. This technology will be on display at Adobe MAX, running through Thursday, Oct. 12, in person and virtually. ]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Generative AI is helping creatives across many industries bring ideas to life at unprecedented speed.</p>
<p>This technology will be on display at <a href="https://max.adobe.com/">Adobe MAX</a>, running through Thursday, Oct. 12, in person and virtually.</p>
<p><iframe loading="lazy" title="Accelerating 3D Workflows with RTX and AI" width="500" height="281" src="https://www.youtube.com/embed/7RnbPP6VdbE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Adobe is putting the power of generative AI into the hands of creators with the release of <a href="https://www.adobe.com/sensei/generative-ai/firefly.html">Adobe Firefly.</a> Using NVIDIA GPUs, Adobe is bringing new opportunities for artists and more looking to accelerate generative AI — unleashing generative AI enhancements for millions of users. Firefly is now available as a standalone app and integrated with other Adobe apps.</p>
<p>Recent updates to Adobe’s most popular apps — including for Adobe Premiere Pro, Lightroom, After Effects and Substance 3D Stager, Modeler and Sampler — bring new AI features to creators. And <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/">GeForce RTX</a> and <a href="https://www.nvidia.com/en-us/design-visualization/desktop-graphics/">NVIDIA RTX</a> GPUs help accelerate these apps and AI effects, providing massive time savings.</p>
<p>Video editors can use AI to improve dialogue quality with the <em>Enhance Speech (beta)</em> function and work faster with GPU accelerated decoding of ARRIRAW camera original digital film clips up to 60% faster on RTX GPUs compared to on an Apple MacBook Pro 16 M2 Max in Premiere Pro. Plus, take advantage of improved rotoscoping quality with the Next-Gen Roto Brush (version 3.0) feature now available in After Effects.</p>
<p>Photographers and 2D artists now have new Lens Blur effects in Lightroom, complementing ongoing optimizations that improve performance in its Select Object, Select People and Select Sky features.</p>
<p>These advanced features are further enhanced by <a href="https://www.nvidia.com/en-us/studio/">NVIDIA Studio Drivers</a>, free for RTX GPU owners, which add performance and reliability. The October Studio Driver is available for download now.</p>
<p>Finally, 3D artist SouthernShotty returns to <i>In the NVIDIA Studio</i> to share his <a href="https://portal.adobe.com/widget/adobe/m23/exhibitorcatalog/exhibitor/NVIDIAStudio">3D montage</a> of a mix of beautifully hand-crafted worlds — built with Adobe apps and Blender and featuring AI-powered workflows accelerated by his GeForce RTX 4090 Laptop GPU.</p>
<h2><b>MAXimizing Creativity</b></h2>
<p>Adobe Creative Cloud and Substance 3D apps run fastest on NVIDIA RTX GPUs — and recent updates show continued time-saving performance gains.</p>
<figure id="attachment_67370" aria-describedby="caption-attachment-67370" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-enchanced-speech-blog-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67370" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-enchanced-speech-blog-1280w-672x183.png" alt="" width="672" height="183" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-enchanced-speech-blog-1280w-672x183.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-enchanced-speech-blog-1280w-400x109.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-enchanced-speech-blog-1280w-768x209.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-enchanced-speech-blog-1280w-842x229.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-enchanced-speech-blog-1280w-406x111.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-enchanced-speech-blog-1280w-188x51.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-enchanced-speech-blog-1280w.png 1281w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67370" class="wp-caption-text">Tested on NVIDIA Studio laptops with GeForce RTX 4050 and 4090 Laptop GPUs with Intel Core i9 13th Gen; MacBook Pro 14&#8243; with M2 Pro; and MacBook Pro 16&#8243; with M2 Max. Performance measures total time to apply Enhanced Speech effect to video clip within Adobe Premiere Pro.</figcaption></figure>
<p>Premiere Pro’s <i>Enhance Speech (beta)</i> feature, currently in beta, uses AI to remove noise and improve the quality of dialogue clips so that they sound professionally recorded. Tasks are completed 8x faster with a GeForce RTX 4090 Laptop GPU compared to MacBook Pro 16 with M2 Max.</p>
<figure id="attachment_67373" aria-describedby="caption-attachment-67373" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-arriraw-blog-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67373" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-arriraw-blog-1280w-672x182.png" alt="" width="672" height="182" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-arriraw-blog-1280w-672x182.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-arriraw-blog-1280w-400x108.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-arriraw-blog-1280w-768x208.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-arriraw-blog-1280w-842x228.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-arriraw-blog-1280w-406x110.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-arriraw-blog-1280w-188x51.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/nvidia-studio-itns-wk78-ada-perf-charts-dark-r3_premiere-pro-arriraw-blog-1280w.png 1281w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67373" class="wp-caption-text">Tested on NVIDIA Studio laptops with GeForce RTX 4050 and 4090 Laptop GPUs with Intel Core i9 13th Gen; MacBook Pro 14&#8243; with M2 Pro; and MacBook Pro 16&#8243; with M2 Max. Performance measures total time to apply export ARRIRAW footage within Adobe Premiere Pro.</figcaption></figure>
<p>Premiere Pro professionals use ARRIRAW footage — the only format that fully retains a camera’s natural color response and great exposure latitude. ARRIRAW video exports can be done 1.6x faster on GeForce RTX 4090 Laptop GPUs than on the MacBook Pro 16 with M2 Max.</p>
<p>Additionally, After Effects users can access the Next-Gen Roto Brush feature in beta, powered by a brand-new AI model. It’s ideal for isolating subjects such as overlapping limbs, hair and other transparencies more easily, saving time.</p>
<p>RTX GPUs shine in 3D workloads. Substance 3D Stager’s new AI-powered, GPU-accelerated denoiser allows almost instantaneous photorealistic rendering.</p>
<p>Substance 3D Modeler’s recent Hardware Ray Tracing in Capture Mode capability uses NVIDIA technology to export high-quality screenshots 2.4x faster than before.</p>
<p>Meanwhile, Substance 3D Sampler’s AI UpScale feature increases detail for low-quality textures and its Image to Material feature makes it easier to create high-quality materials from a single photograph.</p>
<figure id="attachment_67376" aria-describedby="caption-attachment-67376" style="width: 520px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/Lens-BLur.png"><img decoding="async" loading="lazy" class="size-large wp-image-67376" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/Lens-BLur-520x500.png" alt="" width="520" height="500" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/Lens-BLur-520x500.png 520w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Lens-BLur-400x384.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Lens-BLur-468x450.png 468w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Lens-BLur-224x215.png 224w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Lens-BLur-104x100.png 104w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Lens-BLur.png 641w" sizes="(max-width: 520px) 100vw, 520px" /></a><figcaption id="caption-attachment-67376" class="wp-caption-text">Lens Blur in Adobe Lightroom.</figcaption></figure>
<p>Photographers have long used the popular Super Resolution feature in Adobe Camera Raw, which is supported by Photoshop, and gives 3x faster performance on a GeForce RTX 4090 Laptop GPU compared to a MacBook Pro 16 M2 Max. Now, Lightroom users have AI-driven capabilities with the Lens Blur feature for applying realistic lens blur effects, Point Color for precise color adjustments to speed up color correction, and High Dynamic Range Output for edits and renders in an HDR color space.</p>
<h2><b>Adobe Firefly Glows #76B900</b></h2>
<p>Adobe Firefly provides users with generative AI features, utilizing NVIDIA GPUs in the cloud.</p>
<p><iframe loading="lazy" title="Adobe Firefly: A New Era of Creativity" width="500" height="281" src="https://www.youtube.com/embed/f_2KsIwoV4Q?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Firefly features such as <a href="https://www.adobe.com/products/photoshop/ai.html">Generative Fill</a> — to add, remove and expand content in Photoshop, and <a href="https://www.adobe.com/products/photoshop/ai.html">Generative Expand</a> to expand scenes with generative content — help complete tasks instantly in Adobe Photoshop.</p>
<figure id="attachment_67379" aria-describedby="caption-attachment-67379" style="width: 640px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/Generative-Fill-Adobe-Photoshop.png"><img decoding="async" loading="lazy" class="size-full wp-image-67379" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/Generative-Fill-Adobe-Photoshop.png" alt="" width="640" height="361" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/Generative-Fill-Adobe-Photoshop.png 640w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Generative-Fill-Adobe-Photoshop-400x226.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Generative-Fill-Adobe-Photoshop-381x215.png 381w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Generative-Fill-Adobe-Photoshop-177x100.png 177w" sizes="(max-width: 640px) 100vw, 640px" /></a><figcaption id="caption-attachment-67379" class="wp-caption-text">Adobe Firefly-powered feature Generative Fill in Adobe Photoshop.</figcaption></figure>
<p>Adobe Illustrator offers the <a href="https://www.adobe.com/products/illustrator/generative-recolor.html">Generative Recolor</a> feature, which enables graphic designers to explore a wide variety of colors, palettes and themes in their work without having to do tedious manual recoloring. Discovering the perfect combination of colors now takes just a few seconds.</p>
<figure id="attachment_67382" aria-describedby="caption-attachment-67382" style="width: 640px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/Generative-Recolor-Adobe-Illustrator.png"><img decoding="async" loading="lazy" class="size-full wp-image-67382" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/Generative-Recolor-Adobe-Illustrator.png" alt="" width="640" height="361" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/Generative-Recolor-Adobe-Illustrator.png 640w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Generative-Recolor-Adobe-Illustrator-400x226.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Generative-Recolor-Adobe-Illustrator-381x215.png 381w, https://blogs.nvidia.com/wp-content/uploads/2023/10/Generative-Recolor-Adobe-Illustrator-177x100.png 177w" sizes="(max-width: 640px) 100vw, 640px" /></a><figcaption id="caption-attachment-67382" class="wp-caption-text">Adobe Firefly-powered feature Generative Recolor in Adobe Illustrator.</figcaption></figure>
<p>Adobe Express offers the <a href="https://www.adobe.com/express/">Text to Image</a> feature to create incredible imagery from standard prompts, and the <a href="https://www.adobe.com/express/">Text Effects</a> feature helps stylize standard text for use in creating flyers, resumes, social media reels and more.</p>
<p>These powerful AI capabilities were developed with the creative community in mind — guided by AI ethics principles of content and data transparency — to ensure ethically and morally responsible output.</p>
<p>NVIDIA technology will continue to support new Adobe Firefly-powered features from the cloud as they become available to photographers, illustrators, designers, video editors, 3D artists and more.</p>
<h2><b>MAXed Out AI Fun</b></h2>
<p>Independent filmmaker and artist SouthernShotty knows the challenges of producing content alone and how daunting the process can be.</p>
<figure id="attachment_67385" aria-describedby="caption-attachment-67385" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-island-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67385" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-island-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-island-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-island-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-island-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-island-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-island-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-island-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-island-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67385" class="wp-caption-text">SouthernShotty’s artwork invokes childlike emotions with impressive visuals.</figcaption></figure>
<div class="simplePullQuote right"><p>“I’m a big fan of the NVIDIA Studio Driver support, because it adds stability and reliability.” &#8211; SouthernShotty</p>
</div>
<p>As such, SouthernShotty is always looking for tools and techniques to ease the creative process. To accelerate his workflow, he combined new Adobe AI capabilities accelerated by his GeForce RTX 4090 GPU to achieve incredible efficiency.</p>
<p>The artist kept his 3D models fairly simple, focusing on textures to ensure that the world would match his vision. He deployed one of his favorite features, the AI-powered <i>Image to Material</i> in Adobe Substance 3D Sampler, to convert images to physically based rendering textures.</p>
<p>&nbsp;</p>
<figure id="attachment_67388" aria-describedby="caption-attachment-67388" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-1-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67388" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-1-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-1-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-1-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-1-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-1-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-1-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-1-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-1-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67388" class="wp-caption-text">Applying textures in Blender.</figcaption></figure>
<div class="simplePullQuote right"><p>“It’s so fast that I can pretty much preview my entire scene in real time and see the final result before I ever hit the render button.” &#8211; SouthernShotty</p>
</div>
<p>RTX-accelerated light and ambient occlusion baking allowed SouthernShotty to realize the desired visual effect in seconds.</p>
<p>His RTX GPU continued to play an essential role as he used Blender Cycles’ RTX-accelerated OptiX ray tracing in the viewport for interactive, photorealistic rendering.</p>
<p>As the 3D montage progresses, the main character appears and reappears in several new environments. Each new location is featured for only a second or two, but SouthernShotty still needed to create a fully fleshed out environment for each.</p>
<p><iframe loading="lazy" title="Blender 3D - How FAST is the NVIDIA GeForce RTX 4090 !?" width="500" height="281" src="https://www.youtube.com/embed/uzzHT12pIVE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>Normally this would take a substantial amount of time, but an AI assist from Adobe Firefly helped speed the process.</p>
<figure id="attachment_67391" aria-describedby="caption-attachment-67391" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-2-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67391" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-2-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-2-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-2-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-2-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-2-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-2-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-2-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-2-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67391" class="wp-caption-text">Adobe is committed to developing generative AI responsibly, with creators at the center.</figcaption></figure>
<p>SouthernShotty opened the app, entered “fantasy mushroom forest” as the text prompt and then made minor adjustments by tinkering with the digital art, golden hour, for lighting, and wide-angle settings for composition. When satisfied with the result, he downloaded the image for further editing in Photoshop.</p>
<figure id="attachment_67394" aria-describedby="caption-attachment-67394" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-3-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67394" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-3-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-3-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-3-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-3-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-3-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-3-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-3-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-max-southernshotty-3-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67394" class="wp-caption-text">An entirely new image is generated in minutes with Adobe Firefly, powered by GeForce RTX GPUs.</figcaption></figure>
<p>SouthernShotty then used the AI-powered Generative Fill feature to remove unwanted background elements. He used the Neural Filters optimization to color match a castle element added in the background, then used Generative Fill again to effortlessly blend the castle in with the trees.</p>
<p>Finally, SouthernShotty used the Neural Filters optimization in the new Lens Blur feature to add depth to the scene — first exporting depth as a separate layer and then editing in Blender to complete the scene.</p>
<figure id="attachment_67397" aria-describedby="caption-attachment-67397" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-adobe-max-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67397" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-adobe-max-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-adobe-max-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-adobe-max-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-adobe-max-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-adobe-max-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-adobe-max-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-adobe-max-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/studio-southernshotty-wk78-adobe-max-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67397" class="wp-caption-text">Editing the depth map in Blender.</figcaption></figure>
<p>“My entire process was sprinkled with GPU-acceleration and AI-enabled features,” said SouthernShotty. “In Blender, the GeForce RTX 4090 GPU accelerated everything — but especially the live render view in my viewport, which was crucial to visualizing my scenes.”</p>
<p>Check out SouthernShotty’s <a href="https://www.youtube.com/@SouthernShotty/featured">YouTube channel</a> for Blender tutorials on characters, animation, rigging and more.</p>
<p><i>Follow NVIDIA Studio on </i><a href="https://www.instagram.com/nvidiastudio/"><i>Instagram</i></a><i>, </i><a href="https://twitter.com/NVIDIAStudio"><i>Twitter</i></a><i> and </i><a href="https://www.facebook.com/NVIDIAStudio/"><i>Facebook</i></a><i>. Access tutorials on the </i><a href="https://www.youtube.com/channel/UCDeQdW6Lt6nhq3mLM4oLGWw"><i>Studio YouTube channel</i></a><i> and get updates directly in your inbox by subscribing to the </i><a href="https://www.nvidia.com/en-us/studio/?nvmid=subscribe-creators-mail-icon"><i>Studio newsletter</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/adobe-max-2023-nv-blog-header-preview-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/adobe-max-2023-nv-blog-header-preview-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[MAXimum AI Performance: Latest Adobe Updates Accelerated by NVIDIA GPUs Improve Workflows for Millions of Creatives]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Keeping an AI on Quakes: Researchers Unveil Deep Learning Model to Improve Forecasts</title>
		<link>https://blogs.nvidia.com/blog/2023/10/06/quakes-deep-learning-forecasts/</link>
		
		<dc:creator><![CDATA[Scott Martin]]></dc:creator>
		<pubDate>Fri, 06 Oct 2023 16:00:54 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Research]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Science]]></category>
		<category><![CDATA[Social Impact]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67327</guid>

					<description><![CDATA[A research team is aiming to shake up the status quo for earthquake models. Researchers from the Universities of California at Berkeley and Santa Cruz, and the Technical University of Munich recently released a paper describing a new model that delivers deep learning to earthquake forecasting. Dubbed RECAST, the model can use larger datasets and <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/06/quakes-deep-learning-forecasts/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>A research team is aiming to shake up the status quo for earthquake models.</p>
<p>Researchers from the Universities of California at Berkeley and Santa Cruz, and the Technical University of Munich recently <a href="https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2023GL103909">released a paper</a> describing a new model that delivers deep learning to earthquake forecasting.</p>
<p>Dubbed RECAST, the model can use larger datasets and offer greater flexibility than the current model standard, ETAS, which has improved only incrementally since its development in 1988, it argues.</p>
<p>The paper’s authors — Kelian Dascher-Cousineau, Oleksandr Shchur, Emily Brodsky and Stephan Günnemann — trained the model on <a href="https://www.nvidia.com/en-us/design-visualization/desktop-graphics/">NVIDIA GPU workstations</a>.</p>
<p>“There’s a whole field of research that explores how to improve ETAS,” said Dacher-Cousineau, a postdoctoral researcher at UC Berkeley. “It’s an immensely useful model that has been used a lot, but it’s been frustratingly hard to improve on it.”</p>
<h2><b>AI Drives Seismology Ahead </b></h2>
<p>The promise of RECAST is that its model flexibility, self-learning capability and ability to scale will enable it to interpret larger datasets and make better predictions during earthquake sequences, he said.</p>
<p>Model advances with improved forecasts could help agencies such as the U.S. Geological Survey and its counterparts elsewhere offer better information to those who need to know. Firefighters and other first responders entering damaged buildings, for example, could benefit from more reliable forecasts on aftershocks.</p>
<p>“There’s a ton of room for improvement within the forecasting side of things. And for a variety of reasons, our community hasn’t really dove into the machine learning side of things, partly because of being conservative and partly because these are really impactful decisions,” said Dacher-Cousineau.</p>
<h2><b>RECAST Model Moves the Needle</b></h2>
<p>While past work on aftershock predictions has relied on statistical models, this doesn’t scale to handle the larger datasets becoming available from an explosion of newly enhanced data capabilities, according to the researchers.</p>
<p>The RECAST model architecture builds on developments in neural temporal point processes, which are probabilistic generative models for continuous time event sequences. In a nutshell, the model has an encoder-decoder neural network architecture used for predicting the timing of a next event based on a history of past events.</p>
<p>Dacher-Cousineau said that releasing and benchmarking the model in the paper demonstrates that it can quickly learn to do what ETAS can do, while it holds vast potential to do more.</p>
<p>“Our model is a generative model that, just like a natural language processing model, you can generate paragraphs and paragraphs of words, and you can sample it and make synthetic catalogs,” said Dacher-Cousineau. “Part of the paper is there to convince old-school seismologists that this is a model that’s doing the right thing — we’re not overfitting.”</p>
<h2><b>Boosting Earthquake Data With Enhanced Catalogs </b></h2>
<p>Earthquake catalogs, or records of earthquake data, for particular geographies can be small. That’s because to this day many come from seismic analysts who interpret scribbles of raw data that comes from seismometers. But this, too, is an area where AI researchers are building models to autonomously interpret these P waves and other signals in the data in real time.</p>
<p>Enhanced data is meanwhile helping to fill the void. With the labeled data in earthquake catalogs, machine learning engineers are revisiting these sources of raw data and building enhanced catalogs to get 10x to 100x the number of earthquakes for training data and categories.</p>
<p>“So it’s not necessarily that we put out more instruments to gather data but rather that we enhance the datasets,” said Dacher-Cousineau.</p>
<h2><b>Applying Larger Datasets to Other Settings</b></h2>
<p>With the larger datasets, the researchers are starting to see improvements from RECAST over the standard ETAS model.</p>
<p>To advance the state of the art in earthquake forecasting, Dascher-Cousineau is working with a team of undergraduates at UC Berkeley to train earthquake catalogs on multiple regions for better predictions.</p>
<p>“I have the natural language processing analogies in mind, where it seems very plausible that earthquake sequences in Japan are useful to inform earthquakes in California,” he said. “And you can see that going in the right direction.”</p>
<p><i>Learn about synthetic data generation with </i><a href="https://developer.nvidia.com/omniverse/replicator"><i>NVIDIA Omniverse Replicator</i></a></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/SFQuakePost-and-Grant-Avenue-Look-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1298"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/SFQuakePost-and-Grant-Avenue-Look-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Keeping an AI on Quakes: Researchers Unveil Deep Learning Model to Improve Forecasts]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Brains of the Operation: Atlas Meditech Maps Future of Surgery With AI, Digital Twins</title>
		<link>https://blogs.nvidia.com/blog/2023/10/05/atlas-meditech-brain-surgery-ai-digital-twins/</link>
		
		<dc:creator><![CDATA[Mona Flores]]></dc:creator>
		<pubDate>Thu, 05 Oct 2023 13:00:43 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[Healthcare and Life Sciences]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Social Impact]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67294</guid>

					<description><![CDATA[Just as athletes train for a game or actors rehearse for a performance, surgeons prepare ahead of an operation. Now, Atlas Meditech is letting brain surgeons experience a new level of realism in their pre-surgery preparation with AI and physically accurate simulations. Atlas Meditech, a brain-surgery intelligence platform, is adopting tools — including the MONAI <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/05/atlas-meditech-brain-surgery-ai-digital-twins/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Just as athletes train for a game or actors rehearse for a performance, surgeons prepare ahead of an operation.</p>
<p>Now, Atlas Meditech is letting brain surgeons experience a new level of realism in their pre-surgery preparation with AI and physically accurate simulations.</p>
<p>Atlas Meditech, a brain-surgery intelligence platform, is adopting tools — including the <a href="https://docs.nvidia.com/monai/index.html">MONAI</a> medical imaging framework and <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> 3D development platform — to build AI-powered decision support and high-fidelity surgery rehearsal platforms. Its mission: improving surgical outcomes and patient safety.</p>
<p>“The Atlas provides a collection of multimedia tools for brain surgeons, allowing them to mentally rehearse an operation the night before a real surgery,” said Dr. Aaron Cohen-Gadol, founder of Atlas Meditech and its nonprofit counterpart, Neurosurgical Atlas. “With accelerated computing and digital twins, we want to transform this mental rehearsal into a highly realistic rehearsal in simulation.”</p>
<p>Neurosurgical Atlas offers case studies, surgical videos and 3D models of the brain to more than a million online users. Dr. Cohen-Gadol, also a professor of neurological surgery at Indiana University School of Medicine, estimates that more than 90% of brain surgery training programs in the U.S. — as well as tens of thousands of neurosurgeons in other countries — use the Atlas as a key resource during residency and early in their surgery careers.</p>
<p>Atlas Meditech’s Pathfinder software is integrating AI algorithms that can suggest safe surgical pathways for experts to navigate through the brain to reach a lesion.</p>
<p>And with NVIDIA Omniverse, a platform for connecting and building custom 3D pipelines and metaverse applications, the team aims to create custom virtual representations of individual patients’ brains for surgery rehearsal.</p>
<h2><b>Custom 3D Models of Human Brains</b></h2>
<p>A key benefit of Atlas Meditech’s advanced simulations — either onscreen or in immersive virtual reality — is the ability to customize the simulations, so that surgeons can practice on a virtual brain that matches the patient’s brain in size, shape and lesion position.</p>
<p>“Every patient’s anatomy is a little different,” said Dr. Cohen-Gadol. “What we can do now with physics and advanced graphics is create a patient-specific model of the brain and work with it to see and virtually operate on a tumor. The accuracy of the physical properties helps to recreate the experience we have in the real world during an operation.”</p>
<p>To create digital twins of patients’ brains, the Atlas Pathfinder tool has adopted MONAI Label, which can support radiologists by automatically annotating MRI and CT scans to segment normal structures and tumors.</p>
<p>“MONAI Label is the gateway to any healthcare project because it provides us with the opportunity to segment critical structures and protect them,” said Dr. Cohen-Gadol. “For the Atlas, we’re training MONAI Label to act as the eyes of the surgeon, highlighting what is a normal vessel and what’s a tumor in an individual patient’s scan.”</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_040-scaled.jpg"><img decoding="async" loading="lazy" class="aligncenter size-large wp-image-67298" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_040-672x378.jpg" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_040-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_040-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_040-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_040-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_040-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_040-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_040-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_040-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_040-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>With a segmented view of a patient’s brain, Atlas Pathfinder can adjust its 3D brain model to morph to the patient’s specific anatomy, capturing how the tumor deforms the normal structure of their brain tissue.</p>
<p>Based on the visualization — which radiologists and surgeons can modify to improve the precision — Atlas Pathfinder suggests the safest surgical approaches to access and remove a tumor without harming other parts of the brain. Each approach links out to the Atlas website, which includes a written tutorial of the operative plan.</p>
<p>“AI-powered decision support can make a big difference in navigating a highly complex 3D structure where every millimeter is critical,” Dr. Cohen-Gadol said.</p>
<h2><b>Realistic Rehearsal Environments for Practicing Surgeons </b></h2>
<p>Atlas Meditech is using NVIDIA Omniverse to develop a virtual operating room that can immerse surgeons into a realistic environment to rehearse upcoming procedures. In the simulation, surgeons can modify how the patient and equipment are positioned.</p>
<p>Using a VR headset, surgeons will be able to work within this virtual environment, going step by step through the procedure and receiving feedback on how closely they are adhering to the target pathway to reach the tumor. AI algorithms can be used to predict how brain tissue would shift as a surgeon uses medical instruments during the operation, and apply that estimated shift to the simulated brain.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_024-scaled.jpg"><img decoding="async" loading="lazy" class="aligncenter size-large wp-image-67302" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_024-672x378.jpg" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_024-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_024-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_024-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_024-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_024-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_024-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_024-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_024-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/AppStill_024-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>“The power to enable surgeons to enter a virtual, 3D space, cut a piece of the skull and rehearse the operation with a simulated brain that has very similar physical properties to the patient would be tremendous,” said Dr. Cohen-Gadol.</p>
<p>To better simulate the brain’s physical properties, the team adopted <a href="https://developer.nvidia.com/physx-sdk">NVIDIA PhysX</a>, an advanced real-time physics simulation engine that’s part of NVIDIA Omniverse. Using haptic devices, they were able to experiment with adding haptic feedback to the virtual environment, mimicking the feeling of working with brain tissue.</p>
<h2><b>Envisioning AI, Robotics in the Future of Surgery Training</b></h2>
<p>Dr. Cohen-Gadol believes that in the coming years AI models will be able to further enhance surgery by providing additional insights during a procedure. Examples include warning surgeons about critical brain structures that are adjacent to the area they’re working in, tracking medical instruments during surgery, and providing a guide to next steps in the surgery.</p>
<p>Atlas Meditech plans to explore the <a href="https://www.nvidia.com/en-us/clara/medical-devices/">NVIDIA Holoscan</a> platform for streaming AI applications to power these real-time, intraoperative insights. Applying AI analysis to a surgeon’s actions during a procedure can provide the surgeon with useful feedback to improve their technique.</p>
<p>In addition to being used for surgeons to rehearse operations, Dr. Cohen-Gadol says that digital twins of the brain and of the operating room could help train intelligent medical instruments such as microscope robots using <a href="https://developer.nvidia.com/isaac-sim">Isaac Sim</a>, a robotics simulation application developed on Omniverse.</p>
<p>View <a href="https://www.nvidia.com/en-us/on-demand/session/gtcfall22-a41130/">Dr. Cohen-Gadol’s presentation at NVIDIA GTC</a>.</p>
<p><a href="https://www.nvidia.com/en-us/industries/healthcare-life-sciences/healthcare-news-sign-up/"><i>Subscribe to NVIDIA healthcare news</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/AtlasMeditech_featureimage_blogcrop.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/AtlasMeditech_featureimage_blogcrop-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Brains of the Operation: Atlas Meditech Maps Future of Surgery With AI, Digital Twins]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Fall in Line for October With Nearly 60 New Games, Including Latest Game Pass Titles to Join the Cloud</title>
		<link>https://blogs.nvidia.com/blog/2023/10/05/geforce-now-thursday-oct-5/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 05 Oct 2023 13:00:25 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67333</guid>

					<description><![CDATA[October brings more than falling leaves and pumpkin spice lattes for GeForce NOW members. Get ready for nearly 60 new games to stream, including Forza Motorsport and 16 more PC Game Pass titles. Assassin’s Creed Mirage leads 29 new games to hit the GeForce NOW library this week. In addition, catch a challenge to earn <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/05/geforce-now-thursday-oct-5/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>October brings more than falling leaves and pumpkin spice lattes for <a href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a> members. Get ready for nearly 60 new games to stream, including <i>Forza Motorsport </i>and 16 more PC Game Pass titles.</p>
<p><i>Assassin’s Creed Mirage</i> leads 29 new games to hit the GeForce NOW library this week. In addition, catch a challenge to earn in-game rewards for <i>World of Warship </i>players.</p>
<h2><b>Leap Into the Cloud</b></h2>
<figure id="attachment_67341" aria-describedby="caption-attachment-67341" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-67341" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Assassins_Creed_Mirage-672x336.jpg" alt="Assassin's Creed Mirage on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Assassins_Creed_Mirage-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Assassins_Creed_Mirage-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Assassins_Creed_Mirage-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Assassins_Creed_Mirage-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Assassins_Creed_Mirage-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Assassins_Creed_Mirage-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Assassins_Creed_Mirage-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Assassins_Creed_Mirage-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Assassins_Creed_Mirage.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-67341" class="wp-caption-text"><em>Nothing is true. Everything is permitted … in the cloud.</em></figcaption></figure>
<p>It’s not an illusion — Ubisoft’s <i>Assassin’s Creed Mirage </i>launches in the cloud this week. <i>Mirage</i> was created as an homage to the first <i>Assassin’s Creed </i>games and pays tribute to the series’ well-loved roots.</p>
<p>Join the powerful proto-Assassin order — the Hidden Ones — as a 17-year-old street thief named Basim Ibn Is’haq as he learns to become a master assassin. Stalk the streets of a bustling and historically accurate ninth-century Baghdad — the perfect urban setting to seamlessly parkour across rooftops, scale tall towers and flee guards while uncovering a conspiracy that threatens the city and Basim’s future destiny.</p>
<p>Take a Leap of Faith into a GeForce NOW Ultimate membership and explore this new open world at up to 4K resolution and 120 frames per second. Ultimate members get exclusive access to GeForce RTX 4080 servers in the cloud, making it the easiest upgrade around.</p>
<h2><b>No Tricks, Only Treats</b></h2>
<p>Don’t be spooked — GeForce NOW has plenty of treats for members this month. More PC Game Pass games are coming soon to the cloud, including <i>Forza Motorsport</i> from Turn 10 Studios and Xbox Game Studios and the <i>Dishonored </i>series from Arkane and Bethesda.</p>
<p>Catch some action (with a little stealth, magic and combat mixed in) with the <i>Dishonored</i> franchise. Dive into a struggle of power and revenge that revolves around the assassination of the Empress of the Isles. Members can follow the whole story starting with the original <i>Dishonored</i> game, up through the latest entry, <i>Dishonored: Death of an Outsider</i>, when the series launches in the cloud this month.</p>
<p>Jump into all the action with an <a href="http://geforcenow.com/membership">Ultimate or Priority</a> account today, for higher performance and faster access to stream over 1,700 games.</p>
<p>Check out the spooktacular list for October:</p>
<ul>
<li><i>Star Trek: Infinite </i>(New release on <a href="https://store.steampowered.com/app/1622900?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 12)</li>
<li><i>Lords of the Fallen </i>(New release on <a href="https://store.steampowered.com/app/1501750?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a href="https://www.epicgames.com/store/p/lords-of-the-fallen-2-46fdd6?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>, Oct. 13)</li>
<li><i>Wizard with a Gun </i>(New release on <a href="https://store.steampowered.com/app/1150530?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 17)</li>
<li><i>Alaskan Road Truckers </i>(New release <a href="https://store.steampowered.com/app/849100?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a href="https://store.epicgames.com/en-US/p/alaskan-road-truckers-d31463">Epic Games Store</a>, Oct. 18)</li>
<li><i>Hellboy: Web of Wyrd </i>(New release on <a href="https://store.steampowered.com/app/2160480?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 18)</li>
<li><i>HOT WHEELS UNLEASHED 2 &#8211; Turbocharged </i>(New release on <a href="https://store.steampowered.com/app/2051120?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 19)</li>
<li><i>Laika Aged Through Blood </i>(New release on <a href="https://store.steampowered.com/app/1796220?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 19)</li>
<li><i>Cities: Skylines II </i>(New release on <a href="https://store.steampowered.com/app/949230?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a href="https://www.xbox.com/games/store/cities-skylines-ii/9ndntv0l9n87?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox </a>and available on PC Game Pass, Oct. 24)</li>
<li><i>Ripout </i>(New release on <a href="https://store.steampowered.com/app/1558830?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct 24)</li>
<li><i>War Hospital </i>(New release on <a href="https://store.steampowered.com/app/1553000?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 26)</li>
<li><i>Alan Wake 2 </i>(New release on <a href="https://store.epicgames.com/en-US/p/alan-wake-2">Epic Games Store</a>, Oct. 26)</li>
<li><i>Headbangers: Rhythm Royale </i>(New release on <a href="https://store.steampowered.com/app/1761620?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a href="https://www.xbox.com/games/store/headbangers-rhythm-royale/9ngrwj03gr1n?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox </a>and available on PC Game Pass, Oct. 31)</li>
<li><i>Jusant </i>(New release on <a href="https://store.steampowered.com/app/1977170?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a href="https://www.xbox.com/games/store/jusant/9PJB1ZRJDCBQ?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox </a>and available on PC Game Pass, Oct. 31)</li>
<li><i>Bad North </i>(<a href="https://www.xbox.com/games/store/bad-north-jotunn-edition/9N4T8VCMQVDT?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Daymare 1994: Sandcastle </i>(<a href="https://store.steampowered.com/app/1530470?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>For The King </i>(<a href="https://www.xbox.com/games/store/for-the-king/9NS1CD1V4BKW?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Forza Motorsport </i>(<a href="https://store.steampowered.com/app/2440510?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a href="https://www.xbox.com/games/store/forza-motorsport-standard-edition/9PLKVSWR299F?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass)</li>
<li><i>Heretic’s Fork</i> (<a href="https://store.steampowered.com/app/2181610?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Moonbreaker </i>(<a href="https://store.steampowered.com/app/845890?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Metro Simulator 2 </i>(<a href="https://store.steampowered.com/app/1787480?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Narita Boy </i>(<a href="https://www.xbox.com/games/store/narita-boy/9NT3FGQC1DFR?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Sifu </i>(<a href="https://www.xbox.com/games/store/sifu/9P7PF6ZP3958?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>StalCraft </i>(<a href="https://store.steampowered.com/app/1818450?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Star Renegades</i> (<a href="https://www.xbox.com/games/store/star-renegades/9PK5S1QKV10D?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Streets of Rogue</i> (<a href="https://www.xbox.com/games/store/streets-of-rogue/9NKRBSZXQ2HM?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Supraland </i>(<a href="https://www.xbox.com/games/store/supraland/9P75CZFXMS7N?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>The Surge </i>(<a href="https://www.xbox.com/games/store/the-surge-windows-10-version/9NKLXF5DLBKT?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Tiny Football </i>(<a href="https://store.steampowered.com/app/1887010?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Vampire Survivors </i>(<a href="https://store.steampowered.com/app/1794680?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a href="https://www.xbox.com/games/store/vampire-survivors/9PD5BM2Z8C4L?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>VEILED EXPERTS </i>(<a href="https://store.steampowered.com/app/1934780?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Yes, Your Grace</i> (<a href="https://www.xbox.com/games/store/yes-your-grace/9NX14Q0QZD1T?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
</ul>
<h2><b>Come Sail Away</b></h2>
<figure id="attachment_67338" aria-describedby="caption-attachment-67338" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="size-large wp-image-67338" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-World_of_Warships_GFN_Event-672x336.jpg" alt="" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-World_of_Warships_GFN_Event-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-World_of_Warships_GFN_Event-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-World_of_Warships_GFN_Event-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-World_of_Warships_GFN_Event-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-World_of_Warships_GFN_Event-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-World_of_Warships_GFN_Event-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-World_of_Warships_GFN_Event-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-World_of_Warships_GFN_Event-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-World_of_Warships_GFN_Event.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-67338" class="wp-caption-text"><em>A new challenge awaits on the open sea.</em></figcaption></figure>
<p><i>World of Warships</i> is launching a new in-game event this week exclusive to GeForce NOW members. From Oct. 5-9, those streaming the game on GeForce NOW will be prompted to complete a special in-game challenge chain, only available from the cloud, to earn economic reward containers and one-day GeForce NOW Priority trials. Aspiring admirals can learn more about these challenges on the <i>World of Warships</i> <a href="https://worldofwarships.com/en/news/sales-and-events/geforce-now-event-2023/">blog</a> and <a href="https://twitter.com/WorldofWarships?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">social channels</a>.</p>
<p>Those new to <i>World of Warships </i>can activate the invite code “GEFORCENOW” in the game starting today to claim exclusive rewards, including a seven-day Premium <i>World of Warships</i> account, 300 doubloons, credits and economic boosters. Once 15 battles are completed, players can choose one of the following tech tree ships to speed up game progress: Japanese destroyer Isokaze, American cruiser Phoenix, German battleship Moltke or British aircraft carrier Hermes.</p>
<figure id="attachment_67335" aria-describedby="caption-attachment-67335" style="width: 672px" class="wp-caption aligncenter"><img decoding="async" loading="lazy" class="wp-image-67335 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Age_of_Empires_II-672x336.jpg" alt="Age Of Empires II on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Age_of_Empires_II-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Age_of_Empires_II-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Age_of_Empires_II-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Age_of_Empires_II-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Age_of_Empires_II-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Age_of_Empires_II-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Age_of_Empires_II-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Age_of_Empires_II-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/GFN_Thursday-Age_of_Empires_II.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-67335" class="wp-caption-text"><em>Conquer the cloud.</em></figcaption></figure>
<p>The leaves may be falling, but new games are always coming to the cloud. Dive into the action now with 29 new games this week:</p>
<ul>
<li><i>Battle Shapers </i>(New release on <a href="https://store.steampowered.com/app/1421290?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 3)</li>
<li><i>Disgaea 7: Vows of the Virtueless </i>(New release on <a href="https://store.steampowered.com/app/2250600?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 3)</li>
<li><i>Station to Station</i> (New release on <a href="https://store.steampowered.com/app/2272400?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 3)</li>
<li><i>The Lamplighter’s League </i>(New release on <a href="https://store.steampowered.com/app/1167750?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a href="https://www.xbox.com/games/store/the-lamplighters-league-pc-edition/9PG4M7W805RB?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass, Oct. 3)</li>
<li><i>Thief Simulator 2 </i>(New release on <a href="https://store.steampowered.com/app/1332720?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 4)</li>
<li><i>Heads Will Roll: Reforged </i>(New release on <a href="https://store.steampowered.com/app/687800?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Oct. 4)</li>
<li><i>Assassin’s Creed Mirage </i>(New release on <a href="https://store.ubi.com/62ea4f8e09372571f2736a71.html#ucid=AFL-ID_152062&amp;maltcode=geforcenow_convst_AFL_geforcenow_vg__STORE____&amp;addinfo=">Ubisoft</a>, Oct. 5)</li>
<li><i>Age of Empires II: Definitive Edition </i>(<a href="https://www.xbox.com/games/store/age-of-empires-ii-definitive-edition/9n2z748spmtm?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Arcade Paradise </i>(<a href="https://www.xbox.com/games/store/arcade-paradise/9N2FQCGWH32D?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>The Ascent </i>(<a href="https://www.xbox.com/games/store/the-ascent/C27QL5JBKQ8M?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Citizen Sleeper </i>(<a href="https://www.xbox.com/games/store/citizen-sleeper/9N6F97F9WGL0?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Dicey Dungeons </i>(<a href="https://www.xbox.com/games/store/dicey-dungeons/9PC4C9NLP3ZD?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Godlike Burger</i> (<a href="https://www.epicgames.com/store/p/godlike-burger-4150a0?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
<li><i>Greedfall </i>(<a href="https://www.xbox.com/games/store/greedfall-gold-edition-windows-10/9PM3JCK0CXHC?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Hypnospace Outlaw </i>(<a href="https://www.xbox.com/games/store/hypnospace-outlaw/9PM3NVB3PXG0?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Killer Frequency </i>(<a href="https://www.xbox.com/games/store/killer-frequency/9mtmss84nw5r?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Lonely Mountains: Downhill </i>(<a href="https://www.xbox.com/games/store/lonely-mountains-downhill/9MV6MCVLT8GR?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Metro 2033 Redux </i>(<a href="https://www.xbox.com/games/store/metro-2033-redux/9NQ387M27TMV?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Metro: Last Light Redux </i>(<a href="https://www.xbox.com/games/store/metro-last-light-redux/9NM5W1TDNQZ3?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>MudRunner </i>(<a href="https://www.xbox.com/games/store/mudrunner/9N58J0GM9MRP?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>Potion Craft: Alchemist Simulator </i>(<a href="https://www.xbox.com/games/store/potion-craft-alchemist-simulator/9MW7WD7J3PPK?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Shadow Gambit: The Cursed Crew </i>(<a href="https://www.epicgames.com/store/p/shadow-gambit-the-cursed-crew-0bca60?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
<li><i>Slayers X: Terminal Aftermath: Vengance of the Slayer </i>(<a href="https://www.xbox.com/games/store/slayers-x-terminal-aftermath-vengance-of-the-slayer/9P2Q0J0GW1FJ?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Soccer Story</i> (<a href="https://www.xbox.com/games/store/soccer-story/9PJ1045MHZJ0?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li>SOMA (<a href="https://www.xbox.com/games/store/soma/C23M2TC1ZFPJ?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Space Hulk: Tactics </i>(<a href="https://www.xbox.com/games/store/space-hulk-tactics/9PBVRHZT421P?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
<li><i>SpiderHeck</i> (<a href="https://www.xbox.com/games/store/spiderheck/9N0TRF57SMQH?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>SUPERHOT: MIND CONTROL DELETE </i>(<a href="https://www.xbox.com/en-us/games/store/superhot-mind-control-delete/9nrh78b682l8">Xbox</a>, available on Microsoft Store)</li>
<li><i>Surviving Mars </i>(<a href="https://www.xbox.com/games/store/surviving-mars-digital-deluxe-edition/9N9C8081CZD2?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on Microsoft Store)</li>
</ul>
<h2><b>Surprises in September</b></h2>
<p>On top of the 24 games announced in September, an additional 45 joined the cloud last month:</p>
<ul>
<li><i>Void Crew</i> (New release on <a href="https://store.steampowered.com/app/1063420?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Sept. 7)</li>
<li><i>Tavernacle!</i> (New release on <a href="https://store.steampowered.com/app/1937820?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Sept. 11)</li>
<li><i>Gunbrella</i> (New release on <a href="https://store.steampowered.com/app/1580180?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Sept. 13)</li>
<li><i>HumanitZ </i>(New release on <a href="https://store.steampowered.com/app/1766060?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Sept. 18)</li>
<li><i>These Doomed Isles</i> (New release on <a href="https://store.steampowered.com/app/1840710?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Sept. 25)</li>
<li><i>Overpass 2 </i>(New release on <a href="https://store.steampowered.com/app/1830630?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Sept. 28)</li>
<li><i>911 Operator</i> (<a href="https://www.epicgames.com/store/p/911-operator-585edd?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
<li><i>A Plague Tale: Requiem </i>(<a href="https://www.xbox.com/games/store/a-plague-tale-requiem---windows/9N97RC576957?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Amnesia: The Bunker </i>(<a href="https://www.xbox.com/games/store/amnesia-the-bunker/9PC15H56NGJK?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Airborne Kingdom </i>(<a href="https://www.epicgames.com/store/p/airborne-kingdom?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
<li><i>Atomic Heart </i>(<a href="https://www.xbox.com/games/store/atomic-heart-windows/9N9KXP8RRPB5?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>BlazBlue: Cross Tag Battle </i>(<a href="https://www.xbox.com/games/store/blazblue-cross-tag-battle-special-edition/9NFWDRBZXDL9?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Bramble: The Mountain King </i>(<a href="https://www.xbox.com/games/store/bramble-the-mountain-king/9NHDFTCL691C?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Call of the Wild: The Angler </i>(<a href="https://www.xbox.com/games/store/call-of-the-wild-the-angler/9NDC5LJS839S?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Chained Echoes</i> (<a href="https://www.xbox.com/games/store/chained-echoes/9N1WWRPJ12FK?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Danganronpa V3: Killing Harmony</i> (<a href="https://www.xbox.com/games/store/danganronpa-v3-killing-harmony-anniversary-edition/9PMM6V93MRKW?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Descenders </i>(<a href="https://www.xbox.com/games/store/descenders/C37XBX7DCBZ0?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Doom Eternal </i>(<a href="https://www.xbox.com/games/store/Doom-eternal-standard-edition-pc/9PK09BL31FK1?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Dordogne </i>(<a href="https://www.xbox.com/games/store/dordogne/9pbc65kxpv5v?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Eastern Exorcist </i>(<a href="https://www.xbox.com/games/store/eastern-exorcist/9NQCJ132C6KW?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Figment 2: Creed Valley </i>(<a href="https://www.xbox.com/games/store/figment-2-creed-valley/9NZWW8K1KX6S?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Hardspace: Shipbreaker </i>(<a href="https://www.xbox.com/games/store/hardspace-shipbreaker-pc-version/9MW8RMSBB5QH?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Insurgency: Sandstorm </i>(<a href="https://www.xbox.com/games/store/insurgency-sandstorm-windows/9NN46PGWPJ98?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>I Am Fish </i>(<a href="https://www.xbox.com/games/store/i-am-fish/9mxgj8jzl0lk?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Last Call BBS</i> (<a href="https://www.xbox.com/games/store/last-call-bbs/9NF0SNBQWN63?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>The Legend of Tianding</i> (<a href="https://www.xbox.com/games/store/the-legend-of-tianding/9NCBTKXHFF8K?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>The Matchless Kungfu</i> (<a href="https://store.steampowered.com/app/1696440?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Mechwarrior 5: Mercenaries</i> (<a href="https://www.xbox.com/games/store/mechwarrior-5-mercenaries/9PB86W3JK8Z5?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Monster Sanctuary</i> (<a href="https://www.xbox.com/games/store/monster-sanctuary-windows-version/9PBWWQGXVMKC?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Opus Magnum</i> (<a href="https://www.xbox.com/games/store/opus-magnum/9P5QNLWDKHPF?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Pizza Possum </i>(New release on <a href="https://store.steampowered.com/app/1951230?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Sept. 28)</li>
<li><i>A Plague Tale: Innocence </i>(<a href="https://www.xbox.com/games/store/a-plague-tale-innocence-windows-10/9ND0CG3LM22K?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Quake II </i>(<a href="https://store.steampowered.com/app/2320?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a href="https://www.epicgames.com/store/p/quake-ii?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a> and <a href="https://www.xbox.com/games/store/quake-ii/9P7L9H478GGV?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Remnant II</i> (<a href="https://www.epicgames.com/store/p/remnant-2?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
<li><i>Road 96 </i>(<a href="https://www.xbox.com/games/store/road-96/9NVBKDF85W8T?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Shadowrun: Hong Kong &#8211; Extended Edition </i>(<a href="https://www.xbox.com/games/store/shadowrun-hong-kong---extended-edition-pc/9NK2DT3K7DTF?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>SnowRunner </i>(<a href="https://www.xbox.com/games/store/snowrunner-3-year-anniversary-edition-windows/9PMQF911J6N9?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Soulstice </i>(New release on <a href="https://www.epicgames.com/store/p/soulstice?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>, free on Sept. 28)</li>
<li><i>Space Hulk: Deathwing &#8211; Enhanced Edition</i> (<a href="https://www.xbox.com/games/store/space-hulk-deathwing-enhanced-edition-windows-10/9N8DKG2R0HKZ?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Spacelines from the Far Out </i>(<a href="https://www.xbox.com/games/store/spacelines-from-the-far-out/9N23WV1HGLTQ?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Superhot </i>(<a href="https://www.xbox.com/games/store/superhot-windows-10/9NV17MJB26PG?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Totally Reliable Delivery Service </i>(<a href="https://www.xbox.com/games/store/totally-reliable-delivery-service/9NZG72SH3H4W?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Vampyr</i> (<a href="https://www.xbox.com/games/store/vampyr/9NT2QNP382V6?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
<li><i>Warhammer 40,000: Battlesector </i>(<a href="https://www.xbox.com/games/store/warhammer-40000-battlesector/9PFLW6WZVJC7?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Yooka-Laylee and the Impossible Lair </i>(<a href="https://www.xbox.com/games/store/yooka-laylee-and-the-impossible-lair/9NB1LCQDPPM1?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
</ul>
<p><i>Halo Infinite </i>and <i>Kingdoms Reborn </i>didn’t make it in September. Stay tuned to GFN Thursday for more updates.</p>
<p>What are you planning to play this weekend? Let us know on <a href="https://www.twitter.com/nvidiagfn">Twitter</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">Predict the game you’ll be playing in the cloud most this October… <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/2601.png" alt="☁" class="wp-smiley" style="height: 1em; max-height: 1em;" /><img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f447.png" alt="👇" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f329.png" alt="🌩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a href="https://twitter.com/NVIDIAGFN/status/1709599258788184519?ref_src=twsrc%5Etfw">October 4, 2023</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/gfn-thursday-10-5-nv-blog-1280x680-no-cta.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/gfn-thursday-10-5-nv-blog-1280x680-no-cta-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Fall in Line for October With Nearly 60 New Games, Including Latest Game Pass Titles to Join the Cloud]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>A Mine-Blowing Breakthrough: Open-Ended AI Agent Voyager Autonomously Plays ‘Minecraft’</title>
		<link>https://blogs.nvidia.com/blog/2023/10/04/ai-jim-fan/</link>
		
		<dc:creator><![CDATA[Kristen Yee]]></dc:creator>
		<pubDate>Wed, 04 Oct 2023 21:04:52 +0000</pubDate>
				<category><![CDATA[The AI Podcast]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67324</guid>

					<description><![CDATA[For NVIDIA Senior AI Scientist Jim Fan, the video game Minecraft served as the “perfect primordial soup” for his research on open-ended AI agents. In the latest AI Podcast episode, host Noah Kravitz spoke with Fan on using large language models to create AI agents — specifically to create Voyager, an AI bot built with <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/04/ai-jim-fan/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>For NVIDIA Senior AI Scientist Jim Fan, the video game <i>Minecraft</i> served as the “perfect primordial soup” for his research on open-ended AI agents.</p>
<p>In the latest <a href="https://blogs.nvidia.com/ai-podcast/">AI Podcast</a> episode, host Noah Kravitz spoke with Fan on using <a href="https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/">large language models</a> to create AI agents — specifically to create Voyager, an AI bot built with Chat GPT-4 that can autonomously play <i>Minecraft</i>.</p>
<p>AI agents are models that “can proactively take actions and then perceive the world, see the consequences of its actions, and then improve itself,” Fan said. Many current AI agents are programmed to achieve specific objectives, such as beating a game as quickly as possible or answering a question. They can work autonomously toward a particular output but lack a broader decision-making agency.</p>
<p>Fan wondered if it was possible to have a “truly open-ended agent that can be prompted by arbitrary natural language to do open-ended, even creative things.”</p>
<p>But he needed a flexible playground in which to test that possibility.</p>
<p>“And that’s why we found <i>Minecraft </i>to be almost a perfect primordial soup for open-ended agents to emerge, because it sets up the environment so well,” he said. <i>Minecraft </i>at its core, after all, doesn’t set a specific key objective for players other than to survive and freely explore the open world.</p>
<p>That became the springboard for Fan’s project, MineDojo, which eventually led to the creation of the AI bot Voyager.</p>
<p>“Voyager leverages the power of Chat GPT-4 to write code in Javascript to execute in the game,” Fan explained. “GPT-4 then looks at the output, and if there’s an error from JavaScript or some feedback from the environment, GPT-4 does a self-reflection and tries to debug the code.”</p>
<p>The bot learns from its mistakes and stores the correctly implemented programs in a skill library for future use, allowing for “lifelong learning.”</p>
<p>In-game, Voyager can autonomously explore for hours, adapting its decisions based on its environment and developing skills to combat monsters and find food when needed.</p>
<p>“We see all these behaviors come from the Voyager setup, the skill library and also the coding mechanism,” Fan explained. “We did not preprogram any of these behaviors.”</p>
<p>He then spoke more generally about the rise and trajectory of LLMs. He foresees strong applications in software, gaming and robotics and increasingly pressing conversations surrounding AI safety.</p>
<p>Fan encourages those looking to get involved and work with LLMs to “just do something,” whether that means using <a href="https://www.nvidia.com/en-us/training/">online resources</a> or experimenting with beginner-friendly, CPU-based AI models.</p>
<p><iframe loading="lazy" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/1627125918%3Fsecret_token%3Ds-PgUquX2ckQi&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true" width="100%" height="166" frameborder="no" scrolling="no"></iframe></p>
<div style="font-size: 10px; color: #cccccc; line-break: anywhere; word-break: normal; overflow: hidden; white-space: nowrap; text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif; font-weight: 100;"><a style="color: #cccccc; text-decoration: none;" title="The AI Podcast" href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">The AI Podcast</a> · <a style="color: #cccccc; text-decoration: none;" title="NVIDIA’s Jim Fan delves into large language models and their industry impact - Ep. 204" href="https://soundcloud.com/theaipodcast/ai-jim-fan/s-PgUquX2ckQi" target="_blank" rel="noopener">NVIDIA’s Jim Fan delves into large language models and their industry impact &#8211; Ep. 204</a></div>
<h2>You Might Also Like</h2>
<p><a href="https://soundcloud.com/theaipodcast/jules-anh-tuan-nguyen-explains-how-ai-lets-amputee-control-prosthetic-hand-video-games-ep-149">Jules Anh Tuan Nguyen Explains How AI Lets Amputee Control Prosthetic Hand, Video Games<br />
</a>A postdoctoral researcher at the University of Minnesota discusses his efforts to allow amputees to control their prosthetic limb — right down to the finger motions — with their minds.</p>
<p><a href="https://soundcloud.com/theaipodcast/ai-overjet">Overjet’s Ai Wardah Inam on Bringing AI to Dentistry<br />
</a>Overjet, a member of <a href="https://www.nvidia.com/en-us/startups/">NVIDIA Inception</a>, is moving fast to bring AI to dentists’ offices. Dr. Wardah Inam, CEO of the company, discusses using AI to improve patient care.</p>
<p><a href="https://soundcloud.com/theaipodcast/ai-luis-voloch">Immunai CTO and Co-Founder Luis Voloch on Using Deep Learning to Develop New Drugs<br />
</a>Luis Voloch talks about tackling the challenges of the immune system with a machine learning and data science mindset.</p>
<h2>Subscribe to the AI Podcast: Now Available on Amazon Music</h2>
<p><a href="https://music.amazon.com/podcasts/956857d0-9461-4496-a07e-24be0539ee82/the-ai-podcast">The AI Podcast is now available through Amazon Music</a>.</p>
<p>In addition, get the <a href="https://blogs.nvidia.com/ai-podcast/">AI Podcast</a> through <a href="https://itunes.apple.com/us/podcast/the-ai-podcast/id1186480811?mt=2&amp;adbsc=social_20161220_68874946&amp;adbid=811257941365882882&amp;adbpl=tw&amp;adbpr=61559439">iTunes</a>, <a href="https://podcasts.google.com/?feed=aHR0cHM6Ly9mZWVkcy5zb3VuZGNsb3VkLmNvbS91c2Vycy9zb3VuZGNsb3VkOnVzZXJzOjI2NDAzNDEzMy9zb3VuZHMucnNz">Google Podcasts</a>, <a href="https://play.google.com/music/listen?u=0#/ps/I4kyn74qfrsdhrm35mcrf3igxzm">Google Play</a>, <a href="https://castbox.fm/channel/The-AI-Podcast-id433488?country=us">Castbox</a>, DoggCatcher, <a href="https://overcast.fm/itunes1186480811/the-ai-podcast">Overcast</a>, <a href="https://player.fm/series/the-ai-podcast">PlayerFM</a>, Pocket Casts, <a href="http://www.podbay.fm/show/1186480811">Podbay</a>, <a href="https://www.podbean.com/podcast-detail/cjgnp-4a6e0/The-AI-Podcast">PodBean</a>, PodCruncher, PodKicker, <a href="https://soundcloud.com/theaipodcast">Soundcloud</a>, <a href="https://open.spotify.com/show/4TB4pnynaiZ6YHoKmyVN0L">Spotify</a>, <a href="http://www.stitcher.com/s?fid=130629&amp;refid=stpr">Stitcher</a> and <a href="https://tunein.com/podcasts/Technology-Podcasts/The-AI-Podcast-p940829/">TuneIn</a>.</p>
<p>Make the AI Podcast better. Have a few minutes to spare? Fill out <a href="http://survey.podtrac.com/start-survey.aspx?pubid=I5V0tOQFNS8j&amp;ver=short">this listener survey</a>.</p>
<p>&nbsp;</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2018/05/ai-podcast.jpg"
			type="image/jpeg"
			width="1400"
			height="931"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2018/05/ai-podcast-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[A Mine-Blowing Breakthrough: Open-Ended AI Agent Voyager Autonomously Plays ‘Minecraft’]]></media:title>
			<media:description type="html">NVIDIA AI Podcast</media:description>
			</media:content>
			</item>
		<item>
		<title>How AI Helps Fight Wildfires in California</title>
		<link>https://blogs.nvidia.com/blog/2023/10/04/ai-wildfires-california/</link>
		
		<dc:creator><![CDATA[Brian Caulfield]]></dc:creator>
		<pubDate>Wed, 04 Oct 2023 15:00:55 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Climate]]></category>
		<category><![CDATA[GPU]]></category>
		<category><![CDATA[Social Impact]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67306</guid>

					<description><![CDATA[California has a new weapon against the wildfires that have devastated the state: AI. A freshly launched system powered by AI trained on NVIDIA GPUs promises to provide timely alerts to first responders across the Golden State every time a blaze ignites. The ALERTCalifornia initiative, a collaboration between California’s wildfire fighting agency CAL FIRE and <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/04/ai-wildfires-california/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>California has a new weapon against the wildfires that have devastated the state: AI.</p>
<p>A freshly launched system powered by AI trained on NVIDIA GPUs promises to provide timely alerts to first responders across the Golden State every time a blaze ignites.</p>
<p>The ALERTCalifornia initiative, a collaboration between California’s wildfire fighting agency CAL FIRE and the University of California, San Diego, uses advanced AI developed by DigitalPath.</p>
<p>Harnessing the raw power of NVIDIA GPUs and aided by a network of thousands of cameras dotting the Californian landscape, DigitalPath has refined a <a href="https://blogs.nvidia.com/blog/2018/09/05/whats-the-difference-between-a-cnn-and-an-rnn/" target="_blank" rel="noopener">convolutional neural network</a> to spot signs of fire in real time.</p>
<h2><strong>A Mission That’s Close to Home</strong></h2>
<p><a href="https://www.digitalpath.net/" target="_blank" rel="noopener">DigitalPath</a> CEO Jim Higgins said it’s a mission that means a lot to the 100-strong technology partner, which is nestled in the Sierra Nevada foothills in Chico, Calif., a short drive from the town of Paradise, where the state’s deadliest wildfire killed 85 people in 2018.</p>
<p>“It’s one of the main reasons we’re doing this,” Higgins said of the wildfire, the deadliest and most destructive in the history of the most populous U.S. state. “We don’t want people to lose their lives.”</p>
<p>The ALERTCalifornia initiative is based at UC San Diego’s Jacobs School of Engineering, the Qualcomm Institute and the Scripps Institution of Oceanography.</p>
<p>The program manages a network of thousands of monitoring cameras and sensor arrays and collects data that provides actionable, real-time information to inform public safety.</p>
<p>The AI program started in June and was initially deployed in six of Cal Fire’s command centers. This month it expanded to all of CAL FIRE’s 21 command centers.</p>
<figure id="attachment_67307" aria-describedby="caption-attachment-67307" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire1.png"><img decoding="async" loading="lazy" class="wp-image-67307 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire1-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire1-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire1-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire1-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire1-1536x864.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire1-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire1-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire1-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire1-1280x720.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire1.png 1920w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67307" class="wp-caption-text">ALERTCalifornia, powered by DigitalPath, can detect fires from cameras positioned across the golden state.</figcaption></figure>
<p>DigitalPath began by building out a management platform for a network of cameras used to confirm California wildfires after a 911 call.</p>
<p>The company quickly realized there would be no way to have people examine images from the thousands of cameras relaying images to the system every ten to fifteen seconds.</p>
<p>So Ethan Higgins, the company’s system architect, turned to AI.</p>
<p>The team began by training a convolutional neural network on a cloud-based system running an NVIDIA A100 Tensor Core GPU and later transitioned to a system running on eight A100 GPUs.</p>
<p>The AI model is crucial to examining a system that sees almost 8 million images a day streaming in from over 1,000 first-party cameras, primarily in California, and thousands more from third-party sources nationwide, he said.</p>
<h2><strong>Impact of Wildfires</strong></h2>
<figure id="attachment_67311" aria-describedby="caption-attachment-67311" style="width: 299px" class="wp-caption alignleft"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire2.png"><img decoding="async" loading="lazy" class="wp-image-67311 " src="https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire2-348x500.png" alt="" width="299" height="430" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire2-348x500.png 348w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire2-278x400.png 278w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire2-768x1105.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire2-1068x1536.png 1068w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire2-313x450.png 313w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire2-149x215.png 149w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire2-70x100.png 70w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire2.png 1167w" sizes="(max-width: 299px) 100vw, 299px" /></a><figcaption id="caption-attachment-67311" class="wp-caption-text">All anomalies being tracked throughout California as of Sept. 20, 2023. Image Credit: DigitalPath</figcaption></figure>
<p>It’s arriving just in time.</p>
<p>Wildfires have ravaged California over the past decade, burning millions of acres of land, destroying thousands of homes and businesses and claiming hundreds of lives.</p>
<p><a href="https://www.fire.ca.gov/our-impact/statistics" target="_blank" rel="noopener">According to CAL FIRE</a>, in 2020 alone, the state experienced five of its six largest and seven of its 20 most destructive wildfires.</p>
<p>And the <a href="https://calmatters.org/california-wildfire-map-tracker/" target="_blank" rel="noopener">total dollar damage of wildfires in California from 2019 to 2021 was estimated at over $25 billion</a>.</p>
<p>The new system promises to give first responders a crucial tool to prevent such conflagrations.</p>
<p>In fact, during a recent interview with DigitalPath, the system detected two separate fires in Northern California as they ignited.</p>
<p>Every day, the system detects between 50 and 300 events, offering invaluable real-time information to local first responders.</p>
<h2><strong>Beyond Detection: Enhancing Capabilities</strong></h2>
<figure id="attachment_67314" aria-describedby="caption-attachment-67314" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire3.png"><img decoding="async" loading="lazy" class="wp-image-67314 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire3-672x481.png" alt="" width="672" height="481" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire3-672x481.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire3-400x286.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire3-768x549.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire3-629x450.png 629w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire3-301x215.png 301w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire3-140x100.png 140w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire3-1280x916.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/29fire3.png 1462w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67314" class="wp-caption-text">Example of multiple cameras detecting a single anomaly. Image Credit: DigitalPath.</figcaption></figure>
<p>But AI is just part of the story.</p>
<p>The system is also a case study in how innovative companies can use AI to amplify their unique capabilities.</p>
<p>One of DigitalPath’s breakthroughs is its system’s ability to identify the same fire captured from diverse camera angles. DigitalPath’s system efficiently filters imagery down to a human-digestible level. The system filters 8 million daily images down to just 100 alerts, or 1.25 thousandths of one percent of total images captured.</p>
<p>“The system was designed from the start with human processing in mind,” Higgins said, ensuring that authorities receive a single, consolidated notification for every incident.</p>
<p>“We’ve got to catch every fire we can,” he adds.</p>
<h2><strong>Expanding Horizons</strong></h2>
<p>DigitalPath eventually hopes to expand its detection technology to help California detect more kinds of natural disasters.</p>
<p>And having proven its worth in California, DigitalPath is now in talks with state and county officials and university research teams across the fire-prone Western United States under its ALERTWest subsidiary.</p>
<p>Their goal: to help partners replicate the success of UC San Diego and ALERTCalifornia, potentially shielding countless lives and homes from the wrath of wildfires.</p>
<p><i>Featured image credit: </i><a href="https://www.flickr.com/photos/slworking/29034137667" target="_blank" rel="noopener"><i>SLworking2, via Flickr</i></a><i>, Creative Commons license, some rights reserved. </i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/29firemain1080.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/29firemain1080-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[How AI Helps Fight Wildfires in California]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Meet the Maker: Robotics Student Rolls Out Autonomous Wheelchair With NVIDIA Jetson</title>
		<link>https://blogs.nvidia.com/blog/2023/10/03/kabilan-kb-autonomous-wheelchair/</link>
		
		<dc:creator><![CDATA[Kristen Yee]]></dc:creator>
		<pubDate>Tue, 03 Oct 2023 15:00:04 +0000</pubDate>
				<category><![CDATA[Autonomous Machines]]></category>
		<category><![CDATA[Inner Geek]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Computer Vision]]></category>
		<category><![CDATA[Education]]></category>
		<category><![CDATA[Embedded Computing]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Robotics]]></category>
		<category><![CDATA[Social Impact]]></category>
		<category><![CDATA[Synthetic Data Generation]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67273</guid>

					<description><![CDATA[With the help of AI, robots, tractors and baby strollers — even skate parks — are becoming autonomous. One developer, Kabilan KB, is bringing autonomous-navigation capabilities to wheelchairs, which could help improve mobility for people with disabilities. The undergraduate from the Karunya Institute of Technology and Sciences in Coimbatore, India, is powering his autonomous wheelchair <a class="read-more" href="https://blogs.nvidia.com/blog/2023/10/03/kabilan-kb-autonomous-wheelchair/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>With the help of AI, robots, <a href="https://blogs.nvidia.com/blog/2022/12/01/mondavi-monarch-smart-electric-jetson-tractor/" target="_blank" rel="noopener">tractors</a> and <a href="https://blogs.nvidia.com/blog/2023/01/18/ella-stroller-jetson/" target="_blank" rel="noopener">baby strollers</a> — even <a href="https://blogs.nvidia.com/blog/2023/06/12/kirk-kaiser-jetson-self-driving-skate-park/" target="_blank" rel="noopener">skate parks</a> — are becoming autonomous. One developer, Kabilan KB, is bringing autonomous-navigation capabilities to wheelchairs, which could help improve mobility for people with disabilities.</p>
<p><img decoding="async" loading="lazy" class="wp-image-67283 alignright" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/image1-300x400.png" alt="" width="233" height="311" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/image1-300x400.png 300w, https://blogs.nvidia.com/wp-content/uploads/2023/10/image1-375x500.png 375w, https://blogs.nvidia.com/wp-content/uploads/2023/10/image1-768x1023.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/image1-1153x1536.png 1153w, https://blogs.nvidia.com/wp-content/uploads/2023/10/image1-338x450.png 338w, https://blogs.nvidia.com/wp-content/uploads/2023/10/image1-161x215.png 161w, https://blogs.nvidia.com/wp-content/uploads/2023/10/image1-75x100.png 75w, https://blogs.nvidia.com/wp-content/uploads/2023/10/image1-1280x1706.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/10/image1.png 1500w" sizes="(max-width: 233px) 100vw, 233px" /></p>
<p>The undergraduate from the Karunya Institute of Technology and Sciences in Coimbatore, India, is powering his autonomous wheelchair project using the <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/" target="_blank" rel="noopener">NVIDIA Jetson</a> platform for edge AI and robotics.</p>
<p>The autonomous motorized wheelchair is connected to depth and lidar sensors — along with USB cameras — which allow it to perceive the environment and plan an obstacle-free path toward a user’s desired destination.</p>
<p>“A person using the motorized wheelchair could provide the location they need to move to, which would already be programmed in the autonomous navigation system or path-planned with assigned numerical values,” KB said. “For example, they could press ‘one’ for the kitchen or ‘two’ for the bedroom, and the autonomous wheelchair will take them there.”</p>
<p>An <a href="https://developer.nvidia.com/embedded/jetson-nano-developer-kit" target="_blank" rel="noopener">NVIDIA Jetson Nano Developer Kit</a> processes data from the cameras and sensors in real time. It then uses deep learning-based computer vision models to detect obstacles in the environment.</p>
<p><img decoding="async" loading="lazy" class="aligncenter wp-image-67274 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2023/10/diagram-image-672x422.png" alt="" width="672" height="422" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/10/diagram-image-672x422.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/10/diagram-image-400x251.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/10/diagram-image-768x482.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/10/diagram-image-1536x964.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/10/diagram-image.png 2048w, https://blogs.nvidia.com/wp-content/uploads/2023/10/diagram-image-717x450.png 717w, https://blogs.nvidia.com/wp-content/uploads/2023/10/diagram-image-342x215.png 342w, https://blogs.nvidia.com/wp-content/uploads/2023/10/diagram-image-159x100.png 159w, https://blogs.nvidia.com/wp-content/uploads/2023/10/diagram-image-1280x804.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></p>
<p>The developer kit acts as the brain of the autonomous system — generating a 2D map of its surroundings to plan a collision-free path to the destination — and sends updated signals to the motorized wheelchair to help ensure safe navigation along the way.</p>
<h2><strong>About the Maker</strong></h2>
<p>KB, who has a background in mechanical engineering, became fascinated with AI and robotics during the pandemic, when he spent his free time searching up educational YouTube videos on the topics.</p>
<p>He’s now working toward a bachelor’s degree in robotics and automation at the Karunya Institute of Technology and Sciences and aspires to one day launch a robotics startup.</p>
<p>KB, a self-described supporter of self-education, has also received several certifications from the <a href="https://www.nvidia.com/en-us/training/" target="_blank" rel="noopener">NVIDIA Deep Learning Institute</a>, including “Building Video AI Applications at the Edge on Jetson Nano” and “Develop, Customize and Publish in Omniverse With Extensions.”</p>
<p>Once he learned the basics of robotics, he began experimenting with simulation in <a href="https://www.nvidia.com/en-us/omniverse/" target="_blank" rel="noopener">NVIDIA Omniverse</a>, a platform for building and operating 3D tools and applications based on the <a href="https://www.nvidia.com/en-us/omniverse/usd/" target="_blank" rel="noopener">OpenUSD</a> framework.</p>
<p>“Using Omniverse for simulation, I don’t need to invest heavily in prototyping models for my robots, because I can use <a href="https://blogs.nvidia.com/blog/2021/06/08/what-is-synthetic-data/" target="_blank" rel="noopener">synthetic data</a> generation instead,” he said. “It’s the software of the future.”</p>
<h2><strong>His Inspiration</strong></h2>
<p>With this latest NVIDIA Jetson project, KB aimed to create a device that could be helpful for his cousin, who has a mobility disorder, and other people with disabilities who might not be able to control a manual or motorized wheelchair.</p>
<p>“Sometimes, people don’t have the money to buy an electric wheelchair,” KB said. “In India, only upper- and middle-class people can afford them, so I decided to use the most basic type of motorized wheelchair available and connect it to the Jetson to make it autonomous.”</p>
<p>The personal project was funded by the Program in Global Surgery and Social Change, which is jointly positioned under the Boston Children’s Hospital and Harvard Medical School.</p>
<h2><b>His Jetson Project</b></h2>
<p>After purchasing the basic motorized wheelchair, KB connected its motor hub with the NVIDIA Jetson Nano and lidar and depth cameras.</p>
<p>He trained the AI algorithms for the autonomous wheelchair using YOLO object detection on the Jetson Nano, as well as the Robot Operating System, or ROS, a popular software for building robotics applications.</p>
<p>The wheelchair can tap these algorithms to perceive and map its environment and plan a collision-free path.</p>
<p>“The NVIDIA Jetson Nano’s real-time processing speed prevents delays or lags for the user,” said KB, who’s been working on the project’s prototype since June. The developer dives into the technical components of the autonomous wheelchair on <a href="https://medium.com/@kabilankb2003/autonomous-wheelchair-using-nvidia-jetson-nano-84268525763c" target="_blank" rel="noopener">his blog</a>. <a href="https://www.youtube.com/watch?v=cqt5zgTIMlE" target="_blank" rel="noopener">A demo</a> of the autonomous wheelchair has also been featured on the Karunya Innovation and Design Studio YouTube channel.</p>
<p>Looking forward, he envisions his project could be expanded to allow users to control a wheelchair using brain signals from electroencephalograms, or EEGs, that are connected to machine learning algorithms.</p>
<p>“I want to make a product that would let a person with a full mobility disorder control their wheelchair by simply thinking, ‘I want to go there,’” KB said.</p>
<p><i>Learn more about the </i><a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/" target="_blank" rel="noopener"><i>NVIDIA Jetson platform</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/autonomouswheelchair.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/10/autonomouswheelchair-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Meet the Maker: Robotics Student Rolls Out Autonomous Wheelchair With NVIDIA Jetson]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>CG Geek Makes VFX Look Easy This Week ‘In the NVIDIA Studio’</title>
		<link>https://blogs.nvidia.com/blog/2023/10/03/cg-geek-blender/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Tue, 03 Oct 2023 13:00:33 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[In the NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA Studio]]></category>
		<category><![CDATA[Rendering]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67223</guid>

					<description><![CDATA[Releasing a 3D tutorial dubbed The Easiest VFX Tutorial Ever takes supreme confidence and the skills to back it up. Steve Lund a.k.a. CG Geek — the featured artist of this week’s In the NVIDIA Studio installment — has both in spades.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p><i>Editor’s note: This post is part of our weekly </i><a href="https://blogs.nvidia.com/blog/tag/in-the-nvidia-studio/"><i>In the NVIDIA Studio</i></a><i> series, which celebrates featured artists, offers creative tips and tricks, and demonstrates how </i><a href="https://www.nvidia.com/en-us/studio/"><i>NVIDIA Studio</i></a><i> technology improves creative workflows. We’re also deep-diving on new </i><a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/"><i>GeForce RTX 40 Series GPU</i></a><i> features, technologies and resources and how they dramatically accelerate content creation.</i></p>
<p>Releasing a 3D tutorial dubbed <i>The Easiest VFX Tutorial Ever</i> takes supreme confidence and the skills to back it up.</p>
<p>Steve Lund a.k.a. CG Geek — the featured artist of this week’s <i>In the NVIDIA Studio</i> installment — has both in spades. It’s no surprise that over 1 million people have subscribed to his YouTube channel, which features tutorials on animation and visual effects (VFX) as well as select tech reviews.</p>
<p><iframe loading="lazy" title="the Easiest VFX Tutorial Ever." width="500" height="281" src="https://www.youtube.com/embed/cNbVl6LCEFI?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>CG Geek has been a content creator for over 13 years, starting with videos on stop-motion animation before moving on to 3D software. Films and movies are his primary sources of inspiration. He grew up creating short films with his family — experimenting with and implementing video effects and 3D characters — which became a critical foundation for his current work.</p>
<p>Artists can strengthen their creative arsenal with the new <a href="https://www.microsoft.com/en-us/d/surface-laptop-studio-2/8rqr54krf1dz">Microsoft Surface Laptop Studio 2</a>, available for pickup today. It’s powered by <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4060-4060ti/">GeForce RTX 4060</a>, GeForce RTX 4050 or <a href="https://www.nvidia.com/en-us/design-visualization/rtx-professional-laptops/?ncid=ref-pr-584767#">NVIDIA RTX 2000 Ada Generation</a> Laptop GPUs with 13th Gen Intel Core processors, up to 64GB of RAM and a 2TB SSD. It features a bright, vibrant 14.4-inch PixelSense Flow touchscreen, a 120Hz refresh rate, and Dolby Vision IQ and HDR to deliver sharper colors.</p>
<figure id="attachment_67227" aria-describedby="caption-attachment-67227" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/itns-week75-navia-left-1280w-1.png"><img decoding="async" loading="lazy" class="size-large wp-image-67227" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/itns-week75-navia-left-1280w-1-672x340.png" alt="" width="672" height="340" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/itns-week75-navia-left-1280w-1-672x340.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/itns-week75-navia-left-1280w-1-400x202.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/itns-week75-navia-left-1280w-1-768x388.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/itns-week75-navia-left-1280w-1-842x426.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/09/itns-week75-navia-left-1280w-1-406x205.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/09/itns-week75-navia-left-1280w-1-188x95.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/09/itns-week75-navia-left-1280w-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67227" class="wp-caption-text">The versatile Microsoft Surface Laptop Studio 2.</figcaption></figure>
<h2><b>The Easiest VFX Tutorial Ever</b></h2>
<p>CG Geek also happens to be a geek for Blender, free for 3D enthusiasts, who regularly create impressive, individualistic art.</p>
<p>“I love the amazing Blender 3D community,” he said. “Whenever you need inspiration or creative feedback, they’re the most helpful, kind and talented collective of ever-growing artists.”</p>
<p>CG Geek wanted to make a tutorial that could prove that virtually anyone could get started in VFX with relative ease, from anywhere, at any time.</p>
<figure id="attachment_67230" aria-describedby="caption-attachment-67230" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-outdoor-wideshot4-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67230" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-outdoor-wideshot4-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-outdoor-wideshot4-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-outdoor-wideshot4-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-outdoor-wideshot4-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-outdoor-wideshot4-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-outdoor-wideshot4-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-outdoor-wideshot4-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-outdoor-wideshot4-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67230" class="wp-caption-text">Work on VFX from anywhere — even the outdoors.</figcaption></figure>
<p>The first step, he instructs, is to capture video footage. To keep things simple, CG Geek recommends mounting a camera or mobile device to a tripod. Note that the camera lens determines the focal length and sensor size — critical details to input in Blender later in the process.</p>
<figure id="attachment_67233" aria-describedby="caption-attachment-67233" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube1-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67233" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube1-1280w-672x370.png" alt="" width="672" height="370" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube1-1280w-672x370.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube1-1280w-400x220.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube1-1280w-768x423.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube1-1280w-817x450.png 817w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube1-1280w-390x215.png 390w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube1-1280w-182x100.png 182w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube1-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67233" class="wp-caption-text">Keep track of the camera’s focal length and sensor size.</figcaption></figure>
<p>Keep a close eye on the video footage lighting for shadows and light intensity — it helps to snap a straight-down photo of the environment the 3D element will populate, namely for light bounces, to help create more realistic shadows.</p>
<figure id="attachment_67236" aria-describedby="caption-attachment-67236" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube2-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67236" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube2-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube2-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube2-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube2-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube2-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube2-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube2-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube2-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67236" class="wp-caption-text">Seasoned visual effects artists can capture and scan the entire 3D area.</figcaption></figure>
<p>Next, secure a 3D model. Create one with guidance from an <a href="https://www.nvidia.com/en-us/studio/blog/">NVIDIA Studio blog</a> or watch detailed tutorials on the <a href="https://www.youtube.com/channel/UCDeQdW6Lt6nhq3mLM4oLGWw">Studio YouTube channel</a>. Alternatively, look online for a 3D model equipped with basic physically based rendering materials, as well as a roughness and normal map.</p>
<figure id="attachment_67239" aria-describedby="caption-attachment-67239" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube3-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67239" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube3-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube3-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube3-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube3-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube3-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube3-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube3-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube3-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67239" class="wp-caption-text">Sketchfab is an excellent resource for acquiring 3D models.</figcaption></figure>
<p>Next, combine the video footage and 3D materials. Open Blender, import the video footage and line up the 3D grid floor to the surface where the model will be presented. The 3D grid doubles as a shadow catcher that will grab the shadows being cast from the 3D elements. With an added texture, lighting will bounce back against the object, resulting in heightened realism.</p>
<figure id="attachment_67242" aria-describedby="caption-attachment-67242" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube4-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67242" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube4-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube4-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube4-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube4-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube4-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube4-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube4-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube4-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67242" class="wp-caption-text">The 3D grid floor will determine where the 3D model will be placed.</figcaption></figure>
<p>Then, light the 3D model to match the video footage. Most commonly, this is achieved by acquiring a high-dynamic range image (HDRI), a panorama with lighting data. CG Geek recommends Poly Haven for free, high-quality HDRIs. The key is picking one that resembles the lighting, color, shadow and intensity of the video footage.</p>
<figure id="attachment_67245" aria-describedby="caption-attachment-67245" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube5-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67245" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube5-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube5-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube5-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube5-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube5-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube5-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube5-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube5-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67245" class="wp-caption-text">Poly Haven has HDRIs for use in VFX work.</figcaption></figure>
<p>Use the HDRI lighting to align the sun’s rotation with the shadows of the footage, adding further realism.</p>
<figure id="attachment_67248" aria-describedby="caption-attachment-67248" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube6-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67248" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube6-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube6-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube6-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube6-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube6-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube6-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube6-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube6-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67248" class="wp-caption-text">Lighting adjustments in Blender.</figcaption></figure>
<p>From there, import camera information into Blender and render out passes for the 3D model over a transparent background in Cycles. Create as many render layers as possible for added post-render editing flexibility, especially in compositing. Shadowcatcher, glossy passes, Z depth and ambient occlusion layers are recommended for advanced users.</p>
<figure id="attachment_67251" aria-describedby="caption-attachment-67251" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube7-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67251" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube7-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube7-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube7-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube7-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube7-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube7-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube7-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube7-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67251" class="wp-caption-text">Speedy renders in Blender on NVIDIA Studio hardware.</figcaption></figure>
<p>These layers can then be combined in popular creative apps like Adobe Premiere Pro, After Effects, Blackmagic Design’s DaVinci Resolve or any of the over 100 NVIDIA RTX GPU-accelerated apps. This workflow, in particular, will be completed in Blender’s custom compositor.</p>
<figure id="attachment_67254" aria-describedby="caption-attachment-67254" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube8-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67254" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube8-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube8-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube8-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube8-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube8-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube8-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube8-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-youtube8-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67254" class="wp-caption-text">Speedy renders in Blender.</figcaption></figure>
<p>Add shadows to the live footage with a multiple overlay. Then, carry over the 3D elements render layer to adjust the intensity of the shadows, helping them mesh better with the video capture. Individual layers can be edited to match the desired tone.</p>
<p>CG Geek made use of Blender Cycles’ RTX-accelerated OptiX ray tracing in the viewport. “Rendering in Cycles with multiple render layers and passes, along with the NVIDIA OptiX Denoiser, made animations and early tests a breeze,” he said.</p>
<div class="simplePullQuote right"><p>“All my rendering changes can be visualized in real time thanks to the power of NVIDIA Studio before ever even hitting that button.” &#8211; CG Geek </p>
</div>
<p>Finally, perform simple masking on areas where the 3D model passes in front of or behind objects. CG Geek’s <a href="https://www.youtube.com/watch?v=lA6AuZYbLyQ">one-minute YouTube tutorial</a> can help guide this process. DaVinci Resolve or Premiere Pro’s AI-powered magic mask features can further speed the process by automatically masking background elements, saving the effort of painstakingly editing videos frame by frame.</p>
<p>These AI features are all accelerated by the GeForce RTX 4070 GPU equipped in CG Geek’s ASUS Zenbook 14 NVIDIA Studio laptop.</p>
<figure id="attachment_67257" aria-describedby="caption-attachment-67257" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-wip-renderlayers-1280w.png"><img decoding="async" loading="lazy" class="size-large wp-image-67257" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-wip-renderlayers-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-wip-renderlayers-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-wip-renderlayers-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-wip-renderlayers-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-wip-renderlayers-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-wip-renderlayers-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-wip-renderlayers-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-cg-geek-wk77-wip-renderlayers-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67257" class="wp-caption-text">An entire workflow in a single shot.</figcaption></figure>
<div class="simplePullQuote right"><p>“NVIDIA Studio laptops powered by RTX GPUs are great for portability and speed in a compact form factor.” &#8211; CG Geek</p>
</div>
<p>For CG Geek, getting reps in, making mistakes and strengthening weaknesses are the keys to evolving as an artist. “Don’t get hung up on the details!” he stressed. “Give yourself a deadline and then get started on another project.”</p>
<p>For more on the basics of 3D VFX and CGI with Blender, accelerated by the <a href="https://www.nvidia.com/en-us/studio/">NVIDIA Studio</a> platform and RTX GPUs, watch his featured five-minute <a href="https://www.youtube.com/watch?v=cNbVl6LCEFI">tutorial</a>.</p>
<figure id="attachment_67263" aria-describedby="caption-attachment-67263" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-itns-cg-geek-wk77-featured-setup-1280w.png"><img decoding="async" loading="lazy" class="wp-image-67263 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-itns-cg-geek-wk77-featured-setup-1280w-672x228.png" alt="" width="672" height="228" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-itns-cg-geek-wk77-featured-setup-1280w-672x228.png 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-itns-cg-geek-wk77-featured-setup-1280w-400x136.png 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-itns-cg-geek-wk77-featured-setup-1280w-768x261.png 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-itns-cg-geek-wk77-featured-setup-1280w-842x286.png 842w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-itns-cg-geek-wk77-featured-setup-1280w-406x138.png 406w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-itns-cg-geek-wk77-featured-setup-1280w-188x64.png 188w, https://blogs.nvidia.com/wp-content/uploads/2023/09/studio-itns-cg-geek-wk77-featured-setup-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-67263" class="wp-caption-text">Content creator CG Geek.</figcaption></figure>
<p>Check out CG Geek on <a href="https://www.youtube.com/@CGGeek/featured">YouTube</a>.</p>
<p><i>Follow NVIDIA Studio on </i><a href="https://www.instagram.com/nvidiastudio/"><i>Instagram</i></a><i>, </i><a href="https://twitter.com/NVIDIAStudio"><i>Twitter</i></a><i> and </i><a href="https://www.facebook.com/NVIDIAStudio/"><i>Facebook</i></a><i>. Access tutorials on the </i><a href="https://www.youtube.com/channel/UCDeQdW6Lt6nhq3mLM4oLGWw"><i>Studio YouTube channel</i></a><i> and get updates directly in your inbox by subscribing to the </i><a href="https://www.nvidia.com/en-us/studio/?nvmid=subscribe-creators-mail-icon"><i>Studio newsletter</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/09/nv-blog-header-preview-1280x680-3.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/09/nv-blog-header-preview-1280x680-3-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[CG Geek Makes VFX Look Easy This Week ‘In the NVIDIA Studio’]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Heeding Huang’s Law: Video Shows How Engineers Keep the Speedups Coming</title>
		<link>https://blogs.nvidia.com/blog/2023/09/29/huangs-law-dally-hot-chips/</link>
		
		<dc:creator><![CDATA[Rick Merritt]]></dc:creator>
		<pubDate>Fri, 29 Sep 2023 15:00:00 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[NVIDIA Life]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Events]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[GPU]]></category>
		<category><![CDATA[Hardware]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Inference]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[NVIDIA Ampere Architecture]]></category>
		<category><![CDATA[NVIDIA Hopper Architecture]]></category>
		<category><![CDATA[NVIDIA Research]]></category>
		<category><![CDATA[NVLink]]></category>
		<category><![CDATA[Parallel Computing]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=67172</guid>

					<description><![CDATA[In a talk, now available online, NVIDIA Chief Scientist Bill Dally describes a tectonic shift in how computer performance gets delivered in a post-Moore’s law era. Each new processor requires ingenuity and effort inventing and validating fresh ingredients, he said in a recent keynote address at Hot Chips, an annual gathering of chip and systems <a class="read-more" href="https://blogs.nvidia.com/blog/2023/09/29/huangs-law-dally-hot-chips/">Read article &#62;</a>]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>In a talk, now available <a href="https://youtu.be/rsxCZAE8QNA">online</a>, NVIDIA Chief Scientist Bill Dally describes a tectonic shift in how computer performance gets delivered in a post-Moore’s law era.</p>
<p>Each new processor requires ingenuity and effort inventing and validating fresh ingredients, he said in a recent keynote address at Hot Chips, an annual gathering of chip and systems engineers. That’s radically different from a generation ago, when engineers essentially relied on the physics of ever smaller, faster chips.</p>
<p>The team of more than 300 that Dally leads at NVIDIA Research helped deliver a whopping 1,000x improvement in single GPU performance on AI inference over the past decade (see chart below).</p>
<p>It’s an astounding increase that <a href="https://spectrum.ieee.org/move-over-moores-law-make-way-for-huangs-law">IEEE Spectrum</a> was the first to dub “Huang’s Law” after NVIDIA founder and CEO Jensen Huang. The label was later popularized by <a href="https://www.wsj.com/articles/huangs-law-is-the-new-moores-law-and-explains-why-nvidia-wants-arm-11600488001">a column</a> in the Wall Street Journal.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2023/09/1000x-GPU-Gains-in-10-Years.jpg"><img decoding="async" loading="lazy" class="aligncenter size-large wp-image-67174" src="https://blogs.nvidia.com/wp-content/uploads/2023/09/1000x-GPU-Gains-in-10-Years-672x372.jpg" alt="1000x leap in GPU performance in a decade" width="672" height="372" srcset="https://blogs.nvidia.com/wp-content/uploads/2023/09/1000x-GPU-Gains-in-10-Years-672x372.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2023/09/1000x-GPU-Gains-in-10-Years-400x221.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2023/09/1000x-GPU-Gains-in-10-Years-768x425.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2023/09/1000x-GPU-Gains-in-10-Years-1536x850.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2023/09/1000x-GPU-Gains-in-10-Years-813x450.jpg 813w, https://blogs.nvidia.com/wp-content/uploads/2023/09/1000x-GPU-Gains-in-10-Years-388x215.jpg 388w, https://blogs.nvidia.com/wp-content/uploads/2023/09/1000x-GPU-Gains-in-10-Years-181x100.jpg 181w, https://blogs.nvidia.com/wp-content/uploads/2023/09/1000x-GPU-Gains-in-10-Years-1280x709.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2023/09/1000x-GPU-Gains-in-10-Years.jpg 2014w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>The advance was a response to the equally phenomenal rise of <a href="https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/">large language models</a> used for <a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/">generative AI</a> that are growing by an order of magnitude every year.</p>
<p>“That’s been setting the pace for us in the hardware industry because we feel we have to provide for this demand,” Dally said.</p>
<p>In his talk, Dally detailed the elements that drove the 1,000x gain.</p>
<p>The largest of all, a sixteen-fold gain, came from finding simpler ways to represent the numbers computers use to make their calculations.</p>
<h2><b>The New Math</b></h2>
<p>The latest <a href="https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/">NVIDIA Hopper architecture</a> with its <a href="https://blogs.nvidia.com/blog/2022/03/22/h100-transformer-engine/">Transformer Engine</a> uses a dynamic mix of eight- and 16-bit floating point and integer math. It’s tailored to the needs of today’s generative AI models. Dally detailed both the performance gains and the energy savings the new math delivers.</p>
<p>Separately, his team helped achieve a 12.5x leap by crafting advanced instructions that tell the GPU how to organize its work. These complex commands help execute more work with less energy.</p>
<p>As a result, computers can be “as efficient as dedicated accelerators, but retain all the programmability of GPUs,” he said.</p>
<p>In addition, the <a href="https://www.nvidia.com/en-us/data-center/ampere-architecture/">NVIDIA Ampere architecture</a> added <a href="https://blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/">structural sparsity</a>, an innovative way to simplify the weights in AI models without compromising the model’s accuracy. The technique brought another 2x performance increase and promises future advances, too, he said.</p>
<p>Dally described how <a href="https://www.nvidia.com/en-us/data-center/nvlink/">NVLink</a> interconnects between GPUs in a system and <a href="https://www.nvidia.com/en-us/networking/">NVIDIA networking</a> among systems compound the 1,000x gains in single GPU performance.</p>
<h2><b>No Free Lunch  </b></h2>
<p>Though NVIDIA migrated GPUs from 28nm to 5nm semiconductor nodes over the decade, that technology only accounted for 2.5x of the total gains, Dally noted.</p>
<p>That’s a huge change from computer design a generation ago under Moore’s law, an observation that performance should double every two years as chips become ever smaller and faster.</p>
<p>Those gains were described in part by Denard scaling, essentially a physics formula defined in <a href="https://ieeexplore.ieee.org/document/1050511">a 1974 paper</a> co-authored by IBM scientist Robert Denard. Unfortunately, the physics of shrinking hit natural limits such as the amount of heat the ever smaller and faster devices could tolerate.</p>
<h2><b>An Upbeat Outlook</b></h2>
<p>Dally expressed confidence that Huang’s law will continue despite diminishing gains from Moore’s law.</p>
<p>For example, he outlined several opportunities for future advances in further simplifying how numbers are represented, creating more sparsity in AI models and designing better memory and communications circuits.</p>
<p>Because each new chip and system generation demands new innovations, “it’s a fun time to be a computer engineer,” he said.</p>
<p>Dally believes the new dynamic in computer design is giving NVIDIA’s engineers the three opportunities they desire most: to be part of a winning team, to work with smart people and to work on designs that have impact.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2023/09/HotChips-2023-2292-Dally-KV-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1091"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2023/09/HotChips-2023-2292-Dally-KV-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Heeding Huang’s Law: Video Shows How Engineers Keep the Speedups Coming]]></media:title>
			<media:description type="html">Bill Dally speaking at Hot Chjps 2023</media:description>
			</media:content>
			</item>
	</channel>
</rss>
