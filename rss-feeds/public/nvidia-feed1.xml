<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:media="http://search.yahoo.com/mrss/">

<channel>
	<title>NVIDIA Blog</title>
	<atom:link href="https://blogs.nvidia.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.nvidia.com/</link>
	<description></description>
	<lastBuildDate>Tue, 19 Mar 2024 11:07:52 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.4.3</generator>
	<item>
		<title>GTC Wrap-Up: ‘We Created a Processor for the Generative AI Era,’ NVIDIA CEO Says</title>
		<link>https://blogs.nvidia.com/blog/2024-gtc-keynote/</link>
		
		<dc:creator><![CDATA[Brian Caulfield]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:43:09 +0000</pubDate>
				<category><![CDATA[Accelerated Analytics]]></category>
		<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Driving]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Hardware]]></category>
		<category><![CDATA[Networking]]></category>
		<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Robotics]]></category>
		<category><![CDATA[Software]]></category>
		<category><![CDATA[Virtual Reality]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70538</guid>

					<description><![CDATA[Generative AI promises to revolutionize every industry it touches — all that’s been needed is the technology to meet the challenge. NVIDIA founder and CEO Jensen Huang on Monday introduced that technology — the company’s new Blackwell computing platform — as he outlined the major advances that increased computing power can deliver for everything from		<a class="read-more" href="https://blogs.nvidia.com/blog/2024-gtc-keynote/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Generative AI promises to revolutionize every industry it touches — all that’s been needed is the technology to meet the challenge.</p>
<p>NVIDIA founder and CEO Jensen Huang on Monday introduced that technology — the company’s new Blackwell computing platform — as he outlined the major advances that increased computing power can deliver for everything from software to services, robotics to medical technology and more.</p>
<p>“Accelerated computing has reached the tipping point — general purpose computing has run out of steam,” Huang told more than 11,000 GTC attendees gathered in-person — and many tens of thousands more online — for his keynote address at Silicon Valley’s cavernous SAP Center arena.</p>
<p>“We need another way of doing computing — so that we can continue to scale so that we can continue to drive down the cost of computing, so that we can continue to consume more and more computing while being sustainable. Accelerated computing is a dramatic speedup over general-purpose computing, in every single industry.”</p>
<p><img fetchpriority="high" decoding="async" class="aligncenter size-full wp-image-70552" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/MC1_4926-scaled.jpg" alt="" width="2048" height="1365" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/MC1_4926-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MC1_4926-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MC1_4926-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MC1_4926-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MC1_4926-1536x1024.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MC1_4926-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MC1_4926-323x215.jpg 323w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MC1_4926-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MC1_4926-1280x853.jpg 1280w" sizes="(max-width: 2048px) 100vw, 2048px" /></p>
<p>Huang spoke in front of massive images on a 40-foot tall, 8K screen the size of a tennis court to a crowd packed with CEOs and developers, AI enthusiasts and entrepreneurs, who walked together 20 minutes to the arena from the San Jose Convention Center on a dazzling spring day.</p>
<p>Delivering a massive upgrade to the world’s AI infrastructure, Huang introduced the NVIDIA Blackwell platform to unleash real-time generative AI on trillion-parameter large language models.</p>
<p>Huang presented NVIDIA NIM — a reference to NVIDIA inference microservices — a new way of packaging and delivering software that connects developers with hundreds of millions of GPUs to deploy custom AI of all kinds.</p>
<p>And bringing AI into the physical world, Huang introduced Omniverse Cloud APIs to deliver advanced simulation capabilities.</p>
<p><iframe title="YouTube video player" src="https://www.youtube.com/embed/Y2F8yisiS6E?si=b9Y8b7x-sTGY4n-J" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p>Huang punctuated these major announcements with powerful demos, partnerships with some of the world’s largest enterprises and more than a score of announcements detailing his vision.</p>
<p><a href="https://www.nvidia.com/gtc/">GTC</a> — which in 15 years has grown from the confines of a local hotel ballroom to the world’s most important AI conference — is returning to a physical event for the first time in five years.</p>
<p>This year’s has over 900 sessions — including a panel discussion on transformers moderated by Huang with the eight pioneers who first developed the technology, more than 300 exhibits and 20-plus technical workshops.</p>
<p>It’s an event that’s at the intersection of AI and just about everything. In a stunning opening act to the keynote, Refik Anadol, the world&#8217;s leading AI artist, showed a massive real-time AI data sculpture with wave-like swirls in greens, blues, yellows and reds, crashing, twisting and unraveling across the screen.</p>
<p>As he kicked off his talk, Huang explained that the rise of multi-modal AI — able to process diverse data types handled by different models — gives AI greater adaptability and power. By increasing their parameters, these models can handle more complex analyses.</p>
<p>But this also means a significant rise in the need for computing power. And as these collaborative, multi-modal systems become more intricate — with as many as a trillion parameters — the demand for advanced computing infrastructure intensifies.</p>
<p>“We need even larger models,” Huang said. “We’re going to train it with multimodality data, not just text on the internet, we’re going to train it on texts and images, graphs and charts, and just as we learned watching TV, there’s going to be a whole bunch of watching video.”</p>
<h2>The Next Generation of Accelerated Computing</h2>
<p>In short, Huang said “we need bigger GPUs.” The Blackwell platform is built to meet this challenge. Huang pulled a Blackwell chip out of his pocket and held it up side-by-side with a Hopper chip, which it dwarfed.</p>
<p>Named for David Harold Blackwell — a University of California, Berkeley mathematician specializing in game theory and statistics, and the first Black scholar inducted into the National Academy of Sciences — the new architecture succeeds the NVIDIA Hopper architecture, launched two years ago.</p>
<p>Blackwell delivers 2.5x its predecessor’s performance in FP8 for training, per chip, and 5x with FP4 for inference. It features a fifth-generation NVLink interconnect that’s twice as fast as Hopper and scales up to 576 GPUs.</p>
<p>And the <a href="https://www.nvidia.com/en-us/data-center/gb200-nvl72/">NVIDIA GB200 Grace Blackwell Superchip</a> connects two Blackwell <a href="https://www.nvidia.com/en-us/data-center/b200/">NVIDIA B200 Tensor Core GPUs</a> to the NVIDIA Grace CPU over a 900GB/s ultra-low-power NVLink chip-to-chip interconnect.</p>
<p>Huang held up a board with the system. “This computer is the first of its kind where this much computing fits into this small of a space,” Huang said. “Since this is memory coherent, they feel like it’s one big happy family working on one application together.”</p>
<p>For the highest AI performance, GB200-powered systems can be connected with the NVIDIA Quantum-X800 InfiniBand and Spectrum-X800 Ethernet platforms, also <a href="https://nvidianews.nvidia.com/news/networking-switches-gpu-computing-ai">announced today</a>, which deliver advanced networking at speeds up to 800Gb/s.</p>
<p><img decoding="async" class="aligncenter size-full wp-image-70577" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/VIVY0652-1-scaled.jpg" alt="" width="2048" height="1365" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/VIVY0652-1-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/03/VIVY0652-1-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/VIVY0652-1-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/VIVY0652-1-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/VIVY0652-1-1536x1024.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/03/VIVY0652-1-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2024/03/VIVY0652-1-323x215.jpg 323w, https://blogs.nvidia.com/wp-content/uploads/2024/03/VIVY0652-1-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2024/03/VIVY0652-1-1280x853.jpg 1280w" sizes="(max-width: 2048px) 100vw, 2048px" /></p>
<p>“The amount of energy we save, the amount of networking bandwidth we save, the amount of wasted time we save, will be tremendous,” Huang said. “The future is generative &#8230; which is why this is a brand new industry. The way we compute is fundamentally different. We created a processor for the generative AI era.”</p>
<p>To scale up Blackwell, NVIDIA built a new chip called NVLink Switch. Each can connect four NVLink interconnects at 1.8 terabytes per second and eliminate traffic by doing in-network reduction.</p>
<p>NVIDIA Switch and GB200 are key components of what Huang described as “one giant GPU,” the <a href="https://developer.nvidia.com/blog/nvidia-gb200-nvl72-delivers-trillion-parameter-llm-training-and-real-time-inference/">NVIDIA GB200 NVL72</a>, a multi-node, liquid-cooled, rack-scale system that harnesses Blackwell to offer supercharged compute for trillion-parameter models, with 720 petaflops of AI training performance and 1.4 exaflops of AI inference performance in a single rack.</p>
<p>“There are only a couple, maybe three exaflop machines on the planet as we speak,” Huang said of the machine, which packs 600,000 parts and weighs 3,000 pounds. “And so this is an exaflop AI system in one single rack. Well let’s take a look at the back of it.”</p>
<p>Going even bigger, NVIDIA today also announced its next-generation AI supercomputer — the <a href="https://www.nvidia.com/en-us/data-center/dgx-superpod-gb200/">NVIDIA DGX SuperPOD powered by NVIDIA GB200 Grace Blackwell Superchips</a> — for processing trillion-parameter models with constant uptime for superscale generative AI training and inference workloads.</p>
<p>Featuring a new, highly efficient, liquid-cooled rack-scale architecture, the new DGX SuperPOD is built with NVIDIA DG GB200 systems and provides 11.5 exaflops of AI supercomputing at FP4 precision and 240 terabytes of fast memory — scaling to more with additional racks.</p>
<p>“In the future, data centers are going to be thought of &#8230; as AI factories,” Huang said. “Their goal in life is to generate revenues, in this case, intelligence.”</p>
<p>The industry has already embraced Blackwell.</p>
<p>The <a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing">press release announcing Blackwell</a> includes endorsements from Alphabet and Google CEO Sundar Pichai, Amazon CEO Andy Jassy, Dell CEO Michael Dell, Google DeepMind CEO Demis Hassabis, Meta CEO Mark Zuckerberg, Microsoft CEO Satya Nadella, OpenAI CEO Sam Altman, Oracle Chairman Larry Ellison, and Tesla and xAI CEO Elon Musk.</p>
<p>Blackwell is being adopted by every major global cloud services provider,  pioneering AI companies, system and server vendors, and regional cloud service providers and telcos all around the world.</p>
<p>“The whole industry is gearing up for Blackwell,” which Huang said would be the most successful launch in the company’s history.</p>
<h2>A New Way to Create Software</h2>
<p>Generative AI changes the way applications are written, Huang said.</p>
<p>Rather than writing software, he explained, companies will assemble AI models, give them missions, give examples of work products, review plans and intermediate results.</p>
<p>These packages — NVIDIA NIMs — are built from NVIDIA’s accelerated computing libraries and generative AI models, Huang explained.</p>
<p>“How do we build software in the future? It is unlikely that you’ll write it from scratch or write a whole bunch of Python code or anything like that,” Huang said. “It is very likely that you assemble a team of AIs.”</p>
<p>The microservices support industry-standard APIs so they are easy to connect, work across NVIDIA’s large CUDA installed base, are re-optimized for new GPUs, and are constantly scanned for security vulnerabilities and exposures.</p>
<p>Huang said customers can use NIM microservices off the shelf, or NVIDIA can help build proprietary AI and copilots, teaching a model specialized skills only a specific company would know to create invaluable new services.</p>
<p>“The enterprise IT industry is sitting on a goldmine,” Huang said. “They have all these amazing tools (and data) that have been created over the years. If they could take that goldmine and turn it into copilots, these copilots can help us do things.”</p>
<p>Major tech players are already putting it to work. Huang detailed how NVIDIA is already helping Cohesity, NetApp, SAP, ServiceNow and Snowflake build copilots and virtual assistants. And industries are stepping in, as well.</p>
<p>In telecom, Huang announced the NVIDIA 6G Research Cloud, a generative AI and Omniverse-powered platform to advance the next communications era. It’s built with NVIDIA’s Sionna neural radio framework, NVIDIA Aerial CUDA-accelerated radio access network and the NVIDIA Aerial Omniverse Digital Twin for 6G.</p>
<p>In semiconductor design and manufacturing, Huang announced that, in collaboration with TSMC and Synopsys, NVIDIA is bringing its breakthrough computational lithography platform, cuLitho, to production. This platform will accelerate the most compute-intensive workload in semiconductor manufacturing by 40-60x.</p>
<p>Huang also announced the NVIDIA Earth Climate Digital Twin. The cloud platform — available now — enables interactive, high-resolution simulation to accelerate climate and weather prediction.</p>
<p>The greatest impact of AI will be in healthcare, Huang said, explaining that NVIDIA is already in imaging systems, in gene sequencing instruments and working with leading surgical robotics companies.</p>
<p>NVIDIA is launching a new type of biology software. NVIDIA today launched more than <a href="https://nvidianews.nvidia.com/news/generative-ai-microservices-for-developers">two dozen new microservices</a> that allow healthcare enterprises worldwide to take advantage of the latest advances in generative AI from anywhere and on any cloud. They offer advanced imaging, natural language and speech recognition, and digital biology generation, prediction and simulation.</p>
<h2>Omniverse Brings AI to the Physical World</h2>
<p>The next wave of AI will be AI learning about the physical world, Huang said.</p>
<p>“We need a simulation engine that represents the world digitally for the robot so that the robot has a gym to go learn how to be a robot,” he said. “We call that virtual world Omniverse.”</p>
<p>That’s why NVIDIA today announced that <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse Cloud</a> will be available as APIs, extending the reach of the world’s leading platform for creating industrial digital twin applications and workflows across the entire ecosystem of software makers.</p>
<p>The five new Omniverse Cloud application programming interfaces enable developers to easily integrate core Omniverse technologies directly into existing design and automation software applications for digital twins, or their simulation workflows for testing and validating autonomous machines like robots or self-driving vehicles.</p>
<p>To show how this works, Huang shared a demo of a robotic warehouse — using multi-camera perception and tracking — watching over workers and orchestrating robotic forklifts, which are driving autonomously with the full robotic stack running.</p>
<p>Huang also announced that NVIDIA is bringing Omniverse to Apple Vision Pro, with the new Omniverse Cloud APIs letting developers stream interactive industrial digital twins into the VR headsets.</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-70574" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/MJC_2441-2-scaled.jpg" alt="" width="2048" height="1365" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/MJC_2441-2-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MJC_2441-2-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MJC_2441-2-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MJC_2441-2-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MJC_2441-2-1536x1024.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MJC_2441-2-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MJC_2441-2-323x215.jpg 323w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MJC_2441-2-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2024/03/MJC_2441-2-1280x853.jpg 1280w" sizes="(max-width: 2048px) 100vw, 2048px" /></p>
<p>Some of the world’s largest industrial software makers are embracing Omniverse Cloud APIs, including Ansys, Cadence, Dassault Systèmes for its 3DEXCITE brand, Hexagon, Microsoft, Rockwell Automation, Siemens and Trimble.</p>
<h2>Robotics</h2>
<p>Everything that moves will be robotic, Huang said. The automotive industry will be a big part of that. NVIDIA computers are already in cars, trucks, delivery bots and robotaxis.</p>
<p>Huang announced that BYD, the world’s largest autonomous vehicle company, has selected NVIDIA’s next-generation computer for its AV, building its next-generation EV fleets on DRIVE Thor.</p>
<p>To help robots better see their environment, Huang also announced the Isaac Perceptor software development kit with state-of-the-art multi-camera visual odometry, 3D reconstruction and occupancy map, and depth perception.</p>
<p>And to help make manipulators, or robotic arms, more adaptable, NVIDIA is announcing Isaac Manipulator — a state-of-the-art robotic arm perception, path planning and kinematic control library.</p>
<p>Finally, Huang announced Project GR00T, a general-purpose foundation model for humanoid robots, designed to further the company’s work driving breakthroughs in robotics and embodied AI.</p>
<p>Supporting that effort, Huang unveiled a new computer, Jetson Thor, for humanoid robots based on the NVIDIA Thor system-on-a-chip and significant upgrades to the NVIDIA Isaac robotics platform.</p>
<p>In his closing minutes, Huang brought on stage a pair of diminutive NVIDIA-powered robots from Disney Research.</p>
<p>“The soul of NVIDIA — the intersection of computer graphics, physics, artificial intelligence,” he said. “It all came to bear at this moment.”</p>
<p><iframe title="YouTube video player" src="https://www.youtube.com/embed/Y2F8yisiS6E?si=b9Y8b7x-sTGY4n-J" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/VIVY0672-1-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1365"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/VIVY0672-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[GTC Wrap-Up: ‘We Created a Processor for the Generative AI Era,’ NVIDIA CEO Says]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>All Aboard: NVIDIA Scores 23 World Records for Route Optimization</title>
		<link>https://blogs.nvidia.com/blog/cuopt-route-optimization-metropolis-omniverse/</link>
		
		<dc:creator><![CDATA[Moon Chung]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:10:56 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Robotics]]></category>
		<category><![CDATA[NVIDIA Jetson]]></category>
		<category><![CDATA[Omniverse]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70263</guid>

					<description><![CDATA[With nearly two dozen world records to its name, NVIDIA cuOpt now holds the top spot for 100% of the largest routing benchmarks in the last three years. And this means the route optimization engine allows industries to hop on board for all kinds of cost-saving efficiencies. Kawasaki Heavy Industries and SyncTwin are among the		<a class="read-more" href="https://blogs.nvidia.com/blog/cuopt-route-optimization-metropolis-omniverse/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>With nearly two dozen world records to its name, <a href="https://www.nvidia.com/en-us/ai-data-science/products/cuopt/">NVIDIA cuOpt</a> now holds the top spot for 100% of the largest routing benchmarks in the last three years. And this means the route optimization engine allows industries to hop on board for all kinds of cost-saving efficiencies.</p>
<p><a href="https://global.kawasaki.com/">Kawasaki Heavy Industries</a> and SyncTwin are among the companies that are riding cuOpt for logistics improvements.</p>
<p>Today at GTC 2024, NVIDIA founder and CEO Jensen Huang announced that cuOpt is moving into general availability.</p>
<p>“With cuOpt, NVIDIA is reinventing logistics management and operations research. It is NVIDIA’s pre-quantum computer, driving transformational operational efficiencies for deliveries, service calls, warehouses and factories, and supply chains,” he said.</p>
<p>The NVIDIA cuOpt microservice, part of the <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> software platform, makes accelerated optimization for real-time dynamic rerouting, factory optimization and robotic simulations available to any organization.</p>
<p>Companies can embed cuOpt into the advanced 3D tools, applications and USD-based workflows they develop with <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a>, a software platform for developing and deploying advanced 3D applications and pipelines based on OpenUSD.</p>
<p>Implemented together, cuOpt, Omniverse and <a href="https://developer.nvidia.com/metropolis-for-factories">NVIDIA Metropolis for Factories</a> can help optimize and create safe environments in logistics-heavy facilities that rely on complex automation, precise material flow and human-robot interaction, such as automotive factories, semiconductor fabs and warehouses.</p>
<p>cuOpt has been continuously tested against the best-known solutions on the most studied benchmarks for route optimization, with results up to 100x faster than CPU-based implementations. With 15 records from the Gehring &amp; Homberger vehicle routing benchmark and eight from the Li &amp; Lim pickup and delivery benchmark, cuOpt has demonstrated the world’s best accuracy with the fastest times.</p>
<p>AI promises to deliver logistics efficiencies spanning from transportation networks to manufacturing and much more.</p>
<h2><b>Delivering Cost-Savings for Inspections With cuOpt</b></h2>
<p><a href="https://global.kawasaki.com/">Kawasaki Heavy Industries</a> is a manufacturing company that&#8217;s been building large machinery for more than a hundred years. The Japanese company partnered with Slalom and <a href="https://www.nvidia.com/en-us/case-studies/reinventing-maintenance-operations-with-ai/">used cuOpt to create routing efficiencies</a> for the development of its AI-driven <a href="https://global.kawasaki.com/">Kawasaki Track Maintenance Platform</a>.</p>
<p>Railroad track maintenance is getting an <a href="https://blogs.nvidia.com/blog/autonomous-trains-deep-learning-dgx-drive/">AI makeover worldwide</a>. Traditionally, track inspections and maintenance are time-consuming and difficult to manage to keep trains running on time. But track maintenance is critical for safety and transportation service. Railway companies are automating track inspections with AI and machine learning paired with digital cameras, lasers and gyrometric sensors.</p>
<p><a href="https://global.kawasaki.com/">Kawasaki</a> is harnessing the edge computing of <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/">NVIDIA Jetson AGX Orin</a> to develop track inspections on its Track Maintenance Platform for running onboard trains. The platform enables customers to improve vision models with the data collected on tracks for advances in the inspection capability of the edge-based AI system.</p>
<p>The platform provides maintenance teams data on track conditions that allows them to prioritize repairs, creating increased safety and reliability of operations.</p>
<p>According to <a href="https://global.kawasaki.com/">Kawasaki</a>, it’s estimated that such an AI-driven system can save $218 million a year for seven companies from automating their track inspections.</p>
<h2><b>Creating Manufacturing Efficiencies With cuOpt and Omniverse</b></h2>
<p>A worldwide leader in automotive seating manufacturing has adopted SyncTwin’s digital twin capability, which is driven by Omniverse and cuOpt, to improve its operations with AI.</p>
<p>The global automotive seating manufacturer has a vast network of loading docks for the delivery of raw materials, and forklifts for unloading and transporting them to storage and handling areas to ensure a steady supply to production lines. SyncTwin’s connection to cuOpt delivers routing efficiencies that optimize all of these moving parts — from vehicles to robotic pallet jacks.</p>
<p>As the SyncTwin solution was developed on top of Omniverse and USD, manufacturers can ensure that their various factory planning tools can contribute to the creation of a rich <a href="https://blogs.nvidia.com/blog/what-is-a-digital-twin/">digital twin</a> environment. Plus, they eliminate tedious manual data collection and gain new insights from their previously disconnected data.</p>
<p><a href="https://www.nvidia.com/gtc/session-catalog/?search=%22cuopt%22&amp;tab.allsessions=1700692987788001F1cG#/"><em>Attend GTC</em></a><em> to explore how cuOpt is achieving world-record accuracy and performance to solve complex problems. Learn more about cuOpt world records in our tech blog. <a href="https://www.nvidia.com/en-us/omniverse/">Learn more</a></em><em> about</em> Omniverse.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc24-cuopt-corp-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc24-cuopt-corp-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[All Aboard: NVIDIA Scores 23 World Records for Route Optimization]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>All Eyes on AI: Automotive Tech on Full Display at GTC 2024</title>
		<link>https://blogs.nvidia.com/blog/automotive-tech-gtc-2024/</link>
		
		<dc:creator><![CDATA[Danny Shapiro]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:06:31 +0000</pubDate>
				<category><![CDATA[Driving]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<category><![CDATA[NVIDIA DRIVE]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70515</guid>

					<description><![CDATA[All eyes across the auto industry are on GTC — the global AI conference running in San Jose, Calif., and online through Thursday, March 21 — as the world&#8217;s top automakers and tech leaders converge to showcase the latest models, demo new technologies and dive into the remarkable innovations reshaping the sector. Attendees will experience		<a class="read-more" href="https://blogs.nvidia.com/blog/automotive-tech-gtc-2024/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>All eyes across the auto industry are on <a href="https://www.nvidia.com/gtc/">GTC</a> — the global AI conference running in San Jose, Calif., and online through Thursday, March 21 — as the world&#8217;s top automakers and tech leaders converge to showcase the latest models, demo new technologies and dive into the remarkable innovations reshaping the sector.</p>
<p>Attendees will experience how generative AI and software-defined computing are advancing the automotive landscape and transforming the behind-the-wheel experience to become safer, smarter and more enjoyable.</p>
<h2><b>Automakers Adopting NVIDIA DRIVE Thor</b></h2>
<p>NVIDIA founder and CEO Jensen Huang kicked off GTC with a keynote address in which he revealed that NVIDIA DRIVE Thor, which combines advanced driver assistance technology and in-vehicle infotainment, now features the newly announced NVIDIA Blackwell GPU architecture for transformer and generative AI workloads.</p>
<p>Following the keynote, top EV makers shared how they will integrate <a href="https://nvidianews.nvidia.com/news/nvidia-drive-powers-next-generation-transportation">DRIVE Thor</a> into their vehicles. <b>BYD</b>, the world’s largest electric vehicle maker, is expanding its ongoing collaboration with NVIDIA and building its next-generation EV fleets on DRIVE Thor. <b>Hyper</b>, a premium luxury brand owned by GAC AION, is announcing it has selected DRIVE Thor for its new models, which will begin production in 2025. <b>XPENG </b>will use DRIVE Thor as the AI brain of its next-generation EV fleets. These EV makers join <b>Li Auto</b> and<b> ZEEKR</b>, which previously announced they’re building their future vehicle roadmaps on DRIVE Thor.</p>
<p>Additionally, trucking, robotaxis and goods delivery vehicle makers are announcing support for DRIVE Thor. <b>Nuro</b> is choosing DRIVE Thor to power the Nuro Driver. <b>Plus</b> is announcing that future generations of its level 4 solution, SuperDrive, will run on DRIVE Thor. <b>Waabi</b> is  leveraging DRIVE Thor to deliver the first generative AI-powered autonomous trucking solution to market. <b>WeRide</b>, in cooperation with tier 1 partner Lenovo Vehicle Computing, is creating level 4 autonomous driving solutions for commercial applications built on DRIVE Thor.</p>
<p>And, <b>DeepRoute.ai</b> is unveiling its new smart driving architecture powered by NVIDIA DRIVE Thor, scheduled to launch next year.</p>
<h2><b>Next-Generation Tech on the Show Floor</b></h2>
<p>The GTC exhibit hall is buzzing with excitement as companies showcase the newest vehicle models and offer technology demonstrations.</p>
<p>Attendees have the opportunity to see firsthand the latest NVIDIA-powered vehicles on display,  including <b>Lucid Air</b>, <b>Mercedes-Benz Concept CLA Class</b>, <b>Nuro R3</b>, <b>Polestar 3</b>,<b> Volvo EX90</b>, <b>WeRide Robobus</b>, and an <b>Aurora</b> truck. The Lucid Air is available for test drives during the week.</p>
<p>A wide array of companies are showcasing innovative automotive technology at GTC, including <b>Foretellix</b>, <b>Luminar</b> and <b>MediaTek</b>, which is launching its <a href="https://blogs.nvidia.com/blog/mediatek-intelligent-cabin-solutions/">Dimensity Auto Cockpit</a> chipsets at the show. The new solutions harness NVIDIA’s graphics and AI technologies to help deliver state-of-the-art in-vehicle user experiences, added safety and security capabilities.</p>
<h2><b>Also Announced at GTC: Omniverse Cloud APIs, Generative AI</b><b><br />
</b></h2>
<ul>
<li><a href="https://nvidianews.nvidia.com/news/omniverse-cloud-apis-industrial-digital-twin">Omniverse Cloud APIs</a>, announced today at NVIDIA GTC, are poised to accelerate the path to autonomy by enabling high-fidelity sensor simulation for AV development and validation. Developers and software vendors such as <b>CARLA</b>, <b>MathWorks</b>, <b>MITRE</b>, <b>Foretellix</b> and <b>Voxel51</b> underscore the broad appeal of these APIs in autonomous vehicles.</li>
<li><a href="https://blogs.nvidia.com/blog/generative-ai-in-vehicle-experiences/">Generative AI developers</a> including <b>Cerence</b>, <b>Geely</b>, <b>Li Auto,</b> <b>NIO,</b> <b>SoundHound</b>, <b>Tata Consulting Services</b> and <b>Wayve</b> announced plans to transform the in-vehicle experience by using NVIDIA’s cloud-to-edge technology to help develop intelligent AI assistants, driver and passenger monitoring, scene understanding and more.</li>
</ul>
<h2><b>AI and Automotive Sessions Available Live and on Demand</b><b><br />
</b></h2>
<p>Throughout the week<b>, </b>the world’s foremost experts on automotive technology will lead a broad array of sessions and panels at GTC, including:</p>
<ul>
<li><a href="https://www.nvidia.com/gtc/session-catalog/?search=S62464&amp;ncid=em-prom-975499&amp;tab.allsessions=1700692987788001F1cG#/">How LLMs and Generative AI Will Enhance the Way We Experience Self-Driving Cars<br />
</a><i>Tuesday, March 19, 9 a.m. PT</i></li>
<li><a href="https://www.nvidia.com/gtc/session-catalog/?search=s62621&amp;ncid=em-prom-975499&amp;tab.allsessions=1700692987788001F1cG#/">Accelerating the New Era of Autonomous Vehicles With Generative AI<br />
</a><i>Tuesday, March 19, 10 a.m. PT</i></li>
<li><a href="https://www.nvidia.com/gtc/session-catalog/?search=S62380&amp;ncid=em-prom-975499&amp;tab.allsessions=1700692987788001F1cG#/">Generative AI and Industrial Digitalization in the Automotive Industry<br />
</a><i>Tuesday, March 19, 2 p.m. PT</i></li>
<li><a href="https://www.nvidia.com/gtc/session-catalog/?search=S62804&amp;ncid=em-prom-975499&amp;tab.allsessions=1700692987788001F1cG#/">Accelerating Automotive Workflows With Large Language Models<br />
</a><i>Tuesday, March 19, 3 p.m. PT</i></li>
<li><a href="https://www.nvidia.com/gtc/session-catalog/?search=SE63001&amp;ncid=em-prom-975499&amp;tab.allsessions=1700692987788001F1cG#/">Accelerating the Shift to AI-Defined Vehicles<b><br />
</b></a><i>Thursday, March 21, 8 a.m. PT</i></li>
</ul>
<p>On <a href="https://www.nvidia.com/gtc/sessions/drive-developer-day/">DRIVE Developer Day</a>, taking place Thursday, March 21, NVIDIA’s engineering experts will highlight the latest DRIVE features and developments through a series of deep-dive sessions on how to build safe and robust self-driving systems.</p>
<p>See the full schedule of <a href="https://images.nvidia.com/nvimages/gtc/pdf/GTC24_March_Automotive_Brochure.pdf">automotive programming at GTC</a> and be sure to tune in.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/auto-gtc.jpeg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/auto-gtc-842x450.jpeg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[All Eyes on AI: Automotive Tech on Full Display at GTC 2024]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Generative AI Developers Harness NVIDIA Technologies to Transform In-Vehicle Experiences</title>
		<link>https://blogs.nvidia.com/blog/generative-ai-in-vehicle-experiences/</link>
		
		<dc:creator><![CDATA[Norm Marks]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:05:45 +0000</pubDate>
				<category><![CDATA[Driving]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<category><![CDATA[NVIDIA DRIVE]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70475</guid>

					<description><![CDATA[Cars of the future will be more than just modes of transportation; they’ll be intelligent companions, seamlessly blending technology and comfort to enhance driving experiences, and built for safety, inside and out. NVIDIA GTC, running this week at the San Jose Convention Center, will spotlight the groundbreaking work NVIDIA and its partners are doing to		<a class="read-more" href="https://blogs.nvidia.com/blog/generative-ai-in-vehicle-experiences/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Cars of the future will be more than just modes of transportation; they’ll be intelligent companions, seamlessly blending technology and comfort to enhance driving experiences, and built for safety, inside and out.</p>
<p><a href="https://www.nvidia.com/gtc/">NVIDIA GTC</a>, running this week at the San Jose Convention Center, will spotlight the groundbreaking work NVIDIA and its partners are doing to bring the transformative power of <a href="https://www.nvidia.com/en-us/glossary/generative-ai/#:~:text=Generative%20AI%20enables%20users%20to,or%20other%20types%20of%20data.">generative AI</a>, <a href="https://www.nvidia.com/en-us/glossary/large-language-models/">large language models</a> and visual language models to the mobility sector.</p>
<p>At its booth, NVIDIA will showcase how it’s building automotive assistants to enhance driver safety, security and comfort through enhanced perception, understanding and generative capabilities powered by deep learning and transformer models.</p>
<h2><b>Talking the Talk</b></h2>
<p><a href="https://www.nvidia.com/en-us/deep-learning-ai/solutions/large-language-models/">LLMs</a>, a form of generative AI, largely represent a class of deep-learning architectures known as <a href="https://blogs.nvidia.com/blog/what-is-a-transformer-model/">transformer models</a>, which are neural networks adept at learning context and meaning.</p>
<p>Vision language models are another derivative of generative AI, that offer image processing and language understanding capabilities. Unlike traditional or multimodal LLMs that primarily process and generate text-based data, VLMs can analyze and generate text via images or videos.</p>
<p>And <a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">retrieval-augmented generations</a> allows manufacturers to access knowledge from a specific database or the web to assist drivers.</p>
<p>These technologies together enable <a href="https://developer.nvidia.com/ace">NVIDIA Avatar Cloud Engine</a>, or ACE, and multimodal language models to work together with the <a href="https://developer.nvidia.com/drive">NVIDIA DRIVE</a> platform to let automotive manufacturers develop their own intelligent in-car assistants.</p>
<p>For example, an Avatar configurator can allow designers to build unique, brand-inspired personas for their cars, complete with customized voices and emotional attributes. These AI-animated avatars can engage in natural dialogue, providing real-time assistance, recommendations and personalized interactions.</p>
<p>Furthermore, AI-enhanced surround visualization enhances vehicle safety using 360-degree camera reconstruction, while the intelligent assistant sources external information, such as local driving laws, to inform decision-making.</p>
<p>Personalization is paramount, with AI assistants learning driver and passenger habits and adapting its behavior to suit occupants’ needs.</p>
<h2><b>Generative AI for Automotive in Full Force at GTC </b></h2>
<p>Several NVIDIA partners at GTC are also showcasing their latest generative AI developments using NVIDIA’s edge-to-cloud technology:</p>
<ul>
<li><b>Cerence’s</b> CaLLM is an automotive-specific LLM that serves as the foundation for the company’s next-gen in-car computing platform, running on NVIDIA DRIVE. The platform, unveiled late last year, is the future of in-car interaction, with an automotive- and mobility-specific assistant that provides an integrated in-cabin experience. Cerence is collaborating with NVIDIA engineering teams for deeper integration of CaLLM with the <a href="https://www.nvidia.com/en-us/ai-data-science/foundation-models/">NVIDIA AI Foundation Models</a>. Through joint efforts, Cerence is harnessing <a href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a> as the development platform, applying guardrails for enhanced performance, and leveraging <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> to optimize inference. NVIDIA and Cerence will continue to partner and pioneer this solution together with several automotive OEMs this year.</li>
<li><b>Wavye</b> is helping usher in the new era of Embodied AI for autonomy, their next-generation AV2.0 approach is characterized by a large Embodied AI foundation model that learns to drive self-supervised using AI end-to-end —from sensing, as an input, to outputting driving actions. The British startup has already unveiled its GAIA-1, a generative world model for AV development running on NVIDIA; alongside LINGO-1, a closed-loop driving commentator that uses natural language to enhance the learning and explainability of AI driving models.</li>
<li><b>Li Auto</b> unveiled its multimodal cognitive model, Mind GPT, in June. Built on <a href="https://developer.nvidia.com/tensorrt">NVIDIA TensorRT-LLM</a>, an open-source library, it serves as the basis for the electric vehicle maker’s AI assistant, Lixiang Tongxue, for scene understanding, generation, knowledge retention and reasoning capabilities. Li Auto is currently developing DriveVLM to enhance autonomous driving capabilities, enabling the system to understand complex scenarios, particularly those that are challenging for traditional AV pipelines, such as unstructured roads, rare and unusual objects, and unexpected traffic events. This advanced model is trained on the NVIDIA GPUs and utilizes TensorRT-LLM and <a href="https://developer.nvidia.com/triton-inference-server">NVIDIA Triton Inference Server</a> for data generation in the data center. With inference optimized by NVIDIA DRIVE and TensorRT-LLM, DriveVLMs perform efficiently on embedded systems.</li>
<li><b>NIO</b> launched its NOMI GPT, which offers a number of functional experiences, including NOMI Encyclopedia Q&amp;A, Cabin Atmosphere Master and Vehicle Assistant. With the capabilities enabled by LLMs and an efficient computing platform powered by NVIDIA AI stacks, NOMI GPT is capable of basic speech recognition and command execution functions and can use deep learning to understand and process more complex sentences and instructions inside the car.</li>
<li><b>Geely</b> is working with NVIDIA to provide intelligent cabin experiences, along with accelerated edge-to-cloud deployment. Specifically, Geely is applying generative AI and LLM technology to provide smarter, personalized and safer driving experiences, using natural language processing, dialogue systems and predictive analytics for intelligent navigation and voice assistants. When deploying LLMs into production, Geely uses NVIDIA TensorRT-LLM to achieve highly efficient inference. For more complex tasks or scenarios requiring massive data support, Geely plans to deploy large-scale models in the cloud.</li>
<li><b>Waabi</b> is building AI for self-driving and will use the generative AI capabilities afforded by NVIDIA DRIVE Thor for its breakthrough autonomous trucking solutions, bringing safe and reliable autonomy to the trucking industry.</li>
<li><b>Lenovo</b> is unveiling a new AI acceleration engine, dubbed UltraBoost, which will run on NVIDIA DRIVE, and features an AI model engine and AI compiler tool chains to facilitate the deployment of LLMs within vehicles.</li>
<li><b>SoundHound AI</b> is using NVIDIA to run its in-vehicle voice interface — which combines both real-time and generative AI capabilities — even when a vehicle has no cloud connectivity. This solution also offers drivers access to SoundHound’s Vehicle Intelligence product, which instantly delivers settings, troubleshooting and other information directly from the car manual and other data sources via natural speech, as opposed to through a physical document.</li>
<li><b>Tata Consulting Services</b> (part of the TATA Group), through its AI-based technology and engineering innovation, has built its automotive GenAI suite powered by NVIDIA GPUs and software frameworks. It accelerates the design, development, and validation of software-defined vehicles, leveraging the various LLMs and VLMs for in-vehicle and cloud-based systems.</li>
<li><b>MediaTek</b> is announcing four automotive systems-on-a-chip within its Dimensity Auto Cockpit portfolio, offering powerful AI-based in-cabin experiences for the next generation of intelligent vehicles that span from premium to entry level. To support deep learning capabilities, the Dimensity Auto Cockpit chipsets integrate NVIDIA’s next-gen GPU-accelerated AI computing and <a href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX</a>-powered graphics to run LLMs in the car, allowing vehicles to support chatbots, rich content delivery to multiple displays, driver alertness detection and other AI-based safety and entertainment applications.</li>
</ul>
<p>Check out the many <a href="https://images.nvidia.com/nvimages/gtc/pdf/GTC24_March_Automotive_Brochure.pdf">automotive talks</a> on generative AI and LLMs throughout the week of GTC.</p>
<p><a href="https://www.nvidia.com/gtc/?ncid=GTC-NVHWQQGN">Register today</a> to attend GTC in person, or tune in virtually, to explore how generative AI is making transportation safer, smarter and more enjoyable.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gen-ai-vehicle.png"
			type="image/png"
			width="1600"
			height="851"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gen-ai-vehicle-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Generative AI Developers Harness NVIDIA Technologies to Transform In-Vehicle Experiences]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Unveils Digital Blueprint for Building Next-Gen Data Centers</title>
		<link>https://blogs.nvidia.com/blog/omniverse-next-gen-data-center/</link>
		
		<dc:creator><![CDATA[Dion Harris]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:05:17 +0000</pubDate>
				<category><![CDATA[Data Center]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Universal Scene Description]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70362</guid>

					<description><![CDATA[Designing, simulating and bringing up modern data centers is incredibly complex, involving multiple considerations like performance, energy efficiency and scalability. It also requires bringing together a team of highly skilled engineers across compute and network design, computer-aided design (CAD) modeling, and mechanical, electrical and thermal design. NVIDIA builds the world’s most advanced AI supercomputers and		<a class="read-more" href="https://blogs.nvidia.com/blog/omniverse-next-gen-data-center/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Designing, simulating and bringing up modern data centers is incredibly complex, involving multiple considerations like performance, energy efficiency and scalability.</p>
<p>It also requires bringing together a team of highly skilled engineers across compute and network design, computer-aided design (CAD) modeling, and mechanical, electrical and thermal design.</p>
<p>NVIDIA builds the world’s most advanced AI supercomputers and at GTC unveiled its latest — a large cluster based on the NVIDIA GB200 NVL72 liquid-cooled system. It consists of two racks, each containing 18 <a href="https://www.nvidia.com/en-us/data-center/grace-cpu/">NVIDIA Grace CPUs</a> and 36 <a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing">NVIDIA Blackwell GPUs</a>, connected by fourth-generation <a href="https://www.nvidia.com/en-us/data-center/nvlink/">NVIDIA NVLink</a> switches.</p>
<p>On the show floor, NVIDIA demoed this fully operational data center as a <a href="https://blogs.nvidia.com/blog/what-is-a-digital-twin/">digital twin</a> in <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a>, a platform for connecting and building generative AI-enabled 3D pipelines, tools, applications and services.</p>
<p>To bring up new data centers as fast as possible, NVIDIA first built its digital twin with software tools connected by Omniverse. Engineers unified and visualized multiple CAD datasets in full physical accuracy and photorealism in <a href="https://www.nvidia.com/en-us/omniverse/usd/">Universal Scene Description</a> (OpenUSD) using the <a href="https://www.cadence.com/en_US/home.html" target="_blank" rel="noopener">Cadence</a> Reality digital twin platform, powered by NVIDIA Omniverse APIs.</p>
<p><iframe loading="lazy" title="Accelerating Data Center Design With Digital Twins" width="500" height="281" src="https://www.youtube.com/embed/h68kXLIRilM?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<h2><b>Design, Simulate and Optimize With Enhanced Efficiency and Accuracy</b></h2>
<p>The new GB200 cluster is replacing an existing cluster in one of NVIDIA’s legacy data centers. To start the digital build-out, technology company <a href="https://kinetic-vision.com/" target="_blank" rel="noopener">Kinetic Vision</a> scanned the facility using the <a href="https://www.navvis.com/vlx" target="_blank" rel="noopener">NavVis VLX wearable lidar scanner</a> to produce highly accurate point cloud data and panorama photos.</p>
<p>Then, <a href="https://www.prevu3d.com/solutions/realitymesh-omniverse/" target="_blank" rel="noopener">Prevu3D</a> software was used to remove the existing clusters and convert the point cloud to a 3D mesh. This provided a physically accurate 3D model of the facility, in which the new digital data center could be simulated.</p>
<p>Engineers combined and visualized multiple CAD datasets with enhanced precision and realism by using the Cadence Reality platform. The platform’s integration with Omniverse provided a powerful computing platform that enabled teams to develop OpenUSD-based 3D tools, workflows and applications.</p>
<p><a href="https://nvidianews.nvidia.com/news/omniverse-cloud-apis-industrial-digital-twin">Omniverse Cloud APIs</a> also added interoperability with more tools, including PATCH MANAGER and NVIDIA Air. With PATCH MANAGER, the team designed the physical layout of their cluster and networking infrastructure, ensuring that cabling lengths were accurate and routing was properly configured.</p>
<p>The team used Cadence’s Reality Digital Twin solvers, accelerated by <a href="https://developer.nvidia.com/modulus">NVIDIA Modulus</a> APIs and NVIDIA Grace Hopper, to simulate airflows, as well as the performance of the new liquid-cooling systems from partners like Vertiv and Schneider Electric. The integrated cooling systems in the GB200 trays were simulated and optimized using solutions from Ansys, which brought simulation data into the digital twin.</p>
<p>The demo showed how digital twins can allow users to fully test, optimize and validate data center designs before ever producing a physical system. By visualizing the performance of the data center in the digital twin, teams can better optimize their designs and plan for what-if scenarios.</p>
<p>Users can also enhance data center and cluster designs by balancing disparate sets of boundary conditions, such as cabling lengths, power, cooling and space, in an integrated manner — enabling engineers and design teams to bring clusters online much faster and with more efficiency and optimization than before.</p>
<p>Learn more about <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> and <a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing">NVIDIA Blackwell</a>.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc-ov-corp-blog-announcement-cadence-1280x680-1.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc-ov-corp-blog-announcement-cadence-1280x680-1-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Unveils Digital Blueprint for Building Next-Gen Data Centers]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>New NVIDIA Storage Partner Validation Program Streamlines Enterprise AI Deployments</title>
		<link>https://blogs.nvidia.com/blog/ovx-storage-partner-validation-program/</link>
		
		<dc:creator><![CDATA[Jason Schroedl]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:00:48 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<category><![CDATA[Omniverse Enterprise]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70358</guid>

					<description><![CDATA[A sharp increase in generative AI deployments is driving business innovation for enterprises across industries. But it’s also posing significant challenges for their IT teams, as slowdowns from long and complex infrastructure deployment cycles prevent them from quickly spinning up AI workloads using their own data. To help overcome these barriers, NVIDIA has introduced a		<a class="read-more" href="https://blogs.nvidia.com/blog/ovx-storage-partner-validation-program/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>A sharp increase in generative AI deployments is driving business innovation for enterprises across industries. But it’s also posing significant challenges for their IT teams, as slowdowns from long and complex infrastructure deployment cycles prevent them from quickly spinning up AI workloads using their own data.</p>
<p>To help overcome these barriers, NVIDIA has introduced a storage partner validation program for <a href="https://www.nvidia.com/en-us/omniverse/platform/ovx/">NVIDIA OVX computing systems</a>. The high-performance storage systems leading the way in completing the NVIDIA OVX storage validation are DDN, Dell PowerScale, NetApp, <a href="https://www.purestorage.com/company/newsroom/press-releases/pure-and-nvidia-accelerate-enterprise-ai-adoption-to-meet-growing-demands.html">Pure Storage</a> and WEKA.</p>
<p>NVIDIA OVX servers combine high-performance, GPU-accelerated compute with high-speed storage access and low-latency networking to address a range of complex AI and graphics-intensive workloads. Chatbots, summarization and search tools, for example, require large amounts of data, and high-performance storage is critical to maximize system throughput.</p>
<p>To help enterprises pair the right storage with NVIDIA-Certified OVX servers, the new program provides a standardized process for partners to validate their storage appliances. They can use the same framework and testing that’s needed to validate storage for the <a href="https://www.nvidia.com/en-sg/data-center/dgx-basepod/">NVIDIA DGX BasePOD</a> reference architecture.</p>
<p>To achieve validation, partners must complete a suite of NVIDIA tests measuring storage performance and input/out scaling across multiple parameters that represent the demanding requirements of various enterprise AI workloads. This includes combinations of different I/O sizes, varying numbers of threads, buffered I/O vs. direct I/O, random reads, re-reads and more.</p>
<p>Each test is run multiple times to verify the results and gather the required data, which is then audited by NVIDIA engineering teams to determine whether the storage system has passed.</p>
<p>The program offers prescriptive guidance to ensure optimal storage performance and scalability for enterprise AI workloads with NVIDIA OVX systems. But the overall design remains flexible, so customers can tailor their system and storage choices to fit their existing data center environments and bring accelerated computing to wherever their data resides.</p>
<p>Generative AI use cases have fundamentally different requirements than traditional enterprise applications, so IT teams must carefully consider their compute, networking, storage and software choices to ensure high performance and scalability.</p>
<p><a href="https://www.nvidia.com/en-us/data-center/products/certified-systems/">NVIDIA-Certified Systems</a> are tested and validated to provide enterprise-grade performance, manageability, security and scalability for AI workloads. Their flexible reference architectures help deliver faster, more efficient and more cost-effective deployments than independently building from the ground up.</p>
<p>Powered by<a href="https://www.nvidia.com/en-us/data-center/l40s/"> NVIDIA L40S GPUs</a>, OVX servers include<a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/"> NVIDIA AI Enterprise</a> software with <a href="https://www.nvidia.com/en-us/networking/quantum2/">NVIDIA Quantum-2 InfiniBand</a> or<a href="https://www.nvidia.com/en-us/networking/spectrumx/"> NVIDIA Spectrum-X</a> Ethernet networking, as well as<a href="https://www.nvidia.com/en-us/networking/products/data-processing-unit/"> NVIDIA BlueField-3 DPUs</a>. They’re optimized for generative AI workloads, including training for smaller LLMs (for example, Llama 2 7B or 70B), fine-tuning existing models and inference with high throughput and low latency.</p>
<p><a href="https://docs.nvidia.com/certification-programs/nvidia-certified-systems/index.html#nvidia-certified-systems-ovx-list">NVIDIA-Certified OVX servers</a> are now available and shipping from global system vendors, including GIGABYTE, Hewlett Packard Enterprise and <a href="https://news.lenovo.com/pressroom/press-releases/lenovo-unveils-hybrid-ai-solutions-delivering-power-of-generative-ai-to-enterprises-with-nvidia">Lenovo</a>. Comprehensive, enterprise-grade support for these servers is provided by each system builder, in collaboration with NVIDIA.</p>
<h2><b>Availability </b></h2>
<p>Validated storage solutions for NVIDIA-Certified OVX servers are now available, and reference architectures will be published over the coming weeks by each of the storage and system vendors. Learn more about <a href="https://www.nvidia.com/en-us/data-center/products/ovx/">NVIDIA OVX Systems</a>.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/ovx-keynote-blog-header.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/ovx-keynote-blog-header-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[New NVIDIA Storage Partner Validation Program Streamlines Enterprise AI Deployments]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>From Atoms to Supercomputers: NVIDIA, Partners Scale Quantum Computing</title>
		<link>https://blogs.nvidia.com/blog/cuda-q-ecosystem/</link>
		
		<dc:creator><![CDATA[Elica Kyoseva]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:00:47 +0000</pubDate>
				<category><![CDATA[Supercomputing]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Customer Stories]]></category>
		<category><![CDATA[Financial Services]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[New GPU Uses]]></category>
		<category><![CDATA[Science]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70242</guid>

					<description><![CDATA[The latest advances in quantum computing include investigating molecules, deploying giant supercomputers and building the quantum workforce with a new academic program. Researchers in Canada and the U.S. used a large language model to simplify quantum simulations that help scientists explore molecules. “This new quantum algorithm opens the avenue to a new way of combining		<a class="read-more" href="https://blogs.nvidia.com/blog/cuda-q-ecosystem/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>The latest advances in quantum computing include investigating molecules, deploying giant supercomputers and building the quantum workforce with a new academic program.</p>
<p>Researchers in Canada and the U.S. used a large language model to simplify quantum simulations that help scientists explore molecules.</p>
<p>“This new quantum algorithm opens the avenue to a new way of combining quantum algorithms with machine learning,” said Alan Aspuru-Guzik, a professor of chemistry and computer science at the University of Toronto, who led the team.</p>
<p>The effort used <a href="https://developer.nvidia.com/cuda-q">CUDA-Q</a>, a hybrid programming model for GPUs, CPUs and the <a href="https://blogs.nvidia.com/blog/what-is-a-qpu/">QPUs</a> quantum systems use. The team ran its research on <a href="https://blogs.nvidia.com/blog/eos/">Eos</a>, NVIDIA’s H100 GPU supercomputer.</p>
<p>Software from the effort will be made available for researchers in fields like healthcare and chemistry. Aspuru-Guzik will detail the work in <a href="https://www.nvidia.com/gtc/session-catalog/?search=quantum&amp;tab.allsessions=1700692987788001F1cG&amp;search=quantum#/session/1696280074506001Ts8y">a talk</a> at GTC.</p>
<h2><b>Quantum Scales for Fraud Detection</b></h2>
<p>At HSBC, one of the world’s largest banks, researchers designed a quantum machine learning application that can detect fraud in digital payments.</p>
<p>The bank’s quantum machine learning algorithm simulated a whopping 165 qubits on NVIDIA GPUs. Research papers typically don’t extend beyond 40 of these fundamental calculating units quantum systems use.</p>
<p>HSBC used machine learning techniques implemented with CUDA-Q and<a href="https://docs.nvidia.com/cuda/cuquantum/latest/cutensornet/index.html"> cuTensorNet</a> software on NVIDIA GPUs to overcome challenges simulating quantum circuits at scale. Mekena Metcalf, a quantum computing research scientist at HSBC (pictured above), will present her work in a<a href="https://www.nvidia.com/gtc/session-catalog/?tab.allsessions=1700692987788001F1cG&amp;search=quantum#/session/1693079968998001x10J"> session at GTC</a>.</p>
<h2><b>Raising a Quantum Generation</b></h2>
<p>In education, NVIDIA is working with nearly two dozen universities to prepare the next generation of computer scientists for the quantum era. The collaboration will design curricula and teaching materials around CUDA-Q.</p>
<p>&#8220;Bridging the divide between traditional computers and quantum systems is essential to the future of computing,&#8221; said Theresa Mayer, vice president for research at Carnegie Mellon University. &#8220;NVIDIA is partnering with institutions of higher education, Carnegie Mellon included, to help students and researchers navigate and excel in this emerging hybrid environment.&#8221;</p>
<p>To help working developers get hands-on with the latest tools, NVIDIA co-sponsored QHack, a quantum hackathon in February. The winning project, developed by Gopesh Dahale of Qkrishi — a quantum company in Gurgaon, India — used CUDA-Q to develop an algorithm to simulate a material critical in designing better batteries.</p>
<h2><b>A Trio of New Systems</b></h2>
<p>Two new systems being deployed further expand the ecosystem for hybrid quantum-classical computing.</p>
<p>The largest of the two, <a href="https://nvidianews.nvidia.com/news/nvidia-powers-japans-abci-q-supercomputer-for-quantum-research">ABCI-Q</a> at Japan’s National Institute of Advanced Industrial Science and Technology, will be one of the largest supercomputers dedicated to research in quantum computing. It will use CUDA-Q on <a href="https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/">NVIDIA H100 GPUs</a> to advance the nation’s efforts in the field.</p>
<p>In Denmark, the Novo Nordisk Foundation will lead on the deployment of an <a href="https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fdata-center%2Fdgx-superpod%2F&amp;data=05%7C02%7Cgrainville%40nvidia.com%7C5a3284deff074ef3643c08dc479a76a0%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638463976012098204%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&amp;sdata=CFCvpXPiz8u7ATwwhmqr6AUjmlyyd2iJgC7iZH3A6cU%3D&amp;reserved=0">NVIDIA DGX SuperPOD</a>, a significant part of which will be dedicated to research in quantum computing in alignment with the country’s national plan to advance the technology.</p>
<p>The new systems join Australia’s Pawsey Supercomputing Research Centre, which <a href="https://nvidianews.nvidia.com/news/nvidia-accelerates-quantum-computing-exploration-at-australias-pawsey-supercomputing-centre">announced</a> in February it will run CUDA-Q on <a href="https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/">NVIDIA Grace Hopper Superchips</a> at its National Supercomputing and Quantum Computing Innovation Hub.</p>
<h2><b>Partners Drive CUDA-Q Forward</b></h2>
<p>In other news, Israeli startup Classiq released at GTC a new integration with CUDA-Q. Classiq’s quantum circuit synthesis lets high-level functional models automatically generate optimized quantum programs, so researchers can get the most out of today’s quantum hardware and expand the scale of their work on future algorithms.</p>
<p>Software and service provider QC Ware is <a href="https://www.prnewswire.com/news-releases/accelerating-the-development-of-new-molecules---promethium-to-empower-ml-models-for-drug-discovery-using-nvidia-quantum-cloud-302090961.html">integrating its Promethium quantum chemistry package</a> with the just-announced <a href="https://nvidianews.nvidia.com/news/nvidia-launches-cloud-quantum-computer-simulation-microservices">NVIDIA Quantum Cloud</a>.</p>
<p>ORCA Computing, a quantum systems developer headquartered in London, released <a href="https://orcacomputing.com/orca-computing-unveils-first-demonstration-of-a-hybrid-algorithm-utilizing-the-orca-pt-1-photonic-quantum-processor-and-nvidia-cuda-quantum/">results</a> running quantum machine learning on its photonics processor with CUDA-Q. In addition, ORCA was selected to build and supply a quantum computing testbed for the UK’s National Quantum Computing Centre which will include an NVIDIA GPU cluster using CUDA-Q.</p>
<p>Nvidia and Infleqtion, a quantum technology leader, partnered to bring cutting-edge quantum-enabled solutions to <a href="https://www.infleqtion.com/quantum-software-updates/quantum-cybersecurity-gets-a-boost-infleqtion-at-dcm3">Europe’s largest cyber-defense exercise</a> with NVIDIA-enabled Superstaq software.</p>
<p>A cloud-based platform for quantum computing, <a href="https://qbraid.com/blog/unleashing-the-power-of-nvidia-cuda-quantum-with-qbraid">qBraid</a>, is integrating CUDA-Q into its developer environment. And California-based BlueQubit described in a <a href="https://www.bluequbit.io/bluequbit-nvidia-managed-platform">blog</a> how NVIDIA’s quantum technology, used in its research and GPU service, provides the fastest and largest quantum emulations possible on GPUs.</p>
<h2><b>Get the Big Picture at GTC</b></h2>
<p>To learn more, watch a session about <a href="https://www.nvidia.com/gtc/session-catalog/?tab.allsessions=1700692987788001F1cG&amp;search=quantum#/session/1694875146306001JiaF">how NVIDIA is advancing quantum computing</a> and attend an <a href="https://www.nvidia.com/gtc/session-catalog/?tab.allsessions=1700692987788001F1cG&amp;search=quantum#/session/1695925756553001O1NU">expert panel</a> on the topic, both at <a href="https://www.nvidia.com/gtc/">NVIDIA GTC</a>, a global AI conference, running March 18-21 at the San Jose Convention Center.</p>
<p>And get the full view from NVIDIA founder and CEO Jensen Huang in his <a href="https://www.nvidia.com/gtc/keynote/">GTC keynote</a>.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/HSBC-quantum-researcher.jpg"
			type="image/jpeg"
			width="1647"
			height="878"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/HSBC-quantum-researcher-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[From Atoms to Supercomputers: NVIDIA, Partners Scale Quantum Computing]]></media:title>
			<media:description type="html">Picture of HSBC quantum resear4ch scientist, Mekena Metcalf</media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Maxine Developer Platform to Transform $10 Billion Video Conferencing Industry</title>
		<link>https://blogs.nvidia.com/blog/maxine-developer-video-conferencing/</link>
		
		<dc:creator><![CDATA[Trisha Tripathi]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:00:45 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<category><![CDATA[Video Streaming]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70331</guid>

					<description><![CDATA[Video conferencing has allowed many to be productive from anywhere. Now NVIDIA is boosting the productivity of the developers of video conferencing, call center and streaming applications within the $10 billion industry by allowing them to easily integrate AI into their workflows. The new release of the Maxine AI Developer Platform transforms the creation of		<a class="read-more" href="https://blogs.nvidia.com/blog/maxine-developer-video-conferencing/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Video conferencing has allowed many to be productive from anywhere.</p>
<p>Now NVIDIA is boosting the productivity of the developers of video conferencing, call center and streaming applications within the $10 billion industry by allowing them to easily integrate AI into their workflows.</p>
<p>The new release of the Maxine AI Developer Platform transforms the creation of state-of-the-art, real-time video conferencing applications with features enabling enhanced user flexibility, engagement and efficiency.</p>
<p>Available through the <a href="https://docs.nvidia.com/ai-enterprise/index.html">NVIDIA AI Enterprise</a> software platform, Maxine allows developers to tap into the latest AI-driven features — such as enhanced video and audio quality and augmented reality effects — to turn users’ everyday video calls into engaging, collaborative experiences.</p>
<h2><b>Expanding Video Conferencing With New Maxine Features </b></h2>
<p>The Maxine AI Developer Platform enables developers to easily access and integrate real-time, AI-enhanced features that increase the quality of engagement for video conferencing users.</p>
<p>Features like noise reduction, video denoising and upscaling, and studio voice improve the quality of audio and video streams. With advanced capabilities like eye-gaze correction, live portrait and future features such as video relighting and cloud microservice Maxine 3D, developers can enhance video conferencing engagement and personal connection.</p>
<p>The platform extends the utility of the state-of-the-art AI models for audio, video and augmented reality effects with multiple ways for developers to deliver Maxine features with offerings of software development kits, microservices, and even application programming interface (API) endpoints delivered from NVIDIA’s cloud infrastructure.</p>
<p>Maxine production feature updates available now include:<b></b></p>
<ul>
<li><b>Eye Contact</b>: The improved eye contact model provides gaze redirection with natural eye movements for deeper meeting participant engagement.</li>
<li><b>Voice Font</b>: This new model matches the speaker’s voice to a target voice while keeping linguistic information and prosody (rhythm and tone) unchanged.</li>
<li><b>Background Noise Reduction (BNR) 2.0</b>: This model updates noise reduction for human listening and for language encoding with a specific effort to decrease encoding word error rates.</li>
</ul>
<p>New features available for early access this spring include:</p>
<ul>
<li><b>Speech Live Portrait</b>: This model allows a user to drive their portrait with direct speech or any audio source, allowing users to always look their best during a conference call.</li>
<li><b>Studio Voice</b>: This model enables ordinary headset, laptop and desktop microphones to deliver the sound of a high-end studio mic, allowing users to always sound their best during a conference call.</li>
</ul>
<p>The <a href="https://developer.nvidia.com/maxine-microservice-early-access">Maxine early access program</a> shares preproduction and prerelease builds of upcoming features in order to get feedback from developers on the utility and refinement of Maxine models. In this release we are asking developers for feedback on features early in the development pipeline including:</p>
<ul>
<li><b>Maxine 3D</b>: Previously shown as a <a href="https://youtu.be/sZefJHL8X_4?si=wR_7-B4gt70mfh9D">research demonstration</a> at SIGGRAPH 2023, this cloud microservice offers a new level of engagement for video conferencing with real-time NeRF technology lifting 2D video to 3D.</li>
<li><b>Video Relighting</b>: This new model uses a high-dynamic-range image to light the user, enabling seamless matching of user lighting with various background images.</li>
<li><b>API Endpoints</b>: API Endpoints offer developers the flexibility of accessing Maxine features through NVIDIA cloud infrastructure, making Maxine integration even easier.</li>
</ul>
<h2><b>Jugo and Arsenal Football Club Score Major Goals </b></h2>
<p>Sporting events are the ultimate human experience, uniting teams and fans beyond borders and language barriers. Jugo, using Maxine’s AI Green Screen feature, offers a digital platform for virtual events that enables companies to create immersive experiences with Unreal Engine that bring fans together from all over the world without the use of a full production studio.</p>
<p>Arsenal FC, a powerhouse franchise in England’s Premier League, is collaborating with Jugo to revolutionize the way the soccer club engages with its 600 million global fan base. The collaboration offers new virtual sports entertainment experiences to boost engagement for global supporters. Jugo brings the power of real, human interaction into Arsenal events, creating realistic virtual connections between supporters and the club’s sports heroes.</p>
<p>“The Jugo Experience platform is transforming the market for brands in their pursuit of global awareness and engagement,” said Richard Stirk, CEO of Jugo Experience. “Arsenal F.C. is the perfect example of a global brand extension. The flexibility in creating an immersive brand experience is a key to Jugo’s offering and the Maxine AI Developer Platform is a basic building block of this flexibility.”</p>
<h2><b>Setting a New Standard of AI-Enhanced Video Conferencing </b></h2>
<p>Among the first customers to tap into the newest set of features within the early access program to create a professional audio-visual studio from commodity cameras and microphones are <a href="http://gemelo.ai">Gemelo</a>, Pexip, Spectacle and VideoRequest.</p>
<p>“Gemelo has been involved in testing prerelease builds of Maxine models for a number of years now, and we value the chance to give early input on Maxine features as they’re developed,” said Paul Jaski, CEO of Gemelo. “The latest feature, Speech Live Portrait, will provide our customers with greater flexibility in creating customized video messaging, opening the doors to a new era of personalization.”</p>
<p>“Pexip welcomes the chance to test development versions of Maxine features and help guide the final product models,” said Ian Mortimer, chief technology officer at Pexip. “In testing the newest version of Maxine BNR, we are seeing significant improvements in intelligibility and speech quality and plan to continue refining our testing parameters to help optimize for accuracy in AI translation pipelines.”</p>
<p>“The NVIDIA Maxine Eye Contact API significantly simplified our path to providing engaging video processing capabilities to the users of our Spectacle app, eliminating the need to worry about infrastructure and resource-intensive integrations,” said Benjamin Portman, president of Spectacle. “With it, we were able to create a proof of concept within a matter of days, speeding up our production application deployment timeline.”</p>
<p>“Our early testing of Maxine Studio Voice enabled an impressive look into what is now possible with AI-enhanced production and video testimonials,” said Joe Tyler, chief technology officer at VideoRequest. “The new Maxine BNR and Eye Contact features will help elevate the quality of our customer’s videos by overcoming their challenging recording environments.”</p>
<h2><b>Availability </b></h2>
<p>Learn more about <a href="https://developer.nvidia.com/maxine">NVIDIA Maxine</a>, which is available now on <a href="https://docs.nvidia.com/ai-enterprise/index.html">NVIDIA AI Enterprise</a>.</p>
<p><i>See </i><a href="https://www.nvidia.com/en-us/about-nvidia/legal-info/"><b><i>notice</i></b></a><i> regarding software product information.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/MAxine_Jugo_1280x680.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/MAxine_Jugo_1280x680-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Maxine Developer Platform to Transform $10 Billion Video Conferencing Industry]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Edify Unlocks 3D Generative AI, New Image Controls for Visual Content Providers</title>
		<link>https://blogs.nvidia.com/blog/edify-3d-generative-ai-custom-fine-tuning/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:00:40 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Cloud Services]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<category><![CDATA[Rendering]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<category><![CDATA[Synthetic Data Generation]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70502</guid>

					<description><![CDATA[NVIDIA Edify, a multimodal architecture for visual generative AI, is entering a new dimension. 3D asset generation is among the latest capabilities Edify offers developers and visual content providers, who will also be able to exert more creative control over AI image generation. Multimedia content and data provider Shutterstock is rolling out early access to		<a class="read-more" href="https://blogs.nvidia.com/blog/edify-3d-generative-ai-custom-fine-tuning/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>NVIDIA Edify, a multimodal architecture for visual generative AI, is entering a new dimension.</p>
<p>3D asset generation is among the latest capabilities Edify offers developers and visual content providers, who will also be able to exert more creative control over AI image generation.</p>
<p>Multimedia content and data provider Shutterstock is rolling out <a href="https://www.turbosquid.com/ai-3d-generator/early-access" target="_blank" rel="noopener">early access to an API</a>, or application programming interface, built on the Edify architecture that lets creators use text prompts or images to rapidly generate 3D objects for virtual scenes.</p>
<p>Visual content creator and marketplace Getty Images will add custom fine-tuning capabilities to its commercially safe <a href="https://www.gettyimages.com/ai/generation/about" target="_blank" rel="noopener">Generative AI service</a>, helping enterprise customers generate visuals that adhere to brand guidelines and styles. The service will also incorporate new features to offer customers even further control of their generated images.</p>
<p>Developers can test drive pretrained Edify models by Getty Images and Shutterstock as APIs through NVIDIA NIM, a collection of microservices for inference announced today at <a href="https://www.nvidia.com/gtc/">NVIDIA GTC</a>. Developers can also train and deploy their own generative AI models using the Edify architecture through <a href="https://www.nvidia.com/en-us/gpu-cloud/picasso/">NVIDIA Picasso</a>, an AI foundry built on <a href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a>.</p>
<p>NVIDIA and Adobe are collaborating to bring new 3D generative AI technologies built on Edify to millions of Firefly and Creative Cloud creators.</p>
<p>Livestreaming platform Be.Live is using the <a href="https://www.nvidia.com/en-us/gpu-cloud/picasso/">NVIDIA Picasso</a> foundry service to provide real-time generative AI that enables the automated creation of visuals and an interactive experience for audiences. <a href="https://blogs.nvidia.com/blog/bria-builds-responsible-generative-ai-using-nemo-picasso/">Bria</a>, a holistic platform tailored for businesses developing responsible visual generative AI, has adopted Picasso to run inference. And creative studio <a href="https://blogs.nvidia.com/blog/pinar-demirdag-cuebric/">Cuebric</a> is enhancing filmmaking and content creation by developing Picasso-powered generative AI applications to build immersive virtual environments.</p>
<h2><b>Speedy 3D: Shutterstock 3D AI Generator Now in Early Access</b></h2>
<p>Shutterstock’s 3D AI Services, available in early access, will enable creators to generate virtual objects for set dressing and ideation. This capability can drastically reduce the time needed to prototype a scene, giving artists more time to focus on hero characters and objects.</p>
<p><iframe loading="lazy" title="Shutterstock GTC 2024" src="https://player.vimeo.com/video/923615820?h=979600c557&amp;dnt=1&amp;app_id=122963" width="500" height="281" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write"></iframe></p>
<p style="text-align: center;"><i>Shutterstock 3D generator in action. Video courtesy of Shutterstock.</i></p>
<p>Using the tools, creative professionals will be able to rapidly create assets from text prompts or reference images and choose from a selection of popular 3D formats to export their files. The Edify 3D-based service will also come with built-in safeguards to filter generated content.</p>
<p>The commercially safe model was trained on Shutterstock’s licensed data. Shutterstock has compensated hundreds of thousands of artists, with anticipated payments to millions more, for the role their content IP has played in training generative technology.</p>
<figure id="attachment_70506" aria-describedby="caption-attachment-70506" style="width: 1999px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="wp-image-70506 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/image3.png" alt="3d generated rainforest flora and fauna " width="1999" height="1500" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/image3.png 1999w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image3-400x300.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image3-666x500.png 666w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image3-768x576.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image3-1536x1153.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image3-600x450.png 600w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image3-287x215.png 287w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image3-133x100.png 133w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image3-1280x960.png 1280w" sizes="(max-width: 1999px) 100vw, 1999px" /><figcaption id="caption-attachment-70506" class="wp-caption-text">Assets created using Shutterstock 3D AI generator, rendered and arranged as a flat-lay image. Image courtesy of Shutterstock.</figcaption></figure>
<p>At GTC, HP and Shutterstock are showcasing a collaboration to enhance custom 3D printing using Edify 3D, providing designers with limitless prototype options.</p>
<p>Shutterstock’s 3D AI generator enables designers to rapidly iterate on concepts, creating digital assets that HP can convert to 3D printable models through automated workflows. HP 3D printers will then turn these models into physical prototypes to help inspire product designs.</p>
<p>Mattel is enabling 3D generative AI capabilities from Shutterstock that can accelerate the design ideation process. With AI tools, toy designers can visualize their ideas for new products with simple text descriptions. By lowering the technical barrier to creating high-fidelity concept design, designers can explore a broader pool of their ideas and iterate faster.</p>
<p>Shutterstock is also building Edify-based tools to <a href="https://blogs.nvidia.com/blog/shutterstock-generative-ai-picasso-360-hdri/">light 3D scenes</a> using 360 HDRi environments generated from text or image prompts.</p>
<p>Dassault Systèmes, through its leading 3DEXCITE applications for 3D content creation, and CGI studio Katana are incorporating Shutterstock generative 360 HDRi APIs into their workflows based on <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a>, a computing platform for developing <a href="https://www.nvidia.com/en-us/omniverse/usd/">Universal Scene Description (OpenUSD)</a>-based 3D workflows and applications.</p>
<p><a href="https://newsroom.accenture.com/news/2024/accenture-teams-with-nvidia-to-showcase-ai-powered-immersive-client-experiences-for-defender" target="_blank" rel="noopener">Accenture Song</a>, the world’s largest tech-powered creative group, is using the Omniverse platform to generate high-fidelity Defender vehicles from computer-aided design data for marketing purposes. Coupled with generative AI microservices powered by Edify, Accenture Song is enabling the creation of cinematic, interactive 3D environments via conversational prompts. The result is a fully immersive 3D scene that harmonizes realistic generated environments with a digital twin of the Defender vehicle.</p>
<h2><b>Take Control: Turn Creative Vision Into Reality With Custom Fine-Tuning From Getty Images</b></h2>
<p>Getty Images continues to expand the capabilities offered through its commercially safe generative AI service, which provides users indemnification for the content they generate.</p>
<p>At January’s CES show, Getty Images released <a href="https://blogs.nvidia.com/blog/nvidia-picasso-istock-generative-ai/">Edify-powered APIs for inpainting</a>, to add, remove or replace objects in an image, and outpainting, to expand the creative canvas. Those features are now available on both Gettyimages.com and iStock.com.</p>
<p>Starting in May, Getty Images will also offer services to custom fine-tune the Edify foundation model to a company’s brand and visual style.</p>
<p>The services will feature a no-code, self-service method for companies to upload a proprietary dataset, review automatically generated tags, submit fine-tuning tasks and review the results before deploying to production.</p>
<p>As part of custom fine-tuning tools, Getty Images will release a collection of APIs that provide finer control over image output, one of the biggest challenges in generative AI.</p>
<p>Developers will soon be able to access Sketch, Depth and Segmentation features — which allow users to provide a sketch to guide the AI’s image generation; copy the composition of a reference image via depth map; and segment parts of an image to add, remove or retouch a character or object.</p>
<p>Getty Images’ API services are already being used by leading creatives and advertisers, including:</p>
<ul>
<li><b>Dentsu Inc.</b>: The Japan-based ad agency is using Getty Images’ generative AI API service to power MAFA: Manga Anime For All, an app that can create manga and anime-style content for marketing use cases. Dentsu Creative is also using NVIDIA Picasso to fine-tune Getty Images’ model for leading membership warehouse retailer Sam’s Club.</li>
<li><b>McCann</b>: The creative agency harnessed generative AI to help develop an innovative game for its client Reckitt’s over-the-counter cold medicine Mucinex, in which customers can interact with the brand’s mascot.</li>
<li><b>Refik Anadol Studio</b>: Known for using generative AI in its artwork, the studio will be showcasing a <a href="https://blogs.nvidia.com/blog/ai-refik-anadol-gtc-2024/">rainforest-inspired art installation at GTC</a>, created using Getty Images’ AI model fine-tuned with Refik’s rainforest catalog.</li>
<li><b>WPP: </b>The marketing and communications services company is partnering with The Coca-Cola Company to explore how fine-tuning Getty Images’ model can help to build custom visuals that meet brand styles and guidelines.</li>
</ul>
<figure id="attachment_70509" aria-describedby="caption-attachment-70509" style="width: 1450px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-full wp-image-70509" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/image1.png" alt="rainforest-themed AI art" width="1450" height="686" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/image1.png 1450w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image1-400x189.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image1-672x318.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image1-768x363.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image1-842x398.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image1-406x192.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image1-188x89.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/03/image1-1280x606.png 1280w" sizes="(max-width: 1450px) 100vw, 1450px" /><figcaption id="caption-attachment-70509" class="wp-caption-text">Large Nature Model: Living Archive installation at GTC 2024 by Refik Anadol Studios</figcaption></figure>
<p>Learn more about <a href="https://www.nvidia.com/en-us/gpu-cloud/picasso/">NVIDIA Picasso</a> and try Edify-powered NIMs from Getty Images and Shutterstock at ai.nvidia.com.</p>
<p><i>Discover the latest in generative AI at </i><a href="https://www.nvidia.com/gtc/"><i>NVIDIA GTC</i></a><i>, a global AI developer conference, running in San Jose, Calif., and online through Thursday, March 21. </i></p>
<p><i>Watch the GTC keynote address by NVIDIA founder and CEO Jensen Huang below:</i></p>
<p><iframe loading="lazy" title="Don’t Miss This Transformative Moment in AI" width="500" height="281" src="https://www.youtube.com/embed/Y2F8yisiS6E?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p><i>Collage at top shows assets created by Edify-powered Shutterstock 3D AI generator on left, courtesy of Shutterstock. Images on right show Edify sketch-to-image capabilities, demonstrated by NVIDIA.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/GTC-Edify-key-visual.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/GTC-Edify-key-visual-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Edify Unlocks 3D Generative AI, New Image Controls for Visual Content Providers]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Make It So: Software Speeds Journey to Post-Quantum Cryptography</title>
		<link>https://blogs.nvidia.com/blog/cupqc-quantum-cryptography/</link>
		
		<dc:creator><![CDATA[Sam Stanwyck]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:00:39 +0000</pubDate>
				<category><![CDATA[Supercomputing]]></category>
		<category><![CDATA[Cloud Services]]></category>
		<category><![CDATA[Cybersecurity]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<category><![CDATA[high]]></category>
		<category><![CDATA[New GPU Uses]]></category>
		<category><![CDATA[NVIDIA Hopper Architecture]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70246</guid>

					<description><![CDATA[The journey to the future of secure communications is about to jump to warp drive. NVIDIA cuPQC brings accelerated computing to developers working on cryptography for the age of quantum computing. The cuPQC library harnesses the parallelism of GPUs for their most demanding security algorithms. Refactoring Security for the Quantum Era   Researchers have known for		<a class="read-more" href="https://blogs.nvidia.com/blog/cupqc-quantum-cryptography/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>The journey to the future of secure communications is about to jump to warp drive.</p>
<p>NVIDIA cuPQC brings accelerated computing to developers working on cryptography for the age of quantum computing. The cuPQC library harnesses the parallelism of GPUs for their most demanding security algorithms.</p>
<h2><b>Refactoring Security for the Quantum Era  </b></h2>
<p>Researchers have known for years that quantum computers will be able to break the public keys used today to secure communications. As these systems approach readiness, government and industry initiatives have been ramping up to address this vital issue.</p>
<p>The U.S. National Institute of Standards and Technology, for example, is expected to introduce the first standard algorithms for post-quantum cryptography as early as this year.</p>
<p>Cryptographers working on advanced algorithms to replace today’s public keys need powerful systems to design and test their work.</p>
<h2><b>Hopper Delivers up to 500x Speedups With cuPQC</b></h2>
<p>In its first benchmarks, cuPQC accelerated Kyber — an algorithm proposed as a standard for securing quantum-resistant keys — by up to 500x running on an <a href="https://resources.nvidia.com/en-us-tensor-core">NVIDIA H100 Tensor Core GPU</a> compared with a CPU.</p>
<p>The speedups will be even greater with <a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing">NVIDIA Blackwell architecture GPUs</a>, given Blackwell’s enhancements for the integer math used in cryptography and other high performance computing workloads.</p>
<p>&#8220;Securing data against quantum threats is a critically important problem, and we&#8217;re excited to work with NVIDIA to optimize post-quantum cryptography,&#8221; said Douglas Stebila, co-founder of the Open Quantum Safe project, a group spearheading work in the emerging field.</p>
<h2><b>Accelerating Community Efforts</b></h2>
<p>The project is a part of the newly formed Post-Quantum Cryptography Alliance, hosted by the Linux Foundation.</p>
<p>The alliance funds open source projects to develop post-quantum libraries and applications. NVIDIA is a member of the alliance with seats on both its steering and technical committees.</p>
<p>NVIDIA is also collaborating with cloud service providers such as Amazon Web Services (AWS), Google Cloud and Microsoft Azure on testing cuPQC.</p>
<p>In addition, leading companies in post-quantum cryptography such as evolutionQ, PQShield, QuSecure and SandboxAQ are collaborating with NVIDIA, many with plans to integrate cuPQC into their offerings.</p>
<p>&#8220;Different use cases will require a range of approaches for optimal acceleration,” said Ben Packman, a senior vice president at PQShield. &#8220;We are delighted to explore cuPQC with NVIDIA.&#8221;</p>
<h2><b>Learn More at GTC</b></h2>
<p>Developers working on post-quantum cryptography can sign up for updates on cuPQC <a href="https://developer.nvidia.com/cupqc">here</a>.</p>
<p>To learn more, watch a <a href="https://www.nvidia.com/gtc/session-catalog/?tab.allsessions=1700692987788001F1cG&amp;search=quantum#/session/1694875146306001JiaF">session</a> about how NVIDIA is advancing quantum computing and attend an <a href="https://www.nvidia.com/gtc/session-catalog/?tab.allsessions=1700692987788001F1cG&amp;search=quantum#/session/1695925756553001O1NU">expert panel</a> on the topic at <a href="https://www.nvidia.com/gtc/">NVIDIA GTC</a>, a global AI conference, running through March 21 at the San Jose Convention Center and online.</p>
<p>Get the full view from NVIDIA founder and CEO Jensen Huang in his <a href="https://www.nvidia.com/gtc/keynote/">GTC keynote</a>.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/Cybersecurity-KV-NVDAM2.jpg"
			type="image/jpeg"
			width="1600"
			height="850"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/Cybersecurity-KV-NVDAM2-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Make It So: Software Speeds Journey to Post-Quantum Cryptography]]></media:title>
			<media:description type="html">pix of cybersecurity developers</media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Brings Generative AI for Digital Humans, New RTX Technologies and More DLSS 3.5 Games to GDC</title>
		<link>https://blogs.nvidia.com/blog/generative-ai-digital-humans-rtx-dlss-gdc/</link>
		
		<dc:creator><![CDATA[Scott Martin]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:00:20 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[NVIDIA DLSS]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70383</guid>

					<description><![CDATA[Generative AI is capable of creating more realistic verbal and facial expressions for digital humans than ever before. This week at GDC 2024, NVIDIA announced that leading AI application developers across a wide range of industries are using NVIDIA digital human technologies to create lifelike avatars for commercial applications and dynamic game characters. NVIDIA enables		<a class="read-more" href="https://blogs.nvidia.com/blog/generative-ai-digital-humans-rtx-dlss-gdc/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Generative AI is capable of creating more realistic verbal and facial expressions for digital humans than ever before.</p>
<p>This week at GDC 2024, NVIDIA announced that leading AI application developers across a wide range of industries are using NVIDIA digital human technologies to create lifelike avatars for commercial applications and dynamic game characters. NVIDIA enables developers with state-of-the-art digital human technologies, including <a href="https://developer.nvidia.com/ace">NVIDIA ACE</a> for speech and animation, <a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/">NVIDIA NeMo</a> for language, and <a href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX</a> for ray-traced rendering.</p>
<p>Developers showcased new digital human technology demos that used NVIDIA ACE microservices at GDC.</p>
<h2><b>Embracing ACE: Partners Transforming Pixels Into Personalities  </b></h2>
<p>Top game and digital human developers are pioneering ways ACE and generative AI technologies can be used to transform interactions between players and NPCs in games and applications.</p>
<p>Developers embracing ACE include: <a href="https://www.convai.com/">Convai</a>, <a href="https://www.cyberagent.co.jp/en/">Cyber Agent</a>, <a href="https://www.datamonsters.com/">Data Monsters</a>, <a href="https://www2.deloitte.com/us/en.html">Deloitte</a>, <a href="https://www.hippocraticai.com/">HippocraticAI</a>, <a href="https://www.igoodi.eu/en/">IGOODI</a>, <a href="https://inworld.ai/">Inworld AI</a>, <a href="https://media.monks.com/">Media.Monks</a>, <a href="https://www.mihoyo.com/en/">miHoYo</a>, <a href="https://www.neteasegames.com/">NetEase Games</a>, <a href="https://www.perfectworld.com/">Perfect World Games</a>, <a href="https://openstream.ai/">Openstream</a>, <a href="http://en.ourpalm.com/">OurPalm</a>, <a href="https://quantiphi.com/">Quantiphi</a>, <a href="https://www.rakuten-sec.co.jp/">Rakuten Securities</a>, <a href="https://www.slalom.com/us/en">Slalom</a>, <a href="https://www.softserveinc.com/en-us">SoftServe,</a> <a href="https://www.tencent.com/">Tencent</a>, <a href="https://www.ubisoft.com/en-us/">Ubisoft</a>, <a href="https://www.digitalhumans.com/">UneeQ</a> and <a href="https://unionavatars.com/">Unions Avatars</a>.</p>
<h2><b>Demos Showcase New NVIDIA Digital Human Technologies</b></h2>
<p>NVIDIA worked with developers Inworld AI and UneeQ on a series of new demos to display the potential of digital human technologies.</p>
<p><a href="https://inworld.ai/">Inworld AI</a> created <a href="https://www.youtube.com/watch?v=uryeFhnNzEs">Covert Protocol</a> in partnership with NVIDIA, allowing players to become a skilled private detective, pushing the possibilities of non-playable character interactions. The demo taps into NVIDIA Riva automatic speech recognition (ASR) and NVIDIA Audio2Face microservices alongside the Inworld Engine.</p>
<p>The Inworld Engine brings together cognition, perception and behavior systems to create an immersive narrative along with the beautifully crafted RTX-rendered environments and art.</p>
<p><a href="https://www.digitalhumans.com/">UneeQ</a> is a digital human platform specialized in creating high-fidelity AI-powered 3D avatars for a range of enterprise applications. UneeQ’s digital humans power interactive experiences for brands enabling them to communicate with customers in real-time to give them confidence in their purchases. UneeQ integrated NVIDIA Audio2Face microservice into its platform and combined it with <a href="https://www.digitalhumans.com/features/synanim">Synanim ML</a> to create highly realistic avatars for a better customer experience and engagement.</p>
<h2><b>New NVIDIA RTX Technologies for Dynamic Scenes</b></h2>
<p><a href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX </a>revolutionized gaming several years ago by offering a collection of rendering technologies that enable real-time path tracing in games and applications.</p>
<p>The latest addition, <a href="https://developer.nvidia.com/rtx/ray-tracing/rtxgi">Neural Radiance Cache (NRC)</a>, is an AI-driven RTX algorithm to handle indirect lighting in fully dynamic scenes, without the need to bake static lighting for geometry and materials beforehand.</p>
<p>Adding flexibility for developers, NVIDIA is introducing Spatial Hash Radiance Cache (<a href="https://developer.nvidia.com/rtx/ray-tracing/rtxgi">SHaRC</a>), which offers similar benefits as NRC but without using a neural network, and with compatibility on any DirectX or Vulkan ray tracing-capable GPU.</p>
<h2><b>RTX. It’s On: More RTX and DLSS 3.5 Titles </b></h2>
<p>There are now over 500 <a href="https://www.nvidia.com/en-us/geforce/news/nvidia-rtx-games-engines-apps/">RTX games and applications</a> that have revolutionized the ways people play and create with ray tracing, <a href="https://www.nvidia.com/en-us/geforce/technologies/dlss/">NVIDIA DLSS</a> and AI-powered technologies.  And gamers have a lot to look forward to with more full ray tracing and DLSS 3.5 titles coming.</p>
<p>Our latest innovation, <a href="https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-3-5-ray-reconstruction">NVIDIA DLSS 3.5</a>, features new DLSS Ray Reconstruction technology. When activated, DLSS Ray Reconstruction replaces hand-tuned ray tracing denoisers with a new unified AI model that enhances ray tracing in supported games, elevating image quality to new heights.</p>
<p>Full Ray Tracing and DLSS 3.5 are coming to both <i>Black Myth: Wukong</i> and <i>NARAKA: BLADEPOINT</i>. And <i>Portal with RTX</i> is available now with RTX DLSS 3.5, enhancing its already beautiful full ray tracing. DLSS 3.5 With Ray Reconstruction will also be coming soon to the NVIDIA RTX Remix Open Beta, enabling modders to add RTX technologies like full ray tracing and DLSS 3.5 into classic games.</p>
<p><i>Star Wars Outlaws</i> will launch with DLSS 3 and ray-traced effects. Ray tracing joins DLSS 3 in <i>Diablo IV</i> March 26. <i>The First Berserker: Khazan</i> will launch with DLSS 3. And <i>Sengoku Destiny</i> introduced support for DLSS 3 and is available now.</p>
<h2><b>See our Partner Ecosystem at GDC</b></h2>
<p>NVIDIA and our partners will showcase the latest in digital human technologies throughout the week of GDC. Here&#8217;s a quick snapshot:</p>
<ul>
<li style="font-weight: 300;" aria-level="1"><b>Inworld AI (Booth P1615):</b> Attendees will get the chance to try out Covert Protocol for themselves live at GDC.</li>
<li style="font-weight: 300;" aria-level="1"><b>Oracle Cloud Infrastructure (Booth S941): </b>See Covert Protocol in action, and discover the “code assist” ability of Retrieval Augmented Generation (RAG).<a href="https://eventreg.oracle.com/profile/web/index.cfm?PKwebID=0x861632abcd&amp;source=WWMK240117P00023:em:ip:ie:pt:::RC_WWMK230720P00051:NVIDIARegistration"> Register and join</a> an exclusive networking event with Oracle and NVIDIA AI experts on March 21, open to women and allies in the gaming industry to build connections with leading voices in AI.</li>
<li style="font-weight: 300;" aria-level="1"><b>Dell Technologies and International Game Developer Association (Booth S1341): </b>Playtest while building a game-ready asset on a workstation with large GPU memory. Speak to<a href="https://sophie.digitalhumans.com/"> Sophie</a>, an AI-powered assistant created by<a href="https://www.digitalhumans.com/"> UneeQ</a> and powered by NVIDIA ACE. Attendees can see the latest debugging and profiling tools for making ray-traced games​ in the latest <a href="https://developer.nvidia.com/nsight-graphics">Nsight Graphics</a> ray tracing demo.</li>
<li style="font-weight: 300;" aria-level="1"><b>AWS: </b>Developers can <a href="https://awspartnershowcase.splashthat.com/">register and join</a> NVIDIA, AWS, game studios, and technology partners as they discuss the game tech used to build, innovate, and maximize growth of today&#8217;s games at the AWS for Games Partner Showcase on March 20th.</li>
</ul>
<p>Stop by these key sessions:</p>
<ul>
<li style="font-weight: 300;" aria-level="1"><a href="https://schedule.gdconf.com/session/transforming-gameplay-with-ai-npcs-presented-by-inworld-ai-nvidia-and-oracle/903328">Transforming Gameplay with AI NPCs</a>: This session featuring Nathan Yu, director of product at Inworld AI, Rajiv Gandhi, master principal cloud architect at Oracle Cloud and Yasmina Benkhoui, generative AI strategic partnerships lead at NVIDIA, will showcase successful examples of developers using AI NPCs to drive core game loops and mechanics that keep players engaged and immersed. Attendees will gain a deeper understanding of the potential of AI NPCs to create new and immersive experiences for players.</li>
<li style="font-weight: 300;" aria-level="1"><a href="https://schedule.gdconf.com/session/alan-wake-2-a-deep-dive-into-path-tracing-technology-presented-by-nvidia/903204">Alan Wake 2: A Deep Dive into Path Tracing Technology</a>: This session will take a deep dive into the path tracing and NVIDIA DLSS Ray Reconstruction technologies implemented in Remedy Entertainment&#8217;s Alan Wake 2. Developers can discover how these cutting-edge techniques can enhance the visual experience of games.</li>
</ul>
<p>Download our <a href="https://images.nvidia.com/aem-dam/en-zz/Solutions/events/GDC/gdc-24-disneyland-show-map-3109452-r14.pdf">show guide</a> to keep this summary on hand while at the show.</p>
<h2><b>Get Started</b></h2>
<p>Developers can start their journey on <a href="https://developer.nvidia.com/ace">NVIDIA ACE</a> by applying for our early access program to get in-development AI models.</p>
<p>If you want to explore available models, evaluate and access NVIDIA NIM, a set of easy-to-use microservices designed to accelerate the deployment of generative AI, for RIVA ASR and Audio2Face on <a href="https://build.nvidia.com">ai.nvidia.com</a> today.</p>
<p>RTXGI’s NRC and SHaRC algorithms are also <a href="https://developer.nvidia.com/rtx/ray-tracing/rtxgi">available now</a> as an experimental branch.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/GDC.png"
			type="image/png"
			width="1600"
			height="900"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/GDC-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Brings Generative AI for Digital Humans, New RTX Technologies and More DLSS 3.5 Games to GDC]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Johnson &#038; Johnson MedTech Works With NVIDIA to Broaden AI’s Reach in Surgery</title>
		<link>https://blogs.nvidia.com/blog/johnson-and-johnson-medtech-ai-surgery/</link>
		
		<dc:creator><![CDATA[David Niewolny]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:00:15 +0000</pubDate>
				<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Networking]]></category>
		<category><![CDATA[Robotics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Embedded Computing]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<category><![CDATA[Healthcare and Life Sciences]]></category>
		<category><![CDATA[Inference]]></category>
		<category><![CDATA[Jetson]]></category>
		<category><![CDATA[Social Impact]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70496</guid>

					<description><![CDATA[AI — already used to connect, analyze and offer predictions based on operating room data — will be critical to the future of surgery, boosting operating room efficiency and clinical decision-making. That’s why NVIDIA is working with Johnson &#38; Johnson MedTech to test new AI capabilities for the company’s connected digital ecosystem for surgery. It		<a class="read-more" href="https://blogs.nvidia.com/blog/johnson-and-johnson-medtech-ai-surgery/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>AI — already used to connect, analyze and offer predictions based on operating room data — will be critical to the future of surgery, boosting operating room efficiency and clinical decision-making.</p>
<p>That’s why NVIDIA is working with <a href="https://www.jnjmedtech.com/en-US/news-events/johnson-johnson-medtech-working-nvidia-scale-ai-surgery" target="_blank" rel="noopener">Johnson &amp; Johnson MedTech</a> to test new AI capabilities for the company’s connected digital ecosystem for surgery. It aims to enable open innovation and accelerate the delivery of real-time insights at scale to support medical professionals before, during and after procedures.</p>
<p>J&amp;J MedTech is in 80% of the world’s operating rooms and <a href="https://jnjinstitute.com/" target="_blank" rel="noopener">trains more than 140,000 healthcare professionals</a> each year through its education programs.</p>
<p>Seeking to bring together its legacy and digital ecosystem in surgery with NVIDIA’s leading AI solutions — including the <a href="https://www.nvidia.com/en-us/edge-computing/products/igx/">NVIDIA IGX</a> edge computing platform and <a href="https://www.nvidia.com/en-us/clara/holoscan/">NVIDIA Holoscan</a> edge AI platform for building medical devices — J&amp;J MedTech can accelerate the infrastructure needed to deploy AI-powered software applications for surgery. IGX and Holoscan can support secure, real-time processing from devices across the operating room to provide clinical insights and improve surgical outcomes.</p>
<p>Unveiled at <a href="https://www.nvidia.com/gtc/">NVIDIA GTC</a>, the global AI conference taking place March 18-21 in San Jose, Calif., and online, this work could also facilitate the deployment of third-party models and applications developed across the digital surgery ecosystem by providing a common AI compute platform.</p>
<p>“AI models are currently being created by experts in surgery in various parts of the world,” said Shan Jegatheeswaran, vice president and global head of digital at J&amp;J MedTech. “If we can create a trusted, open ecosystem that enables and accelerates coordination, it would create a flywheel of innovation where different groups can collaborate and connect at scale, improving access to advanced analytics across the surgical experience.”</p>
<h2><b>An Open Ecosystem for AI Innovation: Building on NVIDIA Holoscan and IGX </b></h2>
<p>J&amp;J MedTech is working with NVIDIA to test how industrial-grade edge AI capabilities purpose-built for medical environments could benefit surgery.</p>
<p>“Our connected digital ecosystem will help break down the traditional barriers to entry for developers seeking to build applications and deploy analytics in the operating room,” Jegatheeswaran said. “We’re making it simpler for those who want to participate in the surgical workflow by eliminating the heavy lifting of building a secure, enterprise-grade platform.”</p>
<p>NVIDIA Holoscan accelerates the development and deployment of real-time AI applications to process data streams.</p>
<p>Holoscan includes reference pipelines to build AI applications for a variety of medical use cases, including endoscopy, ultrasound and other sensors. It runs on NVIDIA IGX, which includes <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/">NVIDIA Jetson Orin</a> modules, <a href="https://www.nvidia.com/en-us/design-visualization/rtx-a6000/">NVIDIA RTX A6000 GPUs</a> and <a href="https://www.nvidia.com/en-us/networking/ethernet-adapters/">NVIDIA ConnectX</a> networking technology to enable high-speed data streaming from medical devices or operating room video feeds.</p>
<p>NVIDIA supports the IGX software stack with <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a>, the enterprise operating system for production-grade AI.</p>
<h2><b>Fueling Surgical AI With Device Data</b></h2>
<p>The J&amp;J MedTech team envisions the potential of NVIDIA-accelerated edge analytics behind its connected digital ecosystem as an enabler of AI-powered applications fueled by device, patient and other surgical data.</p>
<p>Developers could leverage continuous learning, where an algorithm improves based on data collected by the deployed device. Real-world footage collected by an endoscope, for example, could be used to refine an AI model that identifies organs, tissue and potential tumors in real time on an operating room display to support clinical decision-making.</p>
<p>“Surgical technologies will get more intelligent over time, bringing the power of advanced analytics to surgeons and hospitals,” said Jegatheeswaran. “A collection of AI models could act like driver-assistance technology for surgeons, amplifying their ability to deliver care while reducing cognitive load.”</p>
<p>One example is AI that removes personally identifiable information from surgical videos so they can be used downstream for research purposes — or, when processed in real time, enable hospitals to bring in external experts through telepresence to consult during a surgery while maintaining patient privacy.</p>
<p>Future applications could enable surgeons to interact with chatbots to gain insights about a patient’s medical history or best practices for handling certain complications. Other models could improve operating room efficiency by using video feeds to understand when a procedure is almost complete, alerting the next surgical team that a room will soon be available.</p>
<p><i>Discover the latest in </i><a href="https://www.nvidia.com/gtc/sessions/healthcare/"><i>AI and healthcare</i></a><i> at GTC, running in San Jose, Calif., and online through Thursday, March 21. Tune in to a </i><a href="https://www.nvidia.com/gtc/session-catalog/?regcode=so-yout-599399-vt25&amp;ncid=so-yout-599399-vt25&amp;tab.allsessions=1700692987788001F1cG&amp;search=S62604#/session/1696538040602001SPK9"><i>special address on generative AI in healthcare</i></a><i> delivered by Kimberly Powell, vice president of healthcare at NVIDIA, on Tuesday at 8 a.m. PT.</i></p>
<p><i>Watch the GTC keynote address by NVIDIA founder and CEO Jensen Huang below:</i></p>
<p><iframe loading="lazy" title="Don’t Miss This Transformative Moment in AI" width="500" height="281" src="https://www.youtube.com/embed/Y2F8yisiS6E?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/hc-corp-blog-gtc24-jnj-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/hc-corp-blog-gtc24-jnj-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Johnson & Johnson MedTech Works With NVIDIA to Broaden AI’s Reach in Surgery]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>BNY Mellon, First Global Bank to Deploy AI Supercomputer Powered by NVIDIA DGX SuperPOD With DGX H100</title>
		<link>https://blogs.nvidia.com/blog/bny-mellon-superpod/</link>
		
		<dc:creator><![CDATA[Malcolm deMayo]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 22:00:05 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Hardware]]></category>
		<category><![CDATA[Supercomputing]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70376</guid>

					<description><![CDATA[Moving fast to accelerate its AI journey, BNY Mellon, a global financial services company celebrating its 240th anniversary, revealed Monday that it has become the first major bank to deploy an NVIDIA DGX SuperPOD with DGX H100 systems. Thanks to the strong collaborative relationship between NVIDIA Professional Services and BNY Mellon, the team was able		<a class="read-more" href="https://blogs.nvidia.com/blog/bny-mellon-superpod/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Moving fast to accelerate its AI journey, <a href="https://www.bnymellon.com/us/en.html">BNY Mellon</a>, a global financial services company celebrating its 240th anniversary, revealed Monday that it has become the first major bank to deploy an <a href="https://www.nvidia.com/en-us/data-center/dgx-superpod/">NVIDIA DGX SuperPOD</a> with <a href="https://www.nvidia.com/en-us/data-center/dgx-h100">DGX H100 systems</a>.</p>
<p>Thanks to the strong collaborative relationship between <a href="https://www.nvidia.com/en-us/ai-data-science/professional-services/">NVIDIA Professional Services</a> and BNY Mellon, the team was able to install and configure the DGX SuperPOD ahead of typical timelines.</p>
<p>The system, equipped with dozens of NVIDIA DGX systems and <a href="https://www.nvidia.com/en-us/networking/products/infiniband/">NVIDIA InfiniBand</a> networking and based on the DGX SuperPOD reference architecture, delivers computer processing performance that hasn’t been seen before at BNY Mellon.</p>
<p>“Key to our technology strategy is empowering our clients through scalable, trusted platforms and solutions,” said BNY Mellon Chief Information Officer <a href="https://www.bnymellon.com/us/en/about-us/leadership/engle.html">Bridget Engle</a>. “By deploying NVIDIA’s AI supercomputer, we can accelerate our processing capacity to innovate and launch AI-enabled capabilities that help us manage, move and keep our clients’ assets safe.”</p>
<p>Powered by its new system, BNY Mellon plans to use <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise software</a> to support the build and deployment of AI applications and manage AI infrastructure.</p>
<h2>NVIDIA AI Software: A Key Component in BNY Mellon’s Toolbox</h2>
<p>Founded by Alexander Hamilton in 1784, BNY Mellon oversees nearly $50 trillion in assets for its clients and helps companies and institutions worldwide access the money they need, support governments in funding local projects, safeguard investments for millions of individuals and more.</p>
<p>BNY Mellon has long been at the forefront of AI and accelerated computing in the <a href="https://www.nvidia.com/en-us/industries/finance/">financial services industry</a>. Its AI Hub has more than 20 AI-enabled solutions in production. These solutions support predictive analytics, automation and anomaly detection, among other capabilities.</p>
<p>While the firm recognizes that AI presents opportunities to enhance its processes to reduce risk across the organization, it is also actively working to consider and manage potential risks associated with AI through its robust risk management and governance processes.</p>
<p>Some of the use cases supported by DGX SuperPOD include deposit forecasting, payment automation, predictive trade analytics and end-of-day cash balances.</p>
<p>More are coming. The company identified more than 600 opportunities in AI during a firmwide exercise last year, and dozens are already in development using such NVIDIA AI Enterprise software as <a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/nemo-framework/">NVIDIA NeMo</a>, <a href="https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/">NVIDIA Triton Inference Server</a> and <a href="https://www.nvidia.com/en-us/data-center/base-command/">NVIDIA Base Command</a>.</p>
<p>Triton Inference Server is inference-serving software that streamlines AI inferencing or puts trained AI models to work.</p>
<p>Base Command powers the DGX SuperPOD, delivering the best of NVIDIA software that enables businesses and their data scientists to accelerate AI development.</p>
<p>NeMo is an end-to-end platform for developing custom generative AI, anywhere. It includes tools for training, and retrieval-augmented generation, guardrailing and toolkits, data curation tools, and pretrained models, offering enterprises an easy, cost-effective, and fast way to adopt generative AI.</p>
<h2>Fueling Innovation Through Top Talent</h2>
<p>With the new DGX SuperPOD, these tools will enable BNY Mellon to streamline and accelerate innovation within the firm and across the global financial system.</p>
<p>Hundreds of data scientists, solutions architects and risk, control and compliance professionals have been using the <a href="https://www.nvidia.com/en-us/data-center/dgx-platform">NVIDIA DGX platform</a>, which delivers the world’s leading solutions for enterprise AI development at scale, for several years.</p>
<p>By leveraging their new NVIDIA DGX SuperPOD will help the company rapidly expand its on-premises AI infrastructure.</p>
<p>The new system also underscores the company’s commitment to adopting new technologies and attracting top talent across the world to help drive its innovation agenda forward.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/finance-corp-blog-superpod-1280x680-2.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/finance-corp-blog-superpod-1280x680-2-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[BNY Mellon, First Global Bank to Deploy AI Supercomputer Powered by NVIDIA DGX SuperPOD With DGX H100]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Isaac Taps Generative AI for Manufacturing and Logistics Applications</title>
		<link>https://blogs.nvidia.com/blog/isaac-generative-ai-manufacturing-logistics/</link>
		
		<dc:creator><![CDATA[Gerard Andrews]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 21:54:57 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Robotics]]></category>
		<category><![CDATA[Isaac]]></category>
		<category><![CDATA[NVIDIA Jetson]]></category>
		<category><![CDATA[Omniverse]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70271</guid>

					<description><![CDATA[The NVIDIA Isaac robotics platform is tapping into the latest generative AI and advanced simulation technologies to accelerate AI-enabled robotics. At GTC today, NVIDIA announced Isaac Manipulator and Isaac Perceptor — a collection of foundation models, robotics tools and GPU-accelerated libraries. On stage before a crowd of 10,000-plus, NVIDIA founder and CEO Jensen Huang demonstrated		<a class="read-more" href="https://blogs.nvidia.com/blog/isaac-generative-ai-manufacturing-logistics/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>The <a href="https://www.nvidia.com/en-us/deep-learning-ai/industries/robotics/">NVIDIA Isaac robotics platform</a> is tapping into the latest generative AI and advanced simulation technologies to accelerate AI-enabled robotics.</p>
<p>At <a href="https://www.nvidia.com/gtc/">GTC</a> today, NVIDIA announced <a href="https://developer.nvidia.com/isaac/manipulator">Isaac Manipulator</a> and <a href="https://developer.nvidia.com/isaac/perceptor">Isaac Perceptor</a> — a collection of foundation models, robotics tools and GPU-accelerated libraries.</p>
<p>On stage before a crowd of 10,000-plus, NVIDIA founder and CEO Jensen Huang demonstrated <a href="https://nvidianews.nvidia.com/news/foundation-model-isaac-robotics-platform">Project GR00T</a>, which stands for Generalist Robot 00 Technology, a general-purpose foundation model for humanoid robot learning. Project GR00T leverages  various tools from the NVIDIA Isaac robotics platform to create AI for humanoid robots.</p>
<p>“Building foundation models for general humanoid robots is one of the most exciting problems to solve in AI today,” said Huang. “The enabling technologies are coming together for leading roboticists around the world to take giant leaps toward artificial general robotics.”</p>
<p>NVIDIA also announced a new computer for humanoid robots based on the NVIDIA Thor system-on-a-chip, and new tools for the NVIDIA Isaac robotics platform, including Isaac Lab for robot learning and <a href="https://developer.nvidia.com/blog/scale-ai-enabled-robotics-development-workloads-with-nvidia-osmo/">NVIDIA OSMO</a> for hybrid-cloud workflow orchestration, which are instrumental in the development of Project GR00T and foundation models for robots.</p>
<h2><b>Introducing Isaac Manipulator for Robotic Arms</b></h2>
<p>&nbsp;</p>
<p style="text-align: center;"><img loading="lazy" decoding="async" class="alignnone wp-image-70405" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/Orchestrator-1-400x166.png" alt="" width="809" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/Orchestrator-1-400x166.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Orchestrator-1-672x279.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Orchestrator-1-768x319.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Orchestrator-1-842x350.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Orchestrator-1-406x169.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Orchestrator-1-188x78.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Orchestrator-1-1280x532.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Orchestrator-1.png 1374w" sizes="(max-width: 809px) 100vw, 809px" /></p>
<p>NVIDIA Isaac Manipulator offers a collection of state-of-the-art motion generation and modular AI capabilities for robotic arms, with a robust collection of foundation models and GPU-accelerated libraries.</p>
<p>Robotics developers can use combinations of software components customized for specific tasks to perceive and interact with surroundings, enabling the building of scalable and repeatable workflows for dynamic manipulation tasks by accelerating AI model training and task programming. <img loading="lazy" decoding="async" class="size-full wp-image-70449 alignright" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/cuMotion.gif" alt="" width="320" height="240" /></p>
<p>“Incorporating new tools for foundation model generation into the Isaac platform accelerates the development of smarter, more flexible robots that can be generalized to do many tasks,&#8221; said Deepu Talla, vice president of robotics and edge computing at NVIDIA.</p>
<p>Leading robotics companies Yaskawa, Solomon, PickNik Robotics, READY Robotics, Franka Robotics, and Universal Robots, a Teradyne company, are partnering with NVIDIA to bring Isaac Manipulator to their customers.</p>
<p>&#8220;By bringing NVIDIA AI tools and capabilities to Yaskawa&#8217;s automation solutions, we&#8217;re pushing the boundaries of where robots can be deployed across industries,“ said Masahiro Ogawa, President,  Yaskawa. “This will significantly influence various industries.&#8221;</p>
<p><img loading="lazy" decoding="async" class="alignnone size-medium wp-image-70412" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/LeftMacNCheese-400x302.gif" alt="" width="400" height="302" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/LeftMacNCheese-400x302.gif 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/LeftMacNCheese-284x215.gif 284w, https://blogs.nvidia.com/wp-content/uploads/2024/03/LeftMacNCheese-132x100.gif 132w" sizes="(max-width: 400px) 100vw, 400px" /></p>
<p>NVIDIA is introducing foundation models to augment existing robot manipulation systems. These will help develop robots to sense, adapt and reprogram for varied environments and applications in smart manufacturing, handling pick-and-place tasks, machine tending and assembly with the following:</p>
<p><img loading="lazy" decoding="async" class="wp-image-70527 alignright" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/FoundationGrasp-Lego-Jensen-v2.gif" alt="" width="455" height="256" /></p>
<ul>
<li style="font-weight: 400;" aria-level="1"><a href="https://arxiv.org/pdf/2312.08344.pdf">FoundationPose</a> is a pioneering foundation model for 6D pose estimation and tracking of previously unseen objects.</li>
<li style="font-weight: 400;" aria-level="1">cuMotion taps into the parallel processing of NVIDIA GPUs for solving robot motion planning problems at industrial scale by running many trajectory optimizations at the same time to provide the best solution.</li>
<li style="font-weight: 400;" aria-level="1">FoundationGrasp is a transformer based model that can make dense grasp predictions for unknown 3D objects.</li>
<li style="font-weight: 400;" aria-level="1">SyntheticaDETR is an object detection model for indoor environments that allows faster detection, rendering and training with new objects.</li>
</ul>
<h2><b>Introducing Isaac Perceptor for Autonomous Mobile Robots Visual AI</b></h2>
<p><img loading="lazy" decoding="async" class="alignnone wp-image-70425 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/NovaOrin-400x132.jpg" alt="" width="724" height="239" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/NovaOrin-400x132.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NovaOrin-406x134.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NovaOrin-188x62.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NovaOrin.jpg 624w" sizes="(max-width: 724px) 100vw, 724px" /></p>
<p>Manufacturing and fulfillment operations are adopting <a href="https://blogs.nvidia.com/blog/isaac-amr-nova-orin-autonomous-mobile-robots/">autonomous mobile robots</a> (AMRs) to improve efficiency and worker safety as well as to reduce error rates and costs.</p>
<p>Isaac Perceptor provides multi-camera, 360-degree vision capabilities, offering early industry partners  such as ArcBest, BYD and KION Group advanced visual AI for their AMR installations that assist in material handling operations.</p>
<p>The NVIDIA Nova Orin DevKit — created in collaboration with <a href="https://robotics.segway.com/nova-dev-kit/">Segway Robotics</a> and <a href="https://leopardimaging.com/nvidia-nova-devkit/">Leopard Imaging</a> — allows companies to quickly develop, evaluate and deploy Isaac Perceptor.</p>
<p>“ArcBest is collaborating with NVIDIA to bring leading-edge machine vision technology into the logistics space,” said Michael Newcity, chief innovation officer of ArcBest and president of ArcBest Technologies. “Using the Isaac Perceptor platform in our Vaux Smart Autonomy AMR forklifts and reach trucks enables better perception, semantic-aware navigation and 3D mapping for obstacle detection in material handling processes across warehouses, distribution centers and manufacturing facilities.”</p>
<h2><b>Project GR00T for Humanoid Robotics Development Takes a Bow</b></h2>
<p>Demonstrated at GTC, GR00T-powered humanoid robots can take multimodal instructions — text, video and demonstrations — as well as their previous interactions to produce the desired action for the robot. GR00T was shown on four humanoid robots from different companies, including Agility Robotics, <a href="https://apptronik.com/news-collection/apptronik-collaborates-with-nvidia">Apptronik</a>, Fourier Intelligence and Unitree Robotics.</p>
<p>Humanoid robots are complex systems that require heterogeneous computing to meet the needs of high frequency low level controls, sensor fusion and perception, task planning and human-robot interaction. NVIDIA unveiled a new Jetson Thor-based computer for humanoid robots, built on the NVIDIA Thor SoC.</p>
<p>Jetson Thor includes a next-generation GPU based on the NVIDIA Blackwell Architecture with a transformer engine delivering 800 teraflops of 8-bit floating point AI performance to run multimodal generative AI models like GR00T. With an integrated functional safety processor, a high-performance CPU cluster and 100GB of ethernet bandwidth, it significantly simplifies design and integration efforts.</p>
<p>&nbsp;</p>
<p><img loading="lazy" decoding="async" class="size-medium wp-image-70534 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc24-issac-robotics-blog-3167701-1280x680-1-400x213.jpg" alt="" width="400" height="213" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc24-issac-robotics-blog-3167701-1280x680-1-400x213.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc24-issac-robotics-blog-3167701-1280x680-1-672x357.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc24-issac-robotics-blog-3167701-1280x680-1-768x408.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc24-issac-robotics-blog-3167701-1280x680-1-842x450.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc24-issac-robotics-blog-3167701-1280x680-1-406x215.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc24-issac-robotics-blog-3167701-1280x680-1-188x100.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc24-issac-robotics-blog-3167701-1280x680-1.jpg 1280w" sizes="(max-width: 400px) 100vw, 400px" /></p>
<p>&nbsp;</p>
<p>Project GR00T uses Isaac tools that are available to robotics developers for building and testing foundation models. These include Isaac Lab, a new lightweight simulation app built in Isaac Sim to train this humanoid robot model at scale, and OSMO, a cloud workflow orchestration platform for managing the training and simulation workloads.</p>
<h2><b>Accelerating Robot Learning With Isaac Lab</b></h2>
<p>Robots that require advanced locomotion skills, whether with walking or grasping, need to use deep reinforcement learning in a simulated environment and be trained repeatedly in a virtual environment to learn skills. However, this utility becomes more useful when the model transfers to the real robot deployment, which has been demonstrated with Project GR00T.</p>
<p>As the successor to Isaac Gym, Isaac Lab benefits from <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> technologies for physics-informed, photorealistic, perception-based reinforcement learning tasks. Isaac Lab is an open-source, performance-optimized application for robot learning built on the Isaac Sim platform. It incorporates reinforcement learning APIs and a developer-friendly tasking framework.</p>
<h2><b>Enabling Cloud-Native Robotics Workflow Scheduling With NVIDIA OSMO </b></h2>
<p><img loading="lazy" decoding="async" class="wp-image-70531 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/OSMO-1.gif" alt="" width="938" height="528" /></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><a href="https://developer.nvidia.com/blog/scale-ai-enabled-robotics-development-workloads-with-nvidia-osmo/">NVIDIA OSMO</a> scales workloads across distributed environments. For robotics workloads with complex multi-stage and multi-container workflows, the platform provides a location-agnostic deployment option and dataset management and traceability features for deployed models.</p>
<p>“Boston Dynamics employs a range of machine learning, reinforcement learning and AI technologies to power our robots,” said Pat Marion, machine learning and perception lead at Boston Dynamics. “To effectively manage the large training workloads, we’re using NVIDIA OSMO, an infrastructure solution that lets our machine learning engineers streamline their workflows and dedicate their expertise to tackling the hard robotics problems.”</p>
<p>OSMO supports GR00T, for example, by concurrently running models on NVIDIA DGX for training and NVIDIA OVX servers for live reinforcement learning in simulation. This workload involves generating and training models iteratively in a loop. OSMO’s ability to manage and schedule workloads across distributed environments allows for the seamless coordination of DGX and OVX systems, enabling efficient and iterative model development. Once the model is ready for testing and validation, OSMO can uniquely orchestrate software-in-the-loop workflows on OVX (x86-64) as well as hardware-in-the-loop workflows with NVIDIA Jetson (aarch64) compute resources.</p>
<h2><b>Supporting the ROS Ecosystem of Developers</b></h2>
<p>NVIDIA joined the Open Source Robotics Alliance (OSRA) as a founding member and platinum sponsor. OSRA is a new initiative by Open Source Robotics Foundation to foster collaboration, innovation and technical guidance in the robotics community by supporting several open-source robotics projects, including the Robot Operating System (ROS).</p>
<p>“The increasing capability of autonomous robots is driving a rise in demand for more powerful but still energy-efficient onboard computing,” said Vanessa Yamzon Orsi, CEO of Open Robotics. “The ROS community is experiencing this demand firsthand, and our users are increasingly taking advantage of advanced accelerated computing hardware from industry leaders such as NVIDIA.&#8221;</p>
<p><i>NVIDIA Isaac Perceptor with Nova Orin evaluation kit, Isaac Manipulator, Isaac Lab and OSMO will be made available to customers and partners in the second quarter of this year. </i><a href="https://developer.nvidia.com/project-gr00t"><i>Learn more about Project GR00T</i></a><i>. </i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/Image-Isaac-Robotics-GenAI-Expansion.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/Image-Isaac-Robotics-GenAI-Expansion-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Isaac Taps Generative AI for Manufacturing and Logistics Applications]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Omniverse Expands Worlds Using Apple Vision Pro</title>
		<link>https://blogs.nvidia.com/blog/omniverse-apple-vision-pro/</link>
		
		<dc:creator><![CDATA[Max Bickley]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 21:45:36 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Virtual Reality]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Rendering]]></category>
		<category><![CDATA[Universal Scene Description]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70350</guid>

					<description><![CDATA[NVIDIA is bringing OpenUSD-based Omniverse enterprise digital twins to the Apple Vision Pro. Announced today at NVIDIA GTC, a new software framework built on Omniverse Cloud APIs, or application programming interfaces, lets developers easily send their Universal Scene Description (OpenUSD) industrial scenes from their content creation applications to the NVIDIA Graphics Delivery Network (GDN), a		<a class="read-more" href="https://blogs.nvidia.com/blog/omniverse-apple-vision-pro/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>NVIDIA is bringing OpenUSD-based Omniverse enterprise digital twins to the Apple Vision Pro.</p>
<p>Announced today at NVIDIA GTC, a new software framework built on <a href="https://nvidianews.nvidia.com/news/omniverse-cloud-apis-industrial-digital-twin">Omniverse Cloud APIs</a>, or application programming interfaces, lets developers easily send their <a href="https://www.nvidia.com/en-us/omniverse/usd/">Universal Scene Description (OpenUSD)</a> industrial scenes from their content creation applications to the <a href="https://www.nvidia.com/en-us/omniverse/solutions/stream-3d-apps/">NVIDIA Graphics Delivery Network (GDN)</a>, a global network of graphics-ready data centers that can stream advanced 3D experiences to Apple Vision Pro.</p>
<p>In a demo unveiled at the global AI conference, NVIDIA presented an interactive, physically accurate <a href="https://blogs.nvidia.com/blog/what-is-a-digital-twin/#:~:text=At%20GTC%2C%20NVIDIA%20unveiled%20Omniverse,for%20training%20deep%20neural%20networks.">digital twin</a> of a car streamed in full fidelity to Apple Vision Pro’s high-resolution displays.</p>
<p>The demo featured a designer wearing the Vision Pro, using a car configurator application developed by CGI studio <a href="https://www.katanaus.com/">Katana</a> on the Omniverse platform. The designer toggles through paint and trim options and even enters the vehicle — leveraging the power of spatial computing by blending 3D photorealistic environments with the physical world.</p>
<p><iframe loading="lazy" title="Supercharging Car Configurator Personalization With Generative AI" width="500" height="281" src="https://www.youtube.com/embed/LAVXuDK83iA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<h2><b>Bringing the Power of RTX Enterprise Cloud Rendering to Spatial Computing</b></h2>
<p>Spatial computing has emerged as a powerful technology for delivering immersive experiences and seamless interactions between people, products, processes and physical spaces. Industrial enterprise use cases require incredibly high-resolution displays and powerful sensors operating at high frame rates to make manufacturing experiences true to reality.</p>
<p>This new Omniverse-based workflow combines Apple Vision Pro groundbreaking high-resolution displays with NVIDIA’s powerful RTX cloud rendering to deliver spatial computing experiences with just the device and an internet connection.</p>
<p>This cloud-based approach allows real-time physically based renderings to be streamed seamlessly to Apple Vision Pro, delivering high-fidelity visuals without compromising details of the massive, engineering fidelity datasets.</p>
<p>“The breakthrough ultra-high-resolution displays of Apple Vision Pro, combined with photorealistic rendering of OpenUSD content streamed from NVIDIA accelerated computing, unlocks an incredible opportunity for the advancement of immersive experiences,” said Mike Rockwell, vice president of the Vision Products Group at Apple. “Spatial computing will redefine how designers and developers build captivating digital content, driving a new era of creativity and engagement.”</p>
<p>“Apple Vision Pro is the first untethered device which allows for enterprise customers to realize their work without compromise,” said Rev Lebaredian, vice president of simulation at NVIDIA. “We look forward to our customers having access to these amazing tools.”</p>
<p>The workflow also introduces hybrid rendering, a groundbreaking technique that combines local and remote rendering on the device. Users can render fully interactive experiences in a single application from Apple’s native <a href="https://developer.apple.com/xcode/swiftui/">SwiftUI</a> and <a href="https://developer.apple.com/augmented-reality/realitykit/">Reality Kit</a> with the Omniverse RTX Renderer streaming from GDN.</p>
<p>NVIDIA GDN, available in over 130 countries, taps NVIDIA’s global cloud-to-edge streaming infrastructure to deliver smooth, high-fidelity, interactive experiences. By moving heavy compute tasks to GDN, users can tackle the most demanding rendering use cases, no matter the size or complexity of the dataset.</p>
<h2><b>Enhancing Spatial Computing Workloads Across Use Cases</b></h2>
<p>The Omniverse-based workflow showed potential for a wide range of use cases. For example, designers could use the technology to see their 3D data in full fidelity, with no loss in quality or model decimation. This means designers can interact with trustworthy simulations that look and behave like the real physical product. This also opens new channels and opportunities for e-commerce experiences.</p>
<p>In industrial settings, factory planners can view and interact with their full engineering factory datasets, letting them optimize their workflows and identify potential bottlenecks.</p>
<p>For developers and independent software vendors, NVIDIA is building the capabilities that would allow them to use the native tools on Apple Vision Pro to seamlessly interact with existing data in their applications.</p>
<p>Learn more about <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> and <a href="https://www.nvidia.com/en-us/omniverse/solutions/stream-3d-apps/">GDN</a>.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc-ov-corp-blog-announcement-vision-pro-1280x680-1.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc-ov-corp-blog-announcement-vision-pro-1280x680-1-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Omniverse Expands Worlds Using Apple Vision Pro]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA and Siemens Bring Immersive Visualization and Generative AI to Industrial Design and Manufacturing</title>
		<link>https://blogs.nvidia.com/blog/siemens-immersive-visualization-generative-ai/</link>
		
		<dc:creator><![CDATA[Rev Lebaredian]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 21:42:10 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70366</guid>

					<description><![CDATA[Generative AI and digital twins are changing the way companies in multiple industries design, manufacture and operate their products. Siemens, a leading technology company for automation, digitalization and sustainability, announced today at NVIDIA GTC that it is expanding its partnership with NVIDIA by adopting new NVIDIA Omniverse Cloud APIs, or application programming interfaces, with its		<a class="read-more" href="https://blogs.nvidia.com/blog/siemens-immersive-visualization-generative-ai/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Generative AI and digital twins are changing the way companies in multiple industries design, manufacture and operate their products.</p>
<p>Siemens, a leading technology company for automation, digitalization and sustainability, announced today at <a href="https://www.nvidia.com/gtc/">NVIDIA GTC</a> that it is expanding its partnership with NVIDIA by adopting new <a href="https://nvidianews.nvidia.com/news/omniverse-cloud-apis-industrial-digital-twin">NVIDIA Omniverse Cloud APIs</a>, or application programming interfaces, with its <a href="https://www.sw.siemens.com/en-US/digital-transformation/">Siemens Xcelerator</a> platform applications, starting with Teamcenter X. Teamcenter X is Siemens’ industry-leading cloud-based product lifecycle management (PLM) software.</p>
<p><a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> is a platform of APIs and services based on <a href="https://www.nvidia.com/en-us/omniverse/usd/">Universal Scene Description (OpenUSD)</a> that enables developers to build <a href="https://www.nvidia.com/en-us/glossary/generative-ai/#:~:text=Generative%20AI%20enables%20users%20to,or%20other%20types%20of%20data.">generative AI</a>-powered tools, applications and services for industrial <a href="https://blogs.nvidia.com/blog/what-is-a-digital-twin/">digital twins</a> and automation.</p>
<p>Enterprises of all sizes depend on Teamcenter software, part of the Siemens Xcelerator platform, to develop and deliver products at scale. By connecting <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> with Teamcenter X, Siemens will be able to provide engineering teams with the ability to make their physics-based digital twins more immersive and photorealistic, helping eliminate workflow waste and reduce errors.</p>
<p>Through the use of Omniverse APIs, workflows such as applying materials, lighting environments and other supporting scenery assets in physically based renderings will be dramatically accelerated using generative AI.</p>
<p>AI integrations will also allow engineering data to be contextualized as it would appear in the real world, allowing other stakeholders — from sales and marketing teams to decision-makers and customers — to benefit from deeper insight and understanding of real-world product appearance.</p>
<h2>Unifying and Visualizing Complex Industrial Datasets</h2>
<p>Traditionally, companies have relied heavily on physical prototypes and costly modifications to complete large-scale industrial projects and build complex, connected products. That approach is expensive and error-prone, limits innovation and slows time to market.</p>
<p>By connecting Omniverse Cloud APIs to the Xcelerator platform, Siemens will enable its customers to enhance their digital twins with physically based rendering, helping supercharge industrial-scale design and manufacturing projects. With the ability to connect generative AI APIs or agents, users can effortlessly generate 3D objects or high-dynamic range image backgrounds to view their assets in context.</p>
<p>This means that companies like <a href="https://english.hhi.co.kr/">HD Hyundai</a>, a leader in sustainable ship manufacturing, can unify and visualize complex engineering projects directly within Teamcenter X. At NVIDIA GTC, Siemens and NVIDIA demonstrated how HD Hyundai could use the software to visualize digital twins of liquified natural gas carriers, which can comprise over 7 million discrete parts, helping validate their product before moving to production.</p>
<p>Interoperable, photoreal and physics-based digital twins like these accelerate engineering collaboration and allow customers to minimize workflow waste, save time and costs, and reduce risk of manufacturing defects.</p>
<p><iframe loading="lazy" title="Siemens Teamcenter X Powered by NVIDIA Omniverse APIs" width="500" height="281" src="https://www.youtube.com/embed/rrb2tPHiLRo?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<h2>Combining Digital and Physical Worlds With Omniverse APIs</h2>
<p>Omniverse Cloud APIs enable data interoperability and physically based rendering for industrial-scale design and manufacturing projects in Teamcenter X. This starts with a real-time, embedded, photoreal viewport powered by the USD Render and USD Write APIs, which engineers can use to interactively navigate, edit and iterate on a shared model of their live data.</p>
<p>The USD Query API lets Teamcenter X users navigate and interact with physically accurate scenes, while the USD Notify API automatically provides real-time design and scene updates. To facilitate cloud-based collaboration and data exchange, Teamcenter X will leverage the Omniverse Channel API to establish a secure connection between multiple users across devices.</p>
<p>In the future, Siemens plans to bring NVIDIA accelerated computing, generative AI and Omniverse to more of its Siemens Xcelerator portfolio.</p>
<p>Learn more about <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a>, <a href="https://www.sw.siemens.com/en-US/digital-transformation/">Siemens Xcelerator</a> and the <a href="https://www.nvidia.com/en-us/industrial/industrial-sector/siemens">partnership</a>.</p>
<p><i>Get started with </i><a href="https://www.nvidia.com/en-us/omniverse/"><i>NVIDIA Omniverse</i></a><i>, access</i><a href="https://developer.nvidia.com/usd"> <i>OpenUSD</i></a><i> resources, and learn how</i><a href="https://www.nvidia.com/en-us/omniverse/enterprise/"> <i>Omniverse Enterprise can connect your team</i></a><i>. Stay up to date on</i><a href="https://www.instagram.com/nvidiaomniverse/"> <i>Instagram</i></a><i>,</i><a href="https://medium.com/@nvidiaomniverse"> <i>Medium</i></a><i> and</i><a href="https://twitter.com/nvidiaomniverse"> <i>Twitter</i></a><i>. For more, join the</i><a href="https://www.nvidia.com/en-us/omniverse/community/"> <i>Omniverse community</i></a><i> on the </i><a href="https://forums.developer.nvidia.com/c/omniverse/300"><i> </i><i>forums</i></a><i>,</i><a href="https://discord.com/invite/XWQNJDNuaC"> <i>Discord server</i></a><i>,</i><a href="https://www.twitch.tv/nvidiaomniverse"> <i>Twitch</i></a><i> and</i><a href="https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA"> <i>YouTube</i></a><i> channels.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc-ov-corp-blog-announcement-siemens-1280x680-1.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc-ov-corp-blog-announcement-siemens-1280x680-1-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA and Siemens Bring Immersive Visualization and Generative AI to Industrial Design and Manufacturing]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Supercharges Autonomous System Development with Omniverse Cloud APIs</title>
		<link>https://blogs.nvidia.com/blog/omniverse-cloud-apis/</link>
		
		<dc:creator><![CDATA[Zvi Greenstein]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 21:40:58 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70372</guid>

					<description><![CDATA[While simulation is critical for training, testing and deploying autonomy,  achieving real-world fidelity is incredibly challenging. It requires accurate modeling of the physics and behavior of an autonomous system’s sensors and surroundings. Designed to address this challenge by delivering large-scale, high-fidelity sensor simulation, Omniverse Cloud APIs, announced today at NVIDIA GTC, are poised to accelerate		<a class="read-more" href="https://blogs.nvidia.com/blog/omniverse-cloud-apis/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>While simulation is critical for training, testing and deploying autonomy,  achieving real-world fidelity is incredibly challenging.</p>
<p>It requires accurate modeling of the physics and behavior of an autonomous system’s sensors and surroundings.</p>
<p>Designed to address this challenge by delivering large-scale, high-fidelity sensor simulation, <a href="https://nvidianews.nvidia.com/news/omniverse-cloud-apis-industrial-digital-twin">Omniverse Cloud APIs</a>, announced today at <a href="https://www.nvidia.com/gtc/">NVIDIA GTC</a>, are poised to accelerate the path to autonomy. They bring together a rich ecosystem of simulation tools, applications and sensors.</p>
<p><iframe loading="lazy" title="Autonomous Vehicle Sensor Simulation, Powered by Omniverse Cloud APIs" width="500" height="281" src="https://www.youtube.com/embed/XeHtw36h-eI?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>The application programming interfaces address the critical need for high-fidelity sensor simulations to safely explore the myriad real-world scenarios autonomous systems will encounter.</p>
<p>In addition, the Omniverse Cloud platform offers application developers access to a range of powerful <a href="https://www.nvidia.com/en-us/omniverse/usd/">Universal Scene Description</a> (OpenUSD), <a href="https://developer.nvidia.com/rtx/ray-tracing">RTX</a> and generative AI-enabled service-level cloud APIs to bring interoperability and physically based rendering to next-generation tools.</p>
<h2>Simulation Key to Unlocking New Levels of Safety</h2>
<p>As demand increases for robots, AVs, and other AI systems, developers are seeking to accelerate their workflows. Sensor data powers these systems’ perception capabilities, enabling them to comprehend their environment and make informed decisions in real time.</p>
<p>Traditionally, developers have used real-world data for training, testing and validation.</p>
<p>However, these methods are limited in covering rare scenarios or data that can’t be captured in the real world. Sensor simulation provides a seamless way to effectively test countless “what if” scenarios and diverse environmental conditions.</p>
<p>With Omniverse Cloud APIs, developers can enhance the workflows they’re already using with high-fidelity sensor simulation to tackle the challenge of developing full-stack autonomy.</p>
<p>This not only streamlines the development process but also lowers the barriers to entry for companies of virtually all sizes developing autonomous machines.</p>
<h2>The Ecosystem Advantage</h2>
<p>By bringing together an expansive ecosystem of simulators, verification and validation (V&amp;V) tools, content and sensor developers, the Omniverse Cloud APIs enable a universal environment for AI system development.</p>
<p>Developers and software vendors such as <a href="https://carla.org/2024/03/18/nvidia-omniverse-cloud-apis/">CARLA</a>, MathWorks, <a href="https://www.mitre.org/news-insights/fact-sheet/mitre-digital-proving-ground">MITRE</a>, <a href="https://www.foretellix.com/end-to-end-av-development-nvidia-omniverse/">Foretellix</a> and <a href="https://www.prnewswire.com/news-releases/voxel51-accelerates-autonomous-vehicle-development-with-nvidia-omniverse-integration-302091979.html">Voxel51</a> underscore the broad appeal of these APIs in autonomous vehicles.</p>
<p><a href="http://carla.org">CARLA</a> is an open-source AV simulator used by more than 100,000 developers. With Omniverse Cloud APIs, CARLA users can enhance their existing workflows with high-fidelity sensor simulation.</p>
<p>Similarly, <a href="https://www.mitre.org/">MITRE</a>, a nonprofit that operates federally funded R&amp;D centers and is dedicated to improving safety in technology, is building a Digital Proving Ground for the AV industry to validate self-driving solutions. The DPG will use the Omniverse APIs to enable core sensor simulation capabilities for their developers.</p>
<p><a href="https://www.mathworks.com/">MathWorks</a> and <a href="https://www.foretellix.com/">Foretellix</a> provide critical simulation tools for authoring, executing, monitoring, and debugging of testing scenarios. As <a href="https://www.youtube.com/watch?v=XeHtw36h-eI">the GTC demo showed</a>, combining such simulation and test automation tools with the APIs forms a powerful test environment for AV development. On the showfloor, Foretellix is showing an in-depth look at this solution in Booth 630.</p>
<p>And, by integrating the APIs with <a href="https://voxel51.com">Voxel51’s</a> FiftyOne platform, developers can easily visualize and organize ground-truth data generated in simulation for streamlined training and testing.</p>
<p>Leading industrial-sensor solution provider <a href="https://www.sick.com/us/en/">SICK AG</a> is working on integrating these APIs in its sensor development process to reduce the number of physical prototypes, iterate quickly on design modifications and validate the eventual performance. These validated sensor models can eventually be used by autonomous systems developers in their applications.</p>
<p>Developers will also have access to sensor models from a variety of manufacturers, including lidar makers <a href="https://www.hesaitech.com/">Hesai</a>, <a href="https://innoviz.tech/">Innoviz Technologies</a>, <a href="https://www.luminartech.com/">Luminar</a>, <a href="https://microvision.com/">MicroVision</a>, <a href="https://www.robosense.ai/en">Robosense</a>, and <a href="https://www.seyond.com/">Seyond</a>, visual sensor suppliers <a href="https://www.ovt.com/">OMNIVISION,</a> <a href="https://www.onsemi.com/">onsemi</a>, and <a href="https://www.sony-semicon.com/en/index.html">Sony Semiconductor Solutions</a>, and <a href="https://www.continental.com/en/">Continental</a>, <a href="https://www.hella.com/techworld/us/Forvia-72713/">FORVIA HELLA</a>, and <a href="https://arberobotics.com/">Arbe</a> for radar.</p>
<p>Additionally, AI/ML developers can call on these APIs to generate large and diverse sets of <a href="https://www.nvidia.com/en-us/use-cases/synthetic-data/">synthetic data</a> — critical input for training and validating perception models that power these autonomous systems.</p>
<h2>Empowering Developers and Accelerating Innovation</h2>
<p>By reducing the traditional barriers to high-fidelity sensor simulation, NVIDIA Omniverse Cloud APIs empower developers to address complex AI problems without significant infrastructure overhauls.</p>
<p>This democratization of access to advanced simulation tools promises to accelerate innovation, allowing developers to quickly adapt to and integrate the latest technological advancements into their testing and development processes.</p>
<p><a href="https://developer.nvidia.com/omniverse">Apply here</a> for early access to Omniverse Cloud APIs.</p>
<p><i>Get started with </i><a href="https://www.nvidia.com/en-us/omniverse/"><i>NVIDIA Omniverse</i></a><i>, access</i><a href="https://developer.nvidia.com/usd"> <i>OpenUSD</i></a><i> resources, and learn how</i><a href="https://www.nvidia.com/en-us/omniverse/enterprise/"> <i>Omniverse Enterprise can connect your team</i></a><i>. Stay up to date on</i><a href="https://www.instagram.com/nvidiaomniverse/"> <i>Instagram</i></a><i>,</i><a href="https://medium.com/@nvidiaomniverse"> <i>Medium</i></a><i> and</i><a href="https://twitter.com/nvidiaomniverse"> <i>Twitter</i></a><i>. For more, join the</i><a href="https://www.nvidia.com/en-us/omniverse/community/"> <i>Omniverse community</i></a><i> on the </i><a href="https://forums.developer.nvidia.com/c/omniverse/300"><i> </i><i>forums</i></a><i>,</i><a href="https://discord.com/invite/XWQNJDNuaC"> <i>Discord server</i></a><i>,</i><a href="https://www.twitch.tv/nvidiaomniverse"> <i>Twitch</i></a><i> and</i><a href="https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA"> <i>YouTube</i></a><i> channels.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc-ov-corp-blog-announcement-fisheye-1280x680-1.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc-ov-corp-blog-announcement-fisheye-1280x680-1-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Supercharges Autonomous System Development with Omniverse Cloud APIs]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Staying in Sync: NVIDIA Combines Digital Twins With Real-Time AI for Industrial Automation</title>
		<link>https://blogs.nvidia.com/blog/ai-digital-twins-industrial-automation-demo/</link>
		
		<dc:creator><![CDATA[Adam Scraba]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 21:38:41 +0000</pubDate>
				<category><![CDATA[Robotics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[GTC 2024]]></category>
		<category><![CDATA[Industrial and Manufacturing]]></category>
		<category><![CDATA[Isaac]]></category>
		<category><![CDATA[Metropolis]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<category><![CDATA[Visual Computing]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70267</guid>

					<description><![CDATA[Real-time AI is helping with the heavy lifting in manufacturing, factory logistics and robotics. In such industries — often involving bulky products, expensive equipment, cobot environments and logistically complex facilities — a simulation-first approach is ushering in the next phase of automation. NVIDIA founder and CEO Jensen Huang today demonstrated in his GTC keynote how		<a class="read-more" href="https://blogs.nvidia.com/blog/ai-digital-twins-industrial-automation-demo/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Real-time AI is helping with the heavy lifting in manufacturing, factory logistics and robotics.</p>
<p>In such industries — often involving bulky products, expensive equipment, cobot environments and logistically complex facilities — a simulation-first approach is ushering in the next phase of automation.</p>
<p>NVIDIA founder and CEO Jensen Huang today demonstrated in his <a href="https://www.nvidia.com/gtc/keynote/" target="_blank" rel="noopener">GTC keynote</a> how developers can use <a href="https://blogs.nvidia.com/blog/what-is-a-digital-twin/" target="_blank" rel="noopener">digital twins</a> to develop, test and refine their large-scale, real-time AIs entirely in simulation before rolling them out in industrial infrastructure, saving significant time and cost.</p>
<p>NVIDIA <a href="https://www.nvidia.com/en-us/omniverse/" target="_blank" rel="noopener">Omniverse</a>, <a href="https://www.nvidia.com/en-us/autonomous-machines/intelligent-video-analytics-platform/" target="_blank" rel="noopener">Metropolis</a>, <a href="https://www.nvidia.com/en-us/deep-learning-ai/industries/robotics/" target="_blank" rel="noopener">Isaac</a> and <a href="https://www.nvidia.com/en-us/ai-data-science/products/cuopt/" target="_blank" rel="noopener">cuOpt</a> interact in AI gyms where developers can train AI agents to help robots and humans navigate unpredictable or complex events.</p>
<p><iframe loading="lazy" title="Fusing Real-Time AI With Digital Twins" width="500" height="281" src="https://www.youtube.com/embed/JQdyzQdMRS0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe></p>
<p>In the demo above, a digital twin of a 100,000-square-foot warehouse — built using the NVIDIA Omniverse platform for developing and connecting <a href="https://www.nvidia.com/en-us/omniverse/usd/" target="_blank" rel="noopener">OpenUSD</a> applications — operates as a simulation environment for dozens of digital workers and multiple autonomous mobile robots (AMRs), vision AI agents and sensors.</p>
<p>Each AMR, running the NVIDIA <a href="https://developer.nvidia.com/isaac/perceptor" target="_blank" rel="noopener">Isaac Perceptor</a> multi-sensor stack, processes visual information from six sensors, all simulated in the digital twin.</p>
<p>At the same time, the NVIDIA Metropolis platform for vision AI creates a single centralized map of worker activity across the entire warehouse, fusing together data from 100 simulated ceiling-mounted camera streams with <a href="https://developer.nvidia.com/metropolis-microservices" target="_blank" rel="noopener">multi-camera tracking</a>. This centralized occupancy map helps inform optimal AMR routes calculated by the NVIDIA cuOpt engine for solving complex routing problems.</p>
<p>cuOpt, a record-breaking optimization AI microservice, solves complex routing problems with multiple constraints using GPU-accelerated evolutionary algorithms.</p>
<p>All of this happens in real time, while <a href="https://docs.nvidia.com/isaac/doc/cloud/01_overview.html" target="_blank" rel="noopener">Isaac Mission Control</a> coordinates the entire fleet using map data and route graphs from cuOpt to send and execute AMR commands.</p>
<h2><b>An AI Gym for Industrial Digitalization</b></h2>
<p><a href="https://developer.nvidia.com/blog/introduction-to-llm-agents/#:~:text=What%20is%20an%20AI%20agent%3F" target="_blank" rel="noopener">AI agents</a> can assist in large-scale industrial environments by, for example, managing fleets of robots in a factory or identifying streamlined configurations for human-robot collaboration in supply chain distribution centers. To build these complex agents, developers need digital twins that function as AI gyms — physically accurate environments for AI evaluation, simulation and training.</p>
<p>Such software-in-the-loop AI testing enables AI agents and AMRs to adapt to real-world unpredictability.</p>
<p>In the above demo, an incident occurs along an AMR’s planned route, blocking the path and preventing it from picking up a pallet. NVIDIA Metropolis updates an occupancy grid, mapping all humans, robots and objects in a single view. cuOpt then plans an optimal route, and the AMR responds accordingly to minimize downtime.</p>
<p>With Metropolis vision <a href="https://www.nvidia.com/en-us/ai-data-science/foundation-models/" target="_blank" rel="noopener">foundation models</a> powering the <a href="https://www.nvidia.com/en-us/solutions/robotics-and-edge-computing/vision-ai/visual-insight-agent" target="_blank" rel="noopener">NVIDIA Visual Insight Agent (VIA)</a> framework, AI agents can be built to help operations teams answer questions like, “What situation occurred in aisle three of the factory?” And the generative AI-powered agent offers immediate insights such as, “Boxes fell from the shelves at 3:30 p.m., blocking the aisle.”</p>
<p>Developers can use the VIA framework to build AI agents capable of processing large amounts of live or archived videos and images with vision-language models — whether deployed at the edge or in the cloud. This new generation of visual AI agents will help nearly every industry summarize, search and extract actionable insights from video using natural language.</p>
<p>All of these AI functions can be enhanced through continuous, simulation-based training and are deployed as modular <a href="https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/" target="_blank" rel="noopener">NVIDIA NIM</a> inference microservices.</p>
<p><i>Learn more about the latest advancements in </i><a href="https://www.nvidia.com/gtc/sessions/generative-ai/" target="_blank" rel="noopener"><i>generative AI</i></a><i> and </i><a href="https://www.nvidia.com/gtc/sessions/industrial-digitalization/" target="_blank" rel="noopener"><i>industrial digitalization</i></a><i> at </i><a href="https://www.nvidia.com/gtc/" target="_blank" rel="noopener"><i>NVIDIA GTC</i></a><i>, a global AI conference running through Thursday, March 21, at the San Jose Convention Center and online.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc24-metropolis-industrial-automation-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gtc24-metropolis-industrial-automation-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Staying in Sync: NVIDIA Combines Digital Twins With Real-Time AI for Industrial Automation]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
	</channel>
</rss>
