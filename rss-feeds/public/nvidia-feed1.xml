<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:media="http://search.yahoo.com/mrss/">

<channel>
	<title>NVIDIA Blog</title>
	<atom:link href="https://blogs.nvidia.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.nvidia.com/</link>
	<description></description>
	<lastBuildDate>Tue, 16 Apr 2024 16:05:13 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.4.3</generator>
	<item>
		<title>New NVIDIA RTX A400 and A1000 GPUs Enhance AI-Powered Design and Productivity Workflows</title>
		<link>https://blogs.nvidia.com/blog/ampere-rtx-a400-a1000-ai/</link>
		
		<dc:creator><![CDATA[Stacy Ozorio]]></dc:creator>
		<pubDate>Tue, 16 Apr 2024 16:00:41 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[Rendering]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71084</guid>

					<description><![CDATA[AI integration across design and productivity applications is becoming the new standard, fueling demand for advanced computing performance. This means professionals and creatives will need to tap into increased compute power, regardless of the scale, complexity or scope of their projects. To meet this growing need, NVIDIA is expanding its RTX professional graphics offerings with		<a class="read-more" href="https://blogs.nvidia.com/blog/ampere-rtx-a400-a1000-ai/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>AI integration across design and productivity applications is becoming the new standard, fueling demand for advanced computing performance. This means professionals and creatives will need to tap into increased compute power, regardless of the scale, complexity or scope of their projects.</p>
<p>To meet this growing need, NVIDIA is expanding its <a href="https://www.nvidia.com/en-us/design-visualization/desktop-graphics/">RTX professional graphics</a> offerings with two new <a href="https://www.nvidia.com/en-us/design-visualization/ampere-architecture/">NVIDIA Ampere architecture</a>-based GPUs for desktops: the NVIDIA RTX A400 and NVIDIA RTX A1000.</p>
<p>They expand access to AI and ray-tracing technology, equipping professionals with the tools they need to transform their daily workflows.</p>
<h2><b>A New Era of Creativity, Performance and Efficiency</b></h2>
<p>The RTX A400 GPU introduces accelerated ray tracing and AI to the RTX 400 series GPUs. With 24 Tensor Cores for AI processing, it surpasses traditional CPU-based solutions, enabling professionals to run cutting-edge AI applications, such as intelligent chatbots and copilots, directly on their desktops.</p>
<p>The GPU delivers real-time ray tracing so creators can build vivid, physically accurate 3D renders that push the boundaries of creativity and realism.</p>
<p>The A400 also includes four display outputs, a first for its series. This makes it ideal for high-density display environments, which are critical for industries like financial services, command and control, retail, and transportation.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A400_Dark_KV_01_v004_AW.png"><img fetchpriority="high" decoding="async" class="aligncenter size-large wp-image-71142" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A400_Dark_KV_01_v004_AW-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A400_Dark_KV_01_v004_AW-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A400_Dark_KV_01_v004_AW-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A400_Dark_KV_01_v004_AW-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A400_Dark_KV_01_v004_AW-1536x864.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A400_Dark_KV_01_v004_AW.png 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A400_Dark_KV_01_v004_AW-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A400_Dark_KV_01_v004_AW-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A400_Dark_KV_01_v004_AW-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A400_Dark_KV_01_v004_AW-1280x720.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>The NVIDIA RTX A1000 GPU brings Tensor Cores and RT Cores to the RTX 1000 series GPUs for the first time, unlocking accelerated AI and ray-tracing performance for creatives and professionals.</p>
<p>With 72 Tensor Cores, the A1000 offers a tremendous upgrade over the previous generation, delivering over 3x faster generative AI processing for tools like Stable Diffusion. In addition, its 18 RT Cores speed graphics and rendering tasks by up to 3x, accelerating professional workflows such as 2D and 3D computer-aided design (CAD), product and architectural design, and 4K video editing.</p>
<p>The A1000 also excels in video processing, handling up to 38% more encode streams and offering 2x faster decode performance over the previous generation.</p>
<p>With a sleek, single-slot design and consuming just 50W, the A400 and A1000 GPUs bring impressive features to compact, energy-efficient workstations.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/A1000-3QTR-TopLeft-scaled.jpg"><img decoding="async" class="aligncenter wp-image-71145 size-medium" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/A1000-3QTR-TopLeft-400x298.jpg" alt="" width="400" height="298" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/A1000-3QTR-TopLeft-400x298.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/A1000-3QTR-TopLeft-672x500.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/A1000-3QTR-TopLeft-768x571.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/A1000-3QTR-TopLeft-1536x1143.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/A1000-3QTR-TopLeft-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/04/A1000-3QTR-TopLeft-605x450.jpg 605w, https://blogs.nvidia.com/wp-content/uploads/2024/04/A1000-3QTR-TopLeft-289x215.jpg 289w, https://blogs.nvidia.com/wp-content/uploads/2024/04/A1000-3QTR-TopLeft-134x100.jpg 134w, https://blogs.nvidia.com/wp-content/uploads/2024/04/A1000-3QTR-TopLeft-1280x952.jpg 1280w" sizes="(max-width: 400px) 100vw, 400px" /></a></p>
<h2><b>Expanding the Reach of RTX</b></h2>
<p>These new GPUs empower users with cutting-edge AI, graphics and compute capabilities to boost productivity and unlock creative possibilities. Advanced workflows involving ray-traced renders and AI are now within reach, allowing professionals to push the boundaries of their work and achieve stunning levels of realism.</p>
<p>Industrial planners can use ‌these new powerful and energy-efficient computing solutions for <a href="https://blogs.nvidia.com/blog/what-is-edge-computing/">edge</a> deployments. Creators can boost editing and rendering speeds to produce richer visual content. Architects and engineers can seamlessly transition ideas from 3D CAD concepts into tangible designs. Teams working in smart spaces can use the GPUs for real-time data processing, AI-enhanced security and digital signage management in space-constrained settings. And healthcare professionals can achieve quicker, more precise medical imaging analyses.</p>
<p>Financial professionals have always used expansive, high-resolution visual workspaces for more effective trading, analysis and data management. With the RTX A400 GPU supporting up to four 4K displays natively, financial services users can now achieve a high display density with fewer GPUs, streamlining their setups and reducing costs.</p>
<h2><b>Next-Generation Features and Accelerated Performance </b></h2>
<p>The NVIDIA RTX A400 and A1000 GPUs are equipped with features designed to supercharge everyday workflows, including:</p>
<ul>
<li><b>Second-generation RT Cores:</b> Real-time ray tracing, photorealistic, physically based rendering and visualization for all professional workflows, including architectural drafting, 3D design and content creation, where accurate lighting and shadow simulations can greatly enhance the quality of work.</li>
<li><b>Third-generation Tensor Cores: </b>Accelerates AI-augmented tools and applications such as generative AI, image rendering denoising and deep learning super sampling to improve image generation speed and quality.</li>
<li><b>Ampere architecture-based CUDA cores: </b>Up to 2x the single-precision floating point throughput of the previous generation for significant speedups in graphics and compute workloads<b>.</b></li>
<li><b>4GB or 8GB of GPU memory: </b>4GB of GPU memory with the A400 GPU and 8GB with the A1000 GPU accommodate a range of professional needs, from basic graphic design and photo editing to more demanding 3D modeling with textures or high-resolution editing and data analyses. The GPUs also feature increased memory bandwidth over the previous generation for quicker data processing and smoother handling of larger datasets and scenes.</li>
<li><b>Encode and decode engines: </b>With seventh-generation encode (<a href="https://developer.nvidia.com/video-codec-sdk">NVENC</a>) and fifth-generation decode (NVDEC) engines, the GPUs offer efficient video processing to support high-resolution video editing, streaming and playback with ultra-low latency. Inclusion of AV1 decode enables higher efficiency and smoother playback of more video formats.</li>
</ul>
<h2><b>Availability</b><b> </b></h2>
<p>The NVIDIA RTX A1000 GPU is now available through global distribution partners such as PNY and Ryoyo Electric. The RTX A400 GPU is expected to be available from channel partners starting in May, with anticipated availability from manufacturers in the summer.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A1000_Dark_KV_01_v008_AW.png"
			type="image/png"
			width="2048"
			height="1152"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/RTX_A1000_Dark_KV_01_v008_AW-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[New NVIDIA RTX A400 and A1000 GPUs Enhance AI-Powered Design and Productivity Workflows]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>To Cut a Long Story Short: Video Editors Benefit From DaVinci Resolve’s New AI Features Powered by RTX</title>
		<link>https://blogs.nvidia.com/blog/studio-davinci-resolve-sketchup-driver/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Tue, 16 Apr 2024 13:00:54 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[In the NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA Studio Driver]]></category>
		<category><![CDATA[Rendering]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71090</guid>

					<description><![CDATA[Blackmagic Design’s DaVinci Resolve released version 19 adding the IntelliTrack AI point tracker and UltraNR AI-powered features to further streamline video editing workflows.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p><i>Editor’s note: This post is part of our </i><a href="https://blogs.nvidia.com/blog/tag/in-the-nvidia-studio/"><i>In the NVIDIA Studio</i></a><i> series, which celebrates featured artists, offers creative tips and tricks, and demonstrates how </i><a href="https://www.nvidia.com/en-us/studio/"><i>NVIDIA Studio</i></a><i> technology improves creative workflows. We’re also deep diving on new </i><a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/"><i>GeForce RTX 40 Series GPU</i></a><i> features, technologies and resources, and how they dramatically accelerate content creation.</i></p>
<p>Video editors have more to look forward to than just April showers.</p>
<p>Blackmagic Design’s DaVinci Resolve released version 19, adding the IntelliTrack AI point tracker and UltraNR AI-powered features to further streamline video editing workflows.</p>
<p>The NAB 2024 trade show is bringing together thousands of content professionals from all corners of the broadcast, media and entertainment industries, with video editors and livestreamers seeking ways to improve their creative workflows with <a href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX</a> technology.</p>
<p>The recently launched Design app SketchUp 2024 introduced a new graphics engine that uses DirectX 12, which renders scenes 2.5x faster than the previous engine.</p>
<p>April also brings the latest NVIDIA Studio Driver, which optimizes the latest creative app updates, available for download today.</p>
<p>And this week’s featured <i>In the NVIDIA Studio </i>artist Rakesh Kumar created his captivating 3D scene <i>The Rooted Vault</i> using RTX acceleration.</p>
<h2><b>Video Editor’s DaVinci Code</b></h2>
<p>DaVinci Resolve is a powerful video editing package with color correction, visual effects, motion graphics and audio post-production all in one software tool. Its elegant, modern interface is easy to learn for new users, while offering powerful capabilities for professionals.</p>
<p>Two new AI features make video editing even more efficient: the IntelliTrack AI point tracker for object tracking, stabilization and audio panning, and UltraNR, which uses AI for spatial noise reduction — doing so 3x faster on the GeForce RTX 4090 vs. the Mac M2 Ultra.</p>
<p>All DaVinci Resolve AI effects are accelerated on RTX GPUs by <a href="https://developer.nvidia.com/tensorrt">NVIDIA TensorRT</a>, boosting AI performance by up to 2x. The update also includes acceleration for Beauty, Edge Detect and Watercolor effects, doubling performance on NVIDIA GPUs.</p>
<p>For more information, check out the DaVinci Resolve <a href="https://www.blackmagicdesign.com/products/davinciresolve">website</a>.</p>
<h2><b>SketchUp Steps Up</b></h2>
<p>SketchUp 2024 is a professional-grade 3D design software toolkit for designing buildings and landscapes, commonly used by designers and architects.</p>
<p>The new app, already receiving positive reviews, introduced a robust graphics engine that uses DirectX 12, which increases frames per second (FPS) by a factor of 2.5x over the previous engine. Navigating and orbiting complex models feels considerably lighter and faster with quicker, more predictable performance.</p>
<p>In testing, the scene below runs 4.5x faster FPS using the NVIDIA RTX 4090 vs. the Mac M2 Ultra and other competitors.</p>
<figure id="attachment_71119" aria-describedby="caption-attachment-71119" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1.png"><img decoding="async" class="size-large wp-image-71119" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/sketchup-studio-rakesh-kumar-wk105-audio-tagging-1280w-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71119" class="wp-caption-text">2.5x faster FPS with the GeForce RTX 4090 GPU. Image courtesy of Trimble SketchUp.</figcaption></figure>
<p>SketchUp 2024 also unlocks import and export functionality for <a href="https://www.nvidia.com/en-us/omniverse/usd/">OpenUSD</a> files to efficiently manage the interoperability of complex 3D scenes and animations across numerous 3D apps.</p>
<p>Get the <a href="https://blog.sketchup.com/article/unlock-enhanced-visuals-inside-sketchup-2024-2">full release details</a>.</p>
<h2><b>Art Rooted in Nature</b></h2>
<p>Rakesh Kumar’s passion for 3D modeling and animation stemmed from his love for gaming and storytelling.</p>
<p>“My goal is to inspire audiences and take them to new realms by showcasing the power of immersive storytelling, captivating visuals and the idea of creating worlds and characters that evoke emotions,” said Kumar.</p>
<p>His scene <i>The Rooted Vault</i> aims to convey the beauty of the natural world, transporting viewers to a serene setting filled with the soothing melodies of nature.</p>
<div style="width: 1280px;" class="wp-video"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video class="wp-video-shortcode" id="video-71090-1" width="1280" height="734" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-breakdown-video-1280w.mp4?_=1" /><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-breakdown-video-1280w.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-breakdown-video-1280w.mp4</a></video></div>
<p>&nbsp;</p>
<p>Kumar began by gathering reference material.</p>
<figure id="attachment_71103" aria-describedby="caption-attachment-71103" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w.png"><img loading="lazy" decoding="async" class="size-large wp-image-71103" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-pure-ref-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71103" class="wp-caption-text">There’s reference sheets … and then there’s reference sheets.</figcaption></figure>
<p>He then used Autodesk Maya to block out the basic structure and piece together the house as a series of modules. GPU-accelerated viewport graphics ensured fast, interactive 3D modeling and animations.</p>
<p>Next, Kumar used ZBrush to sculpt high-resolution details into the modular assets.</p>
<figure id="attachment_71115" aria-describedby="caption-attachment-71115" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w.png"><img loading="lazy" decoding="async" class="size-large wp-image-71115" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-tree-trunks-sculpted-in-zbrush-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71115" class="wp-caption-text">Fine details applied in ZBrush.</figcaption></figure>
<div class="simplePullQuote right"><p>“I chose an NVIDIA RTX GPU-powered system for real-time ray tracing to achieve lifelike visuals, reliable performance for smoother workflows, faster render times and industry-standard software compatibility.” — Rakesh Kumar</p>
</div>
<p>He used the ZBrush decimation tool alongside Unreal Engine’s Nanite workflow to efficiently create most of the modular building props.</p>
<p>Traditional poly-modeling workflows for the walls enabled vertex blending shaders for seamless texture transitions.</p>
<p>Textures were created with Adobe Substance 3D Painter. Kumar’s RTX GPU used RTX-accelerated light and ambient occlusion to bake and optimize assets in mere seconds.</p>
<p>Kumar moved the project to Unreal Engine 5, where near-final finishing touches such as lighting, shadows and visual effects were applied.</p>
<figure id="attachment_71112" aria-describedby="caption-attachment-71112" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w.png"><img loading="lazy" decoding="async" class="size-large wp-image-71112" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-rgba-mask-in-substance-painter-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71112" class="wp-caption-text">Textures applied in Adobe Substance 3D Painter.</figcaption></figure>
<p>GPU acceleration played a crucial role in real-time rendering, allowing him to instantly see and adjust the scene.</p>
<figure id="attachment_71125" aria-describedby="caption-attachment-71125" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1.png"><img loading="lazy" decoding="async" class="size-large wp-image-71125" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-premiere-pro-1280w-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71125" class="wp-caption-text">Adobe Premiere Pro has a vast selection of GPU-accelerated features.</figcaption></figure>
<p>Kumar then moved to Blackmagic Design’s DaVinci Resolve to color grade the scene for the desired mood and aesthetic, before he began final editing in Premiere Pro, adding transitions and audio.</p>
<p>“While the initial concept required significant revisions, the final result demonstrates the iterative nature of artistic creation — all inspired by my mentors, friends and family, who were always there to support me,” Kumar said.</p>
<figure id="attachment_71122" aria-describedby="caption-attachment-71122" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1.png"><img loading="lazy" decoding="async" class="size-large wp-image-71122" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-672x218.png" alt="" width="672" height="218" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-672x218.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-400x130.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-768x249.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-842x273.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-406x132.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1-188x61.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-artist-feature-1280w-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71122" class="wp-caption-text">3D artist Rakesh Kumar.</figcaption></figure>
<p>Check out Kumar’s latest work on <a href="https://www.instagram.com/rakesh__art/">Instagram</a>.</p>
<p><i>Follow NVIDIA Studio on </i><a href="https://www.instagram.com/nvidiastudio/"><i>Instagram</i></a><i>, </i><a href="https://twitter.com/NVIDIAStudio"><i>X</i></a><i> and </i><a href="https://www.facebook.com/NVIDIAStudio/"><i>Facebook</i></a><i>. Access tutorials on the </i><a href="https://www.youtube.com/channel/UCDeQdW6Lt6nhq3mLM4oLGWw"><i>Studio YouTube channel</i></a><i> and get updates directly in your inbox by subscribing to the </i><a href="https://www.nvidia.com/en-us/studio/?nvmid=subscribe-creators-mail-icon"><i>Studio newsletter</i></a><i>.</i><b> </b></p>
</div>]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2024/04/studio-rakesh-kumar-wk105-breakdown-video-1280w.mp4" length="1833156" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/roots-nv-blog-header-preview-1280x680-2.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/roots-nv-blog-header-preview-1280x680-2-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[To Cut a Long Story Short: Video Editors Benefit From DaVinci Resolve’s New AI Features Powered by RTX]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>AI Is Tech’s ‘Greatest Contribution to Social Elevation,’ NVIDIA CEO Tells Oregon State Students</title>
		<link>https://blogs.nvidia.com/blog/oregon-state-higher-ed/</link>
		
		<dc:creator><![CDATA[Brian Caulfield]]></dc:creator>
		<pubDate>Mon, 15 Apr 2024 13:00:59 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Education]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71074</guid>

					<description><![CDATA[Amid a flurry of investments in higher education, NVIDIA founder Jensen Huang highlights potential for AI to serve as a “collaborator” as alma mater breaks ground on $213 million research complex. ]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>AI promises to bring the full benefits of the digital revolution to billions across the globe, NVIDIA CEO Jensen Huang said Friday during a conversation with Oregon State University President Jayathi Murthy.</p>
<p>“I believe that artificial intelligence is the technology industry’s single greatest contribution to social elevation, to lift all of the people that have historically been left behind,” Huang told more than 2,000 faculty, students and staff gathered for his conversation with Murthy.</p>
<p>The talk was the highlight of a forum marking the groundbreaking for a new research building that will be named for Huang and his wife, Lori, both Oregon State alumni.</p>
<p>The facility positions Oregon State as a leader not just in the semiconductor industry but also at the intersection of high performance computing and a growing number of fields.</p>
<figure id="attachment_71079" aria-describedby="caption-attachment-71079" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="wp-image-71079 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-672x448.png" alt="" width="672" height="448" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-672x448.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-400x267.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-768x512.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-1536x1024.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-675x450.png 675w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-323x215.png 323w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-150x100.png 150w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking-1280x853.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/04/groundbreaking.png 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-71079" class="wp-caption-text">Friday’s event at Oregon State University followed the groundbreaking for the Jen-Hsun Huang and Lori Mills Huang Collaborative Innovation Complex. Image courtesy of Oregon State University.</figcaption></figure>
<p>Those innovations have world-changing implications.</p>
<p>Huang said those who know a programming language such as C++ typically have greater opportunities.</p>
<p>“Because programming is so hard, the number of people who have benefitted from this, putting it to use for their economic prosperity, has been limited,” Huang said.</p>
<p>AI unlocks that and more.</p>
<p>“So you essentially have a collaborator with you at all times, essentially have a tutor at all times, and so I think the ability for AI to elevate all of the people left behind is quite extraordinary,” he added.</p>
<p>Huang’s appearance in Corvallis, Oregon, capped off a week of announcements underscoring NVIDIA’s commitment to preparing the future workforce with advanced AI, data science and high performance computing training.</p>
<p>On Tuesday, NVIDIA announced that it would participate in a <a href="https://blogs.nvidia.com/blog/partnership-universities-teach-ai-skills/">$110 million partnership between Japan and the United States</a>, which would include funding for university research.</p>
<p>On Wednesday, <a href="https://coe.gatech.edu/news/2024/04/georgia-tech-unveils-new-ai-makerspace-collaboration-nvidia">Georgia Tech announced a new NVIDIA-powered supercomputer</a> that will help prepare undergraduate students to solve complex challenges with AI and HPC.</p>
<p>And later this month, NVIDIA founder Chris Malachowsky will be inducted into the Hall of Fame for the University of Florida’s Department of Electrical &amp; Computer Engineering, following the November inauguration of the university’s $150 million Malachowsky Hall for Data Science &amp; Information Technology.</p>
<h2>Educating Future Leaders for ‘New Industrial Revolution’</h2>
<p>NVIDIA has been investing in universities for decades, providing computing resources, advanced training curricula, donations and other support.</p>
<p>These contributions enable students and professors to access the high performance computing necessary for groundbreaking results at a key moment in the history of the industry.</p>
<p>“We’re at the beginning of a new industrial revolution, and the reason why I say that is because an industrial revolution produces something new that was impossible to produce in the past,” Huang said.</p>
<p>“And in this new world, you can apply electricity, and what’s going to come out of it is a whole bunch of floating-point numbers. We call them tokens, and those tokens are essentially artificial intelligence,” Huang said.</p>
<p>“And so this industrial revolution is going to be manufacturing intelligence at a very large scale,” Huang said.</p>
<h2>OSU Breaks Ground on $213 Million Research Complex</h2>
<p>Friday’s event in Oregon highlighted the Huangs’ commitment to education and reflected the couple’s deep personal ties to Oregon State, where the two met.</p>
<p>The conversation with Murthy followed the groundbreaking for the Jen-Hsun Huang and Lori Mills Huang Collaborative Innovation Complex, which took place Friday morning on the Corvallis campus.</p>
<p>When it opens in 2026, the 150,000-square-foot, $213 million complex — supported by a $50 million gift from the Huangs — will increase Oregon State’s support for the semiconductor and technology industry in Oregon and beyond.</p>
<p>Harnessing one of the nation’s most powerful NVIDIA supercomputers, the complex will bring together faculty and students to solve critical challenges facing the world in areas such as climate science, clean energy and water resources.</p>
<p>Huang sees the center — and AI — as helping put the benefits of computing at the service of people doing work across a broad range of disciplines.</p>
<p>Oregon State is one of the world’s premier schools in forestry, Huang said, adding that “let’s just face it, it’s very unlikely that somebody who was in forestry, it’s not impossible, but C++ is probably not your thing,” Huang said.</p>
<p>Thanks to ChatGPT, you can “now use a computer to apply it to your field of science and apply this computing technology to revolutionize your work.”</p>
<p>That makes learning how to think — and how to collaborate — more important than ever, Huang said. It’s “no different than if I gave you a partner to collaborate with you to solve problems,” Huang said.</p>
<p>“You still need to know how to collaborate, how to prompt, how to frame a problem, how to refine the solution, how to iterate on it and how to change your mind.”</p>
<h2>NVIDIA Joins $110 Million Partnership to Help Universities Teach AI Skills</h2>
<p>The groundbreaking at Oregon State is just one of several announcements highlighting NVIDIA’s global commitment to advancing the global technology industry.</p>
<p>Last week, the Biden Administration announced a new $110 million AI partnership between Japan and the United States, including an initiative to fund research through collaboration between the University of Washington and the University of Tsukuba.</p>
<p>As part of this, NVIDIA is committing $25 million to a collaboration with Amazon to bring the latest technologies to the University of Washington, in Seattle, and the University of Tsukuba, northeast of Tokyo.</p>
<h2>Georgia Tech Unveils New AI Makerspace in Collaboration With NVIDIA</h2>
<p>And on Wednesday, Georgia Tech’s College of Engineering established an AI supercomputing hub dedicated to teaching students.</p>
<p>The AI Makerspace was launched in collaboration with NVIDIA. College leaders call it a “digital sandbox” for students to understand and use AI. Initially focusing on undergraduates, the AI Makerspace aims to democratize access to computing resources typically reserved for researchers or technology companies.</p>
<p>Students will access the cluster online as part of their coursework. The Makerspace will also better position students after graduation as they work with AI professionals and help shape future applications.</p>
<h2>‘Beginning of a New World’</h2>
<p>To be sure, AI has limits, Huang explained. “It’s no different than when you work with teammates or lab partners; you’re guiding each other along because you know each other’s weaknesses and strengths,” he said.</p>
<p>However, Huang said now is a fantastic time to get an education and prepare for a career.</p>
<p>“This is the beginning of a new world and this is the best of times to go to school — the whole world is changing, right? New technology and new capabilities, new instruments and new ways to learn,” Huang said.</p>
<p><em>Images courtesy of Oregon State University.</em></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/oregon-state.jpg"
			type="image/jpeg"
			width="2048"
			height="1215"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/oregon-state-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[AI Is Tech’s ‘Greatest Contribution to Social Elevation,’ NVIDIA CEO Tells Oregon State Students]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Bethesda’s ‘Fallout’ Titles Join GeForce NOW</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-fallout-series/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 11 Apr 2024 13:00:16 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71056</guid>

					<description><![CDATA[Welcome to the wasteland, Vault Dwellers. Bethesda’s Fallout 4 and Fallout 76 are bringing post-nuclear adventures to the cloud. These highly acclaimed action role-playing games lead 10 new titles joining GeForce NOW this week. Announced as coming to GeForce NOW at CES, Honkai: Star Rail is targeting a release this quarter. Stay tuned for future		<a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-fallout-series/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Welcome to the wasteland, Vault Dwellers. Bethesda’s <i>Fallout 4</i> and <i>Fallout 76</i> are bringing post-nuclear adventures to the cloud.</p>
<p>These highly acclaimed action role-playing games lead 10 new titles joining <a href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a> this week.</p>
<p>Announced as <a href="https://blogs.nvidia.com/blog/ces-2024-geforce-now-activision-blizzard-day-passes-g-sync/">coming to GeForce NOW at CES</a>, <i>Honkai: Star Rail</i> is targeting a release this quarter. Stay tuned for future updates.</p>
<h2><b>Vault Into the Cloud</b></h2>
<p>Adventurers needed, whether for mapping the irradiated wasteland or shaping the fate of humanity.</p>
<figure id="attachment_71060" aria-describedby="caption-attachment-71060" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-71060" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-672x378.jpg" alt="Fallout 4 on GeForce NOW" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4-1280x720.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_4.jpg 1920w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-71060" class="wp-caption-text"><em>Don’t let Dogmeat venture out alone.</em></figcaption></figure>
<p>Embark on a journey through ruins of the post-apocalyptic Commonwealth in <i>Fallout 4</i>. As the sole survivor of Vault 111, navigate a world destroyed by nuclear war, make choices to reshape the wasteland and rebuild society one settlement at a time. With a vast, open world, dynamic crafting systems and a gripping storyline, the game offers an immersive single-player experience that challenges dwellers to emerge as beacons of hope for humanity’s remnants.</p>
<figure id="attachment_71063" aria-describedby="caption-attachment-71063" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-71063" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-672x378.jpg" alt="Fallout 76 on GeForce NOW" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Fallout_76-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-71063" class="wp-caption-text"><em>Dust off your Pip-Boy and stream &#8216;Fallout 76&#8217; from the cloud.</em></figcaption></figure>
<p>Plus, in <i>Fallout 76</i>, head back to the early days of post-nuclear Appalachia and experience the <i>Fallout</i> universe’s largest, most dynamic world. Encounter unique challenges, build portable player homes called C.A.M.P.s, and cooperate or compete with other survivors in the mountainous lands in West Virginia.</p>
<p>Join the proud ranks of Vault survivors in the cloud today and stream these titles, including Creation Club content for <i>Fallout 4</i>, across devices. With longer gaming sessions and faster access to servers, GeForce NOW members can play anywhere, anytime, and at up to 4K resolution, streaming with an <a href="http://geforcenow.com">Ultimate membership</a>. The games come just in time for those tuning into the <i>Fallout </i>series TV adaptation, released today, for a <i>Fallout</i>-filled week.</p>
<h2><b>Go Big or Go Home</b></h2>
<figure id="attachment_71066" aria-describedby="caption-attachment-71066" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-71066" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-672x336.jpg" alt="Gigantic: Rampage Edition on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Gigantic_Rampage_Edition.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-71066" class="wp-caption-text"><em>Larger than life MOBA now streaming on GeForce NOW.</em></figcaption></figure>
<p><i>Gigantic: Rampage Edition </i>promises big fun with epic 5v5 matches, crossplay support, an exciting roster of heroes and more. Rush to the cloud to jump into the latest game from Arc Games and team with four other players to control objectives and take down the opposing team’s mighty Guardian. Think fast, be bold and go gigantic!</p>
<p>Look forward to these new games this week:</p>
<ul>
<li><i>Gigantic: Rampage Edition </i>(New release on <a href="https://store.steampowered.com/app/1924490?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 9)</li>
<li><i>Inkbound 1.0</i> (New release, on <a href="https://store.steampowered.com/app/1062810/Inkbound/">Steam</a>, April 9)</li>
<li><i>Broken Roads </i>(New release on <a href="https://store.steampowered.com/app/1403440?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 10)</li>
<li><i>Infection Free Zone </i>(New release on <a href="https://store.steampowered.com/app/1465460?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 11)</li>
<li><i>Shadow of the Tomb Raider: Definitive Edition </i>(New release on <a href="https://www.xbox.com/games/store/shadow-of-the-tomb-raider-definitive-edition/BNQQ3WVBNZCQ?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass, April 11)</li>
<li><i>Backpack Battles </i>(<a href="https://store.steampowered.com/app/2427700?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Fallout 4 </i>(<a href="https://store.steampowered.com/app/377160?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Fallout 76 </i>(<a href="https://store.steampowered.com/app/1151340?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a href="https://www.xbox.com/games/store/fallout-76-pc/9nkgnmnk3k3z?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Ghostrunner </i>(<a href="https://www.epicgames.com/store/p/ghostrunner?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>, free April 11-18)</li>
<li><i>Terra Invicta</i> (<a href="https://www.xbox.com/games/store/terra-invicta-game-preview/9pb0glgxqz83?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
</ul>
<p>What are you planning to play this weekend? Let us know on <a href="https://www.twitter.com/nvidiagfn">X</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">can&#39;t help 𝙛𝙖𝙡𝙡ing into the cloud</p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f329.png" alt="🌩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a href="https://twitter.com/NVIDIAGFN/status/1778090566644801863?ref_src=twsrc%5Etfw">April 10, 2024</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-April_11.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-April_11-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Bethesda’s ‘Fallout’ Titles Join GeForce NOW]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Combating Corruption With Data: Cleanlab and Berkeley Research Group on Using AI-Powered Investigative Analytics  </title>
		<link>https://blogs.nvidia.com/blog/cleanlab-podcast/</link>
		
		<dc:creator><![CDATA[Brian Caulfield]]></dc:creator>
		<pubDate>Wed, 10 Apr 2024 13:00:49 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71052</guid>

					<description><![CDATA[Talk about scrubbing data. Curtis Northcutt, cofounder and CEO of Cleanlab, and Steven Gawthorpe, senior data scientist at Berkeley Research Group, speak about Cleanlab’s groundbreaking approach to data curation with Noah Kravitz, host of NVIDIA’s AI Podcast, in an episode recorded live at the NVIDIA GTC global AI conference. The startup’s tools enhance data reliability		<a class="read-more" href="https://blogs.nvidia.com/blog/cleanlab-podcast/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Talk about scrubbing data. Curtis Northcutt, cofounder and CEO of Cleanlab, and Steven Gawthorpe, senior data scientist at Berkeley Research Group, speak about Cleanlab’s groundbreaking approach to data curation with Noah Kravitz, host of NVIDIA’s <a href="https://soundcloud.com/theaipodcast">AI Podcast</a>, in an episode recorded live at the <a href="https://www.nvidia.com/gtc/">NVIDIA GTC</a> global AI conference. The startup’s tools enhance data reliability and trustworthiness through sophisticated error identification and correction algorithms. Northcutt and Gawthorpe provide insights into how AI-powered data analytics can help combat economic crimes and corruption and discuss the intersection of AI, data science and ethical governance in fostering a more just society.</p>
<p>Cleanlab is a member of the <a href="https://www.nvidia.com/startups/?nvid=nv-int-tblg-295718-vt33">NVIDIA Inception</a> program for cutting-edge startups.</p>
<p>Stay tuned for more episodes recorded live from GTC.</p>
<p><iframe loading="lazy" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/1797413893%3Fsecret_token%3Ds-bfbkVACWPn4&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true" width="100%" height="166" frameborder="no" scrolling="no"></iframe></p>
<div style="font-size: 10px; color: #cccccc; line-break: anywhere; word-break: normal; overflow: hidden; white-space: nowrap; text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif; font-weight: 100;"><a style="color: #cccccc; text-decoration: none;" title="The AI Podcast" href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">The AI Podcast</a> · <a style="color: #cccccc; text-decoration: none;" title="Cleanlab's Curtis Northcutt and Berkeley Research Group's Steven Gawthorpe on AI for Fighting Crime" href="https://soundcloud.com/theaipodcast/cleanlabs-curtis-northcutt-and-berkeley-research-groups-steven-gawthorpe-on-ai-for-fighting-crime-ep-218/s-bfbkVACWPn4" target="_blank" rel="noopener">Cleanlab&#8217;s Curtis Northcutt and Berkeley Research Group&#8217;s Steven Gawthorpe on AI for Fighting Crime</a></div>
<h2><b>Time Stamps</b></h2>
<p>1:05: Northcutt on Cleanlab’s inception and mission<br />
2:41: What Cleanlab offers its customers<br />
4:24: The human element in Cleanlab’s data verification<br />
8:57: Gawthorpe on the core functions, aims of the Berkeley Research Group<br />
10:42: Gawthorpe’s approach to data collection and analysis in fraud investigations<br />
16:38: Cleanlab’s one-click solution for generating machine learning models<br />
18:30: The evolution of machine learning and its impact on data analytics<br />
20:07: Future directions in data-driven crimefighting</p>
<h2><b>You Might Also Like…</b></h2>
<p><a href="https://soundcloud.com/theaipodcast/legal"><b>The Case for Generative AI in the Legal Field &#8211; Ep. 210</b><b><br />
</b><br />
</a>Thomson Reuters, the global content and technology company, is transforming the legal industry with generative AI. In the latest episode of NVIDIA’s AI Podcast, host Noah Kravitz spoke with Thomson Reuters’ Chief Product Officer David Wong about its potential — and implications.</p>
<p><a href="https://soundcloud.com/theaipodcast/making-machines-mindful"><b>Making Machines Mindful: NYU Professor Talks Responsible AI &#8211; Ep. 205</b></a></p>
<p>Artificial intelligence is now a household term. Responsible AI is hot on its heels. Julia Stoyanovich, associate professor of computer science and engineering at NYU and director of the university’s Center for Responsible AI, wants to make the terms “AI” and “responsible AI” synonymous.</p>
<p><a href="https://soundcloud.com/theaipodcast/mlcommons"><b>MLCommons’ David Kanter, NVIDIA’s Daniel Galvez on Publicly Accessible Datasets &#8211; Ep. 167</b></a></p>
<p>On this episode of NVIDIA’s AI Podcast, host Noah Kravitz spoke with David Kanter, founder and executive director of MLCommons, and NVIDIA senior AI developer technology engineer Daniel Galvez, about the democratization of access to speech technology and how ML Commons is helping advance the research and development of machine learning for everyone.</p>
<p><a href="https://soundcloud.com/theaipodcast/ai-migel-tissera"><b>Metaspectral’s Migel Tissera on AI-Based Data Management &#8211; Ep. 155</b></a></p>
<p>Cofounder and CTO of Metaspectral speaks with‌ ‌‌NVIDIA‌ ‌AI‌ ‌Podcast‌‌ ‌host‌ ‌Noah‌ ‌Kravitz‌ ‌about‌ ‌how‌ ‌Metaspectral’s‌ ‌technologies‌ ‌help‌ ‌space‌ ‌explorers‌ ‌make‌ ‌quicker‌ ‌and‌ ‌better‌ ‌use‌ ‌of‌ ‌the‌ ‌massive‌ ‌amounts‌ ‌of‌ ‌image‌ ‌data‌ ‌they‌ ‌collect‌ ‌out‌ ‌in‌ ‌the‌ ‌cosmos.‌ ‌</p>
<h2><b>Subscribe to the AI Podcast</b></h2>
<p>Get the<a href="https://blogs.nvidia.com/ai-podcast/"> AI Podcast</a> through<a href="https://itunes.apple.com/us/podcast/the-ai-podcast/id1186480811?mt=2&amp;adbsc=social_20161220_68874946&amp;adbid=811257941365882882&amp;adbpl=tw&amp;adbpr=61559439"> iTunes</a>,<a href="https://podcasts.google.com/?feed=aHR0cHM6Ly9mZWVkcy5zb3VuZGNsb3VkLmNvbS91c2Vycy9zb3VuZGNsb3VkOnVzZXJzOjI2NDAzNDEzMy9zb3VuZHMucnNz"> Google Podcasts</a>,<a href="https://play.google.com/music/listen?u=0#/ps/I4kyn74qfrsdhrm35mcrf3igxzm"> Google Play</a>, <a href="https://music.amazon.com/podcasts/956857d0-9461-4496-a07e-24be0539ee82/the-ai-podcast">Amazon Music, </a><a href="https://castbox.fm/channel/The-AI-Podcast-id433488?country=us">Castbox</a>, DoggCatcher,<a href="https://overcast.fm/itunes1186480811/the-ai-podcast"> Overcast</a>,<a href="https://player.fm/series/the-ai-podcast"> PlayerFM</a>, Pocket Casts,<a href="http://www.podbay.fm/show/1186480811"> Podbay</a>,<a href="https://www.podbean.com/podcast-detail/cjgnp-4a6e0/The-AI-Podcast"> PodBean</a>, PodCruncher, PodKicker,<a href="https://soundcloud.com/theaipodcast"> Soundcloud</a>,<a href="https://open.spotify.com/show/4TB4pnynaiZ6YHoKmyVN0L"> Spotify</a>,<a href="http://www.stitcher.com/s?fid=130629&amp;refid=stpr"> Stitcher</a> and<a href="https://tunein.com/podcasts/Technology-Podcasts/The-AI-Podcast-p940829/"> TuneIn</a>.</p>
<p>Make the AI Podcast better: Have a few minutes to spare? Fill out<a href="http://survey.podtrac.com/start-survey.aspx?pubid=I5V0tOQFNS8j&amp;ver=short"> this listener survey</a>.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2021/08/ai-podcast-2600x1472_-1-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1159"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2021/08/ai-podcast-2600x1472_-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Combating Corruption With Data: Cleanlab and Berkeley Research Group on Using AI-Powered Investigative Analytics  ]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>The Building Blocks of AI: Decoding the Role and Significance of Foundation Models</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-foundation-models/</link>
		
		<dc:creator><![CDATA[Jesse Clayton]]></dc:creator>
		<pubDate>Wed, 10 Apr 2024 13:00:47 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71030</guid>

					<description><![CDATA[Skyscrapers start with strong foundations. The same goes for apps powered by AI.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p><i>Editor’s note: This post is part of the </i><a href="https://blogs.nvidia.com/blog/tag/ai-decoded/"><i>AI Decoded series</i></a><i>, which demystifies AI by making the technology more accessible, and which showcases new hardware, software, tools and accelerations for RTX PC users.</i></p>
<p>Skyscrapers start with strong foundations. The same goes for apps powered by AI.</p>
<p>A <a href="https://blogs.nvidia.com/blog/what-are-foundation-models/">foundation model</a> is an AI neural network trained on immense amounts of raw data, generally with <a href="https://blogs.nvidia.com/blog/supervised-unsupervised-learning/">unsupervised learning</a>.</p>
<p>It’s a type of artificial intelligence model trained to understand and generate human-like language. Imagine giving a computer a huge library of books to read and learn from, so it can understand the context and meaning behind words and sentences, just like a human does.</p>
<figure id="attachment_71041" aria-describedby="caption-attachment-71041" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model.jpg"><img loading="lazy" decoding="async" class="wp-image-71041 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-672x457.jpg" alt="" width="672" height="457" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-672x457.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-400x272.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-768x522.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-662x450.jpg 662w, https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-316x215.jpg 316w, https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model-147x100.jpg 147w, https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-model.jpg 1054w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71041" class="wp-caption-text">Foundation models.</figcaption></figure>
<p>A foundation model’s deep knowledge base and ability to communicate in natural language make it useful for a broad range of applications, including text generation and summarization, copilot production and computer code analysis, image and video creation, and audio transcription and speech synthesis.</p>
<p>ChatGPT, one of the most notable generative AI applications, is a chatbot built with OpenAI’s GPT foundation model. Now in its fourth version, GPT-4 is a large multimodal model that can ingest text or images and generate text or image responses.</p>
<p>Online apps built on foundation models typically access the models from a data center. But many of these models, and the applications they power, can now run locally on PCs and workstations with <a href="https://www.nvidia.com/en-us/geforce/">NVIDIA GeForce</a> and <a href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX</a> GPUs.</p>
<h2><strong>Foundation Model Uses</strong></h2>
<p>Foundation models can perform a variety of functions, including:</p>
<ul>
<li>Language processing: understanding and generating text</li>
<li>Code generation: analyzing and debugging computer code in many programming languages</li>
<li>Visual processing: analyzing and generating images</li>
<li>Speech: generating text to speech and transcribing speech to text</li>
</ul>
<p>They can be used as is or with further refinement. Rather than training an entirely new AI model for each generative AI application — a costly and time-consuming endeavor — users commonly fine-tune foundation models for specialized use cases.</p>
<p>Pretrained foundation models are remarkably capable, thanks to prompts and data-retrieval techniques like <a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">retrieval-augmented generation</a>, or RAG. Foundation models also excel at <a href="https://blogs.nvidia.com/blog/what-is-transfer-learning/">transfer learning</a>, which means they can be trained to perform a second task related to their original purpose.</p>
<p>For example, a general-purpose large language model (LLM) designed to converse with humans can be further trained to act as a customer service chatbot capable of answering inquiries using a corporate knowledge base.</p>
<p>Enterprises across industries are fine-tuning foundation models to get the best performance from their AI applications.</p>
<h2><strong>Types of Foundation Models</strong></h2>
<p>More than 100 foundation models are in use — a number that continues to grow. LLMs and image generators are the two most popular types of foundation models. And many of them are free for anyone to try — on any hardware — in the <a href="https://build.nvidia.com/explore/discover">NVIDIA API Catalog</a>.</p>
<p>LLMs are models that understand natural language and can respond to queries. Google’s <a href="https://build.nvidia.com/google/gemma-7b">Gemma</a> is one example; it excels at text comprehension, transformation and code generation. When asked about the astronomer Cornelius Gemma, it shared that his “contributions to celestial navigation and astronomy significantly impacted scientific progress.” It also provided information on his key achievements, legacy and other facts.</p>
<p>Extending the collaboration <a href="https://blogs.nvidia.com/blog/google-gemma-llm-rtx-ai-pc/">of the Gemma models</a>, accelerated with the NVIDIA TensorRT-LLM on RTX GPUs, <a href="https://developers.googleblog.com/2024/04/gemma-family-expands.html">Google’s CodeGemma</a> brings powerful yet lightweight coding capabilities to the community. CodeGemma models are available as 7B and 2B pretrained variants that specialize in code completion and code generation tasks.</p>
<p>MistralAI’s <a href="https://build.nvidia.com/mistralai/mistral-7b-instruct-v2">Mistral</a> LLM can follow instructions, complete requests and generate creative text. In fact, it helped brainstorm the headline for this blog, including the requirement that it use a variation of the series’ name “AI Decoded,” and it assisted in writing the definition of a foundation model.</p>
<figure id="attachment_71034" aria-describedby="caption-attachment-71034" style="width: 589px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/Hello-world-indeed.png"><img loading="lazy" decoding="async" class="wp-image-71034 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/Hello-world-indeed.png" alt="" width="589" height="275" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/Hello-world-indeed.png 589w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Hello-world-indeed-400x187.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Hello-world-indeed-406x190.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/Hello-world-indeed-188x88.png 188w" sizes="(max-width: 589px) 100vw, 589px" /></a><figcaption id="caption-attachment-71034" class="wp-caption-text">Hello, world, indeed.</figcaption></figure>
<p>Meta’s <a href="https://build.nvidia.com/meta/llama2-70b">Llama 2</a> is a cutting-edge LLM that generates text and code in response to prompts.</p>
<p>Mistral and Llama 2 are available in the <a href="https://www.nvidia.com/en-ph/ai-on-rtx/chat-with-rtx-generative-ai/">NVIDIA ChatRTX</a> tech demo, running on RTX PCs and workstations. ChatRTX lets users personalize these foundation models by connecting them to personal content — such as documents, doctors’ notes and other data — through RAG. It’s accelerated by <a href="https://blogs.nvidia.com/blog/ai-decoded-tensorrt-stable-diffusion-automatic1111">TensorRT-LLM</a> for quick, contextually relevant answers. And because it runs locally, results are fast and secure.</p>
<p>Image generators like StabilityAI’s <a href="https://build.nvidia.com/stabilityai/stable-diffusion-xl">Stable Diffusion XL</a> and <a href="https://build.nvidia.com/stabilityai/sdxl-turbo">SDXL Turbo</a> let users generate images and stunning, realistic visuals. StabilityAI’s video generator, <a href="https://build.nvidia.com/stabilityai/stable-video-diffusion">Stable Video Diffusion</a>, uses a generative diffusion model to synthesize video sequences with a single image as a conditioning frame.</p>
<p>Multimodal foundation models can simultaneously process more than one type of data — such as text and images — to generate more sophisticated outputs.</p>
<p>A multimodal model that works with both text and images could let users upload an image and ask questions about it. These types of models are quickly working their way into real-world applications like customer service, where they can serve as faster, more user-friendly versions of traditional manuals.</p>
<figure id="attachment_71044" aria-describedby="caption-attachment-71044" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/04/image.png"><img loading="lazy" decoding="async" class="size-large wp-image-71044" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/image-672x334.png" alt="" width="672" height="334" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/image-672x334.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-400x199.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-768x382.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-1536x764.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-842x419.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-406x202.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-188x94.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image-1280x637.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/04/image.png 1783w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-71044" class="wp-caption-text">Many foundation models are free to try — on any hardware — in the NVIDIA API Catalog.</figcaption></figure>
<p><a href="https://build.nvidia.com/microsoft/microsoft-kosmos-2">Kosmos 2</a> is Microsoft’s groundbreaking multimodal model designed to understand and reason about visual elements in images.</p>
<h2><strong>Think Globally, Run AI Models Locally </strong></h2>
<p>GeForce RTX and NVIDIA RTX GPUs can run foundation models locally.</p>
<p>The results are fast and secure. Rather than relying on cloud-based services, users can harness apps like ChatRTX to process sensitive data on their local PC without sharing the data with a third party or needing an internet connection.</p>
<p>Users can choose from a rapidly growing catalog of open foundation models to download and run on their own hardware. This lowers costs compared with using cloud-based apps and APIs, and it eliminates latency and network connectivity issues. <i>Generative AI is transforming gaming, videoconferencing and interactive experiences of all kinds. Make sense of what’s new and what’s next by subscribing to the </i><a href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><i>AI Decoded newsletter</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-models-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/foundation-models-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[The Building Blocks of AI: Decoding the Role and Significance of Foundation Models]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Joins $110 Million Partnership to Help Universities Teach AI Skills</title>
		<link>https://blogs.nvidia.com/blog/partnership-universities-teach-ai-skills/</link>
		
		<dc:creator><![CDATA[Ruth Berry]]></dc:creator>
		<pubDate>Tue, 09 Apr 2024 19:01:49 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[HPC Stories]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71006</guid>

					<description><![CDATA[The Biden Administration has announced a new $110 million AI partnership between Japan and the United States that includes an initiative to fund research through a collaboration between the University of Washington and the University of Tsukuba. NVIDIA is committing $25 million in a collaboration with Amazon that aims to bring the latest technologies to		<a class="read-more" href="https://blogs.nvidia.com/blog/partnership-universities-teach-ai-skills/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>The Biden Administration has announced a new $110 million AI partnership between Japan and the United States that includes an initiative to fund research through a collaboration between the University of Washington and the University of Tsukuba.</p>
<p>NVIDIA is committing $25 million in a collaboration with Amazon that aims to bring the latest technologies to the University of Washington, in Seattle, and the University of Tsukuba, which is northeast of Tokyo.</p>
<p>Universities around the world are preparing students for crucial AI skills by providing access to the high performance computing capabilities of supercomputing.</p>
<p>“This collaboration between the University of Washington, University of Tsukuba, Amazon, and NVIDIA will help provide the research and workforce training for our regions’ tech sectors to keep up with the profound impacts AI is having across every sector of our economy,” said Jay Inslee, governor of Washington State.</p>
<h2><b>Creating AI Opportunities for Students</b></h2>
<p>NVIDIA has been investing in universities for decades computing resources, advanced training curriculums, donations, and other support to provide students and professors with access to high performance computing (HPC) for groundbreaking research results.</p>
<p>NVIDIA founder and CEO Jensen Huang and his wife, Lori Huang, <a href="https://blogs.nvidia.com/blog/ai-supercomputer-oregon-state/">donated $50 million</a> to their alma mater Oregon State University — where they met and earned engineering degrees —  to help build one of the world’s fastest supercomputers in a facility bearing their names. This computing center will help students research, develop and apply AI across Oregon State’s top-ranked programs in agriculture, computer sciences, climate science, forestry, oceanography, robotics, water resources, materials sciences and more.</p>
<p>The University of Florida <a href="https://blogs.nvidia.com/blog/starship-for-mind-uf-malachowsky-hall/">recently unveiled</a> Malachowsky Hall, which was made possible with a $50 million donation from NVIDIA co-founder Chris Malachowsky. This new building along with a previous donation of an AI supercomputer is enabling the University of Florida to offer world class AI training and research opportunities.</p>
<h2><b>Strengthening US-Japan AI Research Collaboration</b></h2>
<p>The U.S.-Japan HPC alliance will advance AI research and development and support the two nations’ global leadership in cutting-edge technology.</p>
<p>The University of Washington and Tsukuba University initiative will support research in critical areas where AI can drive impactful change, such as robotics, healthcare, climate change and atmospheric science, among others.</p>
<p>In addition to the university partnership,  NVIDIA recently announced a collaboration with Japan’s National Institute of Advanced Industrial Science and Technology (AIST) on AI and quantum technology.</p>
<h2><b>Addressing Worldwide AI Talent Shortage</b></h2>
<p>Demand for key AI skills is creating a talent shortage worldwide. Some <a href="https://www.ox.ac.uk/news/2023-10-09-expert-comment-ai-demand-booming-right-skills-and-technology-glue-guys">experts calculate </a>there has been a fivefold increase in demand for these skills as a percentage of total U.S. jobs. Universities around the world are looking for ways to prepare students with new skills for the workforce, and corporate-university partnerships are a key tool to help bridge the gap.</p>
<p>NVIDIA <a href="https://blogs.nvidia.com/blog/generative-ai-professional-certification/">unveiled at GTC 2024</a> new professional certifications in generative AI to help enable the next generation of developers to obtain technical credibility in this important domain.</p>
<p><i>Learn more about NVIDIA generative AI courses </i><a href="https://www.nvidia.com/en-us/training/instructor-led-workshops/generative-ai-with-diffusion-models/"><i>here</i></a><i> and </i><a href="https://www.nvidia.com/en-us/learn/certification/generative-ai-llm-associate/"><i>here</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/uow5.jpg"
			type="image/jpeg"
			width="1169"
			height="735"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/uow5-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Joins $110 Million Partnership to Help Universities Teach AI Skills]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Broadcasting Breakthroughs: NVIDIA Holoscan for Media, Available Now, Transforms Live Media With Easy AI Integration</title>
		<link>https://blogs.nvidia.com/blog/holoscan-live-media-ai-solutions/</link>
		
		<dc:creator><![CDATA[Sepi Motamedi]]></dc:creator>
		<pubDate>Tue, 09 Apr 2024 15:00:11 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Graphics]]></category>
		<category><![CDATA[Video Streaming]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70983</guid>

					<description><![CDATA[Whether delivering live sports programming, streaming services, network broadcasts or content on social platforms, media companies face a daunting landscape. Viewers are increasingly opting for interactive and personalized content. Virtual reality and augmented reality continue their drive into the mainstream. New video compression standards are challenging traditional computing infrastructure. And AI is having an impact		<a class="read-more" href="https://blogs.nvidia.com/blog/holoscan-live-media-ai-solutions/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Whether delivering live sports programming, streaming services, network broadcasts or content on social platforms, media companies face a daunting landscape.</p>
<p>Viewers are increasingly opting for interactive and personalized content. Virtual reality and augmented reality continue their drive into the mainstream. New video compression standards are challenging traditional computing infrastructure. And AI is having an impact across the board.</p>
<p>In a situation this dynamic, media companies will benefit most from AI-enabled media solutions that flexibly align with their changing development and delivery needs.</p>
<p>NVIDIA Holoscan for Media, available now, is a software-defined platform that enables developers to easily build live media applications, supercharge them with AI and then deploy them across media platforms.</p>
<h2><b>A New Approach to Media Application Development </b></h2>
<p>Holoscan for Media offers a new approach to development in live media. It simplifies application development by providing an internet protocol (IP)-based, cloud-native architecture that isn’t constrained by dedicated hardware, environments or locations. Instead, it integrates open-source and ubiquitous technologies and streamlines application delivery to customers, all while optimizing costs.</p>
<p>Traditional application development for the live media market relies on dedicated hardware. Because software is tied to that hardware, developers are constrained when it comes to innovating or upgrading applications.</p>
<p>Each deployment type, whether on premises or in the cloud, requires its own build, making development costly and inefficient. Beyond designing an application’s user interface and core functionalities, developers have to build out additional infrastructure services, further eating into research and development budgets.</p>
<p>The most significant challenge is incorporating AI, due to the complexity of building an AI software stack. This prevents many applications in pilot programs from moving to production.</p>
<p>Holoscan for Media eases the integration of AI into application development due to its underlying architecture, which enables software-defined video to be deployed on the same software stack as AI applications, including generative AI-based tools. This benefits vendors and research and development departments looking to incorporate AI apps into live video.</p>
<p>Since the platform is cloud-native, the same architecture can run independent of location, whether in the cloud, on premises or at the edge. Additionally, it’s not tied to a specific device, field-programmable gate array or appliance.</p>
<p>The Holoscan for Media architecture includes services like authentication, logging and security, as well as features that help broadcasters migrate to IP-based technologies, including the SMPTE ST 2110 transport protocol, the precision time protocol for timing and synchronization, and the NMOS controller and registry for dynamic device management.</p>
<h2><b>A Growing Ecosystem of Partners</b></h2>
<p>Beamr, Comprimato, Lawo, Media.Monks, Pebble, RED Digital Cinema, Sony Corporation and Telestream are among the early adopters already transforming live media with Holoscan for Media.</p>
<p>“We use Holoscan for Media as the core infrastructure for our broadcast and media workflow, granting us powerful scale to deliver interest-based content across a wide range of channels and platforms,” said Lewis Smithingham, senior vice president of innovation special operations at Media.Monks, a provider of software-defined production workflows.</p>
<p>“By compartmentalizing applications and making them interoperable, Holoscan for Media allows for easy adoption of new innovations from many different companies in one platform,” said Jeff Goodman, vice president of product management at RED Digital Cinema, a manufacturer of professional digital cinema cameras. “It takes much of the integration complexity out of the equation and will significantly increase the pace of innovation. We are very excited to be a part of it.”</p>
<p>“We believe NVIDIA Holoscan for Media is one of the paths forward to enabling the development of next-generation products and services for the industry, allowing the scaling of GPU power as needed,” said Masakazu Murata, senior general manager of media solutions business at Sony Corporation. “Our M2L-X software switcher prototype running on Holoscan for Media demonstrates how customers can run Sony’s solutions on GPU clusters.”</p>
<p>“Telestream is committed to transforming the media landscape, enhancing efficiency and content experiences without sacrificing quality or user-friendliness,” said Charlie Dunn, senior vice president and general manager at Telestream, a provider of digital media software and solutions. “We’ve seamlessly integrated the Holoscan for Media platform into our INSPECT IP video monitoring solution to achieve a clear and efficient avenue for ST 2110 compliance.”</p>
<h2><b>Experience Holoscan for Media at NAB Show</b></h2>
<p>These partners will showcase how they’re using NVIDIA Holoscan for Media at NAB Show, an event for the broadcast, media and entertainment industry, taking place April 13-17 in Las Vegas.</p>
<p>Explore development on Holoscan for Media and discover applications running on the platform at the <a href="https://nab24.mapyourshow.com/8_0/floorplan/?hallID=SL&amp;selectedBooth=SL8065">Dell Technologies booth</a>. Learn more about <a href="https://www.nvidia.com/en-us/events/nab/">NVIDIA’s presence at NAB Show</a>, including details on sessions and demos on generative AI, software-defined broadcast and immersive graphics.</p>
<p><i>Apply for access to </i><a href="https://developer.nvidia.com/holoscan-for-media"><i>NVIDIA Holoscan for Media</i></a><i> today.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/Holoscan-Corp-Blog.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/Holoscan-Corp-Blog-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Broadcasting Breakthroughs: NVIDIA Holoscan for Media, Available Now, Transforms Live Media With Easy AI Integration]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Start Up Your Engines: NVIDIA and Google Cloud Collaborate to Accelerate AI Development</title>
		<link>https://blogs.nvidia.com/blog/nvidia-google-cloud-ai-development/</link>
		
		<dc:creator><![CDATA[Greg Estes]]></dc:creator>
		<pubDate>Tue, 09 Apr 2024 12:00:48 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Hardware]]></category>
		<category><![CDATA[Software]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GPU]]></category>
		<category><![CDATA[Inception]]></category>
		<category><![CDATA[Inference]]></category>
		<category><![CDATA[NVIDIA DGX]]></category>
		<category><![CDATA[NVIDIA Triton]]></category>
		<category><![CDATA[TensorRT]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=71004</guid>

					<description><![CDATA[NVIDIA and Google Cloud have announced a new collaboration to help startups around the world accelerate the creation of generative AI applications and services. The announcement, made today at Google Cloud Next ‘24 in Las Vegas, brings together the NVIDIA Inception program for startups and the Google for Startups Cloud Program to widen access to		<a class="read-more" href="https://blogs.nvidia.com/blog/nvidia-google-cloud-ai-development/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>NVIDIA and Google Cloud have announced a new collaboration to help startups around the world accelerate the creation of generative AI applications and services.</p>
<p>The announcement, made today at Google Cloud Next ‘24 in Las Vegas, brings together the <a href="https://www.nvidia.com/en-us/startups/?ncid=ref-kc-319196-vt18&amp;sfdcid=Google" target="_blank" rel="noopener">NVIDIA Inception</a> program for startups and the <a href="https://startup.google.com/cloud/">Google for Startups Cloud Program</a> to widen access to cloud credits, go-to-market support and technical expertise to help startups deliver value to customers faster.</p>
<p>Qualified members of NVIDIA Inception, a global program supporting more than 18,000 startups, will have an accelerated path to using Google Cloud infrastructure with access to Google Cloud credits — up to $350,000 for those focused on AI.</p>
<p>Google for Startups Cloud Program members can join NVIDIA Inception and gain access to technological expertise, <a href="https://www.nvidia.com/en-us/training/" target="_blank" rel="noopener">NVIDIA Deep Learning Institute</a> course credits, NVIDIA hardware and software, and more. Eligible members of the Google for Startups Cloud Program also can participate in NVIDIA Inception Capital Connect, a platform that gives startups exposure to venture capital firms interested in the space.</p>
<p>High-growth emerging software makers of both programs can also gain fast-tracked onboarding to Google Cloud Marketplace, co-marketing and product acceleration support.</p>
<p>This collaboration is the latest in a series of announcements the two companies have made to help ease the costs and barriers associated with developing generative AI applications for enterprises of all sizes. Startups in particular are constrained by the high costs associated with AI investments.</p>
<h2><b>It Takes a Full-Stack AI Platform</b></h2>
<p>In February, Google DeepMind unveiled <a href="https://blog.google/technology/developers/gemma-open-models/" target="_blank" rel="noopener">Gemma</a>, a family of state-of-the-art open models. NVIDIA, in collaboration with Google, recently launched optimizations across all <a href="https://blogs.nvidia.com/blog/google-gemma-llm-rtx-ai-pc/" target="_blank" rel="noopener">NVIDIA AI platforms for Gemma</a>, helping to reduce customer costs and speed up innovative work for domain-specific use cases.</p>
<p>Teams from the companies worked closely together to accelerate the performance of Gemma — built from the same research and technology used to create Google DeepMind’s most capable model yet, <a href="https://blog.google/technology/ai/google-gemini-ai/" target="_blank" rel="noopener">Gemini</a> — with <a href="https://developer.nvidia.com/tensorrt#inference">NVIDIA TensorRT-LLM</a>, an open-source library for optimizing large language model inference, when running on NVIDIA GPUs.</p>
<p><a href="https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/" target="_blank" rel="noopener">NVIDIA NIM</a> microservices, part of the <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/" target="_blank" rel="noopener">NVIDIA AI Enterprise</a> software platform, together with <a href="https://cloud.google.com/kubernetes-engine" target="_blank" rel="noopener">Google Kubernetes Engine</a> (GKE) provide a streamlined path for developing AI-powered apps and deploying optimized AI models into production. Built on inference engines including <a href="https://developer.nvidia.com/triton-inference-server" target="_blank" rel="noopener">NVIDIA Triton Inference Server</a> and TensorRT-LLM, NIM supports a wide range of leading AI models and delivers seamless, scalable AI inferencing to accelerate generative AI deployment in enterprises.</p>
<p>The Gemma family of models, including <a href="https://build.nvidia.com/google/gemma-7b" target="_blank" rel="noopener">Gemma 7B</a>, <a href="https://build.nvidia.com/google/recurrentgemma-2b" target="_blank" rel="noopener">RecurrentGemma</a> and <a href="https://build.nvidia.com/google/codegemma-7b" target="_blank" rel="noopener">CodeGemma,</a> are available from the <a href="http://build.nvidia.com" target="_blank" rel="noopener">NVIDIA API catalog</a> for users to try from a browser, prototype with the API endpoints and self-host with NIM.</p>
<p>Google Cloud has made it easier to deploy the <a href="https://www.nvidia.com/en-us/ai-data-science/products/nemo/" target="_blank" rel="noopener">NVIDIA NeMo</a> framework across its platform via GKE and <a href="https://cloud.google.com/hpc-toolkit/docs/overview" target="_blank" rel="noopener">Google Cloud HPC Toolkit</a>. This enables developers to automate and scale the training and serving of generative AI models, allowing them to rapidly deploy turnkey environments through customizable blueprints that jump-start the development process.</p>
<p>NVIDIA NeMo, part of NVIDIA AI Enterprise, is also available in Google Cloud Marketplace, providing customers another way to easily access NeMo and other frameworks to accelerate AI development.</p>
<p>Further widening the availability of NVIDIA-accelerated generative AI computing, Google Cloud also announced the general availability of A3 Mega will be coming next month. The instances are an expansion to its A3 virtual machine family, powered by <a href="https://www.nvidia.com/en-us/data-center/h100/" target="_blank" rel="noopener">NVIDIA H100 Tensor Core GPUs</a>. The new instances will double the GPU-to-GPU network bandwidth from A3 VMs.</p>
<p>Google Cloud’s new Confidential VMs on A3 will also include support for <a href="https://www.nvidia.com/en-us/data-center/solutions/confidential-computing/" target="_blank" rel="noopener">confidential computing</a> to help customers protect the confidentiality and integrity of their sensitive data and secure applications and AI workloads during training and inference — with no code changes while accessing H100 GPU acceleration. These GPU-powered Confidential VMs will be available in Preview this year.</p>
<h2><b>Next Up: NVIDIA Blackwell-Based GPUs</b></h2>
<p>NVIDIA’s newest GPUs based on the <a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener">NVIDIA Blackwell</a> platform will be coming to Google Cloud early next year in two variations: the <a href="https://www.nvidia.com/en-us/data-center/hgx/">NVIDIA </a><a href="https://www.nvidia.com/en-us/data-center/hgx/">HGX B200</a> and the <a href="https://www.nvidia.com/en-us/data-center/gb200-nvl72/">NVIDIA GB200 NVL72</a>.</p>
<p>The HGX B200 is designed for the most demanding AI, data analytics and high performance computing workloads, while the GB200 NVL72 is designed for next-frontier, massive-scale, trillion-parameter model training and real-time inferencing.</p>
<p>The NVIDIA GB200 NVL72 connects 36 Grace Blackwell Superchips, each with two NVIDIA Blackwell GPUs combined with an <a href="https://www.nvidia.com/en-us/data-center/grace-cpu/" target="_blank" rel="noopener">NVIDIA Grace CPU</a> over a 900GB/s chip-to-chip interconnect, supporting up to 72 Blackwell GPUs in one <a href="https://www.nvidia.com/en-us/data-center/nvlink/" target="_blank" rel="noopener">NVIDIA NVLink</a> domain and 130TB/s of bandwidth. It overcomes communication bottlenecks and acts as a single GPU, delivering 30x faster real-time LLM inference and 4x faster training compared to the prior generation.</p>
<p>NVIDIA GB200 NVL72 is a multi-node rack-scale system that will be combined with Google Cloud’s fourth generation of advanced liquid-cooling systems.</p>
<p>NVIDIA announced last month that NVIDIA DGX Cloud, an AI platform for enterprise developers that’s optimized for the demands of generative AI, is generally available on A3 VMs powered by H100 GPUs. DGX Cloud with GB200 NVL72 will also be available on Google Cloud in 2025.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/google-cloud-logo.jpg"
			type="image/jpeg"
			width="1920"
			height="1080"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/google-cloud-logo-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Start Up Your Engines: NVIDIA and Google Cloud Collaborate to Accelerate AI Development]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Ranked by Fortune at No. 3 on ‘100 Best Companies to Work For’ List</title>
		<link>https://blogs.nvidia.com/blog/nvidia-life-fortune-100-best-companies/</link>
		
		<dc:creator><![CDATA[Samantha Zee]]></dc:creator>
		<pubDate>Thu, 04 Apr 2024 18:07:43 +0000</pubDate>
				<category><![CDATA[NVIDIA Life]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70987</guid>

					<description><![CDATA[NVIDIA jumped to No. 3 on the latest list of America’s 100 Best Companies to Work For by Fortune magazine and Great Place to Work. It’s the company’s eighth consecutive year and highest ranking yet on the widely followed list, published today, which more than a thousand businesses vie to land on. NVIDIA ranked sixth		<a class="read-more" href="https://blogs.nvidia.com/blog/nvidia-life-fortune-100-best-companies/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>NVIDIA jumped to No. 3 on the latest list of America’s <a href="https://fortune.com/ranking/best-companies/">100 Best Companies to Work For</a> by <i>Fortune</i> magazine and Great Place to Work.</p>
<p>It’s the company’s eighth consecutive year and highest ranking yet on the widely followed list, published today, which more than a thousand businesses vie to land on. NVIDIA ranked sixth last year.</p>
<p>“Since the COVID pandemic, employees are increasingly prioritizing work-life balance, mission alignment and empathetic workplaces,” <i>Fortune</i> wrote, with hotelier Hilton taking the top spot, followed by Cisco.</p>
<p>“Even as the broader tech sector has shed tens of thousands of jobs, NVIDIA continued its remarkable streak of nearly 15 years without any layoffs,” <i>Fortune</i> noted in its writeup. NVIDIA also was cited for the company’s “flat structure,” which encourages employees to solve problems quickly and collaboratively through projects.</p>
<h2><b>Survey Says: 97% Are Proud to Share They Work at NVIDIA</b></h2>
<p>To identify the top 100, <i>Fortune</i> conducted a detailed employee survey with Great Place to Work that received more than 1.3 million responses from people in the U.S. The <a href="https://www.greatplacetowork.com/certified-company/1000184">survey</a> showed that 97% of NVIDIANs are proud to tell others where they work.</p>
<p>While many tech companies faced a challenging 2023, with an uncertain economy and several of the biggest employers laying off thousands of workers, NVIDIA focused on managing costs, encouraging innovation and offering unique benefits and compensation that supported employees.</p>
<p><i>Learn more about </i><a href="https://www.nvidia.com/en-us/about-nvidia/careers/life-at-nvidia/"><i>NVIDIA life, culture and careers</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/Fortune-GPTW-Feature-Image.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/Fortune-GPTW-Feature-Image-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Ranked by Fortune at No. 3 on ‘100 Best Companies to Work For’ List]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>‘The Elder Scrolls Online’ Joins GeForce NOW for Game’s 10th Anniversary</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-elder-scrolls-online-japan-expansion-april-games-list/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 04 Apr 2024 13:00:47 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70969</guid>

					<description><![CDATA[Rain or shine, a new month means new games. GeForce NOW kicks off April with nearly 20 new games, seven of which are available to play this week. GFN Thursday celebrates the 10-year anniversary of ZeniMax Online Studios’ Elder Scrolls Online by bringing the award-winning online role-playing game (RPG) to the cloud this week. Plus,		<a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-elder-scrolls-online-japan-expansion-april-games-list/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Rain or shine, a new month means new games. <a href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a> kicks off April with nearly 20 new games, seven of which are available to play this week.</p>
<p>GFN Thursday celebrates the 10-year anniversary of ZeniMax Online Studios’ <i>Elder Scrolls</i> <i>Online</i> by bringing the award-winning online role-playing game (RPG) to the cloud this week.</p>
<p>Plus, the <a href="https://www.nvidia.com/en-us/geforce-now/memberships/">GeForce NOW Ultimate membership</a> comes to gamers in Japan for the first time, with new GeForce RTX 4080 SuperPODs online today.</p>
<h2><b>The Rising Sun Goes Ultimate</b></h2>
<figure id="attachment_70973" aria-describedby="caption-attachment-70973" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-70973" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Japan-672x374.png" alt="Japan and GeForce NOW" width="672" height="374" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Japan-672x374.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Japan-400x223.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Japan-768x427.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Japan-1536x855.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Japan.png 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Japan-808x450.png 808w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Japan-386x215.png 386w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Japan-180x100.png 180w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Japan-1280x712.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-70973" class="wp-caption-text"><em>Get ready to drift into the cloud.</em></figcaption></figure>
<p>GeForce NOW is rolling out the green carpet to gamers in Japan, expanding next-generation cloud gaming worldwide. <a href="https://www.nvidia.com/ja-jp/geforce-now/memberships/">The Ultimate membership tier is now available</a> to gamers in the region, delivering up to 4K gaming at up to 120 frames per second, all at ultra-low latency — even on devices without the latest hardware.</p>
<p>Gamers in Japan can now access from the cloud triple-A titles by some of the world’s largest publishers. Capcom’s <i>Street Fighter 6</i> and <i>Resident Evil Village</i> will be coming to GeForce NOW at a later date for members to stream at the highest performance.</p>
<p>GeForce NOW will operate in Japan alongside GeForce NOW Alliance partner and telecommunications company KDDI, which currently offers its customers access to GeForce RTX 3080-powered servers, in addition to its mobile benefits. Plus, new GFNA partners in other regions will be announced this year — stay tuned to GFN Thursdays for details.</p>
<h2><b>A Decade of Adventure</b></h2>
<figure id="attachment_70976" aria-describedby="caption-attachment-70976" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-70976" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Elder_Scrolls_Online-672x378.jpg" alt="Elder Scrolls Online on GeForce NOW" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Elder_Scrolls_Online-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Elder_Scrolls_Online-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Elder_Scrolls_Online-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Elder_Scrolls_Online-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Elder_Scrolls_Online-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Elder_Scrolls_Online-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Elder_Scrolls_Online-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Elder_Scrolls_Online-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-Elder_Scrolls_Online-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-70976" class="wp-caption-text"><em>The cloud is slay-ing.</em></figcaption></figure>
<p>Discover Tamriel from the comfort of almost anywhere with GeForce NOW. Explore the Elder Scrolls universe solo or alongside thousands of other players in <i>The Elder Scrolls Online</i> as it joins the cloud this week for members.</p>
<p>For a decade, <i>Elder Scrolls Online </i>has cultivated a vibrant community of millions of players and a legacy of exciting stories, characters and adventures. Players have explored Morrowind<i>, </i>Summerset, Skyrim and more, thanks to regular updates and chapter releases. The title’s anniversary celebrations <a href="https://www.elderscrollsonline.com/en-us/news/post/65537">kick off in Amsterdam</a> this week, and fans worldwide can join in by streaming the game from the cloud.</p>
<p>Set during Tamriel’s Second Era, a millennium before <i>The Elder Scrolls V: Skyrim</i>, <i>The Elder Scrolls Online</i> has players exploring a massive, ever-growing world. Together they can encounter memorable quests, challenging dungeons, player vs. player battles and more. Gamers can play their way by customizing their characters, looting and crafting new gear, and unlocking and developing their abilities.</p>
<p>Experience the epic RPG with an Ultimate membership and venture forth in the cloud with friends, tapping eight-hour gaming sessions and exclusive access to servers. Ultimate members can effortlessly explore the awe-inspiring fantasy world with the ability to stream at up to 4K and 120 fps, or experience the game at ultrawide resolutions on <a href="https://nvidia.custhelp.com/app/answers/detail/a_id/5462">supported devices</a>.</p>
<h2><b>April Showers Bring New Games</b></h2>
<figure id="attachment_70979" aria-describedby="caption-attachment-70979" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-70979" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-MEGAMAN_X_DiVE_OFFLINE-672x336.jpg" alt="MEGAMAN X DiVE OFFLINE on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-MEGAMAN_X_DiVE_OFFLINE-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-MEGAMAN_X_DiVE_OFFLINE-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-MEGAMAN_X_DiVE_OFFLINE-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-MEGAMAN_X_DiVE_OFFLINE-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-MEGAMAN_X_DiVE_OFFLINE-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-MEGAMAN_X_DiVE_OFFLINE-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-MEGAMAN_X_DiVE_OFFLINE-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-MEGAMAN_X_DiVE_OFFLINE-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-MEGAMAN_X_DiVE_OFFLINE.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-70979" class="wp-caption-text"><em>X marks the cloud.</em></figcaption></figure>
<p>Dive into a new adventure with <i>Mega Man X DiVE Offline </i>from Capcom. It’s the offline, reimagined version of <i>Mega Man X DiVE</i>, featuring the franchise’s classic action, over 100 characters from the original series and an all-new story with hundreds of stages to play. Strengthen characters and weapons with a variety of power-ups — then test them out in the side-scrolling action.</p>
<p>Catch it alongside other new games joining the cloud this week:</p>
<ul>
<li><i>ARK: Survival Ascended </i>(New release on <a href="https://www.xbox.com/games/store/ark-survival-ascended/9p33vjgvphvp?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass, April 1)</li>
<li><i>Thief</i> (New release on <a href="https://www.epicgames.com/store/p/thief-5bb95f?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>, free from April 4-11)</li>
<li><i>Sons of Valhalla </i>(New release on <a href="https://store.steampowered.com/app/1409830?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 5)</li>
<li><i>Elder Scrolls Online </i>(<a href="https://store.steampowered.com/app/306130?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a href="https://www.epicgames.com/store/p/the-elder-scrolls-online?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
<li><i>MEGA MAN X DiVE Offline </i>(<a href="https://store.steampowered.com/app/2183650?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>SUPERHOT: MIND CONTROL DELETE</i> (<a href="https://www.xbox.com/games/store/superhot-mind-control-delete/9NRH78B682L8?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Turbo Golf Racing 1.0</i> (<a href="https://www.xbox.com/games/store/turbo-golf-racing-game-preview/9N6781PMXC02?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
</ul>
<p>And members can look for the following throughout the rest of the month:</p>
<ul>
<li><i>Dead Island 2 </i>(New release on <a href="https://store.steampowered.com/app/934700?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 22)</li>
<li><i>Phantom Fury </i>(New release on <a href="https://store.steampowered.com/app/1733240?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 23)</li>
<li><i>Oddsparks: An Automation Adventure </i>(New release on <a href="https://store.steampowered.com/app/1817800?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, April 24)</li>
<li><i>9-Bit Armies: A Bit Too Far </i>(<a href="https://store.steampowered.com/app/1439750?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Backpack Battles </i>(<a href="https://store.steampowered.com/app/2427700?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Dragon’s Dogma 2 Character Creator &amp; Storage </i>(<a href="https://store.steampowered.com/app/2674810?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Evil West </i>(<a href="https://www.xbox.com/games/store/evil-west/9MW581HCJPM6?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Islands of Insight </i>(<a href="https://store.steampowered.com/app/2071500?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Lightyear Frontier </i>(<a href="https://store.steampowered.com/app/1677110?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a href="https://www.xbox.com/games/store/lightyear-frontier-game-preview/9NDX2927SND9?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Manor Lords</i> (New release on <a href="https://store.steampowered.com/app/1363080?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and Xbox, available on PC Game Pass)</li>
<li><i>Metaball </i>(<a href="https://store.steampowered.com/app/2215910?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Tortuga &#8211; A Pirate’s Tale </i>(<a href="https://store.steampowered.com/app/1604250/Tortuga__A_Pirates_Tale/">Steam</a>)</li>
</ul>
<h2><b>Making the Most of March</b></h2>
<p>In addition to the 30 games announced last month, six more joined the <a href="http://play.geforcenow.com">GeForce NOW library</a>:</p>
<ul>
<li><i>Zoria: Age of Shattering</i> (New release on <a href="https://store.steampowered.com/app/1159090?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, March 7)</li>
<li><i>Deus Ex: Mankind Divided</i> (New release on <a href="https://www.epicgames.com/store/p/deus-ex-mankind-divided-4c6370?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>, free, March 14)</li>
<li><i>Dragon’s Dogma 2 </i>(New release on <a href="https://store.steampowered.com/app/2054970?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, March 21)</li>
<li><i>Diablo IV</i> (<a href="https://shop.battle.net/family/diablo-iv">Xbox</a>, available on PC Game Pass)</li>
<li><i>Granblue Fantasy: Relink </i>(<a href="https://store.steampowered.com/app/881020?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Space Engineers</i> (<a href="https://www.xbox.com/en-US/games/store/space-engineers/9NLV3X229LG1?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
</ul>
<p>Some titles didn’t make it in March. <i>Crown Wars: The Black Prince </i>and <i>Breachway</i> have delayed their launch dates to later this year, and <i>Portal: Revolution </i>will join GeForce NOW in the future. Stay tuned to GFN Thursday for updates.</p>
<p>What are you planning to play this weekend? Let us know on <a href="https://www.twitter.com/nvidiagfn">X</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">respect your gaming elders <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f579.png" alt="🕹" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f329.png" alt="🌩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a href="https://twitter.com/NVIDIAGFN/status/1775553850105069738?ref_src=twsrc%5Etfw">April 3, 2024</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-April_4.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/GFN_Thursday-April_4-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[‘The Elder Scrolls Online’ Joins GeForce NOW for Game’s 10th Anniversary]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>A New Lens: Dotlumen CEO Cornel Amariei on Assistive Technology for the Visually Impaired</title>
		<link>https://blogs.nvidia.com/blog/dotlumen-ai-podcast/</link>
		
		<dc:creator><![CDATA[Kristen Yee]]></dc:creator>
		<pubDate>Wed, 03 Apr 2024 13:00:49 +0000</pubDate>
				<category><![CDATA[The AI Podcast]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Inception]]></category>
		<category><![CDATA[Social Impact]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70961</guid>

					<description><![CDATA[Dotlumen is illuminating a new technology to help people with visual impairments navigate the world. In this episode of NVIDIA’s AI Podcast, recorded live at the NVIDIA GTC global AI conference, host Noah Kravitz spoke with the Romanian startup’s founder and CEO, Cornel Amariei, about developing its flagship Dotlumen Glasses. Equipped with sensors and powered		<a class="read-more" href="https://blogs.nvidia.com/blog/dotlumen-ai-podcast/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Dotlumen is illuminating a new technology to help people with visual impairments navigate the world.</p>
<p>In this episode of NVIDIA’s <a href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">AI Podcast</a>, recorded live at the <a href="https://www.nvidia.com/gtc/" target="_blank" rel="noopener">NVIDIA GTC</a> global AI conference, host Noah Kravitz spoke with the Romanian startup’s founder and CEO, Cornel Amariei, about developing its flagship Dotlumen Glasses.</p>
<p>Equipped with sensors and powered by AI, the glasses compute a safely walkable path for visually impaired individuals and offer haptic — or tactile — feedback on how to proceed via corresponding vibrations. Amariei further discusses the process and challenges of developing assistive technology and its potential for enhancing accessibility.</p>
<p>Dotlumen is a member of the <a href="https://www.nvidia.com/startups/?nvid=nv-int-tblg-295718-vt33" target="_blank" rel="noopener">NVIDIA Inception</a> program for cutting-edge startups.</p>
<p>Stay tuned for more episodes recorded live from GTC.</p>
<p><iframe loading="lazy" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/1791227923%3Fsecret_token%3Ds-XBqirnHOR0N&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true" width="100%" height="166" frameborder="no" scrolling="no"></iframe></p>
<div style="font-size: 10px; color: #cccccc; line-break: anywhere; word-break: normal; overflow: hidden; white-space: nowrap; text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif; font-weight: 100;"><a style="color: #cccccc; text-decoration: none;" title="The AI Podcast" href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">The AI Podcast</a> · <a style="color: #cccccc; text-decoration: none;" title="Dotlumen CEO Cornel Amariei on Assistive Technology for the Visually Impaired" href="https://soundcloud.com/theaipodcast/gtc24-cornel-amariei-inception/s-XBqirnHOR0N" target="_blank" rel="noopener">Dotlumen CEO Cornel Amariei on Assistive Technology for the Visually Impaired</a></div>
<h2><b>Time Stamps</b></h2>
<p>0:52: Background on the glasses</p>
<p>4:28: User experience of the glasses</p>
<p>7:29: How the glasses sense the physical world and compute a walkable path</p>
<p>18:07: The hardest part of the development process</p>
<p>22:20: Expected release and availability of the glasses</p>
<p>25:57: Dotlumen’s technical breakthrough moments</p>
<p>30:19: Other assistive technologies to look out for</p>
<h2><b>You Might Also Like…</b></h2>
<p><a href="https://soundcloud.com/theaipodcast/viome-guru-banavar" target="_blank" rel="noopener"><b>Personalized Health: Viome’s Guru Banavar Discusses Startup’s AI-Driven Approach &#8211; Ep. 216</b><b><br />
</b><br />
</a>Viome Chief Technology Officer Guru Banavar discusses how the startup’s innovations in AI and genomics advance personalized health and wellness.</p>
<p><a href="https://soundcloud.com/theaipodcast/cardiac-caristo-dr-keith-channon" target="_blank" rel="noopener"><b>Cardiac Clarity: Dr. Keith Channon Talks Revolutionizing Heart Health With AI &#8211; Ep. 212</b></a></p>
<p>Here’s some news to still beating hearts: AI is helping bring some clarity to cardiology. Caristo Diagnostics has developed an AI-powered solution for detecting coronary inflammation in cardiac CT scans.</p>
<p><a href="https://soundcloud.com/theaipodcast/matice" target="_blank" rel="noopener"><b>Matice Founder Jessica Whited on Harnessing Regenerative Species for Medical Breakthroughs &#8211; Ep. 198</b></a></p>
<p>Scientists at Matice Biosciences are using AI to study the regeneration of tissues in animals known as super-regenerators, such as salamanders and planarians. The goal of the research is to develop new treatments that will help humans heal from injuries without scarring.</p>
<p><a href="https://soundcloud.com/theaipodcast/ai-stroller" target="_blank" rel="noopener"><b>How GluxKind Created Ella, the AI-Powered Smart Stroller &#8211; Ep. 193</b></a></p>
<p>Imagine a stroller that can drive itself, help users up hills, brake on slopes and provide alerts of potential hazards. That’s what GlüxKind has done with Ella, an award-winning smart stroller that uses the NVIDIA Jetson edge AI and robotics platform to power its AI features.</p>
<h2><b>Subscribe to the AI Podcast</b></h2>
<p>Get the <a href="https://blogs.nvidia.com/ai-podcast/">AI Podcast</a> through <a href="https://itunes.apple.com/us/podcast/the-ai-podcast/id1186480811?mt=2&amp;adbsc=social_20161220_68874946&amp;adbid=811257941365882882&amp;adbpl=tw&amp;adbpr=61559439">iTunes</a>, <a href="https://podcasts.google.com/?feed=aHR0cHM6Ly9mZWVkcy5zb3VuZGNsb3VkLmNvbS91c2Vycy9zb3VuZGNsb3VkOnVzZXJzOjI2NDAzNDEzMy9zb3VuZHMucnNz">Google Podcasts</a>, <a href="https://play.google.com/music/listen?u=0#/ps/I4kyn74qfrsdhrm35mcrf3igxzm">Google Play</a>, <a href="https://music.amazon.com/podcasts/956857d0-9461-4496-a07e-24be0539ee82/the-ai-podcast">Amazon Music</a>, <a href="https://castbox.fm/channel/The-AI-Podcast-id433488?country=us">Castbox</a>, DoggCatcher, <a href="https://overcast.fm/itunes1186480811/the-ai-podcast">Overcast</a>, <a href="https://player.fm/series/the-ai-podcast">PlayerFM</a>, Pocket Casts, <a href="http://www.podbay.fm/show/1186480811">Podbay</a>, <a href="https://www.podbean.com/podcast-detail/cjgnp-4a6e0/The-AI-Podcast">PodBean</a>, PodCruncher, PodKicker,<a href="https://soundcloud.com/theaipodcast"> Soundcloud</a>, <a href="https://open.spotify.com/show/4TB4pnynaiZ6YHoKmyVN0L">Spotify</a>, <a href="http://www.stitcher.com/s?fid=130629&amp;refid=stpr">Stitcher</a> and <a href="https://tunein.com/podcasts/Technology-Podcasts/The-AI-Podcast-p940829/">TuneIn</a>.</p>
<p>Make the AI Podcast better: Have a few minutes to spare? Fill out <a href="http://survey.podtrac.com/start-survey.aspx?pubid=I5V0tOQFNS8j&amp;ver=short">this listener survey</a>.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2018/05/ai-podcast.jpg"
			type="image/jpeg"
			width="1400"
			height="931"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2018/05/ai-podcast-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[A New Lens: Dotlumen CEO Cornel Amariei on Assistive Technology for the Visually Impaired]]></media:title>
			<media:description type="html">NVIDIA AI Podcast</media:description>
			</media:content>
			</item>
		<item>
		<title>Coming Up ACEs: Decoding the AI Technology That’s Enhancing Games With Realistic Digital Humans</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-ace-microservices-digital-humans/</link>
		
		<dc:creator><![CDATA[Jesse Clayton]]></dc:creator>
		<pubDate>Wed, 03 Apr 2024 13:00:41 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70963</guid>

					<description><![CDATA[Non-playable characters often play a crucial role in video game storytelling, but since they’re usually designed with a fixed purpose, they can get repetitive and boring — especially in vast worlds where there are thousands.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p><i>Editor’s note: This post is part of the </i><a href="https://blogs.nvidia.com/blog/tag/ai-decoded/"><i>AI Decoded series</i></a><i>, which demystifies AI by making the technology more accessible, and which showcases new hardware, software, tools and accelerations for RTX PC users.</i></p>
<p>Digital characters are leveling up.</p>
<p>Non-playable characters often play a crucial role in video game storytelling, but since they’re usually designed with a fixed purpose, they can get repetitive and boring — especially in vast worlds where there are thousands.</p>
<p>Thanks in part to incredible advances in visual computing like ray tracing and DLSS, video games are more immersive and realistic than ever, making dry encounters with NPCs especially jarring.</p>
<p>Earlier this year, production microservices for the <a href="https://developer.nvidia.com/ace">NVIDIA Avatar Cloud Engine</a> launched, giving game developers and digital creators an ace up their sleeve when it comes to making lifelike NPCs. ACE microservices allow developers to integrate state-of-the-art generative AI models into digital avatars in games and applications. With ACE microservices, NPCs can dynamically interact and converse with players in-game and in real time.</p>
<p>Leading game developers, studios and startups are already incorporating ACE into their titles, bringing new levels of personality and engagement to NPCs and digital humans.</p>
<h2><b>Bring Avatars to Life With NVIDIA ACE</b></h2>
<p>The process of creating NPCs starts with providing them a backstory and purpose, which helps guide the narrative and ensures contextually relevant dialogue. Then, ACE subcomponents work together to build avatar interactivity and enhance responsiveness.</p>
<p>NPCs tap up to four AI models to hear, process, generate dialogue and respond.</p>
<p>The player’s voice first goes into <a href="https://www.nvidia.com/en-us/ai-data-science/products/riva/">NVIDIA Riva</a>, a technology that builds fully customizable, real-time conversational AI pipelines and turns chatbots into engaging and expressive assistants using GPU-accelerated multilingual speech and translation microservices.</p>
<p>With ACE, <a href="https://www.nvidia.com/en-us/ai-data-science/products/riva/">Riva’s</a> automatic speech recognition (ASR) feature processes what was said and uses AI to deliver a highly accurate transcription in real time. Explore a Riva-powered <a href="https://www.nvidia.com/en-us/ai-data-science/products/riva/#demos">demo of speech-to-text</a> in a dozen languages.</p>
<p>The transcription then goes into an LLM — such as Google’s Gemma, Meta’s Llama 2 or Mistral — and taps Riva’s neural machine translation to generate a natural language text response. Next, Riva’s Text-to-Speech functionality generates an audio response.</p>
<p>Finally, <a href="https://www.nvidia.com/en-us/omniverse/apps/audio2face/">NVIDIA Audio2Face</a> (A2F) generates facial expressions that can be synced to dialogue in many languages. With the microservice, digital avatars can display dynamic, realistic emotions streamed live or baked in during post-processing.</p>
<p>The AI network automatically animates face, eyes, mouth, tongue and head motions to match the selected emotional range and level of intensity. And A2F can automatically infer emotion directly from an audio clip.</p>
<p>Each step happens in real time to ensure fluid dialogue between the player and the character. And the tools are customizable, giving developers the flexibility to build the types of characters they need for immersive storytelling or worldbuilding.</p>
<h2><b>Born to Roll</b></h2>
<p>At GDC and GTC, developers and platform partners showcased demos leveraging NVIDIA ACE microservices — from interactive NPCs in gaming to powerful digital human nurses.</p>
<p>Ubisoft is exploring new types of interactive gameplay with dynamic NPCs. NEO NPCs, the product of its latest research and development project, are designed to interact in real time with players, their environment and other characters, opening up new possibilities for dynamic and emergent storytelling.</p>
<p>The capabilities of these NEO NPCs were showcased through demos, each focused on different aspects of NPC behaviors, including environmental and contextual awareness; real-time reactions and animations; and conversation memory, collaboration and strategic decision-making. Combined, the demos spotlighted the technology’s potential to push the boundaries of game design and immersion.</p>
<p>Using Inworld AI technology, Ubisoft’s narrative team created two NEO NPCs, Bloom and Iron, each with their own background story, knowledge base and unique conversational style. Inworld technology also provided the NEO NPCs with intrinsic knowledge of their surroundings, as well as interactive responses powered by Inworld’s LLM. NVIDIA A2F provided facial animations and lip syncing for the two NPCs real time.</p>
<p><iframe loading="lazy" title="NVIDIA ACE | NVIDIA x Inworld AI - Pushing the Boundaries of Game Characters in Covert Protocol" width="500" height="281" src="https://www.youtube.com/embed/uryeFhnNzEs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>Inworld and NVIDIA set GDC abuzz with a new technology demo called Covert Protocol, which showcased NVIDIA ACE technologies and the Inworld Engine. In the demo, players controlled a private detective who completed objectives based on the outcome of conversations with NPCs on the scene. Covert Protocol unlocked social simulation game mechanics with AI-powered digital characters that acted as bearers of crucial information, presented challenges and catalyzed key narrative developments. This enhanced level of AI-driven interactivity and player agency is set to open up new possibilities for emergent, player-specific gameplay.</p>
<p>Built on Unreal Engine 5, Covert Protocol uses the Inworld Engine and NVIDIA ACE, including NVIDIA Riva ASR and A2F, to augment Inworld’s speech and animation pipelines.</p>
<p>In the latest version of the <a href="https://www.youtube.com/watch?v=psrXGPh80UM">NVIDIA Kairos</a> tech demo built in collaboration with Convai, which was shown at CES, Riva ASR and A2F were used to significantly improve NPC interactivity. Convai’s new framework allowed the NPCs to converse among themselves and gave them awareness of objects, enabling them to pick up and deliver items to desired areas. Furthermore, NPCs gained the ability to lead players to objectives and traverse worlds.</p>
<h2><b>Digital Characters in the Real World</b></h2>
<p>The technology used to create NPCs is also being used to animate avatars and digital humans. Going beyond gaming, task-specific generative AI is moving into healthcare, customer service and more.</p>
<p><iframe loading="lazy" title="Always Available, Real-Time Generative AI Healthcare Agents" width="500" height="281" src="https://www.youtube.com/embed/yg0m8eR7k24?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>NVIDIA collaborated with Hippocratic AI at GTC to extend its healthcare agent solution, showcasing the potential of a generative AI healthcare agent avatar. More work underway to develop a super-low-latency inference platform to power real-time use cases.</p>
<p>“Our digital assistants provide helpful, timely and accurate information to patients worldwide,” said Munjal Shah, cofounder and CEO of Hippocratic AI. “NVIDIA ACE technologies bring them to life with cutting-edge visuals and realistic animations that help better connect to patients.”</p>
<p>Internal testing of Hippocratic’s initial AI healthcare agents is focused on chronic care management, wellness coaching, health risk assessments, social determinants of health surveys, pre-operative outreach and post-discharge follow-up.</p>
<p><iframe loading="lazy" title="Sophie is a UneeQ digital human – and she&#039;s powered by Synanim and NVIDIA Audio2Face" width="500" height="281" src="https://www.youtube.com/embed/jK1Go1GVRLI?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>UneeQ is an autonomous digital human platform focused on AI-powered avatars for customer service and interactive applications. UneeQ integrated the NVIDIA A2F microservice into its platform and combined it with its Synanim ML synthetic animation technology to create highly realistic avatars for enhanced customer experiences and engagement.</p>
<p>“UneeQ combines NVIDIA animation AI with our own Synanim ML synthetic animation technology to deliver real-time digital human interactions that are emotionally responsive and deliver dynamic experiences powered by conversational AI,” said Danny Tomsett, founder and CEO at UneeQ.</p>
<h2><b>AI in Gaming</b></h2>
<p>ACE is one of the many NVIDIA AI technologies that bring games to the next level.</p>
<ul>
<li><a href="https://www.nvidia.com/en-us/geforce/technologies/dlss/">NVIDIA DLSS</a> is a breakthrough graphics technology that uses AI to increase frame rates and improve image quality on GeForce RTX GPUs.</li>
<li><a href="https://www.nvidia.com/en-us/geforce/rtx-remix/">NVIDIA RTX Remix</a> enables modders to easily capture game assets, automatically enhance materials with generative AI tools and quickly create stunning RTX remasters with full ray tracing and DLSS.</li>
<li>NVIDIA Freestyle, accessed through the new <a href="https://www.nvidia.com/en-us/geforce/news/nvidia-app-beta-download/">NVIDIA app</a> beta, lets users personalize the visual aesthetics of more than 1,200 games through real-time post-processing filters, with features like RTX HDR, RTX Dynamic Vibrance and more.</li>
<li>The <a href="https://www.nvidia.com/en-us/geforce/broadcasting/broadcast-app/">NVIDIA Broadcast app</a> transforms any room into a home studio, giving livestream AI-enhanced voice and video tools, including noise and echo removal, virtual background and AI green screen, auto-frame, video noise removal and eye contact.</li>
</ul>
<p>Experience the latest and greatest in AI-powered experiences with NVIDIA RTX PCs and workstations, and make sense of what’s new, and what’s next, with AI Decoded.</p>
<p><i>Get weekly updates directly in your inbox by subscribing to the </i><a href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><i>AI Decoded newsletter</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/ai-gaming-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/04/ai-gaming-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Coming Up ACEs: Decoding the AI Technology That’s Enhancing Games With Realistic Digital Humans]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Greater Scope: Doctors Get Inside Look at Gut Health With AI-Powered Endoscopy</title>
		<link>https://blogs.nvidia.com/blog/ai-powered-endoscopy-odin-vision-olympus/</link>
		
		<dc:creator><![CDATA[Isha Salian]]></dc:creator>
		<pubDate>Thu, 28 Mar 2024 15:00:11 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Software]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Healthcare and Life Sciences]]></category>
		<category><![CDATA[Inception]]></category>
		<category><![CDATA[Inference]]></category>
		<category><![CDATA[Social Impact]]></category>
		<category><![CDATA[TensorRT]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70912</guid>

					<description><![CDATA[From humble beginnings as a university spinoff to an acquisition by the leading global medtech company in its field, Odin Vision has been on an accelerated journey since its founding less than five years ago. An alum of the NVIDIA Inception program for cutting-edge startups, Odin Vision builds cloud-connected AI software that helps clinicians detect		<a class="read-more" href="https://blogs.nvidia.com/blog/ai-powered-endoscopy-odin-vision-olympus/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>From humble beginnings as a university spinoff to an acquisition by the leading global medtech company in its field, Odin Vision has been on an accelerated journey since its founding less than five years ago.</p>
<p>An alum of the <a href="https://www.nvidia.com/startups/?nvid=nv-int-tblg-295718-vt33">NVIDIA Inception</a> program for cutting-edge startups, Odin Vision builds cloud-connected AI software that helps clinicians detect and characterize areas of concern during endoscopy, a procedure where a tiny camera mounted on a tube is inserted into the gastrointestinal tract.</p>
<p>Network-connected devices in the endoscopy room capture and stream real-time video data to the cloud, where powerful NVIDIA GPUs run AI inference. The models’ results are then streamed back to the endoscopy room so that clinicians can see the AI insights overlaid on the live video feed with minimal latency.</p>
<p>The startup was in 2022 acquired by Japanese medtech leader Olympus, which has a 70% global market share in gastrointestinal endoscopic equipment.</p>
<p>“We believe the acquisition brings us much closer to achieving our vision to revolutionize endoscopy through AI and cloud technology,” said Daniel Toth, cofounder and chief technology officer of Odin Vision. “Our software can reach Olympus’ global customer base, enabling us to bring our solutions to as many patients as possible.”</p>
<p>Olympus is also collaborating with NVIDIA on Olympus Office Hours, an advisory program that connects Inception startups with the medical device company’s experts, who will offer deep industry expertise and guidance to help the startups build AI solutions in key areas including gastroenterology, urology and surgery.</p>
<p>Eight leading AI startups have joined the inaugural cohort of the program — which is part of the NVIDIA Inception Alliance for Healthcare, an initiative that brings together medical AI startups with NVIDIA and its healthcare industry partners — to help accelerate their product development and go-to-market goals.</p>
<h2><b>An Extra Set of AIs for Clinicians</b></h2>
<p>Around <a href="https://newsnetwork.mayoclinic.org/discussion/ai-reduces-miss-rate-of-precancerous-polyps-in-colorectal-cancer-screening/#:~:text=The%20rate%20at%20which%20precancerous%20colorectal%20polyps%20is%20missed%20has,that%20had%20standard%20colonoscopy%20first." target="_blank" rel="noopener">a quarter of precancerous polyps are missed</a> during colonoscopies, a kind of endoscopy procedure that examines the lower digestive tract.</p>
<p>While some are missed because the endoscope doesn’t capture video footage of every angle, others remain undetected by clinicians. That’s where AI can help provide a second set of eyes to support clinical decision-making.</p>
<p>Seamless AI integration into the video feeds that medical professionals view during an endoscopy provides an extra data source that can help doctors detect and remove polyps sooner, helping prevent cancer development.</p>
<p>“Polyps develop slowly, and can take five or 10 years to appear as cancer,” Toth said. “If a clinician can detect and remove them in time, it can help save lives.”</p>
<p>CADDIE, the company’s AI software for detecting and classifying polyps, has received the CE Mark of regulatory approval in Europe and is deployed across hospitals in the U.K., Spain, Germany, Poland and Italy — with plans for use in the U.S as well.</p>
<p>Odin Vision also has AI software that has received the CE Mark to assist gastroscopy, where doctors inspect the esophagus for signs of throat cancer.</p>
<h2><b>Accelerated Inference for Real-Time Insights</b></h2>
<p>Odin Vision began as a research project by two professors and a Ph.D. student at University College London who were developing AI techniques for polyp detection. In 2019, they teamed with Toth and Odin’s CEO, Peter Mountney, both from Siemens Healthineers, to commercialize their work.</p>
<p>“NVIDIA GPUs were part of our work from the start — they’ve been essential to train our AI models and were part of our first product prototypes for inference, too,” Toth said. “Since moving to a cloud-based deployment, we’ve begun using the NVIDIA Triton Inference Server for dynamic processing in the cloud.”</p>
<p>The team uses <a href="https://www.nvidia.com/en-us/data-center/tensor-cores/">NVIDIA Tensor Core GPUs</a> for accelerated inference — most recently transitioning to <a href="https://www.nvidia.com/en-us/data-center/l4/">NVIDIA L4 GPUs</a>. Adopting <a href="https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/">NVIDIA Triton Inference Server</a> software and the <a href="https://developer.nvidia.com/tensorrt">NVIDIA TensorRT</a> software development kit enabled them to meet the low-latency threshold needed for real-time video-processing AI applications.</p>
<p>In addition to supporting doctors during specific procedures, Odin Vision plans to develop generative AI models that can automate a first draft of the clinical notes doctors prepare afterward — as well as models that can aggregate data across procedures. These would allow endoscopy teams to review analytics and assess how well a procedure is performed compared to clinical guidelines.</p>
<p>“Once you get to a point where there are dozens of AI models tracking different elements of these procedures, we can see if a healthcare professional is inspecting a particular area of the digestive tract for only three minutes, when it’s supposed to take six minutes,” Toth said. “The system can provide a nudge to remind the clinician to follow the guidelines.”</p>
<h2><b>Cloud-Connected Cancer Screening</b></h2>
<p>Membership in NVIDIA Inception provided the Odin Vision team access to technical expertise from NVIDIA and cloud credits through leading cloud service providers.</p>
<p>“Cloud credits helped us massively speed up our technology development and deployment, enabling us to release our products to market months earlier than initially planned,” Toth said. “NVIDIA experts also validated our product concept from a technology perspective and provided consultation about GPU and accelerated software optimizations.”</p>
<p>The team found that a cloud-based solution made it easier to push software updates over the air to deployments across hospital customers.</p>
<p>“Some AI companies are sending boxes that need to sit in clinical sites and require regular maintenance, which can prevent normal clinical workflows from running smoothly,” Toth said. “With network-connected devices, we can instead update a single server and the changes reach all end users at the same time.”</p>
<p><i>Learn more about </i><a href="https://www.nvidia.com/startups/?nvid=nv-int-tblg-295718-vt33"><i>NVIDIA Inception</i></a><i> and </i><a href="https://www.nvidia.com/en-us/industries/healthcare-life-sciences/healthcare-news-sign-up/"><i>subscribe to NVIDIA healthcare news</i></a><i>. </i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/AdobeStock_309555032_crop.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/AdobeStock_309555032_crop-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Greater Scope: Doctors Get Inside Look at Gut Health With AI-Powered Endoscopy]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Get Cozy With ‘Palia’ on GeForce NOW</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-palia/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 28 Mar 2024 13:00:18 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70939</guid>

					<description><![CDATA[Ease into spring with the warm, cozy vibes of Palia, coming to the cloud this GFN Thursday. It’s part of six new titles joining the GeForce NOW library of over 1,800 games. Welcome Home Escape to a cozy world with Palia, a free-to-play massively multiplayer online game from Singularity 6 Corporation. The game, which has		<a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-palia/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>Ease into spring with the warm, cozy vibes of <i>Palia</i>, coming to the cloud this GFN Thursday.</p>
<p>It’s part of six new titles joining the <a href="http://play.geforcenow.com">GeForce NOW library</a> of over 1,800 games.</p>
<h2><b>Welcome Home</b></h2>
<figure id="attachment_70943" aria-describedby="caption-attachment-70943" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-70943" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Palia-672x378.jpg" alt="Palia on GeForce NOW" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Palia-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Palia-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Palia-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Palia-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Palia-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Palia-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Palia-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Palia-1280x720.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Palia.jpg 1920w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-70943" class="wp-caption-text"><em>Better together.</em></figcaption></figure>
<p>Escape to a cozy world with <i>Palia</i>, a free-to-play massively multiplayer online game from Singularity 6 Corporation. The game, which has made its way onto more than 200,000 wishlists on Steam, has launched in the cloud this week.</p>
<p>Farm, fish, craft and explore with friendly villagers across a stunning variety of different biomes — from sprawling flower fields to hilly forests and rocky beaches — in the world of Palia. Inhabit the land, furnish a dream home, unravel ancient mysteries and interact with a vibrant online community.</p>
<p>Get ready for a captivating adventure across devices by streaming <i>Palia</i> from the cloud. GeForce NOW Ultimate and Priority members get faster access to servers and longer gaming sessions over Free members.</p>
<h2><b>Time to Play</b></h2>
<figure id="attachment_70946" aria-describedby="caption-attachment-70946" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-70946" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Milennia-672x336.jpg" alt="Millennia on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Milennia-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Milennia-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Milennia-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Milennia-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Milennia-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Milennia-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Milennia-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Milennia-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/03/GFN_Thursday-Milennia.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-70946" class="wp-caption-text"><em>10,000 years of history, all in the cloud.</em></figcaption></figure>
<p>Shape the course of history across 10,000 years in <i>Millennia</i> from C Prompt Games and Paradox Interactive. GeForce NOW members can customize their own nations, explore unique combinations of traits and adapt to alternative histories in this captivating journey.</p>
<p>In addition, members can look for the following:</p>
<ul>
<li><i>Palia </i>(New release on <a href="https://store.steampowered.com/app/2707930?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, March 25)</li>
<li><i>Bulwark: Falconeer Chronicles </i>(New release on <a href="https://store.steampowered.com/app/290100?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, March 26)</li>
<li><i>Millennia </i>(New release on <a href="https://store.steampowered.com/app/1268590?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, March 26)</li>
<li><i>Outpost: Infinity Siege </i>(New release on <a href="https://store.steampowered.com/app/1566690?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, March 26)</li>
<li><i>SOUTH PARK: SNOW DAY! </i>(New release on <a href="https://store.steampowered.com/app/1214650?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, March 26)</li>
<li><i>Tchia</i> (<a href="https://store.steampowered.com/app/1496590?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
</ul>
<p>What are you planning to play this weekend? Let us know on <a href="https://www.twitter.com/nvidiagfn">X</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">if you could live in a video game, which one would it be and why? <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f914.png" alt="🤔" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/14.0.0/72x72/1f329.png" alt="🌩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a href="https://twitter.com/NVIDIAGFN/status/1773017134517346499?ref_src=twsrc%5Etfw">March 27, 2024</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gfn-thursday-3-28-nv-blog-1280x680-no-copy.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/gfn-thursday-3-28-nv-blog-1280x680-no-copy-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Get Cozy With ‘Palia’ on GeForce NOW]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Software Developers Launch OpenUSD and Generative AI-Powered Product Configurators Built on NVIDIA Omniverse</title>
		<link>https://blogs.nvidia.com/blog/3d-product-configurators-omniverse-openusd/</link>
		
		<dc:creator><![CDATA[Dane Johnston]]></dc:creator>
		<pubDate>Wed, 27 Mar 2024 23:26:27 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[Generative AI]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70924</guid>

					<description><![CDATA[From designing dream cars to customizing clothing, 3D product configurators are ringing in a new era of hyper-personalization that will benefit retailers and consumers. Developers are delivering innovative virtual product experiences and automated personalization using Universal Scene Description (aka OpenUSD), NVIDIA RTX technologies from NVIDIA Omniverse software development kits (SDKs) and application programming interfaces (APIs),		<a class="read-more" href="https://blogs.nvidia.com/blog/3d-product-configurators-omniverse-openusd/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>From designing dream cars to customizing clothing, 3D product configurators are ringing in a new era of hyper-personalization that will benefit retailers and consumers.</p>
<p>Developers are delivering innovative virtual product experiences and automated personalization using <a href="https://www.nvidia.com/en-us/omniverse/usd/">Universal Scene Description (aka OpenUSD)</a>, <a href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX</a> technologies from <a href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> software development kits (SDKs) and application programming interfaces (APIs), and generative AI from <a href="https://blogs.nvidia.com/blog/edify-3d-generative-ai-custom-fine-tuning/">NVIDIA Edify</a> models.</p>
<p>Together, these technologies enable developers to create configurator applications that deliver physically accurate, photoreal <a href="https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/">digital twins</a> of products, revolutionizing the way brands personalize buyer journeys at unprecedented scale.</p>
<p>For example, <a href="https://blog.3ds.com/brands/3dexcite/dassault-systemes-collaborates-with-nvidia-to-showcase-the-future-of-storytelling-with-generative-ai/">Dassault Systèmes’ 3DEXCITE</a> brand is adopting <a href="https://nvidianews.nvidia.com/news/omniverse-cloud-apis-industrial-digital-twin">Omniverse Cloud APIs</a> to enable interoperability with generative AI services, such as Shutterstock’s Edify3D or Edify 360, directly inside its web-based application.</p>
<p>By using NVIDIA Edify-powered models, trained by Shutterstock, Dassault Systèmes can generate stunning 3D environments from text prompts to instantly personalize scenes representing physically accurate products. And with Omniverse APIs, the company can supercharge the web-based app with real-time ray-traced rendering.</p>
<p><iframe loading="lazy" title="The Future of Storytelling with Generative AI and NVIDIA" width="500" height="281" src="https://www.youtube.com/embed/h30brb7-taU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>Many other developers are also building 3D product configurator software and solutions with NVIDIA Omniverse SDKs and APIs.</p>
<p>CGI studio <a href="https://www.katanaus.com/">Katana</a> has developed a content creation application, COATCreate, used by manufacturers such as <a href="https://www.nissanusa.com/">Nissan</a>, that allows marketing assets to be staged and created faster with product digital twins. COATCreate also enables users to <a href="https://blogs.nvidia.com/blog/omniverse-apple-vision-pro/">view and customize the digital twin while wearing an Apple Vision Pro headset</a>, unlocking real-time ray-traced extended reality experiences.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-1.jpg"><img loading="lazy" decoding="async" class="aligncenter size-large wp-image-70931" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-1-672x357.jpg" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-1-672x357.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-1-400x213.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-1-768x408.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-1-842x447.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-1-406x215.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-1-188x100.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-1.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p><a href="https://www.brickland.se/">Brickland</a>, another CGI studio, is developing real-time virtual experiences that allow users to customize clothing by choosing from predefined parameters such as color and material. Through their <a href="https://www.brickland.se/digitex/winter">Digitex initiative</a>, Brickland is expanding into digital textures and allowing consumers to visualize and interact with extreme levels of detail in their 3D assets thanks to RTX real-time rendering</p>
<p><a href="https://configit.com/">Configit</a> connected its powerful configurator logic tool Configit Ace to Omniverse and OpenUSD by streamlining the management of the complex rules system behind the creation of configurators and combining it with the rendering capabilities of Omniverse and RTX. This allows for rapid creation of articulated product configurators and enables the configurator developers to power real-time ray-traced rendering in their solutions.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-2.jpg"><img loading="lazy" decoding="async" class="aligncenter size-large wp-image-70934" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-2-672x357.jpg" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-2-672x357.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-2-400x213.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-2-768x408.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-2-842x447.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-2-406x215.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-2-188x100.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Copy-2.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p><a href="https://www.wpp.com/wpp-iq/2024/03/wpp-puts-itself-at-the-heart-of-collaborative-3d-worlds">WPP</a> has developed a <a href="https://www.wpp.com/en/news/2023/05/wpp-partners-with-nvidia-to-build-generative-ai-enabled-content-engine-for-digital-advertising">content engine that harnesses OpenUSD and AI</a> to enable creative teams to produce high-quality commercial content faster, more efficiently and at scale while remaining aligned with a client’s brand.</p>
<p><a href="https://media.monks.com/">Media.Monks</a> has developed an AI-centric professional managed service that leverages Omniverse called Monks.Flow, which helps brands virtually explore different customizable product designs and unlock scale and hyper-personalization across any customer journey.</p>
<p><a href="https://newsroom.accenture.com/news/2024/accenture-teams-with-nvidia-to-showcase-ai-powered-immersive-client-experiences-for-defender">Accenture Song</a>, the world’s largest tech-powered creative group, is using Omniverse SDKs to generate marketing content for Defender vehicles. Using it with the Edify-powered generative AI microservice, Accenture Song is enabling the creation of cinematic 3D environments via conversational prompts.</p>
<h2><b>Product Digital Twins in the Era of Industrial Digitalization</b></h2>
<p>Forecasts indicate that consumer purchases, including high-value items like vehicles and luxury goods, <a href="https://www.forbes.com/advisor/business/ecommerce-statistics/">will increasingly take place online</a> in the coming decade. 3D product digital twins and automated personalization with generative AI serve as invaluable tools for brands to showcase their products and enhance customer engagement in the changing retail landscape.</p>
<p>3D configurators provide tangible benefits for businesses, including increased average selling prices, reduced return rates and stronger brand loyalty. Once a digital twin is built, it can serve many purposes and be updated to meet shifting consumer preferences with minimal time, cost and effort.</p>
<p><iframe loading="lazy" title="Develop 3D Product Configurators With Generative AI and OpenUSD" width="500" height="281" src="https://www.youtube.com/embed/LAVXuDK83iA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Creating a 3D Product Configurator</b></h2>
<p>The process of creating a 3D product configurator begins with harnessing OpenUSD’s powerful composition engine and interoperability. These features enable developers to create dynamic, interactive experiences that accurately reflect the nuances of each product.</p>
<p>Teams can also integrate generative AI technologies into OpenUSD-based product configurators using NVIDIA Omniverse APIs to enhance the realism and customization options available to users. By leveraging AI, configurators can intelligently adapt to user inputs, offering personalized recommendations and dynamically adjusting product configurations in real time. And with <a href="https://www.nvidia.com/en-us/omniverse/solutions/stream-3d-apps/">NVIDIA Graphics Delivery Network</a> , high-quality, real-time viewports can be embedded into web applications so consumers can browse products in full fidelity, on nearly any device.</p>
<p>The possibilities for 3D product configurators are virtually limitless, applicable across a wide range of industries and use cases.</p>
<p>To start, get <a href="https://www.nvidia.com/en-us/omniverse/#get-started">NVIDIA Omniverse</a> and follow along with a <a href="https://www.youtube.com/watch?v=xoY2Cyxy9pQ&amp;list=PL3jK4xNnlCVdruh1y5cIwJx3FPBwoZtXR&amp;index=1">tutorial series</a>.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Header.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/OV-Dev-Blog-Header-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Software Developers Launch OpenUSD and Generative AI-Powered Product Configurators Built on NVIDIA Omniverse]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Hopper Leaps Ahead in Generative AI at MLPerf</title>
		<link>https://blogs.nvidia.com/blog/tensorrt-llm-inference-mlperf/</link>
		
		<dc:creator><![CDATA[Dave Salvator]]></dc:creator>
		<pubDate>Wed, 27 Mar 2024 15:40:47 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GPU]]></category>
		<category><![CDATA[Hardware]]></category>
		<category><![CDATA[Inference]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[NGC]]></category>
		<category><![CDATA[NVIDIA Hopper Architecture]]></category>
		<category><![CDATA[TensorRT]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70672</guid>

					<description><![CDATA[It’s official: NVIDIA delivered the world’s fastest platform in industry-standard tests for inference on generative AI. In the latest MLPerf benchmarks, NVIDIA TensorRT-LLM — software that speeds and simplifies the complex job of inference on large language models — boosted the performance of NVIDIA Hopper architecture GPUs on the GPT-J LLM nearly 3x over their		<a class="read-more" href="https://blogs.nvidia.com/blog/tensorrt-llm-inference-mlperf/">
			Read Article			<span data-icon="y"></span>
		</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p>It’s official: NVIDIA delivered the world’s fastest platform in industry-standard tests for inference on <a href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/">generative AI</a>.</p>
<p>In the latest MLPerf benchmarks, <a href="https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/">NVIDIA TensorRT-LLM</a> — software that speeds and simplifies the complex job of inference on <a href="https://www.nvidia.com/en-us/glossary/large-language-models/">large language models</a> — boosted the performance of <a href="https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/">NVIDIA Hopper architecture GPUs</a> on the GPT-J LLM nearly 3x over their results just six months ago.</p>
<p>The dramatic speedup demonstrates the power of NVIDIA’s full-stack platform of chips, systems and software to handle the demanding requirements of running generative AI.</p>
<p>Leading companies <a href="https://developer.nvidia.com/blog/nvidia-tensorrt-llm-supercharges-large-language-model-inference-on-nvidia-h100-gpus/">are using</a> TensorRT-LLM to optimize their models. And <a href="https://www.nvidia.com/en-us/launchpad/ai/generative-ai-inference-with-nim/">NVIDIA NIM</a>  — a set of inference microservices that includes inferencing engines like TensorRT-LLM — makes it easier than ever for businesses to deploy NVIDIA’s inference platform.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-1-GPTJ-LLM-3x.jpg"><img loading="lazy" decoding="async" class="aligncenter wp-image-70917 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-1-GPTJ-LLM-3x-672x378.jpg" alt="Chart of NVIDIA Hopper GPUs with TensorRT-LLM on MLPerf Inference GPT-J" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-1-GPTJ-LLM-3x-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-1-GPTJ-LLM-3x-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-1-GPTJ-LLM-3x-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-1-GPTJ-LLM-3x-1536x865.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-1-GPTJ-LLM-3x-799x450.jpg 799w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-1-GPTJ-LLM-3x-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-1-GPTJ-LLM-3x-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-1-GPTJ-LLM-3x-1280x721.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-1-GPTJ-LLM-3x.jpg 2014w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<h2><b>Raising the Bar in Generative AI</b></h2>
<p>TensorRT-LLM running on <a href="https://www.nvidia.com/en-us/data-center/h200/">NVIDIA H200 Tensor Core GPUs</a> — the latest, memory-enhanced Hopper GPUs — delivered the fastest performance running inference in MLPerf’s biggest test of generative AI to date.</p>
<p>The new benchmark uses the largest version of Llama 2, a state-of-the-art large language model packing 70 billion parameters. The model is more than 10x larger than the GPT-J LLM first used in the <a href="https://blogs.nvidia.com/blog/grace-hopper-inference-mlperf/">September benchmarks</a>.</p>
<p>The memory-enhanced H200 GPUs, in their MLPerf debut, used TensorRT-LLM to produce up to 31,000 tokens/second, a record on MLPerf’s Llama 2 benchmark.</p>
<p>The H200 GPU results include up to 14% gains from a custom thermal solution. It’s one example of innovations beyond standard air cooling that systems builders are applying to their <a href="https://www.nvidia.com/en-us/data-center/products/mgx/">NVIDIA MGX</a> designs to take the performance of Hopper GPUs to new heights.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-2-Llama-2-win.jpg"><img loading="lazy" decoding="async" class="aligncenter wp-image-70920 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-2-Llama-2-win-672x371.jpg" alt="Chart of NVIDIA performance on MLPerf inference Llama 2 70B" width="672" height="371" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-2-Llama-2-win-672x371.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-2-Llama-2-win-400x221.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-2-Llama-2-win-768x424.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-2-Llama-2-win-1536x847.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-2-Llama-2-win-816x450.jpg 816w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-2-Llama-2-win-390x215.jpg 390w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-2-Llama-2-win-181x100.jpg 181w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-2-Llama-2-win-1280x706.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/03/NU-MLPerf-2-Llama-2-win.jpg 2018w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<h2><b>Memory Boost for NVIDIA Hopper GPUs</b></h2>
<p>NVIDIA is sampling H200 GPUs to customers today and shipping in the second quarter. They’ll be available soon from nearly 20 leading system builders and cloud service providers.</p>
<p>H200 GPUs pack 141GB of HBM3e running at 4.8TB/s. That’s 76% more memory flying 43% faster compared to H100 GPUs. These accelerators plug into the same boards and systems and use the same software as H100 GPUs.</p>
<p>With HBM3e memory, a single H200 GPU can run an entire Llama 2 70B model with the highest throughput, simplifying and speeding inference.</p>
<h2><b>GH200 Packs Even More Memory</b></h2>
<p>Even more memory — up to 624GB of fast memory, including 144GB of HBM3e — is packed in <a href="https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/">NVIDIA GH200 Superchips</a>, which combine on one module a Hopper architecture GPU and a power-efficient <a href="https://www.nvidia.com/en-us/data-center/grace-cpu/">NVIDIA Grace CPU</a>. NVIDIA accelerators are the first to use HBM3e memory technology.</p>
<p>With nearly 5 TB/second memory bandwidth, GH200 Superchips delivered standout performance, including on memory-intensive MLPerf tests such as <a href="https://blogs.nvidia.com/blog/grace-hopper-recommender-systems/">recommender systems</a>.</p>
<h2><b>Sweeping Every MLPerf Test</b></h2>
<p>On a per-accelerator basis, Hopper GPUs swept every test of AI inference in the latest round of the MLPerf industry benchmarks.</p>
<p>In addition, <a href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/">NVIDIA Jetson Orin</a> remains at the forefront in MLPerf’s edge category. In the last two inference rounds, Orin ran the most diverse set of models in the category, including GPT-J and Stable Diffusion XL.</p>
<p>The MLPerf benchmarks cover today’s most popular AI workloads and scenarios, including generative AI, recommendation systems, natural language processing, speech and computer vision. NVIDIA was the only company to submit results on every workload in the latest round and every round since MLPerf’s data center inference benchmarks began in October 2020.</p>
<p>Continued performance gains translate into lower costs for inference, a large and growing part of the daily work for the millions of NVIDIA GPUs deployed worldwide.</p>
<h2><b>Advancing What’s Possible</b></h2>
<p>Pushing the boundaries of what’s possible, NVIDIA demonstrated three innovative techniques in a special section of the benchmarks called the open division, created for testing advanced AI methods.</p>
<p>NVIDIA engineers used a technique called <a href="https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/">structured sparsity</a> — a way of reducing calculations, first introduced with <a href="https://www.nvidia.com/en-us/data-center/a100/">NVIDIA A100 Tensor Core GPUs</a> — to deliver up to 33% speedups on inference with Llama 2.</p>
<p>A second open division test found inference speedups of up to 40% using pruning, a way of simplifying an AI model — in this case, an LLM — to increase inference throughput.</p>
<p>Finally, an optimization called DeepCache reduced the math required for inference with the Stable Diffusion XL model, accelerating performance by a whopping 74%.</p>
<p>All these results were run on <a href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPUs</a>.</p>
<h2><b>A Trusted Source for Users</b></h2>
<p>MLPerf’s tests are transparent and objective, so users can rely on the results to make informed buying decisions.</p>
<p>NVIDIA’s partners participate in MLPerf because they know it’s a valuable tool for customers evaluating AI systems and services. Partners submitting results on the NVIDIA AI platform in this round included ASUS, Cisco, Dell Technologies, Fujitsu, GIGABYTE, Google, Hewlett Packard Enterprise, Lenovo, Microsoft Azure, Oracle, QCT, Supermicro, VMware (recently acquired by Broadcom) and Wiwynn.</p>
<p>All the software NVIDIA used in the tests is available in the MLPerf repository. These optimizations are continuously folded into containers available on <a href="https://ngc.nvidia.com/catalog">NGC</a>, NVIDIA’s software hub for GPU applications, as well as <a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> — a secure, supported platform that includes NIM inference microservices.</p>
<h2><b>The Next Big Thing  </b></h2>
<p>The use cases, model sizes and datasets for generative AI continue to expand. That’s why MLPerf continues to evolve, adding real-world tests with popular models like Llama 2 70B and Stable Diffusion XL.</p>
<p>Keeping pace with the explosion in LLM model sizes, NVIDIA founder and CEO Jensen Huang announced last week at GTC that the <a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/">NVIDIA Blackwell architecture GPUs</a> will deliver new levels of performance required for the multitrillion-parameter AI models.</p>
<p>Inference for large language models is difficult, requiring both expertise and the full-stack architecture NVIDIA demonstrated on MLPerf with Hopper architecture GPUs and TensorRT-LLM. There’s much more to come.</p>
<p>Learn more about <a href="https://www.nvidia.com/en-us/data-center/resources/mlperf-benchmarks/">MLPerf benchmarks</a> and the <a href="https://developer.nvidia.com/blog/nvidia-h200-tensor-core-gpus-and-nvidia-tensorrt-llm-set-mlperf-llm-inference-records/">technical details</a> of this inference round.</p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/NVIDIA-H200-HGX-image.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/NVIDIA-H200-HGX-image-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Hopper Leaps Ahead in Generative AI at MLPerf]]></media:title>
			<media:description type="html">NVIDIA HGX H200 GPU system running TensorRT-LLM</media:description>
			</media:content>
			</item>
		<item>
		<title>Unlocking Peak Generations: TensorRT Accelerates AI on RTX PCs and Workstations</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-tensorrt-stable-diffusion-automatic1111/</link>
		
		<dc:creator><![CDATA[Jesse Clayton]]></dc:creator>
		<pubDate>Wed, 27 Mar 2024 13:00:13 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=70900</guid>

					<description><![CDATA[As generative AI advances and becomes widespread across industries, the importance of running generative AI applications on local PCs and workstations grows.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"><p><i>Editor’s note: This post is part of the </i><a href="https://blogs.nvidia.com/blog/tag/ai-decoded/"><i>AI Decoded series</i></a><i>, which demystifies AI by making the technology more accessible, and which showcases new hardware, software, tools and accelerations for RTX PC users.</i></p>
<p>As <a href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a> advances and becomes widespread across industries, the importance of running generative AI applications on local PCs and workstations grows. Local <a href="https://blogs.nvidia.com/blog/difference-deep-learning-training-inference-ai/">inference</a> gives consumers reduced latency, eliminates their dependency on the network and enables more control over their data.</p>
<p>NVIDIA GeForce and NVIDIA RTX GPUs feature Tensor Cores, dedicated AI hardware accelerators that provide the horsepower to run generative AI locally.</p>
<p>Stable Video Diffusion is now optimized for the <a href="https://developer.nvidia.com/tensorrt">NVIDIA TensorRT</a> software development kit, which unlocks the highest-performance generative AI on the more than 100 million Windows PCs and workstations powered by RTX GPUs.</p>
<p>Now, the TensorRT extension for the popular Stable Diffusion WebUI by Automatic1111 is adding support for ControlNets, tools that give users more control to refine generative outputs by adding other images as guidance.</p>
<p><iframe loading="lazy" title="Accelerate Stable Diffusion with NVIDIA RTX GPUs" width="500" height="281" src="https://www.youtube.com/embed/fUAEBoJCJW8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>TensorRT acceleration can be put to the test in the new UL Procyon AI Image Generation benchmark, which internal tests have shown accurately replicates real-world performance. It delivered speedups of 50% on a GeForce RTX 4080 SUPER GPU compared with the fastest non-TensorRT implementation.</p>
<h2><b>More Efficient and Precise AI</b></h2>
<p>TensorRT enables developers to access the hardware that provides fully optimized AI experiences. AI performance typically doubles compared with running the application on other frameworks.</p>
<p>It also <a href="https://developer.nvidia.com/blog/new-stable-diffusion-models-accelerated-with-nvidia-tensorrt/">accelerates</a> the most popular generative AI models, like Stable Diffusion and SDXL. <a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1-tensorrt">Stable Video Diffusion</a>, Stability AI’s image-to-video generative AI model, experiences a 40% speedup with TensorRT.</p>
<p>The optimized <a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1-tensorrt">Stable Video Diffusion 1.1 Image-to-Video</a> model can be downloaded on Hugging Face.</p>
<p>Plus, the TensorRT extension for Stable Diffusion WebUI boosts performance by up to 2x — significantly streamlining Stable Diffusion workflows.</p>
<p>With the extension’s latest update, TensorRT optimizations extend to ControlNets — a set of AI models that help guide a diffusion model’s output by adding extra conditions. With TensorRT, ControlNets are 40% faster.</p>
<figure id="attachment_70907" aria-describedby="caption-attachment-70907" style="width: 1280px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/03/Auto1111-ControlNet-Visual_1.jpg"><img loading="lazy" decoding="async" class="size-full wp-image-70907" src="https://blogs.nvidia.com/wp-content/uploads/2024/03/Auto1111-ControlNet-Visual_1.jpg" alt="" width="1280" height="720" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/03/Auto1111-ControlNet-Visual_1.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Auto1111-ControlNet-Visual_1-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Auto1111-ControlNet-Visual_1-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Auto1111-ControlNet-Visual_1-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Auto1111-ControlNet-Visual_1-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Auto1111-ControlNet-Visual_1-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/03/Auto1111-ControlNet-Visual_1-178x100.jpg 178w" sizes="(max-width: 1280px) 100vw, 1280px" /></a><figcaption id="caption-attachment-70907" class="wp-caption-text">TensorRT optimizations extend to ControlNets for improved customization.</figcaption></figure>
<p>Users can guide aspects of the output to match an input image, which gives them more control over the final image. They can also use multiple ControlNets together for even greater control. A ControlNet can be a depth map, edge map, normal map or keypoint detection model, among others.</p>
<p>Download the TensorRT extension for Stable Diffusion Web UI on <a href="https://github.com/NVIDIA/Stable-Diffusion-WebUI-TensorRT">GitHub</a> today.</p>
<h2><b>Other Popular Apps Accelerated by TensorRT</b></h2>
<p>Blackmagic Design <a href="https://blogs.nvidia.com/blog/surface-studio-chaos-dlss-resolve-tensor-rt/">adopted NVIDIA TensorRT acceleration</a> in update 18.6 of DaVinci Resolve. Its AI tools, like Magic Mask, Speed Warp and Super Scale, run more than 50% faster and up to 2.3x faster on RTX GPUs compared with Macs.</p>
<p>In addition, with TensorRT integration, <a href="https://www.topazlabs.com/">Topaz Labs</a> saw an up to 60% performance increase in its Photo AI and Video AI apps — such as photo denoising, sharpening, photo super resolution, video slow motion, video super resolution, video stabilization and more — all running on RTX.</p>
<p>Combining Tensor Cores with TensorRT software brings unmatched generative AI performance to local PCs and workstations. And by running locally, several advantages are unlocked:</p>
<ul>
<li><strong>Performance</strong>: Users experience lower latency, since latency becomes independent of network quality when the entire model runs locally. This can be important for real-time use cases such as gaming or video conferencing. NVIDIA RTX offers the fastest AI accelerators, scaling to more than 1,300 AI trillion operations per second, or TOPS.</li>
<li><strong>Cost</strong>: Users don’t have to pay for cloud services, cloud-hosted application programming interfaces or infrastructure costs for large language model inference.</li>
<li><strong>Always on</strong>: Users can access LLM capabilities anywhere they go, without relying on high-bandwidth network connectivity.</li>
<li><strong>Data privacy</strong>: Private and proprietary data can always stay on the user’s device.</li>
</ul>
<h2><b>Optimized for LLMs</b></h2>
<p>What TensorRT brings to deep learning, <a href="https://developer.nvidia.com/tensorrt#inference">NVIDIA TensorRT-LLM</a> brings to the latest <a href="https://blogs.nvidia.com/blog/ai-decoded-rtx-pc-llms-chatbots/">LLMs</a>.</p>
<p>TensorRT-LLM, an open-source library that accelerates and optimizes LLM inference, includes out-of-the-box support for popular community models, including Phi-2, Llama2, Gemma, Mistral and Code Llama. Anyone — from developers and creators to enterprise employees and casual users — can experiment with TensorRT-LLM-optimized models in the <a href="https://build.nvidia.com/explore/discover">NVIDIA AI Foundation models</a>. Plus, with the <a href="https://blogs.nvidia.com/blog/chat-with-rtx-available-now/">NVIDIA ChatRTX</a> tech demo, users can see the performance of various models running locally on a Windows PC. ChatRTX is built on TensorRT-LLM for optimized performance on RTX GPUs.</p>
<p>NVIDIA is collaborating with the open-source community to develop native TensorRT-LLM connectors to popular application frameworks, including LlamaIndex and LangChain.</p>
<p>These innovations make it easy for developers to use TensorRT-LLM with their applications and experience the best LLM performance with RTX.</p>
<p><i>Get weekly updates directly in your inbox by subscribing to the </i><a href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><i>AI Decoded newsletter</i></a><i>.</i></p>
</div>]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/ai-decoded-tensor-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/03/ai-decoded-tensor-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Unlocking Peak Generations: TensorRT Accelerates AI on RTX PCs and Workstations]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
	</channel>
</rss>
