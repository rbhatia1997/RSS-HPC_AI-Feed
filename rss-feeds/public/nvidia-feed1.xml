<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:media="http://search.yahoo.com/mrss/">

<channel>
	<title>NVIDIA Blog</title>
	<atom:link href="https://blogs.nvidia.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.nvidia.com/</link>
	<description></description>
	<lastBuildDate>Wed, 10 Jul 2024 18:22:08 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.5.5</generator>
	<item>
		<title>Paige Cofounder Thomas Fuchs’ Diagnosis on Improving Cancer Patient Outcomes With AI</title>
		<link>https://blogs.nvidia.com/blog/paige-thomas-fuchs/</link>
		
		<dc:creator><![CDATA[Andy Bui]]></dc:creator>
		<pubDate>Wed, 10 Jul 2024 13:00:12 +0000</pubDate>
				<category><![CDATA[The AI Podcast]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72845</guid>

					<description><![CDATA[Improved cancer diagnostics — and improved patient outcomes — could be among the changes generative AI will bring to the healthcare industry, thanks to Paige, the first company with an FDA-approved tool for cancer diagnosis. In this episode of NVIDIA’s AI Podcast, host Noah Kravitz speaks with Paige cofounder and Chief Scientific Officer Thomas Fuchs.	<a class="read-more" href="https://blogs.nvidia.com/blog/paige-thomas-fuchs/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Improved cancer diagnostics — and improved patient outcomes — could be among the changes generative AI will bring to the healthcare industry, thanks to Paige, the first company with an FDA-approved tool for cancer diagnosis. In this episode of NVIDIA’s <a href="https://blogs.nvidia.com/ai-podcast/">AI Podcast</a>, host Noah Kravitz speaks with Paige cofounder and Chief Scientific Officer Thomas Fuchs. He’s also dean of artificial intelligence and human health at the Icahn School of Medicine at Mount Sinai.</p>
<p>Tune in to hear Fuchs on machine learning and AI applications and how technology brings better precision and care to the medical industry.</p>
<p><iframe src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/1864135386%3Fsecret_token%3Ds-dz9YPLgbXUB&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true" width="100%" height="166" frameborder="no" scrolling="no"></iframe></p>
<div style="font-size: 10px; color: #cccccc; line-break: anywhere; word-break: normal; overflow: hidden; white-space: nowrap; text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif; font-weight: 100;"><a style="color: #cccccc; text-decoration: none;" title="The AI Podcast" href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">The AI Podcast</a> · <a style="color: #cccccc; text-decoration: none;" title="Paige Cofounder Thomas Fuchs’ Diagnosis on Improving Cancer Patient Outcomes With AI" href="https://soundcloud.com/theaipodcast/paige-thomas-fuchs/s-dz9YPLgbXUB" target="_blank" rel="noopener">Paige Cofounder Thomas Fuchs’ Diagnosis on Improving Cancer Patient Outcomes With AI</a></div>
<h2><b>Time Stamps</b></h2>
<p>1:03: Background on Paige and computational pathology<br />
7:28: How AI models use visual pattern recognition to accelerate cancer detection<br />
11:27: Paige’s results using AI in cancer imaging and pathology<br />
15:16: Challenges in cancer detection<br />
17:38: Thomas Fuchs’ background in engineering at JPL and NASA<br />
24:10: AI’s future in the medical industry</p>
<h2><b>You Might Also Like:</b></h2>
<p><a target="_blank" href="https://soundcloud.com/theaipodcast/gtc24-cornel-amariei-inception"><b>Dotlumen CEO Cornel Amariei on Assistive Technology for the Visually Impaired &#8211; Ep. 217</b></a></p>
<p>NVIDIA Inception program member Dotlumen is building AI glasses to help people with visual impairments navigate the world. CEO and founder Cornel Amariei discusses the processes of developing assistive technology and its potential for enhancing accessibility.</p>
<p><a target="_blank" href="https://soundcloud.com/theaipodcast/viome-guru-banavar"><b>Personalized Health: Viome’s Guru Banavar Discusses Startup’s AI-Driven Approach &#8211; Ep. 216</b></a></p>
<p>Viome CTO Guru Banavar discusses innovations in AI and genomics and how technology has advanced personalized health and wellness. Viome aims to tackle the root causes of chronic diseases by analyzing microbiomes and gene expression, transforming biological data into practical recommendations for a holistic approach to wellness.</p>
<p><a target="_blank" href="https://soundcloud.com/theaipodcast/cardiac-caristo-dr-keith-channon"><b>Cardiac Clarity: Dr. Keith Channon Talks Revolutionizing Heart Health With AI &#8211; Ep. 212</b></a></p>
<p>Caristo Diagnostics has developed an AI-powered solution for detecting coronary inflammation in cardiac CT scans. Dr. Keith Channon, cofounder and chief medical officer, discusses how Caristo uses AI to improve treatment plans and risk predictions by providing patient-specific readouts.</p>
<h2><b>Subscribe to the AI Podcast</b></h2>
<p>Get the<a href="https://blogs.nvidia.com/ai-podcast/"> AI Podcast</a> through<a target="_blank" href="https://itunes.apple.com/us/podcast/the-ai-podcast/id1186480811?mt=2&amp;adbsc=social_20161220_68874946&amp;adbid=811257941365882882&amp;adbpl=tw&amp;adbpr=61559439"> iTunes</a>, <a target="_blank" href="https://music.amazon.com/podcasts/956857d0-9461-4496-a07e-24be0539ee82/the-ai-podcast">Amazon Music, </a><a target="_blank" href="https://castbox.fm/channel/The-AI-Podcast-id433488?country=us">Castbox</a>, DoggCatcher,<a target="_blank" href="https://overcast.fm/itunes1186480811/the-ai-podcast"> Overcast</a>,<a target="_blank" href="https://player.fm/series/the-ai-podcast"> PlayerFM</a>, Pocket Casts,<a target="_blank" href="http://www.podbay.fm/show/1186480811"> Podbay</a>,<a target="_blank" href="https://www.podbean.com/podcast-detail/cjgnp-4a6e0/The-AI-Podcast"> PodBean</a>, PodCruncher, PodKicker,<a target="_blank" href="https://soundcloud.com/theaipodcast"> Soundcloud</a>,<a target="_blank" href="https://open.spotify.com/show/4TB4pnynaiZ6YHoKmyVN0L"> Spotify</a>,<a target="_blank" href="http://www.stitcher.com/s?fid=130629&amp;refid=stpr"> Stitcher</a> and<a target="_blank" href="https://tunein.com/podcasts/Technology-Podcasts/The-AI-Podcast-p940829/"> TuneIn</a>.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2021/08/ai-podcast-2600x1472_-1-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1159"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2021/08/ai-podcast-2600x1472_-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Paige Cofounder Thomas Fuchs’ Diagnosis on Improving Cancer Patient Outcomes With AI]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Mission NIMpossible: Decoding the Microservices That Accelerate Generative AI</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-nim/</link>
		
		<dc:creator><![CDATA[Allen Bourgoyne]]></dc:creator>
		<pubDate>Wed, 10 Jul 2024 13:00:08 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72866</guid>

					<description><![CDATA[In the rapidly evolving world of artificial intelligence, generative AI is captivating imaginations and transforming industries.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>In the rapidly evolving world of artificial intelligence, <a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a> is captivating imaginations and transforming industries. Behind the scenes, an unsung hero is making it all possible: microservices architecture.</p>
<h2><b>The Building Blocks of Modern AI Applications</b></h2>
<p>Microservices have emerged as a powerful architecture, fundamentally changing how people design, build and deploy software.</p>
<p>A microservices architecture breaks down an application into a collection of loosely coupled, independently deployable services. Each service is responsible for a specific capability and communicates with other services through well-defined application programming interfaces, or APIs. This modular approach stands in stark contrast to traditional all-in-one architectures, in which all functionality is bundled into a single, tightly integrated application.</p>
<p>By decoupling services, teams can work on different components simultaneously, accelerating development processes and allowing updates to be rolled out independently without affecting the entire application. Developers can focus on building and improving specific services, leading to better code quality and faster problem resolution. Such specialization allows developers to become experts in their particular domain.</p>
<p>Services can be scaled independently based on demand, optimizing resource utilization and improving overall system performance. In addition, different services can use different technologies, allowing developers to choose the best tools for each specific task.</p>
<h2><b>A Perfect Match: Microservices and Generative AI</b></h2>
<p>The microservices architecture is particularly well-suited for developing generative AI applications due to its scalability, enhanced modularity and flexibility.</p>
<p>AI models, especially <a target="_blank" href="https://www.nvidia.com/en-us/glossary/large-language-models/">large language models</a>, require significant computational resources. Microservices allow for efficient scaling of these resource-intensive components without affecting the entire system.</p>
<p>Generative AI applications often involve multiple steps, such as data preprocessing, model inference and post-processing. Microservices enable each step to be developed, optimized and scaled independently. Plus, as AI models and techniques evolve rapidly, a microservices architecture allows for easier integration of new models as well as the replacement of existing ones without disrupting the entire application.</p>
<h2><b>NVIDIA NIM: Simplifying Generative AI Deployment</b></h2>
<p>As the demand for AI-powered applications grows, developers face challenges in efficiently deploying and managing AI models.</p>
<p><a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM inference microservices</a> provide models as optimized containers to deploy in the cloud, data centers, workstations, desktops and laptops. Each NIM container includes the <a href="https://blogs.nvidia.com/blog/what-is-a-pretrained-ai-model/">pretrained AI models</a> and all the necessary runtime components, making it simple to integrate AI capabilities into applications.</p>
<p>NIM offers a game-changing approach for application developers looking to incorporate AI functionality by providing simplified integration, production-readiness and flexibility. Developers can focus on building their applications without worrying about the complexities of data preparation, model training or customization, as NIM inference microservices are optimized for performance, come with runtime optimizations and support industry-standard APIs.</p>
<h2><b>AI at Your Fingertips: NVIDIA NIM on Workstations and PCs</b></h2>
<p>Building enterprise generative AI applications comes with many challenges. While cloud-hosted model APIs can help developers get started, issues related to data privacy, security, model response latency, accuracy, API costs and scaling often hinder the path to production.</p>
<p>Workstations with NIM provide developers with secure access to a broad range of models and performance-optimized inference microservices.</p>
<p>By avoiding the latency, cost and compliance concerns associated with cloud-hosted APIs as well as the complexities of model deployment, developers can focus on application development. This accelerates the delivery of production-ready generative AI applications — enabling seamless, automatic scale out with performance optimization in data centers and the cloud.</p>
<p>The recently announced general availability of the <a href="https://blogs.nvidia.com/blog/llama-3-nim-healthcare-generative-ai/">Meta Llama 3 8B model as a NIM</a>, which can run locally on RTX systems, brings state-of-the-art language model capabilities to individual developers, enabling local testing and experimentation without the need for cloud resources. With NIM running locally, developers can create sophisticated <a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">retrieval-augmented generation (RAG)</a> projects right on their workstations.</p>
<p>Local RAG refers to implementing RAG systems entirely on local hardware, without relying on cloud-based services or external APIs.</p>
<p>Developers can use the Llama 3 8B NIM on workstations with one or more <a target="_blank" href="https://www.nvidia.com/en-us/design-visualization/rtx-6000/">NVIDIA RTX 6000 Ada Generation GPUs</a> or on NVIDIA RTX systems to build end-to-end RAG systems entirely on local hardware. This setup allows developers to tap the full power of Llama 3 8B, ensuring high performance and low latency.</p>
<p>By running the entire RAG pipeline locally, developers can maintain complete control over their data, ensuring privacy and security. This approach is particularly helpful for developers building applications that require real-time responses and high accuracy, such as customer-support chatbots, personalized content-generation tools and interactive virtual assistants.</p>
<p>Hybrid RAG combines local and cloud-based resources to optimize performance and flexibility in AI applications. With <a target="_blank" href="https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/">NVIDIA AI Workbench</a>, developers can get started with the hybrid-RAG Workbench Project — an example application that can be used to run vector databases and embedding models locally while performing inference using NIM in the cloud or data center, offering a flexible approach to resource allocation.</p>
<p>This hybrid setup allows developers to balance the computational load between local and cloud resources, optimizing performance and cost. For example, the vector database and embedding models can be hosted on local workstations to ensure fast data retrieval and processing, while the more computationally intensive inference tasks can be offloaded to powerful cloud-based NIM inference microservices. This flexibility enables developers to scale their applications seamlessly, accommodating varying workloads and ensuring consistent performance.</p>
<p><a target="_blank" href="https://developer.nvidia.com/ace">NVIDIA ACE</a> NIM inference microservices bring digital humans, AI non-playable characters (NPCs) and interactive avatars for customer service to life with generative AI, running on RTX PCs and workstations.</p>
<p>ACE NIM inference microservices for speech — including Riva automatic speech recognition, text-to-speech and neural machine translation — allow accurate transcription, translation and realistic voices.</p>
<p>The NVIDIA Nemotron small language model is a NIM for intelligence that includes INT4 quantization for minimal memory usage and supports roleplay and RAG use cases.</p>
<p>And ACE NIM inference microservices for appearance include Audio2Face and Omniverse RTX for lifelike animation with ultrarealistic visuals. These provide more immersive and engaging gaming characters, as well as more satisfying experiences for users interacting with virtual customer-service agents.</p>
<h2><b>Dive Into NIM</b></h2>
<p>As AI progresses, the ability to rapidly deploy and scale its capabilities will become increasingly crucial.</p>
<p>NVIDIA NIM microservices provide the foundation for this new era of AI application development, enabling breakthrough innovations. Whether building the next generation of AI-powered games, developing advanced <a target="_blank" href="https://www.nvidia.com/en-us/glossary/natural-language-processing/">natural language processing</a> applications or creating intelligent automation systems, users can access these powerful development tools at their fingertips.</p>
<p>Ways to get started:</p>
<ul>
<li>Experience and interact with NVIDIA NIM microservices on <a target="_blank" href="http://ai.nvidia.com">ai.nvidia.com</a>.</li>
<li>Join the <a target="_blank" href="https://developer.nvidia.com/developer-program">NVIDIA Developer Program</a> and get free access to NIM for testing and prototyping AI-powered applications.</li>
<li>Buy an <a target="_blank" href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> license with a free 90-day evaluation period for production deployment and use NVIDIA NIM to self-host AI models in the cloud or in data centers.</li>
</ul>
<p><i>Generative AI is transforming gaming, videoconferencing and interactive experiences of all kinds. Make sense of what’s new and what’s next by subscribing to the </i><a target="_blank" href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><i>AI Decoded newsletter</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/NIMs-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/NIMs-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Mission NIMpossible: Decoding the Microservices That Accelerate Generative AI]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Widescreen Wonder: Las Vegas Sphere Delivers Dazzling Displays</title>
		<link>https://blogs.nvidia.com/blog/sphere-las-vegas/</link>
		
		<dc:creator><![CDATA[Isha Salian]]></dc:creator>
		<pubDate>Tue, 09 Jul 2024 16:00:39 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Media and Entertainment]]></category>
		<category><![CDATA[NVIDIA BlueField]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[Rendering]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72849</guid>

					<description><![CDATA[Sphere, a new kind of entertainment medium in Las Vegas, is joining the ranks of legendary circular performance spaces such as the Roman Colosseum and Shakespeare’s Globe Theater — captivating audiences with eye-popping LED displays that cover nearly 750,000 square feet inside and outside the venue. Behind the screens, around 150 NVIDIA RTX A6000 GPUs	<a class="read-more" href="https://blogs.nvidia.com/blog/sphere-las-vegas/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Sphere, a new kind of entertainment medium in Las Vegas, is joining the ranks of legendary circular performance spaces such as the Roman Colosseum and Shakespeare’s Globe Theater — captivating audiences with eye-popping LED displays that cover nearly 750,000 square feet inside and outside the venue.</p>
<p>Behind the screens, around 150 <a target="_blank" href="https://www.nvidia.com/en-us/design-visualization/rtx-a6000/">NVIDIA RTX A6000 GPUs</a> help power stunning visuals on floor-to-ceiling, 16x16K displays across the Sphere’s interior, as well as 1.2 million programmable LED pucks on the venue’s exterior — the Exosphere, which is the world’s largest LED screen.</p>
<p>Delivering robust network connectivity, <a target="_blank" href="https://www.nvidia.com/en-us/networking/products/data-processing-unit/">NVIDIA BlueField DPUs</a> and <a target="_blank" href="https://www.nvidia.com/en-us/networking/ethernet-adapters/">NVIDIA ConnectX-6 Dx NICs</a> — along with the <a target="_blank" href="https://docs.nvidia.com/doca/sdk/doca-firefly-service/index.html">NVIDIA DOCA Firefly Service</a> and <a target="_blank" href="https://developer.nvidia.com/networking/rivermax">NVIDIA Rivermax software</a> for media streaming — ensure that all the display panels act as one synchronized canvas.</p>
<p>“Sphere is captivating audiences not only in Las Vegas, but also around the world on social media, with immersive LED content delivered at a scale and clarity that has never been done before,” said Alex Luthwaite, senior vice president of show systems technology at Sphere Entertainment. “This would not be possible without the expertise and innovation of companies such as NVIDIA that are critical to helping power our vision, working closely with our team to redefine what is possible with cutting-edge display technology.”</p>
<p>Named <a href="https://time.com/collection/best-inventions-2023/6324099/sphere/" target="_blank" rel="noopener">one of TIME’s Best Inventions of 2023</a>, Sphere hosts original Sphere Experiences, concerts and residencies from the world’s biggest artists, and premier marquee and corporate events.</p>
<p>Rock band U2 opened Sphere with a 40-show run that concluded in March. Other shows include The Sphere Experience featuring Darren Aronofsky’s <i>Postcard From Earth</i>, a specially created multisensory cinematic experience that showcases all of the venue’s immersive technologies, including high-resolution visuals, advanced concert-grade sound, haptic seats and atmospheric effects such as wind and scents.</p>
<figure id="attachment_72853" aria-describedby="caption-attachment-72853" style="width: 2048px" class="wp-caption aligncenter"><img fetchpriority="high" decoding="async" class="size-full wp-image-72853" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/Sphere_092823_2323-2-scaled.jpg" alt="image of the Earth from space displayed in Sphere" width="2048" height="1365" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/Sphere_092823_2323-2-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Sphere_092823_2323-2-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Sphere_092823_2323-2-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Sphere_092823_2323-2-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Sphere_092823_2323-2-1536x1024.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Sphere_092823_2323-2-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Sphere_092823_2323-2-323x215.jpg 323w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Sphere_092823_2323-2-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Sphere_092823_2323-2-1280x853.jpg 1280w" sizes="(max-width: 2048px) 100vw, 2048px" /><figcaption id="caption-attachment-72853" class="wp-caption-text">“Postcard From Earth” is a multisensory immersive experience. Image courtesy of Sphere Entertainment.</figcaption></figure>
<h2><b>Behind the Screens: Visual Technology Fueling the Sphere</b></h2>
<p>Sphere Studios creates video content in its Burbank, Calif., facility, then transfers it digitally to Sphere in Las Vegas. The content is then streamed in real time to rack-mounted workstations equipped with NVIDIA RTX A6000 GPUs, achieving unprecedented performance capable of delivering three layers of 16K resolution at 60 frames per second.</p>
<p>The NVIDIA Rivermax software helps provide media streaming acceleration, enabling direct data transfers to and from the GPU. Combined, the software and hardware acceleration eliminates jitter and optimizes latency.</p>
<p>NVIDIA BlueField DPUs also facilitate precision timing through the DOCA Firefly Service, which is used to synchronize clocks in a network with sub-microsecond accuracy.</p>
<p>“The integration of NVIDIA RTX GPUs, BlueField DPUs and Rivermax software creates a powerful trifecta of advantages for modern accelerated computing, supporting the unique high-resolution video streams and strict timing requirements needed at Sphere and setting a new standard for media processing capabilities,” said Nir Nitzani, senior product director for networking software at NVIDIA. “This collaboration results in remarkable performance gains, culminating in the extraordinary experiences guests have at Sphere.”<i> </i></p>
<h2><b>Well-Rounded: From Simulation to Sphere Stage</b></h2>
<p>To create new immersive content exclusively for Sphere, Sphere Entertainment launched Sphere Studios, which is dedicated to developing the next generation of original immersive entertainment. The Burbank campus consists of numerous development facilities, including a quarter-sized version of Sphere screen in Las Vegas, dubbed Big Dome, which serves as a specialized screening, production facility and lab for content.</p>
<figure id="attachment_72856" aria-describedby="caption-attachment-72856" style="width: 2048px" class="wp-caption aligncenter"><img decoding="async" class="size-full wp-image-72856" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/Big-Dome-Exterior-Credit-Sphere-Entertainment-scaled.jpeg" alt="dome-shaped building flanked by palm trees" width="2048" height="1366" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/Big-Dome-Exterior-Credit-Sphere-Entertainment-scaled.jpeg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Big-Dome-Exterior-Credit-Sphere-Entertainment-400x267.jpeg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Big-Dome-Exterior-Credit-Sphere-Entertainment-672x448.jpeg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Big-Dome-Exterior-Credit-Sphere-Entertainment-768x512.jpeg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Big-Dome-Exterior-Credit-Sphere-Entertainment-1536x1024.jpeg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Big-Dome-Exterior-Credit-Sphere-Entertainment-675x450.jpeg 675w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Big-Dome-Exterior-Credit-Sphere-Entertainment-322x215.jpeg 322w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Big-Dome-Exterior-Credit-Sphere-Entertainment-150x100.jpeg 150w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Big-Dome-Exterior-Credit-Sphere-Entertainment-1280x854.jpeg 1280w" sizes="(max-width: 2048px) 100vw, 2048px" /><figcaption id="caption-attachment-72856" class="wp-caption-text">The Big Dome is 100 feet high and 28,000 square feet. Image courtesy of Sphere Entertainment.</figcaption></figure>
<p>Sphere Studios also developed the Big Sky camera system, which captures uncompressed, 18K images from a single camera, so that the studio can film content for Sphere without needing to stitch multiple camera feeds together. The studio’s custom image processing software runs on Lenovo servers powered by <a target="_blank" href="https://www.nvidia.com/en-us/data-center/a40/">NVIDIA A40 GPUs</a>.</p>
<p>The A40 GPUs also fuel creative work, including 3D video, virtualization and ray tracing. To develop visuals for different kinds of shows, the team works with apps including Unreal Engine, Unity, Touch Designer and Notch.</p>
<p><i>For more, explore upcoming sessions in </i><a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/"><i>NVIDIA’s room at SIGGRAPH</i></a><i> and watch the panel discussion “</i><a target="_blank" href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s63135/"><i>Immersion in Sphere: Redefining Live Entertainment Experiences</i></a><i>” on NVIDIA On-Demand.</i></p>
<p><i>All images courtesy of Sphere Entertainment.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/Sphere-Exosphere.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/Sphere-Exosphere-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Widescreen Wonder: Las Vegas Sphere Delivers Dazzling Displays]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>In It for the Long Haul: Waabi Pioneers Generative AI to Unleash Fully Driverless Autonomous Trucking</title>
		<link>https://blogs.nvidia.com/blog/waabi-autonomous-trucking/</link>
		
		<dc:creator><![CDATA[Norm Marks]]></dc:creator>
		<pubDate>Mon, 08 Jul 2024 15:00:00 +0000</pubDate>
				<category><![CDATA[Driving]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[NVIDIA DRIVE]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72803</guid>

					<description><![CDATA[Artificial intelligence is transforming the transportation industry, helping drive advances in autonomous vehicle (AV) technology. Waabi, a Toronto-based startup, is embracing generative AI to deliver self-driving vehicles at scale — starting with the long-haul trucking sector. At GTC in March, Waabi announced that it will use the NVIDIA DRIVE Thor centralized car computer to bring	<a class="read-more" href="https://blogs.nvidia.com/blog/waabi-autonomous-trucking/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Artificial intelligence is transforming the transportation industry, helping drive advances in autonomous vehicle (AV) technology.</p>
<p>Waabi, a Toronto-based startup, is embracing generative AI to deliver self-driving vehicles at scale — starting with the long-haul trucking sector.</p>
<p>At GTC in March, <a href="https://waabi.ai/nvidia-drivethor/" target="_blank" rel="noopener">Waabi announced that it will use the NVIDIA DRIVE Thor</a> centralized car computer to bring a safe, generative AI-powered autonomous trucking solution — the Waabi Driver —  to market.</p>
<p>As the company plans the launch of fully driverless operations next year, Waabi is reinvigorating the industry with a self-driving solution that’s capital-efficient, can safely handle new scenarios on the road and ultimately scales commercially.</p>
<p>Waabi is developing on <a href="https://developer.nvidia.com/drive/os" target="_blank" rel="noopener">NVIDIA DRIVE OS</a>, the company’s operating system for safe, AI-defined autonomous vehicles.</p>
<p>The innovative startup has pioneered an approach that centers on the combination of two generative AI systems: a “teacher,” called Waabi World, an advanced simulator that trains and validates a “student,” called Waabi Driver, a single, end-to-end AI system that’s capable of human-like reasoning and is fully interpretable.</p>
<p>When paired together, these systems reduce the need for extensive on-road testing and enable a safer, more efficient solution that is highly performant and scalable.</p>
<p>“We are excited to have a deep collaboration with NVIDIA to bring generative AI to the edge, on our vehicles, at scale,” said Raquel Urtasun, founder and CEO of Waabi.</p>
<p><iframe loading="lazy" title="Waabi and NVIDIA: Bringing Generative AI to the Edge" width="500" height="281" src="https://www.youtube.com/embed/iDGwP45B_GA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>Generative AI accelerates the development of AVs by “providing an end-to-end system where, instead of requiring hundreds of engineers to develop a system by hand, it provides the ability to learn foundation models that can run unsupervised by observing and acting on the world,” Urtasun added.</p>
<p>Waabi’s collaboration with NVIDIA is one in a series of milestones, including the company’s <a href="https://waabi.ai/waabi-series-b-announcement/" target="_blank" rel="noopener">$200 million Series B round</a> with participation from NVIDIA, its <a href="https://waabi.ai/waabi-uber-freight/" target="_blank" rel="noopener">work with logistics company Uber Freight</a>, the launch of its first commercial autonomous trucking routes in the U.S., and the opening of a trucking terminal near Dallas to serve as the center of the company’s operations in the Lone Star state.</p>
<p>“What we’re building for autonomous vehicles — combining generative AI-powered simulation with a foundation AI model purpose-built for acting in the physical world — will enable faster, safer and more scalable deployment of this transformative technology around the world,” Urtasun noted on the company’s website.</p>
<p><i>Listen to Urtasun’s </i><a href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62621/" target="_blank" rel="noopener"><i>talk at GTC</i></a><i> for more on the company’s work on using generative AI to develop autonomous vehicles.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/waabi.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/waabi-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[In It for the Long Haul: Waabi Pioneers Generative AI to Unleash Fully Driverless Autonomous Trucking]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>GeForce NOW Beats the Heat With 22 New Games in July</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-july-2024-games-list/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 04 Jul 2024 13:00:36 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72833</guid>

					<description><![CDATA[GeForce NOW is bringing 22 new games to members this month. Dive into the four titles available to stream on the cloud gaming service this week to stay cool and entertained throughout the summer — whether poolside, on a long road trip or in the air-conditioned comfort of home. Plus, get great games at great	<a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-july-2024-games-list/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><a target="_blank" href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a> is bringing 22 new games to members this month.</p>
<p>Dive into the four titles available to stream on the cloud gaming service this week to stay cool and entertained throughout the summer — whether poolside, on a long road trip or in the air-conditioned comfort of home.</p>
<p>Plus, get great games at great deals to stream across devices during the Steam Summer Sale. In total, more than 850 titles on GeForce NOW can be found at discounts in a dedicated Steam Summer Sale row on the GeForce NOW app, from now until July 11.</p>
<h2><b>Time to Grind</b></h2>
<figure id="attachment_72837" aria-describedby="caption-attachment-72837" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-72837" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-The_First_Descendant-672x336.jpg" alt="The First Descendant on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-The_First_Descendant-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-The_First_Descendant-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-The_First_Descendant-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-The_First_Descendant-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-The_First_Descendant-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-The_First_Descendant-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-The_First_Descendant-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-The_First_Descendant-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-The_First_Descendant.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-72837" class="wp-caption-text"><em>Be the first Descendant with the cloud.</em></figcaption></figure>
<p>In <i>The First Descendant</i> from NEXON, take on the role of Descendants tasked with safeguarding the powerful Iron Heart from relentless Vulgus invaders. Set in a captivating sci-fi universe, the game is a third-person co-op action role-playing shooter that seamlessly blends looting mechanics with strategic combat. Engage in intense gunplay, face off against formidable bosses and collect valuable loot while fighting to preserve humanity’s future.</p>
<p>Check out the list of new games this week:</p>
<ul>
<li><i>The Falconeer </i>(Free on <a target="_blank" href="https://www.epicgames.com/store/p/the-falconeer?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store,</a> July 4)</li>
<li><i>The First Descendant </i>(<a target="_blank" href="https://store.steampowered.com/app/2074920?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Star Traders: Frontiers </i>(<a target="_blank" href="https://store.steampowered.com/app/335620?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Wuthering Waves </i>(<a target="_blank" href="https://wutheringwaves.kurogames.com/?utm_source=nvidia&amp;utm_campaign=geforce_now">Native</a> and <a target="_blank" href="https://www.epicgames.com/store/p/wuthering-waves-76ebc5?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a>)</li>
</ul>
<p>And members can look for the following later this month:</p>
<ul>
<li><i>Once Human </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2139460?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 9)</li>
<li><i>Anger Foot </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1978590?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 11)</li>
<li><i>The Crust </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1465470?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 15)</li>
<li><i>Gestalt: Steam &amp; Cinder </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1231990?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 16)</li>
<li><i>Flintlock: The Siege of Dawn  </i>(New release <a target="_blank" href="https://store.steampowered.com/app/1832040?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a target="_blank" href="https://www.xbox.com/games/store/flintlock-the-siege-of-dawn/9PBBQHX6V3PJ?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass, July 18)</li>
<li><i>Dungeons of Hinterberg </i>(New release <a target="_blank" href="https://store.steampowered.com/app/1983260?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a target="_blank" href="https://store.steampowered.com/app/1983260?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass, July 18)</li>
<li><i>Norland </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1857090?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 18)</li>
<li><i>Cataclismo </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1422440?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 22</li>
<li><i>CONSCRIPT </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1286990?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 23)</li>
<li><i>F1 Manager 2024 </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2591280?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 23)</li>
<li><i>EARTH DEFENSE FORCE 6 </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2291060?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 25)</li>
<li><i>Stormgate Early Access </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2012510?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 30)</li>
<li><i>Cyber Knights: Flashpoint </i>(<a target="_blank" href="https://store.steampowered.com/app/1021210?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Content Warning </i>(<a target="_blank" href="https://store.steampowered.com/app/2881650?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Crime Boss: Rockay City </i>(<a target="_blank" href="https://store.steampowered.com/app/2933080?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Gang Beasts </i>(<a target="_blank" href="https://store.steampowered.com/app/285900?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a target="_blank" href="https://www.xbox.com/games/store/gang-beasts/BPQZT43FWD49?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>HAWKED</i> (<a target="_blank" href="https://store.steampowered.com/app/1955960?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Kingdoms and Castles </i>(<a target="_blank" href="https://store.steampowered.com/app/569480?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
</ul>
<h2><b>Jam-Packed June</b></h2>
<p>In addition to the 17 games announced last month, 10 more joined the <a target="_blank" href="http://play.geforcenow.com">GeForce NOW library</a>:<i></i></p>
<ul>
<li><i>Killer Klowns from Outer Space: The Game </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1556100?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, June 4)</li>
<li><i>Sneak Out </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2410490?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, June 6)</li>
<li><i>Beyond Good &amp; Evil &#8211; 20th Anniversary Edition </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2556990/Beyond_Good__Evil__20th_Anniversary_Edition/">Steam</a> and <a target="_blank" href="https://www.ubisoft.com/en-us/game/beyond-good-and-evil/20th-anniversary-edition">Ubisoft</a>, June 24)</li>
<li><i>As Dusk Falls </i>(<a target="_blank" href="https://store.steampowered.com/app/1341820?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a target="_blank" href="https://www.xbox.com/games/store/as-dusk-falls/9NR7XDNVP5SW?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Bodycam </i>(<a target="_blank" href="https://store.steampowered.com/app/2406770/Bodycam/">Steam</a>)</li>
<li><i>Drug Dealer Simulator 2</i> (<a target="_blank" href="https://store.steampowered.com/app/1708850?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Sea of Thieves </i>(<a target="_blank" href="https://store.steampowered.com/app/1172620?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a target="_blank" href="https://www.xbox.com/games/store/sea-of-thieves-2023-edition/9P2N57MC619K?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Skye: The Misty Isle </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1710180?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, June 19)</li>
<li><i>XDefiant</i> (Ubisoft)</li>
<li><i>Tell Me Why </i>(<a target="_blank" href="https://store.steampowered.com/app/1180660?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a target="_blank" href="https://www.xbox.com/games/store/tell-me-why/9NBL0XKVCN5L?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
</ul>
<p><i>Torque Drift 2 </i>didn’t make it in June due to technical issues. Stay tuned to GFN Thursday for updates.</p>
<p>What are you planning to play this weekend? Let us know on <a target="_blank" href="https://www.twitter.com/nvidiagfn">X</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">Fill in the blank:</p>
<p><img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f4c6.png" alt="📆" class="wp-smiley" style="height: 1em; max-height: 1em;" /> Game that you&#39;ve played for over a year: _____<br /><img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f465.png" alt="👥" class="wp-smiley" style="height: 1em; max-height: 1em;" /> Multiplayer game you can&#39;t stop playing: _____<br /><img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f621.png" alt="😡" class="wp-smiley" style="height: 1em; max-height: 1em;" /> Game that makes you rage: _____</p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f329.png" alt="🌩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a target="_blank" href="https://twitter.com/NVIDIAGFN/status/1808531147401359480?ref_src=twsrc%5Etfw">July 3, 2024</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/gfn-thursday-7-4-nv-blog-1280x680-no-cta.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/gfn-thursday-7-4-nv-blog-1280x680-no-cta-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[GeForce NOW Beats the Heat With 22 New Games in July]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Decoding How the Generative AI Revolution BeGAN</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-gan-canvas-app/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Wed, 03 Jul 2024 13:00:55 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72808</guid>

					<description><![CDATA[Generative models have completely transformed the AI landscape — headlined by popular apps such as ChatGPT and Stable Diffusion. ]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><i>Editor’s note: This post is part of the </i><a href="https://blogs.nvidia.com/blog/tag/ai-decoded/" target="_blank" rel="noopener"><i>AI Decoded series</i></a><i>, which demystifies AI by making the technology more accessible, and showcases new hardware, software, tools and accelerations for RTX PC users.</i></p>
<p>Generative models have completely transformed the AI landscape — headlined by popular apps such as ChatGPT and Stable Diffusion.</p>
<p>Paving the way for this boom were foundational AI models and generative adversarial networks (GANs), which sparked a leap in productivity and creativity.</p>
<p>NVIDIA’s <a href="https://blogs.nvidia.com/blog/gaugan-photorealistic-landscapes-nvidia-research/" target="_blank" rel="noopener">GauGAN</a>, which powers the <a href="https://www.nvidia.com/en-us/studio/canvas/" target="_blank" rel="noopener">NVIDIA Canvas app</a>, is one such model that uses AI to transform rough sketches into photorealistic artwork.</p>
<h2><b>How It All BeGAN</b></h2>
<p>GANs are deep learning models that involve two complementary neural networks: a generator and a discriminator.</p>
<p>These neural networks compete against each other. The generator attempts to create realistic, lifelike imagery, while the discriminator tries to tell the difference between what’s real and what’s generated. As its neural networks keep challenging each other, GANs get better and better at making realistic-looking samples.</p>
<p>GANs excel at understanding complex data patterns and creating high-quality results. They&#8217;re used in applications including image synthesis, style transfer, data augmentation and image-to-image translation.</p>
<p>NVIDIA’s GauGAN, named after post-Impressionist painter Paul Gauguin, is an AI demo for photorealistic image generation. Built by NVIDIA Research, it directly led to the development of the NVIDIA Canvas app — and can be experienced for free through the <a target="_blank" href="https://www.nvidia.com/en-us/research/ai-demos/">NVIDIA AI Playground</a>.</p>
<p>GauGAN has been wildly popular since it debuted at NVIDIA GTC in 2019 — used by art teachers, creative agencies, museums and millions more online.</p>
<h2><b>Giving Sketch to Scenery a Gogh</b></h2>
<p>Powered by GauGAN and local <a href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/" target="_blank" rel="noopener">NVIDIA RTX GPUs</a>, NVIDIA Canvas uses AI to turn simple brushstrokes into realistic landscapes, displaying results in real time.</p>
<p>Users can start by sketching simple lines and shapes with a palette of real-world elements like grass or clouds —- referred to in the app as “materials.”</p>
<p><iframe loading="lazy" title="NVIDIA Canvas: New Update | 4x Higher Resolution &amp; 5 New Materials" width="500" height="281" src="https://www.youtube.com/embed/wKztRskmsig?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>The AI model then generates the enhanced image on the other half of the screen in real time. For example, a few triangular shapes sketched using the “mountain” material will appear as a stunning, photorealistic range. Or users can select the “cloud” material and with a few mouse clicks transform environments from sunny to overcast.</p>
<p>The creative possibilities are endless — sketch a pond, and other elements in the image, like trees and rocks, will reflect in the water. Change the material from snow to grass, and the scene shifts from a cozy winter setting to a tropical paradise.</p>
<figure id="attachment_72809" aria-describedby="caption-attachment-72809" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/beachcove.png"><img loading="lazy" decoding="async" class="size-large wp-image-72809" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/beachcove-672x368.png" alt="" width="672" height="368" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/beachcove-672x368.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/beachcove-400x219.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/beachcove-768x420.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/beachcove-1536x840.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/beachcove.png 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/07/beachcove-823x450.png 823w, https://blogs.nvidia.com/wp-content/uploads/2024/07/beachcove-393x215.png 393w, https://blogs.nvidia.com/wp-content/uploads/2024/07/beachcove-183x100.png 183w, https://blogs.nvidia.com/wp-content/uploads/2024/07/beachcove-1280x700.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-72809" class="wp-caption-text">Canvas offers nine different styles, each with 10 variations and 20 materials to play with.</figcaption></figure>
<p>Canvas features a Panorama mode that enables artists to create 360-degree images for use in 3D apps. YouTuber <a href="https://www.youtube.com/watch?v=d2O-kj5KF5w" target="_blank" rel="noopener">Greenskull AI</a> demonstrated Panorama mode by painting an ocean cove, before then importing it into Unreal Engine 5.</p>
<p><iframe loading="lazy" title="NVIDIA Canvas Panorama - Painting in 360° with Ai" width="500" height="281" src="https://www.youtube.com/embed/d2O-kj5KF5w?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p><a href="https://www.nvidia.com/en-us/studio/canvas/" target="_blank" rel="noopener">Download</a> the NVIDIA Canvas app to get started.</p>
<p>Consider exploring <a href="https://www.nvidia.com/en-us/geforce/broadcasting/broadcast-app/" target="_blank" rel="noopener">NVIDIA Broadcast</a>, another AI-powered content creation app that transforms any room into a home studio. Broadcast is free for RTX GPU owners.</p>
<p><i>Generative AI is transforming gaming, videoconferencing and interactive experiences of all kinds. Make sense of what’s new and what’s next by subscribing to the </i><a target="_blank" href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><i>AI Decoded newsletter</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/canvas-app-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/canvas-app-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Decoding How the Generative AI Revolution BeGAN]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>How an NVIDIA Engineer Unplugs to Recharge During Free Days</title>
		<link>https://blogs.nvidia.com/blog/nvidia-life-free-days-2024/</link>
		
		<dc:creator><![CDATA[Haley Hirai]]></dc:creator>
		<pubDate>Fri, 28 Jun 2024 13:00:23 +0000</pubDate>
				<category><![CDATA[NVIDIA Life]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72785</guid>

					<description><![CDATA[On a weekday afternoon, Ashwini Ashtankar sat on the bank of the Doodhpathri River, in a valley nestled in the Himalayas. Taking a deep breath, she noticed that there was no city noise, no pollution — and no work emails. Ashtankar, a senior tools development engineer in NVIDIA’s Pune, India, office, took advantage of the	<a class="read-more" href="https://blogs.nvidia.com/blog/nvidia-life-free-days-2024/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>On a weekday afternoon, Ashwini Ashtankar sat on the bank of the Doodhpathri River, in a valley nestled in the Himalayas. Taking a deep breath, she noticed that there was no city noise, no pollution — and no work emails.</p>
<p>Ashtankar, a senior tools development engineer in NVIDIA’s Pune, India, office, took advantage of the company’s free days — two extra days off per quarter when the whole company disconnects from work — to recharge. Free days are fully paid by NVIDIA, not counted as vacation or as personal time off, and are in addition to country-specific holidays and time-away programs.</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-large wp-image-72789" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/Snow_Peak_Sonmarg-672x378.jpg" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/Snow_Peak_Sonmarg-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Snow_Peak_Sonmarg-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Snow_Peak_Sonmarg-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Snow_Peak_Sonmarg-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Snow_Peak_Sonmarg-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Snow_Peak_Sonmarg-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Snow_Peak_Sonmarg-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Snow_Peak_Sonmarg-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Snow_Peak_Sonmarg-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></p>
<p>Free days give employees time to take an adventure, a breather — or both. Ashtankar and her husband, Dipen Sisodia — also an NVIDIAN — spent it outdoors, hiking up a mountain, playing in snow and exploring forests and lush green meadows.</p>
<p>“My free days give me time to focus on myself and recharge,” said Ashtankar. “We didn’t take our laptops. We were able to completely disconnect, like all NVIDIANs were doing at the same time.”</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-large wp-image-72792" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/Mountain_top_bike-672x378.jpg" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/Mountain_top_bike-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Mountain_top_bike-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Mountain_top_bike-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Mountain_top_bike-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Mountain_top_bike-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Mountain_top_bike-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Mountain_top_bike-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Mountain_top_bike-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Mountain_top_bike-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /></p>
<p>Ashtankar returned to work feeling refreshed and recharged, she said. Her team tests software features of NVIDIA products, focusing on GPU display drivers and the <a target="_blank" href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a> game-streaming service, to make sure bugs are found and addressed before a product reaches customers.</p>
<p>“I take pride in tackling challenges with the highest level of quality and creativity, all in support of delivering the best products to our customers,” she said. “To do that, sometimes the most productive thing we can do is rest and let the soul catch up with the body.”</p>
<p>Ashtankar plans to build her career at NVIDIA for many years to come.</p>
<p>“I’ve never heard of another company that truly cares this much about its employees,” she said.</p>
<p><i>Learn more about </i><a target="_blank" href="https://www.nvidia.com/en-us/about-nvidia/careers/life-at-nvidia/"><i>NVIDIA life, culture and careers</i></a><i>. </i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/Ashwini-Ashtankar-Free-Days-Featured-Photo.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/Ashwini-Ashtankar-Free-Days-Featured-Photo-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[How an NVIDIA Engineer Unplugs to Recharge During Free Days]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>GeForce NOW Unleashes High-Stakes Horror With ‘Resident Evil Village’</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-resident-evil-village/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 27 Jun 2024 13:00:51 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72759</guid>

					<description><![CDATA[Get ready to feel some chills, even amid the summer heat. Capcom’s award-winning Resident Evil Village brings a touch of horror to the cloud this GFN Thursday, part of three new games joining GeForce NOW this week. And a new app update brings a visual enhancement to members, along with new ways to curate their	<a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-resident-evil-village/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Get ready to feel some chills, even amid the summer heat. Capcom’s award-winning <i>Resident Evil Village</i> brings a touch of horror to the cloud this GFN Thursday, part of three new games joining <a target="_blank" href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a> this week.</p>
<p>And a new app update brings a visual enhancement to members, along with new ways to curate their GeForce NOW gaming libraries.</p>
<figure id="attachment_72772" aria-describedby="caption-attachment-72772" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-72772" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-1-672x357.png" alt="Greetings on GFN" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-1-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-1-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-1-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-1-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-1-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-1-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-72772" class="wp-caption-text"><em>#GreetingsFromGFN by @railbeam.</em></figcaption></figure>
<p>Members are showcasing their favorite locations to visit in the cloud. Follow along with #GreetingsFromGFN on @NVIDIAGFN social media accounts and share picturesque scenes from the cloud for a chance to be featured.</p>
<h2><b>The Bell Tolls for All</b></h2>
<figure id="attachment_72769" aria-describedby="caption-attachment-72769" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-72769" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Resident_Evil_Village-672x378.jpg" alt="Resident Evil Village on GeForce NOW" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Resident_Evil_Village-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Resident_Evil_Village-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Resident_Evil_Village-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Resident_Evil_Village-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Resident_Evil_Village-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Resident_Evil_Village-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Resident_Evil_Village-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Resident_Evil_Village-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Resident_Evil_Village-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-72769" class="wp-caption-text"><em>The cloud — big enough, even, for Lady Dimitrescu and her towering castle.</em></figcaption></figure>
<p><i>Resident Evil Village</i>, the follow-up to Capcom’s critically acclaimed <i>Resident Evil 7 Biohazard</i>, delivers a gripping blend of survival-horror and action. Step into the shoes of Ethan Winters, a desperate father determined to rescue his kidnapped daughter.</p>
<p>Set against a backdrop of a chilling European village teeming with mutant creatures, the game includes a captivating cast of characters, including the enigmatic Lady Dimitrescu, who haunts the dimly lit halls of her grand castle. Fend off hordes of enemies, such as lycanthropic villagers and grotesque abominations.</p>
<p>Experience classic survival-horror tactics — such as resource management and exploration — mixed with action featuring intense combat and higher enemy counts.</p>
<p><a target="_blank" href="https://www.nvidia.com/en-us/geforce-now/memberships/">Ultimate and Priority members</a> can experience the horrors of this dark and twisted world in gruesome, mesmerizing detail with support for ray tracing and high dynamic range (HDR) for the most lifelike shadows and sharp visual fidelity when navigating every eerie hallway. Members can stream it all seamlessly from <a target="_blank" href="https://www.nvidia.com/en-us/geforce/rtx/">NVIDIA GeForce RTX</a>-powered servers in the cloud and get a taste of the chills with the <i>Resident Evil Village</i> demo before taking on the towering Lady Dimitrescu in the full game.</p>
<h2><b>I Can See Clearly Now</b></h2>
<p>The latest GeForce NOW app update — version 2.0.64 — adds support for 10-bit color precision. Available for Ultimate members, this feature enhances image quality when streaming on Windows, macOS and <a target="_blank" href="https://www.nvidia.com/en-us/shield/">NVIDIA SHIELD TV</a>.</p>
<figure id="attachment_72766" aria-describedby="caption-attachment-72766" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-72766" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-10_bit_Color_Precision-672x361.png" alt="SDR10 on GeForce NOW" width="672" height="361" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-10_bit_Color_Precision-672x361.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-10_bit_Color_Precision-400x215.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-10_bit_Color_Precision-768x413.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-10_bit_Color_Precision-1536x825.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-10_bit_Color_Precision.png 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-10_bit_Color_Precision-838x450.png 838w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-10_bit_Color_Precision-186x100.png 186w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-10_bit_Color_Precision-1280x688.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-72766" class="wp-caption-text"><em>Rolling out now.</em></figcaption></figure>
<p>10-bit color precision significantly improves the accuracy and richness of color gradients during streaming. Members will especially notice its effects in scenes with detailed color transitions, such as for vibrant skies, dimly lit interiors, and various loading screens and menus. It’s useful for non-HDR displays and non-HDR-supported games. Find the setting in the GeForce NOW app &gt; Streaming Quality &gt; Color Precision, with the recommended default value of 10-bit.</p>
<p>Try it out on the neon-lit streets of <i>Cyberpunk 2077</i> for smoother color transitions, and traverse the diverse landscapes of<i> Assassin’s Creed Valhalla</i> and other games for a more immersive streaming experience.</p>
<p>The update, rolling out now, also brings bug fixes and new ways to curate a member’s in-app game library. For more information, visit the <a target="_blank" href="https://nvidia.custhelp.com/app/answers/detail/a_id/5390">NVIDIA Knowledgebase</a>.</p>
<h2><b>Lights, Camera, Action: New Games</b></h2>
<figure id="attachment_72763" aria-describedby="caption-attachment-72763" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-72763" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Beyond_Good_And_Evil_20th_Anniversary_Edition-672x336.jpg" alt="Beyond Good and Evil 20th Anniversary Edition on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Beyond_Good_And_Evil_20th_Anniversary_Edition-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Beyond_Good_And_Evil_20th_Anniversary_Edition-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Beyond_Good_And_Evil_20th_Anniversary_Edition-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Beyond_Good_And_Evil_20th_Anniversary_Edition-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Beyond_Good_And_Evil_20th_Anniversary_Edition-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Beyond_Good_And_Evil_20th_Anniversary_Edition-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Beyond_Good_And_Evil_20th_Anniversary_Edition-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Beyond_Good_And_Evil_20th_Anniversary_Edition-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Beyond_Good_And_Evil_20th_Anniversary_Edition.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-72763" class="wp-caption-text"><em>Uncover the truth.</em></figcaption></figure>
<p>Join the rebellion as action reporter Jade in <i>Beyond Good &amp; Evil &#8211; 20th Anniversary Edition </i>from Ubisoft. Embark on this epic adventure in up to 4K 60 frames per second with improved graphics and audio, a new speedrun mode, updated achievements and an exclusive anniversary gallery. Enjoy unique new rewards exploring Hillys and discover more about Jade’s past in a new treasure hunt throughout the planet.</p>
<p>Check out the list of new games this week:</p>
<ul>
<li><i>Beyond Good &amp; Evil &#8211; 20th Anniversary Edition </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2556990/Beyond_Good__Evil__20th_Anniversary_Edition/">Steam</a> and <a target="_blank" href="https://www.ubisoft.com/en-us/game/beyond-good-and-evil/20th-anniversary-edition">Ubisoft</a>, June 24)</li>
<li><i>Drug Dealer Simulator 2</i> (<a target="_blank" href="https://store.steampowered.com/app/1708850?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Resident Evil Village </i>(<a target="_blank" href="https://store.steampowered.com/app/1196590?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Resident Evil Village Demo </i>(<a target="_blank" href="https://store.steampowered.com/app/1196590?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
</ul>
<p>What are you planning to play this weekend? Let us know on <a target="_blank" href="https://www.twitter.com/nvidiagfn">X</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">Would you rather have infinite health or infinite ammo in-game? <img src="https://s.w.org/images/core/emoji/15.0.3/72x72/2764.png" alt="❤" class="wp-smiley" style="height: 1em; max-height: 1em;" /><img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f52b.png" alt="🔫" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f329.png" alt="🌩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a target="_blank" href="https://twitter.com/NVIDIAGFN/status/1805632044195516463?ref_src=twsrc%5Etfw">June 25, 2024</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/gfn-thursday-6-27-nv-blog-1280x680-no-copy.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/gfn-thursday-6-27-nv-blog-1280x680-no-copy-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[GeForce NOW Unleashes High-Stakes Horror With ‘Resident Evil Village’]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Into the Omniverse: SyncTwin Helps Democratize Industrial Digital Twins With Generative AI, OpenUSD</title>
		<link>https://blogs.nvidia.com/blog/synctwin-digital-twins-generative-ai-openusd/</link>
		
		<dc:creator><![CDATA[James McKenna]]></dc:creator>
		<pubDate>Thu, 27 Jun 2024 13:00:19 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[Into the Omniverse]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[SIGGRAPH]]></category>
		<category><![CDATA[Universal Scene Description]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72742</guid>

					<description><![CDATA[SyncTwin’s application enables teams to optimize industrial efficiency while enhancing sustainability across manufacturing processes.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><i>Editor’s note: This post is part of </i><a target="_blank" href="https://www.nvidia.com/en-us/omniverse/news/"><i>Into the Omniverse</i></a><i>, a series focused on how technical artists, developers and enterprises can transform their workflows using the latest advances in </i><a target="_blank" href="https://www.nvidia.com/en-us/omniverse/usd/"><i>OpenUSD</i></a><i> and </i><a target="_blank" href="https://www.nvidia.com/en-us/omniverse/usd/"><i>NVIDIA Omniverse</i></a><i>.</i></p>
<p>Efficiency and sustainability are critical for organizations looking to be at the forefront of industrial innovation.</p>
<p>To address the digitalization needs of manufacturing and other industries, <a target="_blank" href="https://www.synctwin.ai/">SyncTwin </a>GmbH — a company that builds software to optimize production, intralogistics and assembly — developed a <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/">digital twin</a> app using <a target="_blank" href="https://www.nvidia.com/en-us/ai-data-science/products/cuopt/">NVIDIA cuOpt</a>, an accelerated optimization engine for solving complex routing problems, and <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a>, a platform of application programming interfaces, software development kits and services that enable developers to build <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/usd/">OpenUSD</a>-based applications.</p>
<p>SyncTwin is harnessing the power of the extensible OpenUSD framework for describing, composing, simulating, and collaborating within 3D worlds to help its customers create physically accurate digital twins of their factories. The digital twins can be used to optimize production and enhance digital precision to meet industrial performance.</p>
<h2><b>OpenUSD’s Role in Modern Manufacturing</b></h2>
<p>Manufacturing workflows are incredibly complex, making effective communication and integration across various domains pivotal to ensuring operational efficiency. The SyncTwin app provides seamless collaboration capabilities for factory plant managers and their teams, enabling them to optimize processes and resources.</p>
<p>The app uses OpenUSD and Omniverse to help make factory planning and operations easier and more accessible by integrating various manufacturing aspects into a cohesive digital twin. Customers can integrate visual data, production details, product catalogs, orders, schedules, resources and production settings <a target="_blank" href="https://www.google.com/url?q=https://developer.nvidia.com/blog/transforming-microsoft-xls-and-ppt-files-into-a-factory-digital-twin-with-openusd/&amp;sa=D&amp;source=docs&amp;ust=1720024234143097&amp;usg=AOvVaw3pcW7Wfq_o95dh5vj0UF4S">from a variety of file formats all in one place with OpenUSD</a>.</p>
<p><img loading="lazy" decoding="async" class=" wp-image-72746 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_blog_steps_app_2.png" alt="" width="816" height="440" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_blog_steps_app_2.png 1928w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_blog_steps_app_2-400x216.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_blog_steps_app_2-672x362.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_blog_steps_app_2-768x414.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_blog_steps_app_2-1536x829.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_blog_steps_app_2-834x450.png 834w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_blog_steps_app_2-399x215.png 399w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_blog_steps_app_2-185x100.png 185w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_blog_steps_app_2-1280x690.png 1280w" sizes="(max-width: 816px) 100vw, 816px" /></p>
<p>The SyncTwin app creates realistic, virtual environments that facilitate seamless interactions between different sectors of factory operations. This capability enables diverse data—including floorplans from Microsoft PowerPoint and warehouse container data from Excel spreadsheets — to be aggregated in a unified digital twin.</p>
<p>The flexibility of OpenUSD allows for non-destructive editing and composition of complex 3D assets and animations, further enhancing the digital twin.</p>
<p>“OpenUSD is the common language bringing all these different factory domains into a single digital twin,” said <a href="https://blogs.nvidia.com/blog/omniverse-developer-michael-wagner/">Michael Wagner</a>, cofounder and chief technology officer of SyncTwin. “The framework can be instrumental in dismantling data silos and enhancing collaborative efficiency across different factory domains, such as assembly, logistics and infrastructure planning.”</p>
<p>Hear Wagner discuss turning PowerPoint and Excel data into digital twin scenarios using the SyncTwin App in a <a target="_blank" href="https://www.linkedin.com/events/7209117147254374401/comments/">LinkedIn livestream</a> on July 4 at 11 a.m. CET.</p>
<p><img loading="lazy" decoding="async" class=" wp-image-72749 aligncenter" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_log6.png" alt="" width="818" height="460" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_log6.png 1920w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_log6-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_log6-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_log6-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_log6-1536x864.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_log6-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_log6-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_log6-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/06/synctwin_log6-1280x720.png 1280w" sizes="(max-width: 818px) 100vw, 818px" /></p>
<h2><b>Pioneering Generative AI in Factory Planning</b></h2>
<p>By integrating <a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a> into its platform, SyncTwin also provides users with data-driven insights and recommendations, enhancing decision-making processes.</p>
<p>This AI integration automates complex analyses, accelerates operations and reduces the need for manual inputs. Learn more about how SyncTwin and other startups are combining the powers of OpenUSD and generative AI to elevate their technologies in this <a target="_blank" href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62644/">NVIDIA GTC session</a>.</p>
<p>Hear SyncTwin and NVIDIA experts discuss how digital twins are unlocking new possibilities in this recent community livestream:</p>
<p><iframe loading="lazy" title="Exploring Physics-Based Digital Twins With OpenUSD" width="500" height="281" src="https://www.youtube.com/embed/4j4doXJ3uao?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>By tapping into the power of OpenUSD and NVIDIA’s AI and optimization technologies, SyncTwin is helping set new standards for factory planning and operations, improving operational efficiency and supporting the vision of sustainability and cost reduction across manufacturing.</p>
<h2><b>Get Plugged Into the World of OpenUSD</b></h2>
<p>Learn more about OpenUSD and meet with NVIDIA experts at <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">SIGGRAPH</a>, taking place July 28-Aug. 1 at the Colorado Convention Center and online. Attend these SIGGRAPH highlights:</p>
<ul>
<li>NVIDIA founder and CEO Jensen Huang’s <a target="_blank" href="https://s2024.siggraph.org/program/keynote-presentations/#speaker-huang">fireside chat</a> on Monday, July 29, covering the latest in generative AI and accelerated computing.</li>
<li><a target="_blank" href="https://s2024.conference-program.org/presentation/?id=ind_101&amp;sess=sess421">OpenUSD Day</a> on Tuesday, July 30, where industry luminaries and developers will showcase how to build 3D pipelines and tools using OpenUSD.</li>
<li><a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">Hands-on OpenUSD</a> training for all skill levels.</li>
</ul>
<p>Check out this <a target="_blank" href="https://www.youtube.com/playlist?list=PL3jK4xNnlCVcUP08kj6eOzvCA82U_JKiy">video series</a> about how OpenUSD can improve 3D workflows. For more resources on OpenUSD, explore the Alliance for OpenUSD <a target="_blank" href="https://forum.aousd.org/">forum</a> and visit the <a target="_blank" href="https://aousd.org/">AOUSD website</a>.</p>
<p><i>Get started with NVIDIA Omniverse by downloading the standard license </i><a target="_blank" href="https://www.nvidia.com/en-us/omniverse/download/"><i>free</i></a><i>, access </i><a target="_blank" href="https://developer.nvidia.com/usd"><i>OpenUSD</i></a><i> resources and learn how </i><a target="_blank" href="https://www.nvidia.com/en-us/omniverse/enterprise/"><i>Omniverse Enterprise</i><i> can connect team</i></a><i>s. Follow Omniverse on </i><a target="_blank" href="https://www.instagram.com/nvidiaomniverse/"><i>Instagram</i></a><i>, </i><a target="_blank" href="https://www.linkedin.com/showcase/nvidia-omniverse/"><i>LinkedIn</i></a><i>, </i><a target="_blank" href="https://medium.com/@nvidiaomniverse"><i>Medium</i></a><i> and </i><a target="_blank" href="https://twitter.com/nvidiaomniverse"><i>X</i></a><i>. For more, join the </i><a target="_blank" href="https://www.nvidia.com/en-us/omniverse/community/"><i>Omniverse community</i></a><i> on the </i><a target="_blank" href="https://forums.developer.nvidia.com/c/omniverse/300"><i>forums</i></a><i>, </i><a target="_blank" href="https://discord.com/invite/XWQNJDNuaC"><i>Discord server</i></a><i> and </i><a target="_blank" href="https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA"><i>YouTube</i></a><i> channel. </i></p>
<p><i>Featured image courtesy of SyncTwin GmbH.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/nv-ov-ito-1280x680-synctwin-noCred.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/nv-ov-ito-1280x680-synctwin-noCred-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Into the Omniverse: SyncTwin Helps Democratize Industrial Digital Twins With Generative AI, OpenUSD]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Thinking Outside the Blox: How Roblox Is Using Generative AI to Enhance User Experiences</title>
		<link>https://blogs.nvidia.com/blog/roblox-anupam-singh/</link>
		
		<dc:creator><![CDATA[Andy Bui]]></dc:creator>
		<pubDate>Wed, 26 Jun 2024 13:00:22 +0000</pubDate>
				<category><![CDATA[The AI Podcast]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72308</guid>

					<description><![CDATA[Roblox is a colorful online platform that aims to reimagine the way that people come together — now that vision is being augmented by generative AI. In this episode of NVIDIA’s AI Podcast, host Noah Kravitz speaks with Anupam Singh, vice president of AI and growth engineering at Roblox, on how the company is using	<a class="read-more" href="https://blogs.nvidia.com/blog/roblox-anupam-singh/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Roblox is a colorful online platform that aims to reimagine the way that people come together — now that vision is being augmented by generative AI. In this episode of NVIDIA’s <a href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">AI Podcast</a>, host Noah Kravitz speaks with Anupam Singh, vice president of AI and growth engineering at Roblox, on how the company is using the technology to enhance virtual experiences with features such as automated chat filters and real-time text translation, which help build inclusivity and user safety. Singh also discusses how generative AI can be used to power coding assistants that help creators focus more on creative expression, rather than spending time manually scripting world-building features.</p>
<p><iframe loading="lazy" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/1851430170%3Fsecret_token%3Ds-Hqyyg8rKctd&amp;color=%23ff5500&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;show_teaser=true" width="100%" height="166" frameborder="no" scrolling="no"></iframe></p>
<div style="font-size: 10px; color: #cccccc; line-break: anywhere; word-break: normal; overflow: hidden; white-space: nowrap; text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif; font-weight: 100;"><a style="color: #cccccc; text-decoration: none;" title="The AI Podcast" href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">The AI Podcast</a> · <a style="color: #cccccc; text-decoration: none;" title="How Roblox Is Using Generative AI to Enhance User Experiences - Ep. 227" href="https://soundcloud.com/theaipodcast/anumpam-singh-roblox/s-Hqyyg8rKctd" target="_blank" rel="noopener">How Roblox Is Using Generative AI to Enhance User Experiences &#8211; Ep. 227</a></div>
<h2>Time Stamps</h2>
<p>1:49: Background on Roblox and user interactions within the platform<br />
6:38: Singh’s insight on AI and machine learning’s role in Roblox’s growth<br />
15:51: Using generative AI to enhance user self-expression<br />
20:04: How generative AI simplifies content creation<br />
24:26: What’s next for Roblox</p>
<h2>You Might Also Like:</h2>
<p><a href="https://soundcloud.com/theaipodcast/mediamonks-lewis-smithingham" target="_blank" rel="noopener"><b>Media.Monks’ Lewis Smithingham on Enhancing Media and Marketing With AI &#8211; Ep. 222</b></a></p>
<p>In this episode, Lewis Smithingham, senior vice president of innovation and special operations at Media.Monks, discusses AI’s potential to enhance the media and entertainment industry. Smithingham delves into Media.Monk’s platform for entertainment and speaks to its vision where AI enhances creativity and allows for more personalized, scalable content creation.</p>
<p><a href="https://soundcloud.com/theaipodcast/legal" target="_blank" rel="noopener"><b>The Case for Generative AI in the Legal Field &#8211; Ep. 210</b></a></p>
<p>AI-driven digital solutions enable law practitioners to search laws and cases intelligently — automating the time-consuming process of drafting and analyzing legal documents. In this episode, Thomson Reuters Chief Product Officer David Wong discusses AI’s potential to help deliver better access to justice.</p>
<p><a href="https://soundcloud.com/theaipodcast/legal" target="_blank" rel="noopener"><b>Deepdub’s Ofir Krakowski on Redefining Dubbing from Hollywood to Bollywood &#8211; Ep. 202</b></a></p>
<p>Deepdub acts as a digital bridge, providing access to content by using generative AI to break down language and cultural barriers in the entertainment landscape. In this episode, Deepdub co-founder and CEO Ofir Krakowski speaks on how AI-driven dubbing helps entertainment companies boost efficiency and increase accessibility.</p>
<h2><b>Subscribe to the AI Podcast</b></h2>
<p>Get the <a href="https://blogs.nvidia.com/ai-podcast/" target="_blank" rel="noopener">AI Podcast</a> through <a href="https://itunes.apple.com/us/podcast/the-ai-podcast/id1186480811?mt=2&amp;adbsc=social_20161220_68874946&amp;adbid=811257941365882882&amp;adbpl=tw&amp;adbpr=61559439" target="_blank" rel="noopener">iTunes</a>, <a href="https://play.google.com/music/listen?u=0#/ps/I4kyn74qfrsdhrm35mcrf3igxzm" target="_blank" rel="noopener">Google Play</a>, <a href="https://music.amazon.com/podcasts/956857d0-9461-4496-a07e-24be0539ee82/the-ai-podcast" target="_blank" rel="noopener">Amazon Music</a>, <a href="https://castbox.fm/channel/The-AI-Podcast-id433488?country=us" target="_blank" rel="noopener">Castbox</a>, DoggCatcher, <a href="https://overcast.fm/itunes1186480811/the-ai-podcast" target="_blank" rel="noopener">Overcast</a>, <a href="https://player.fm/series/the-ai-podcast" target="_blank" rel="noopener">PlayerFM</a>, Pocket Casts, <a href="http://www.podbay.fm/show/1186480811" target="_blank" rel="noopener">Podbay</a>, <a href="https://www.podbean.com/podcast-detail/cjgnp-4a6e0/The-AI-Podcast" target="_blank" rel="noopener">PodBean</a>, PodCruncher, PodKicker, <a href="https://soundcloud.com/theaipodcast" target="_blank" rel="noopener">Soundcloud</a>, <a href="https://open.spotify.com/show/4TB4pnynaiZ6YHoKmyVN0L" target="_blank" rel="noopener">Spotify</a>, <a href="http://www.stitcher.com/s?fid=130629&amp;refid=stpr" target="_blank" rel="noopener">Stitcher</a> and <a href="https://tunein.com/podcasts/Technology-Podcasts/The-AI-Podcast-p940829/" target="_blank" rel="noopener">TuneIn</a>.</p>
<p>Make the AI Podcast better: Have a few minutes to spare? Fill out <a href="http://survey.podtrac.com/start-survey.aspx?pubid=I5V0tOQFNS8j&amp;ver=short" target="_blank" rel="noopener">this listener survey</a>.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/2023-Avatar-Lineup-Avatars-have-been-previously-only-created-by-Roblox-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1365"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/2023-Avatar-Lineup-Avatars-have-been-previously-only-created-by-Roblox-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Thinking Outside the Blox: How Roblox Is Using Generative AI to Enhance User Experiences]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Cut the Noise: NVIDIA Broadcast Supercharges Livestreaming, Remote Work</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-broadcast/</link>
		
		<dc:creator><![CDATA[Brian Choi]]></dc:creator>
		<pubDate>Wed, 26 Jun 2024 13:00:19 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[Media and Entertainment]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72735</guid>

					<description><![CDATA[NVIDIA Broadcast offers AI-powered features that improve audio and video quality for a variety of use cases.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><i>Editor’s note: This post is part of the </i><a href="https://blogs.nvidia.com/blog/tag/ai-decoded/"><i>AI Decoded series</i></a><i>, which demystifies AI by making the technology more accessible, and showcases new hardware, software, tools and accelerations for RTX PC users.</i></p>
<p>AI has changed computing forever. The spotlight has most recently been on generative AI, but AI-accelerated, NVIDIA RTX-powered tools have also been key in improving gaming, content creation and productivity over the years.</p>
<p>The <a target="_blank" href="https://www.nvidia.com/en-us/geforce/broadcasting/broadcast-app/">NVIDIA Broadcast app</a> is one example, using Tensor Cores on a local RTX GPU to seamlessly improve audio and video quality. Paired with the NVIDIA encoder (<a target="_blank" href="https://developer.nvidia.com/video-codec-sdk">NVENC</a>) built into GeForce RTX and NVIDIA RTX GPUs, the app makes it easy to get started as a livestreamer or to look professional during video conference calls.</p>
<h2><b>The Stream Dream</b></h2>
<p><iframe loading="lazy" title="Broadcasting With NVIDIA GeForce RTX | NVENC vs x264 Quality Comparison" width="500" height="281" src="https://www.youtube.com/embed/viWuC4MMXWc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>High-quality livestreaming traditionally required expensive hardware. Many livestreamers relied on software CPU encoding using the x264 software library, which often impacted gameplay quality. This led many to use a dual-PC setup, with one PC focused on gaming and content and the other on encoding the stream. It was complicated to assemble, difficult to troubleshoot and often cost-prohibitive for budding livestreamers.</p>
<p>NVENC is here to help. It’s a dedicated hardware video encoder on NVIDIA GPUs that processes the encoding, freeing up the rest of the system to focus on game and content performance. Industry-leading streaming apps like Open Broadcaster Software (OBS) are adding support for NVENC, paving the way for a new generation of broadcasters on popular platforms like Twitch and YouTube.</p>
<p>Meanwhile, <a target="_blank" href="https://developer.nvidia.com/maxine">NVIDIA Maxine</a> helps solve the issue of expensive equipment. It includes free, AI-powered features like virtual green screens and webcam-based augmented reality tracking that eliminate the need for special equipment like physical green screens or motion- capture suits. Broadcasters first got to experience the technology at TwitchCon 2019, where they tested OBS live on the show floor with an AI-accelerated green screen on a GeForce RTX 2080 GPU.</p>
<p>Maxine’s AI-powered effects debuted for RTX users in the RTX Voice beta, and moved into the <a target="_blank" href="https://www.nvidia.com/en-us/geforce/broadcasting/broadcast-app/">NVIDIA Broadcast</a> app.</p>
<h2><b>Now Showing: NVIDIA Broadcast</b></h2>
<p>NVIDIA Broadcast offers AI-powered features that improve audio and video quality for a variety of use cases. It’s user-friendly, works in any app and is a <a target="_blank" href="https://www.nvidia.com/en-us/geforce/guides/broadcast-app-setup-guide/#Quick-Start-Guide">breeze to set up</a>.</p>
<p>It includes:</p>
<ul>
<li><b>Noise and Acoustic Echo Removal:</b> AI eliminates unwanted background noise from both the mic and inbound audio at the touch of a button.</li>
<li><b>Virtual Backgrounds:</b> Features like Background Removal, Replacement and Blur help customize backgrounds without the need for expensive equipment or complex lighting setups.</li>
<li><b>Eye Contact: </b>AI helps make it appear as though a streamer is looking directly at the camera, even when they’re glancing off camera or taking notes.</li>
<li><b>Auto Frame: </b>Dynamically tracks movements in real time, automatically cropping and zooming moving objects regardless of their position.</li>
<li><b>Vignette:</b> AI applies a darkening effect to the corners of camera images, providing visual contrast to draw attention to the center of the video and adding stylistic flair.</li>
<li><b>Video Noise Removal:</b> Removes visual noise from low-light situations for a cleaner picture.</li>
</ul>
<p>NVIDIA Broadcast works by creating a virtual camera, microphone or speaker in Windows so that users can set up their devices once and use them in any broadcasting, video conferencing or voice chat apps, including Discord, Google Meet, Microsoft Teams, OBS Studio, Slack, Webex and Zoom.</p>
<p>Those with an NVIDIA GeForce RTX, TITAN RTX, NVIDIA RTX or Quadro RTX GPU can use their GPU’s dedicated Tensor Cores to help the app’s AI networks run in real time.</p>
<p>The same AI-powered technology in NVIDIA Broadcast is also available to app developers as a software development kit. Audiovisual technology company Elgato includes Maxine’s AI audio noise removal technology in its <a target="_blank" href="https://help.elgato.com/hc/en-us/articles/7665204543117-Wave-Link-How-to-use-NVIDIA-Broadcast-Noise-Removal-by-Elgato">Wave Link</a> software, while VTube Studio — a popular app for connecting a 3D model to a webcam for streaming as an animated character — offers an <a target="_blank" href="https://store.steampowered.com/app/2178540/VTube_Studio__NVIDIA_Broadcast_Tracker/">RTX-accelerated model tracker</a> plug-in as a free download. Independent developer Xaymar uses NVIDIA Maxine in his <a target="_blank" href="https://www.xaymar.com/projects/voicefx/">VoiceFX</a> plug-in.</p>
<p>Content creators can use this plug-in or Elgato’s virtual studio technology (VST) filter to clean up noise and echo from recordings in post-processing in video editing suites like Adobe Premiere Pro or in digital audio workstations like Ableton Live and Adobe Audition.</p>
<h2><b>(Not) Hearing Is Believing</b></h2>
<p>Since its release, NVIDIA Broadcast has been used by millions.</p>
<p>“I’ve utilized the video noise removal and background replacement the most,” said <a target="_blank" href="https://www.twitch.tv/mr_vudoo">Mr_Vudoo</a>, a Twitch personality and broadcaster. “The eye contact feature was very interesting and quite honestly took me by surprise at how well it worked.”</p>
<p>Unmesh Dinda, host of the YouTube channel <a target="_blank" href="https://www.youtube.com/@PiXimperfect/featured">PiXimperfect</a>, demonstrated NVIDIA Broadcast’s noise-canceling and echo-removal AI features in an extreme scenario. He set an electric fan whirring directly into his microphone and donned a helmet that was intensely hammered on. Even with these loud sounds in the background, Dinda could be heard crystal clear with Broadcast’s noise-removal feature turned on. <a target="_blank" href="https://www.instagram.com/reel/ClTp4_Hp867/?hl=en">The video</a> has racked up more than 12 million views.</p>
<p><iframe loading="lazy" title="Remove ALL Background Noise with One Click! #Shorts" width="500" height="281" src="https://www.youtube.com/embed/bEurXLfzXfw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>NVIDIA Broadcast is also a useful tool for the growing remote workforce. In an article, Tom’s Hardware editor-in-chief Avram Piltch <a target="_blank" href="https://www.tomshardware.com/news/nvidia-broadcast-tested">detailed his testing</a> of the app’s noise reduction features against noisy air conditioners, lawn-mowing neighbors and even a robot-wielding, tantrum-throwing child. Broadcast’s AI audio filters prevailed every time:</p>
<p>“I got my eight-year-old to fake throwing a fit right behind me and, once I enabled noise removal, every whine of ‘I’m not going to bed’ went silent (at least on the recording),” said Piltch. “To double the challenge, we had him throw a tantrum while carrying around a robot car with whirring treads. Once again, NVIDIA Broadcast removed all of the unwanted sound.”</p>
<p>Even everyday scenarios like video calls with a medical professional benefit from NVIDIA Broadcast’s AI-powered background removal.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">Today I was video calling my doctor and she complimented me on how good the background blurring on my webcam looked.</p>
<p>It caught me so off guard, I didn&#39;t know how to respond, so I just gave her a step by step tutorial on how to use NVIDIA Broadcast <img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f629.png" alt="😩" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>&mdash; Prismatic Ash (@_Ashdown) <a target="_blank" href="https://twitter.com/_Ashdown/status/1801547934925742437?ref_src=twsrc%5Etfw">June 14, 2024</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<p>Download <a target="_blank" href="https://www.nvidia.com/en-us/geforce/broadcasting/broadcast-app/">NVIDIA Broadcast</a> for free on any RTX-powered desktop or laptop.</p>
<p><i>Generative AI is transforming gaming, videoconferencing and interactive experiences of all kinds. Make sense of what’s new and what’s next by subscribing to the </i><a target="_blank" href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><i>AI Decoded newsletter</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/broadcast-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/broadcast-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Cut the Noise: NVIDIA Broadcast Supercharges Livestreaming, Remote Work]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>EvolutionaryScale Debuts With ESM3 Generative AI Model for Protein Design</title>
		<link>https://blogs.nvidia.com/blog/evolutionaryscale-esm3-generative-ai-nim-bionemo-h100/</link>
		
		<dc:creator><![CDATA[Anthony Costa]]></dc:creator>
		<pubDate>Tue, 25 Jun 2024 10:00:25 +0000</pubDate>
				<category><![CDATA[Research]]></category>
		<category><![CDATA[BioNeMo]]></category>
		<category><![CDATA[NVIDIA H100]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72720</guid>

					<description><![CDATA[Generative AI has revolutionized software development with prompt-based code generation — protein design is next. EvolutionaryScale today announced the release of its ESM3 model, the third-generation ESM model, which simultaneously reasons over the sequence, structure and functions of proteins, giving protein discovery engineers a programmable platform. The startup, which emerged from the Meta FAIR (Fundamental	<a class="read-more" href="https://blogs.nvidia.com/blog/evolutionaryscale-esm3-generative-ai-nim-bionemo-h100/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Generative AI has revolutionized software development with prompt-based code generation — protein design is next.</p>
<p>EvolutionaryScale today announced the release of its ESM3 model, the third-generation ESM model, which simultaneously reasons over the sequence, structure and functions of proteins, giving protein discovery engineers a programmable platform.</p>
<p>The startup, which emerged from the Meta FAIR (Fundamental AI Research) unit, recently landed funding led by Lux Capital, Nat Friedman and Daniel Gross, with investment from NVIDIA and Amazon.</p>
<p>At the forefront of programmable biology, EvolutionaryScale can assist researchers in engineering proteins that can help target cancer cells, find alternatives to harmful plastics, drive environmental mitigations and more.</p>
<p>EvolutionaryScale is pioneering the frontier of programmable biology with the scale-out model development of ESM3, which used <a target="_blank" href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPUs</a> for the most compute ever put into a biological foundation model. The 98 billion parameter ESM3 model uses roughly 25x more flops and 60x more data than its predecessor, ESM2.</p>
<p>The company, which developed a database of more than 2 billion protein sequences to train its AI model, offers technology that can provide clues applicable to drug development, disease eradication and, literally, how humans have evolved at scale as a species — as its name suggests — for drug discovery researchers.</p>
<h2><b>Accelerating In Silico Biological Research With ESM3</b></h2>
<p>With leaps in training data, EvolutionaryScale aims to accelerate protein discovery with ESM3.</p>
<p>The model was trained on almost 2.8 billion protein sequences sampled from organisms and biomes, allowing scientists to prompt the model to identify and validate new proteins with increasing levels of accuracy.</p>
<p>ESM3 offers significant updates over previous versions. The model is natively generative, and it is an “all to all” model, meaning structure and function annotations can be provided as input rather than just as output.</p>
<p>Once it’s made publicly available, scientists can fine-tune this base model to construct purpose-built models based on their own proprietary data. The boost in protein engineering capabilities due to ESM3’s large-scale generative training across enormous amounts of data offers a time-traveling machine for in silico biological research.</p>
<h2><b>Driving the Next Big Breakthroughs With NVIDIA BioNeMo</b></h2>
<p>ESM3 provides biologists and protein designers with a generative AI boost, helping improve their engineering and understanding of proteins. With simple prompts, it can generate new proteins with a provided scaffold, self-improve its protein design based on feedback and design proteins based on the functionality that the user indicates. These capabilities can be used in tandem in any combination to provide chain-of-thought protein design as if the user were messaging a researcher who had memorized the intricate three-dimensional meaning of every protein sequence known to humans and had learned the language fluently, enabling users to iterate back and forth.</p>
<p>“In our internal testing we’ve been impressed by the ability of ESM3 to creatively respond to a variety of complex prompts,” said Tom Sercu, co-founder and VP of engineering at EvolutionaryScale. “It was able to solve an extremely hard protein design problem to create a novel Green Fluorescent Protein. We expect ESM3 will help scientists accelerate their work and open up new possibilities — we’re looking forward to seeing how it will contribute to future research in the life sciences.”</p>
<p>EvolutionaryScale will be opening an API for closed beta today and code and weights are available for a small open version of ESM3 for non-commercial use. This version is coming soon to <a target="_blank" href="https://www.nvidia.com/en-us/clara/bionemo/">NVIDIA BioNeMo</a>, a generative AI platform for drug discovery. The full ESM3 family of models will soon be available to select customers as an <a target="_blank" href="https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Furl.avanan.click%2Fv2%2F___https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fai%2F%2523referrer%3Dai-subdomain%3Fncid%3Dpa-srch-goog-772333%26_bt%3D697697685508%26_bk%3Dnvidia%2520nim%26_bm%3De%26_bn%3Dg%26_bg%3D165151891361%26gad_source%3D1%26gclid%3DEAIaIQobChMI5pHT7fvjhgMVVXN_AB1KOQFuEAAYASAAEgKvWfD_BwE___.YXAzOm91dGNhc3Q6YTpvOjViNjRiMzE4MDU2MWMwZDcyNThmMmI4NWZhYTViNTJiOjY6NDI2ZTphZWVjMmY1N2JjMjFiM2NhODBiNmFmMjFiZTM2YTQyZWM1NDMwZTZhODdmZTYwOWEyZGRiODc2MWJlMGY2MzUxOmg6VA&amp;data=05%7C02%7Crkelleher%40nvidia.com%7C789b16235c954beed3a908dc8ff9ab8b%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638543549716443890%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&amp;sdata=wD96MHFHOJjySVQjvo%2FqotZ6KMUZX7Lcr%2FMHAbyGW%2FU%3D&amp;reserved=0">NVIDIA NIM</a> microservice, run-time optimized in collaboration with NVIDIA, and supported by an <a target="_blank" href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/">NVIDIA AI Enterprise</a> software license for testing at ai.nvidia.com.</p>
<p>The computing power required to train these models is growing exponentially. ESM3 was trained using the <a target="_blank" href="https://andromeda.ai/">Andromeda cluster</a>, which uses NVIDIA H100 GPUs and NVIDIA Quantum-2 InfiniBand networking.</p>
<p>The ESM3 model will be available on select partner platforms, including Amazon Bedrock, Amazon Sagemaker, AWS HealthOMICs and NVIDIA BioNeMo.</p>
<p><i>See </i><a target="_blank" href="https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fabout-nvidia%2Flegal-info%2F&amp;data=05%7C02%7Cscmartin%40nvidia.com%7C83515b5492594c57a3cb08dc94b213d0%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638548739762646477%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&amp;sdata=KzQTSwL94NtcZYgStTWnnmHHHy53cbxt0r7LKK5pUdQ%3D&amp;reserved=0"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/hc-corp-blog-evo-scale-1280x680-1.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/hc-corp-blog-evo-scale-1280x680-1-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[EvolutionaryScale Debuts With ESM3 Generative AI Model for Protein Design]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Why 3D Visualization Holds Key to Future Chip Designs</title>
		<link>https://blogs.nvidia.com/blog/ansys-omniverse-modulus-accelerate-simulation/</link>
		
		<dc:creator><![CDATA[Baskar Rajagopalan]]></dc:creator>
		<pubDate>Mon, 24 Jun 2024 19:00:44 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[NVIDIA Modulus]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72721</guid>

					<description><![CDATA[Multi-die chips, known as three-dimensional integrated circuits, or 3D-ICs, represent a revolutionary step in semiconductor design. The chips are vertically stacked to create a compact structure that boosts performance without increasing power consumption. However, as chips become denser, they present more complex challenges in managing electromagnetic and thermal stresses. To understand and address this, advanced	<a class="read-more" href="https://blogs.nvidia.com/blog/ansys-omniverse-modulus-accelerate-simulation/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Multi-die chips, known as three-dimensional integrated circuits, or 3D-ICs, represent a revolutionary step in semiconductor design. The chips are vertically stacked to create a compact structure that boosts performance without increasing power consumption.</p>
<p>However, as chips become denser, they present more complex challenges in managing electromagnetic and thermal stresses. To understand and address this, advanced 3D multiphysics visualizations become essential to design and diagnostic processes.</p>
<p>At this week’s <a target="_blank" href="https://www.dac.com/">Design Automation Conference</a>, a global event showcasing the latest developments in chips and systems, Ansys — a company that develops engineering simulation and 3D design software — will share how it’s using NVIDIA technology to overcome these challenges to build the next generation of semiconductor systems.</p>
<p>To enable 3D visualizations of simulation results for their users, Ansys uses <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a>, a platform of application programming interfaces, software development kits, and services that enables developers to easily integrate <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/usd/">Universal Scene Description (OpenUSD)</a> and NVIDIA RTX rendering technologies into existing software tools and simulation workflows.</p>
<p>The platform powers visualizations of 3D-IC results from Ansys solvers so engineers can evaluate phenomena like electromagnetic fields and temperature variations to optimize chips for faster processing, increased functionality and improved reliability.</p>
<p>With Ansys Icepak on the NVIDIA Omniverse platform, engineers can simulate temperatures across a chip according to different power profiles and floor plans. Finding chip hot-spots can lead to better design of the chips themselves, as well as auxiliary cooling devices. However, these 3D-IC simulations are computationally intensive, limiting the number of simulations and design points users can explore.</p>
<p>Using <a target="_blank" href="https://developer.nvidia.com/modulus">NVIDIA Modulus, combined with</a> novel techniques for handling arbitrary power patterns in the Ansys RedHawk-SC electrothermal data pipeline and model training framework, the Ansys R&amp;D team is exploring the acceleration of simulation workflows with AI-based surrogate models. Modulus is an open-source AI framework for building, training and fine-tuning physics-ML models at scale with a simple Python interface.</p>
<p>With the <a target="_blank" href="https://docs.nvidia.com/deeplearning/modulus/modulus-v2209/user_guide/theory/architectures.html">NVIDIA Modulus Fourier neural operator </a>(FNO) architecture, which can parameterize solutions for a distribution of partial differential equations, Ansys researchers created an AI surrogate model that efficiently predicts temperature profiles for any given power profile and a given floor plan defined by system parameters like heat transfer coefficient, thickness and material properties. This model offers near real-time results at significantly reduced computational costs, allowing Ansys users to explore a wider design space for new chips.</p>
<figure id="attachment_72730" aria-describedby="caption-attachment-72730" style="width: 399px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/06/Ansys-Copy.png"><img loading="lazy" decoding="async" class="wp-image-72730 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/Ansys-Copy.png" alt="" width="399" height="133" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/Ansys-Copy.png 399w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Ansys-Copy-188x63.png 188w" sizes="(max-width: 399px) 100vw, 399px" /></a><figcaption id="caption-attachment-72730" class="wp-caption-text">Ansys uses a 3D FNO model to infer temperatures on a chip surface for unseen power profiles, a given die height and heat-transfer coefficient boundary condition.</figcaption></figure>
<p>Following a successful proof of concept, the Ansys team will explore integration of such AI surrogate models for its next-generation RedHawk-SC platform using NVIDIA Modulus.</p>
<p>As more surrogate models are developed, the team will also look to enhance model generality and accuracy through in-situ fine-tuning. This will enable RedHawk-SC users to benefit from faster simulation workflows, access to a broader design space and the ability to refine models with their own data to foster innovation and safety in product development.</p>
<p><i>To see the joint demonstration of 3D-IC multiphysics visualization using NVIDIA Omniverse APIs, </i><a target="_blank" href="https://www.ansys.com/events/dac"><i>visit Ansys at the Design Automation Conference</i></a><i>, running June 23-27, in San Francisco at booth 1308 or watch the </i><a target="_blank" href="https://61dac.conference-program.com/presentation/?id=EF119&amp;sess=sess280"><i>presentation</i></a><i> at the Exhibitor Forum.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/Ansys-Blog-Header.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/Ansys-Blog-Header-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Why 3D Visualization Holds Key to Future Chip Designs]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Crack the Case With ‘Tell Me Why’ and ‘As Dusk Falls’ on GeForce NOW</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-tell-me-why-as-dusk-falls/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 20 Jun 2024 13:00:27 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72692</guid>

					<description><![CDATA[Sit back and settle in for some epic storytelling. Tell Me Why and As Dusk Falls — award-winning, narrative-driven games from Xbox Studios — add to the 1,900+ games in the GeForce NOW library, ready to stream from the cloud.  Members can find more adventures with four new titles available this week. Experience a Metallica	<a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-tell-me-why-as-dusk-falls/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Sit back and settle in for some epic storytelling. <i>Tell Me Why</i> and <i>As Dusk Falls</i> — award-winning, narrative-driven games from Xbox Studios — add to the 1,900+ games in the <a target="_blank" href="http://play.geforcenow.com">GeForce NOW library</a>, ready to stream from the cloud.<a target="_blank" href="http://play.geforcenow.com"> </a></p>
<p>Members can find more adventures with four new titles available this week.</p>
<p>Experience a Metallica concert like no other in <a target="_blank" href="https://www.fortnite.com/news/metallica-lights-up-fortnite-with-a-new-music-experience-festival-season-and-more">“Metallica: Fuel. Fire. Fury.”</a> This journey through six fan-favorite songs features gameplay that matches the intensity. “Metallica: Fuel. Fire. Fury.” will have six different showtimes running June 22-23 in <i>Fortnite</i>. Anyone can get a front-row seat to the interactive music experience by streaming on their mobile device, powered by <a target="_blank" href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a>.</p>
<h2><b>Unravel the Mystery</b></h2>
<p>Whether uncovering family mysteries in Alaska or navigating small-town secrets in Arizona, gamers are set to be drawn into richly woven stories with <i>Tell Me Why</i> and <i>Ask Dusk Falls</i> joining the cloud this week<i>.</i></p>
<figure id="attachment_72699" aria-describedby="caption-attachment-72699" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-72699" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Tell_Me_Why-672x378.jpg" alt="Tell Me Why on GeForce NOW" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Tell_Me_Why-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Tell_Me_Why-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Tell_Me_Why-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Tell_Me_Why-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Tell_Me_Why-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Tell_Me_Why-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Tell_Me_Why-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Tell_Me_Why-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Tell_Me_Why-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-72699" class="wp-caption-text"><em>Ain’t nothing but a great game.</em></figcaption></figure>
<p><i>Tell Me Why</i> — an episodic adventure game from Dontnod Entertainment, the creators of the beloved <i>Life Is Strange</i> series — follows twins Tyler and Alyson Ronan as they reunite after a decade to uncover the mysteries of their troubled childhoods in the fictional town of Delos Crossing, Alaska. Experience true-to-life characters, mature themes and gripping choices.</p>
<figure id="attachment_72696" aria-describedby="caption-attachment-72696" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-72696" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-As_Dusk_Falls-672x378.jpg" alt="As Dusk Falls on GeForce NOW" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-As_Dusk_Falls-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-As_Dusk_Falls-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-As_Dusk_Falls-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-As_Dusk_Falls-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-As_Dusk_Falls-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-As_Dusk_Falls-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-As_Dusk_Falls-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-As_Dusk_Falls-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-As_Dusk_Falls-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-72696" class="wp-caption-text"><em>Every family has secrets.</em></figcaption></figure>
<p>Dive into the intertwined lives of two families over three decades in <i>As Dusk Falls</i> from INTERIOR/NIGHT. Set in small-town Arizona in the 1990s, the game’s unique art style blends 2D character illustrations with 3D environments, creating a visually striking experience. Players’ choices significantly impact the storyline, making each playthrough unique.</p>
<p>GeForce NOW members can now stream these award-winning titles on a variety of devices, including PCs, Macs, <a target="_blank" href="https://www.nvidia.com/en-us/shield/">SHIELD TVs</a> and Android devices. Upgrade to a <a target="_blank" href="http://geforcenow.com/membership">Priority or Ultimate membership</a> to enjoy enhanced streaming quality and performance, including up to 4K resolution and 120 frames per second on supported devices. Jump into these emotionally rich narratives and discover the power of choice in shaping the characters’ destinies.</p>
<h2><b>Wake Up to New Games</b></h2>
<figure id="attachment_72693" aria-describedby="caption-attachment-72693" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-72693" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Still_Wakes_The_Deep-672x336.jpg" alt="Still Wakes the Deep on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Still_Wakes_The_Deep-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Still_Wakes_The_Deep-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Still_Wakes_The_Deep-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Still_Wakes_The_Deep-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Still_Wakes_The_Deep-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Still_Wakes_The_Deep-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Still_Wakes_The_Deep-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Still_Wakes_The_Deep-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Still_Wakes_The_Deep.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-72693" class="wp-caption-text"><em>Run!</em></figcaption></figure>
<p>In <i>Still Wakes the Deep</i> from The Chinese Room and Secret Mode, play as an offshore oil rig worker fighting for dear life through a vicious storm, perilous surroundings and the dark, freezing North Sea waters. All lines of communication have been severed. All exits are gone. All that remains is the need to face the unknowable horror aboard. Live the terror and escape the rig, all from the cloud.</p>
<p>Check out the list of new games this week:</p>
<ul>
<li><i>Still Wakes the Deep </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1622910?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a target="_blank" href="https://www.xbox.com/games/store/1622910?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass, June 18)</li>
<li><i>Skye: The Misty Isle </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1710180?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, June 19)</li>
<li><i>As Dusk Falls </i>(<a target="_blank" href="https://store.steampowered.com/app/1341820?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a target="_blank" href="https://www.xbox.com/games/store/as-dusk-falls/9NR7XDNVP5SW?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Tell Me Why </i>(<a target="_blank" href="https://store.steampowered.com/app/1180660?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a target="_blank" href="https://www.xbox.com/games/store/tell-me-why/9NBL0XKVCN5L?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
</ul>
<figure id="attachment_72702" aria-describedby="caption-attachment-72702" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-72702" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-672x357.png" alt="Greetings From GFN" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/06/GFN_Thursday-Greetings_From_GFN.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-72702" class="wp-caption-text"><em>Make sure to catch #GreetingFromGFN.</em></figcaption></figure>
<p>Plus, #GreetingsFromGFN continues on @NVIDIAGFN social media accounts, with members sharing their favorite locations to visit in the cloud.</p>
<p>What are you planning to play this weekend? Let us know on <a target="_blank" href="https://www.twitter.com/nvidiagfn">X</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">What&#39;s a game that&#39;s still giving you nightmares? <img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f628.png" alt="😨" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f329.png" alt="🌩" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a target="_blank" href="https://twitter.com/NVIDIAGFN/status/1803095326980460884?ref_src=twsrc%5Etfw">June 18, 2024</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/gfn-thursday-6-20-nv-blog-1280x680-no-copy.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/gfn-thursday-6-20-nv-blog-1280x680-no-copy-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Crack the Case With ‘Tell Me Why’ and ‘As Dusk Falls’ on GeForce NOW]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Decoding How NVIDIA AI Workbench Powers App Development</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-workbench-hybrid-rag/</link>
		
		<dc:creator><![CDATA[Jesse Clayton]]></dc:creator>
		<pubDate>Wed, 19 Jun 2024 13:00:25 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72261</guid>

					<description><![CDATA[NVIDIA AI Workbench simplifies AI developer workflows by helping users build their own RAG projects, customize models and more.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><i>Editor’s note: This post is part of the </i><a href="https://blogs.nvidia.com/blog/tag/ai-decoded/"><i>AI Decoded series</i></a><i>, which demystifies AI by making the technology more accessible and showcases new hardware, software, tools and accelerations for NVIDIA RTX PC and workstation users.</i></p>
<p>The demand for tools to simplify and optimize <a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a> development is skyrocketing. Applications based on <a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">retrieval-augmented generation (RAG)</a> — a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from specified external sources — and customized models are enabling developers to tune AI models to their specific needs.</p>
<p>While such work may have required a complex setup in the past, new tools are making it easier than ever.</p>
<p><a target="_blank" href="https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/">NVIDIA AI Workbench</a> simplifies AI developer workflows by helping users build their own RAG projects, customize models and more. It’s part of the <a target="_blank" href="https://developer.nvidia.com/blog/streamline-ai-powered-app-development-with-nvidia-rtx-ai-toolkit-for-windows-rtx-pcs/">RTX AI Toolkit</a> — a suite of tools and software development kits for customizing, optimizing and deploying AI capabilities — launched at <a target="_blank" href="https://www.nvidia.com/en-us/events/computex/">COMPUTEX</a> earlier this month. AI Workbench removes the complexity of technical tasks that can derail experts and halt beginners.</p>
<h2><b>What Is NVIDIA AI Workbench?</b></h2>
<p>Available for free, NVIDIA AI Workbench enables users to develop, experiment with, test and prototype AI applications across GPU systems of their choice — from laptops and workstations to data center and cloud. It offers a new approach for creating, using and sharing GPU-enabled development environments across people and systems.</p>
<p>A simple <a target="_blank" href="https://docs.nvidia.com/ai-workbench/user-guide/latest/installation/overview.html">installation</a> gets users up and running with AI Workbench on a local or remote machine in just minutes. Users can then start a new project or replicate one from <a target="_blank" href="https://github.com/nvidia?q=workbench&amp;type=all&amp;language=&amp;sort">the examples on GitHub</a>. Everything works through GitHub or GitLab, so users can easily collaborate and distribute work. Learn more about <a target="_blank" href="https://docs.nvidia.com/ai-workbench/user-guide/latest/overview/introduction.html">getting started with AI Workbench</a>.</p>
<h2><b>How AI Workbench Helps Address AI Project Challenges</b></h2>
<p>Developing AI workloads can require manual, often complex processes, right from the start.</p>
<p>Setting up GPUs, updating drivers and managing versioning incompatibilities can be cumbersome. Reproducing projects across different systems can require replicating manual processes over and over. Inconsistencies when replicating projects, like issues with data fragmentation and version control, can hinder collaboration. Varied setup processes, moving credentials and secrets, and changes in the environment, data, models and file locations can all limit the portability of projects.</p>
<p>AI Workbench makes it easier for data scientists and developers to manage their work and collaborate across heterogeneous platforms. It integrates and automates various aspects of the development process, offering:<b></b></p>
<ul>
<li><b>Ease of setup:</b> AI Workbench streamlines the process of setting up a developer environment that’s GPU-accelerated, even for users with limited technical knowledge.</li>
<li><b>Seamless collaboration: </b>AI Workbench integrates with version-control and project-management tools like GitHub and GitLab, reducing friction when collaborating.</li>
<li><b>Consistency when scaling from local to cloud:</b> AI Workbench ensures consistency across multiple environments, supporting scaling up or down from local workstations or PCs to data centers or the cloud.<b></b></li>
</ul>
<h2><b>RAG for Documents, Easier Than Ever</b></h2>
<p>NVIDIA offers sample development <a target="_blank" href="https://docs.nvidia.com/ai-workbench/user-guide/latest/overview/projects.html">Workbench Projects</a> to help users get started with AI Workbench. The <a target="_blank" href="https://github.com/NVIDIA/workbench-example-hybrid-rag">hybrid RAG Workbench Project</a> is one example: It runs a custom, text-based RAG web application with a user’s documents on their local workstation, PC or remote system.</p>
<p>Every Workbench Project runs in a “container” — software that includes all the necessary components to run the AI application. The hybrid RAG sample pairs a Gradio chat interface frontend on the host machine with a containerized RAG server — the backend that services a user’s request and routes queries to and from the vector database and the selected <a target="_blank" href="https://www.nvidia.com/en-us/glossary/large-language-models/">large language model</a>.</p>
<p>This Workbench Project supports a wide variety of LLMs <a target="_blank" href="https://github.com/NVIDIA/workbench-example-hybrid-rag?tab=readme-ov-file#table-1-default-supported-models-by-inference-mode">available on NVIDIA’s GitHub page</a>. Plus, the hybrid nature of the project lets users select where to run inference.</p>
<figure id="attachment_72265" aria-describedby="caption-attachment-72265" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/06/git-based-projects.png"><img loading="lazy" decoding="async" class="size-large wp-image-72265" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/git-based-projects-672x382.png" alt="" width="672" height="382" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/git-based-projects-672x382.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/git-based-projects-400x228.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/git-based-projects-768x437.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/git-based-projects-791x450.png 791w, https://blogs.nvidia.com/wp-content/uploads/2024/06/git-based-projects-378x215.png 378w, https://blogs.nvidia.com/wp-content/uploads/2024/06/git-based-projects-176x100.png 176w, https://blogs.nvidia.com/wp-content/uploads/2024/06/git-based-projects-1280x728.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/06/git-based-projects.png 1413w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-72265" class="wp-caption-text">Workbench Projects let users version the development environment and code.</figcaption></figure>
<p>Developers can run the embedding model on the host machine and run inference locally on a Hugging Face Text Generation Inference server, on target cloud resources using NVIDIA inference endpoints like the <a target="_blank" href="http://build.nvidia.com">NVIDIA API catalog</a>, or with self-hosting microservices such as <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> or third-party services.</p>
<p>The hybrid RAG Workbench Project also includes:</p>
<ul>
<li><b>Performance metrics:</b> Users can evaluate how RAG- and non-RAG-based user queries perform across each inference mode. Tracked metrics include Retrieval Time, Time to First Token (TTFT) and Token Velocity.</li>
<li><b>Retrieval transparency:</b> A panel shows the exact snippets of text — retrieved from the most contextually relevant content in the vector database — that are being fed into the LLM and improving the response’s relevance to a user’s query.</li>
<li><b>Response customization:</b> Responses can be tweaked with a variety of parameters, such as maximum tokens to generate, temperature and frequency penalty.</li>
</ul>
<p>To get started with this project, simply <a target="_blank" href="https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/">install AI Workbench</a> on a local system. The hybrid RAG Workbench Project can be brought from GitHub into the user’s account and duplicated to the local system.</p>
<p>More resources are available in the <a target="_blank" href="https://docs.nvidia.com/ai-workbench/user-guide/latest/quickstart/example-projects.html">AI Decoded user guide</a>. In addition, community members provide helpful video tutorials, like the one from Joe Freeman below.</p>
<p><iframe loading="lazy" title="NVIDIA AI Workbench Topology running on Windows, WSL and Linux  with an example" width="500" height="281" src="https://www.youtube.com/embed/MTA3D4LJfc0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Customize, Optimize, Deploy</b></h2>
<p>Developers often seek to customize AI models for specific use cases. Fine-tuning, a technique that changes the model by training it with additional data, can be useful for style transfer or changing model behavior. AI Workbench helps with fine-tuning, as well.</p>
<p>The <a target="_blank" href="https://github.com/NVIDIA/RTX-AI-Toolkit/blob/main/tutorial-llama3-finetune.md">Llama-factory AI Workbench Project</a> enables QLoRa, a fine-tuning method that minimizes memory requirements, for a variety of models, as well as model quantization via a simple graphical user interface. Developers can use public or their own datasets to meet the needs of their applications.</p>
<p>Once fine-tuning is complete, the model can be quantized for improved performance and a smaller memory footprint, then deployed to native Windows applications for local inference or to NVIDIA NIM for cloud inference. Find a complete tutorial for this project on the <a target="_blank" href="https://github.com/NVIDIA/RTX-AI-Toolkit">NVIDIA RTX AI Toolkit repository</a>.</p>
<h2><b>Truly Hybrid — Run AI Workloads Anywhere</b></h2>
<p>The Hybrid-RAG Workbench Project described above is hybrid in more than one way. In addition to offering a choice of inference mode, the project can be run locally on NVIDIA RTX workstations and GeForce RTX PCs, or scaled up to remote cloud servers and data centers.</p>
<p>The ability to run projects on systems of the user’s choice — without the overhead of setting up the infrastructure — extends to all Workbench Projects. Find more examples and instructions for fine-tuning and customization in the AI Workbench <a target="_blank" href="https://docs.nvidia.com/ai-workbench/user-guide/latest/quickstart/quickstart-basic.html">quick-start guide</a>.</p>
<p><i>Generative AI is transforming gaming, videoconferencing and interactive experiences of all kinds. Make sense of what’s new and what’s next by subscribing to the </i><a target="_blank" href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><i>AI Decoded newsletter</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/ai-workbench-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/ai-workbench-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Decoding How NVIDIA AI Workbench Powers App Development]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Light Bulb Moment: NVIDIA CEO Sees Bright Future for AI-Powered Electric Grid</title>
		<link>https://blogs.nvidia.com/blog/ai-electric-grid-edison/</link>
		
		<dc:creator><![CDATA[Rick Merritt]]></dc:creator>
		<pubDate>Tue, 18 Jun 2024 23:40:22 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[Energy]]></category>
		<category><![CDATA[Events]]></category>
		<category><![CDATA[Inception]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[New GPU Uses]]></category>
		<category><![CDATA[NVIDIA Jetson]]></category>
		<category><![CDATA[Omniverse]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72246</guid>

					<description><![CDATA[The electric grid and the utilities managing it have an important role to play in the next industrial revolution that’s being driven by AI and accelerated computing, said NVIDIA founder and CEO Jensen Huang Tuesday at the annual meeting of the Edison Electric Institute (EEI), an association of U.S. and international utilities. “The future of	<a class="read-more" href="https://blogs.nvidia.com/blog/ai-electric-grid-edison/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>The electric grid and the utilities managing it have an important role to play in the next industrial revolution that’s being driven by AI and accelerated computing, said NVIDIA founder and CEO Jensen Huang Tuesday at the annual meeting of the Edison Electric Institute (EEI), an association of U.S. and international utilities.</p>
<p>“The future of digital intelligence is quite bright, and so the future of the energy sector is bright, too,” said Huang in a conversation before an audience of more than a thousand utility and energy industry executives.</p>
<p>Like other companies, utilities will apply AI to increase employee productivity, but “the greatest impact and return is in applying AI in the delivery of energy over the grid,” said Huang, in conversation with Pedro Pizarro, the chair of EEI and president and CEO of Edison International, the parent company of Southern California Edison, one of the nation’s largest electric utilities.</p>
<p>For example, Huang described how grids will use AI-powered smart meters to let customers sell their excess electricity to neighbors.</p>
<p>“You will connect resources and users, just like Google, so your power grid becomes a smart network with a digital layer like an app store for energy,” he said.</p>
<p>“My sense is, like previous industrial revolutions, [AI] will drive productivity to levels that we’ve never seen,” he added.</p>
<h2><b>AI Lights Up Electric Grids</b></h2>
<p>Today, electric grids are mainly one-way systems that link a few big power plants to many users. They’ll increasingly become two-way, flexible and distributed networks with solar and wind farms connecting homes and buildings that sport solar panels, batteries and electric vehicle chargers.</p>
<p>It’s a big job that requires autonomous control systems that process and analyze in real time a massive amount of data — work well suited to AI and <a href="https://blogs.nvidia.com/blog/what-is-accelerated-computing/">accelerated computing</a>.</p>
<p>AI is being applied to use cases across electric grids, thanks to a wide ecosystem of companies using NVIDIA’s technologies.</p>
<p>In a recent <a target="_blank" href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s61864/">GTC session</a>, utility vendor Hubbell and startup Utilidata, a member of the <a target="_blank" href="https://www.nvidia.com/en-us/startups/">NVIDIA Inception</a> program, described a new generation of smart meters using the <a target="_blank" href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/">NVIDIA Jetson</a> platform that utilities will deploy to process and analyze real-time grid data using AI models at the edge. Deloitte <a target="_blank" href="https://www.prnewswire.com/news-releases/deloitte-and-utilidata-establish-first-of-its-kind-collaboration-to-revolutionize-the-us-power-grid-for-a-sustainable-future-302175120.html">announced</a> today its support for the effort.</p>
<p>Siemens Energy detailed in <a target="_blank" href="https://www.nvidia.com/en-us/on-demand/session/gtc24-s62524/">a separate GTC session</a> its work with AI and <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> creating digital twins of transformers in substations to improve predictive maintenance, boosting grid resilience. And <a target="_blank" href="https://resources.nvidia.com/en-us-energy-utilities/maximize-wind-energy">a video</a> reports on how Siemens Gamesa used Omniverse and accelerated computing to optimize turbine placements for a large wind farm.</p>
<p>“Deploying AI and advanced computing technologies developed by NVIDIA enables faster and better grid modernization and we, in turn, can deliver for our customers,” said Maria Pope, CEO of Portland General Electric in Oregon.</p>
<h2><b>NVIDIA Delivers 45,000x Gain in Energy Efficiency</b></h2>
<p>The advances come as NVIDIA drives down the costs and energy needed to deploy AI.</p>
<p>Over the last eight years, NVIDIA increased <a target="_blank" href="https://www.nvidia.com/en-us/glossary/energy-efficiency/">energy efficiency</a> of running AI inference on state-of-the-art large language models a whopping 45,000x, Huang said in his recent <a target="_blank" href="https://www.youtube.com/watch?v=pKXDVsWZmUU">keynote</a> at COMPUTEX.</p>
<p><a target="_blank" href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/">NVIDIA Blackwell architecture GPUs</a> will provide 20x greater energy efficiency than CPUs for AI and <a target="_blank" href="https://www.nvidia.com/en-us/glossary/high-performance-computing/">high-performance computing</a>. If all CPU servers for these jobs transitioned to GPUs, users would save 37 terawatt-hours a year, the equivalent of 25 million metric tons of carbon dioxide and the electricity use of 5 million homes.</p>
<p>That’s why NVIDIA-powered systems swept the top six spots and took seven of the top 10 in <a href="https://blogs.nvidia.com/blog/green500-energy-efficient-supercomputers/">the latest ranking</a> of the <a target="_blank" href="https://top500.org/lists/green500/2024/06/">Green500</a>, a list of the world’s most energy-efficient supercomputers.</p>
<p>In addition, <a href="https://blogs.nvidia.com/blog/ai-energy-study/">a recent report</a> calls for governments to accelerate adoption of AI as a significant new tool to drive energy efficiency across many industries. It cited examples of utilities adopting AI to make the electric grid more efficient.</p>
<p>Learn more about how <a target="_blank" href="https://www.nvidia.com/en-us/industries/energy/power-utilities/">utilities are deploying AI</a> and accelerated computing to improve operations, saving cost and energy.</p>
<p>Watch the full conversation with Huang in <a target="_blank" href="https://eei.app.box.com/s/i6tuw33ji6uflg7ye4qo757ntj1vywyu">this video</a> starting at the three-minute mark.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/Final-Edison-fireside-chat-with-Jensen-Huang.jpg"
			type="image/jpeg"
			width="1461"
			height="779"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/Final-Edison-fireside-chat-with-Jensen-Huang-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Light Bulb Moment: NVIDIA CEO Sees Bright Future for AI-Powered Electric Grid]]></media:title>
			<media:description type="html">Jensen Huang in fireside chat at the Edison Electric Institute</media:description>
			</media:content>
			</item>
		<item>
		<title>Seamless in Seattle: NVIDIA Research Showcases Advancements in Visual Generative AI at CVPR</title>
		<link>https://blogs.nvidia.com/blog/visual-generative-ai-cvpr-research/</link>
		
		<dc:creator><![CDATA[Isha Salian]]></dc:creator>
		<pubDate>Mon, 17 Jun 2024 13:00:36 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Research]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[NVIDIA Research]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Synthetic Data Generation]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72167</guid>

					<description><![CDATA[NVIDIA researchers are at the forefront of the rapidly advancing field of visual generative AI, developing new techniques to create and interpret images, videos and 3D environments. More than 50 of these projects will be showcased at the Computer Vision and Pattern Recognition (CVPR) conference, taking place June 17-21 in Seattle. Two of the papers	<a class="read-more" href="https://blogs.nvidia.com/blog/visual-generative-ai-cvpr-research/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>NVIDIA researchers are at the forefront of the rapidly advancing field of visual generative AI, developing new techniques to create and interpret images, videos and 3D environments.</p>
<p>More than 50 of these projects will be showcased at the <a target="_blank" href="https://www.nvidia.com/en-us/events/cvpr/">Computer Vision and Pattern Recognition (CVPR)</a> conference, taking place June 17-21 in Seattle. Two of the papers — one on the <a target="_blank" href="https://github.com/NVlabs/edm2">training dynamics of diffusion models</a> and another on <a target="_blank" href="https://arxiv.org/abs/2403.16439">high-definition maps for autonomous vehicles</a> — are finalists for CVPR’s Best Paper Awards.</p>
<p>NVIDIA is also the <a href="https://blogs.nvidia.com/blog/auto-research-cvpr-2024/">winner of the CVPR Autonomous Grand Challenge’s End-to-End Driving at Scale</a> track — a significant milestone that demonstrates the company’s use of generative AI for comprehensive self-driving models. The winning submission, which outperformed more than 450 entries worldwide, also received CVPR’s Innovation Award.</p>
<p>NVIDIA’s research at CVPR includes a text-to-image model that can be easily customized to depict a specific object or character, a new model for object pose estimation, a technique to edit neural radiance fields (<a href="https://blogs.nvidia.com/blog/ai-decoded-instant-nerf/">NeRFs</a>) and a visual language model that can understand memes. Additional papers introduce domain-specific innovations for industries including automotive, healthcare and robotics.</p>
<p>Collectively, the work introduces powerful AI models that could enable creators to more quickly bring their artistic visions to life, accelerate the training of autonomous robots for manufacturing, and support healthcare professionals by helping process radiology reports.</p>
<p>“Artificial intelligence, and generative AI in particular, represents a pivotal technological advancement,” said Jan Kautz, vice president of learning and perception research at NVIDIA. “At CVPR, NVIDIA Research is sharing how we’re pushing the boundaries of what’s possible — from powerful image generation models that could supercharge professional creators to autonomous driving software that could help enable next-generation self-driving cars.”</p>
<p>At CVPR, NVIDIA also announced <a target="_blank" href="https://nvidianews.nvidia.com/news/omniverse-microservices-physical-ai">NVIDIA Omniverse Cloud Sensor RTX</a>, a set of microservices that enable physically accurate sensor simulation to accelerate the development of fully autonomous machines of every kind.</p>
<h2><b>Forget Fine-Tuning: JeDi Simplifies Custom Image Generation</b></h2>
<p>Creators harnessing diffusion models, the most popular method for generating images based on text prompts, often have a specific character or object in mind — they may, for example, be developing a storyboard around an animated mouse or brainstorming an ad campaign for a specific toy.</p>
<p>Prior research has enabled these creators to personalize the output of diffusion models to focus on a specific subject using fine-tuning — where a user trains the model on a custom dataset — but the process can be time-consuming and inaccessible for general users.</p>
<p><a target="_blank" href="https://research.nvidia.com/labs/dir/jedi/">JeDi</a>, a paper by researchers from Johns Hopkins University, Toyota Technological Institute at Chicago and NVIDIA, proposes a new technique that allows users to easily personalize the output of a diffusion model within a couple of seconds using reference images. The team found that the model achieves state-of-the-art quality, significantly outperforming existing fine-tuning-based and fine-tuning-free methods.</p>
<p>JeDi can also be combined with <a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">retrieval-augmented generation</a>, or RAG, to generate visuals specific to a database, such as a brand’s product catalog.</p>
<div style="width: 1452px;" class="wp-video"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video class="wp-video-shortcode" id="video-72167-1" width="1452" height="680" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/JeDi-cow-sculpture.mp4?_=1" /><a href="https://blogs.nvidia.com/wp-content/uploads/2024/06/JeDi-cow-sculpture.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/06/JeDi-cow-sculpture.mp4</a></video></div>
<p>&nbsp;</p>
<h2><b>New Foundation Model Perfects the Pose</b></h2>
<p>NVIDIA researchers at CVPR are also presenting <a target="_blank" href="https://nvlabs.github.io/FoundationPose/">FoundationPose</a>, a <a href="https://blogs.nvidia.com/blog/what-are-foundation-models/">foundation model</a> for object pose estimation and tracking that can be instantly applied to new objects during inference, without the need for fine-tuning.</p>
<p>The model, which <a target="_blank" href="https://bop.felk.cvut.cz/leaderboards/pose-estimation-unseen-bop23/core-datasets/">set a new record</a> on a popular benchmark for object pose estimation, uses either a small set of reference images or a 3D representation of an object to understand its shape. It can then identify and track how that object moves and rotates in 3D across a video, even in poor lighting conditions or complex scenes with visual obstructions.</p>
<p>FoundationPose could be used in industrial applications to help autonomous robots identify and track the objects they interact with. It could also be used in augmented reality applications where an AI model is used to overlay visuals on a live scene.</p>
<p><iframe loading="lazy" title="ar maze" width="500" height="281" src="https://www.youtube.com/embed/IvXSd4r12G8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>NeRFDeformer Transforms 3D Scenes With a Single Snapshot</b></h2>
<p>A NeRF is an AI model that can render a 3D scene based on a series of 2D images taken from different positions in the environment. In fields like robotics, NeRFs can be used to generate immersive 3D renders of complex real-world scenes, such as a cluttered room or a construction site. However, to make any changes, developers would need to manually define how the scene has transformed — or remake the NeRF entirely.</p>
<p>Researchers from the University of Illinois Urbana-Champaign and NVIDIA have simplified the process with NeRFDeformer. The method, being presented at CVPR, can successfully transform an existing NeRF using a single RGB-D image, which is a combination of a normal photo and a depth map that captures how far each object in a scene is from the camera.</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-72222" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/Screenshot-2024-06-05-at-5.05.51-PM.png" alt="" width="1316" height="794" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/Screenshot-2024-06-05-at-5.05.51-PM.png 1316w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Screenshot-2024-06-05-at-5.05.51-PM-400x241.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Screenshot-2024-06-05-at-5.05.51-PM-672x405.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Screenshot-2024-06-05-at-5.05.51-PM-768x463.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Screenshot-2024-06-05-at-5.05.51-PM-746x450.png 746w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Screenshot-2024-06-05-at-5.05.51-PM-356x215.png 356w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Screenshot-2024-06-05-at-5.05.51-PM-166x100.png 166w, https://blogs.nvidia.com/wp-content/uploads/2024/06/Screenshot-2024-06-05-at-5.05.51-PM-1280x772.png 1280w" sizes="(max-width: 1316px) 100vw, 1316px" /></p>
<h2><b>VILA Visual Language Model Gets the Picture</b></h2>
<p>A CVPR research collaboration between NVIDIA and the Massachusetts Institute of Technology is advancing the state of the art for vision language models, which are generative AI models that can process videos, images and text.</p>
<p>The group developed <a target="_blank" href="https://arxiv.org/abs/2312.07533">VILA</a>, a family of open-source visual language models that outperforms prior neural networks <a target="_blank" href="https://mmmu-benchmark.github.io/#leaderboard">on key benchmarks</a> that test how well AI models answer questions about images. VILA’s unique pretraining process unlocked new model capabilities, including enhanced world knowledge, stronger in-context learning and the ability to reason across multiple images.</p>
<figure id="attachment_72225" aria-describedby="caption-attachment-72225" style="width: 1999px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-full wp-image-72225" src="https://blogs.nvidia.com/wp-content/uploads/2024/06/VILA.png" alt="figure showing how VILA can reason based on multiple images" width="1999" height="809" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/06/VILA.png 1999w, https://blogs.nvidia.com/wp-content/uploads/2024/06/VILA-400x162.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/06/VILA-672x272.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/06/VILA-768x311.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/06/VILA-1536x622.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/06/VILA-842x341.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/06/VILA-406x164.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/06/VILA-188x76.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/06/VILA-1280x518.png 1280w" sizes="(max-width: 1999px) 100vw, 1999px" /><figcaption id="caption-attachment-72225" class="wp-caption-text">VILA can understand memes and reason based on multiple images or video frames.</figcaption></figure>
<p>The VILA model family can be optimized for inference using the <a target="_blank" href="https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/">NVIDIA TensorRT-LLM</a> open-source library and can be deployed on NVIDIA GPUs in data centers, workstations and even <a target="_blank" href="https://developer.nvidia.com/blog/visual-language-intelligence-and-edge-ai-2-0/">edge devices</a>.</p>
<p>Read more about VILA on the <a target="_blank" href="https://developer.nvidia.com/blog/visual-language-models-on-nvidia-hardware-with-vila/">NVIDIA Technical Blog</a> and <a target="_blank" href="https://github.com/NVlabs/VILA">GitHub</a>.</p>
<h2><b>Generative AI Fuels Autonomous Driving, Smart City Research</b></h2>
<p>A dozen of the NVIDIA-authored CVPR papers focus on autonomous vehicle research. Other AV-related highlights include:</p>
<ul>
<li style="font-weight: 300;" aria-level="1"><a target="_blank" href="http://research.nvidia.com/labs/av-applied-research">NVIDIA’s AV applied research</a>, which <a href="https://blogs.nvidia.com/blog/auto-research-cvpr-2024/">won the CVPR Autonomous Grand Challenge</a>, is featured in <a target="_blank" href="https://www.youtube.com/watch?v=wfpLLSz5iWY">this demo</a>.</li>
<li style="font-weight: 300;" aria-level="1"><a target="_blank" href="https://research.nvidia.com/labs/toronto-ai/">Sanja Fidler</a>, vice president of AI research at NVIDIA, will present on vision language models at the <a target="_blank" href="https://cvpr2024.wad.vision/">Workshop on Autonomous Driving</a> on June 17.</li>
<li style="font-weight: 300;" aria-level="1"><a target="_blank" href="https://arxiv.org/abs/2403.16439">Producing and Leveraging Online Map Uncertainty in Trajectory Prediction</a>, a paper authored by researchers from the University of Toronto and NVIDIA, has been selected as one of 24 finalists for CVPR’s best paper award.</li>
</ul>
<p>Also at CVPR, NVIDIA contributed the largest ever indoor synthetic dataset to the <a href="https://blogs.nvidia.com/blog/ai-city-challenge-omniverse-cvpr/">AI City Challenge</a>, helping researchers and developers advance the development of solutions for smart cities and industrial automation. The challenge’s datasets were generated using <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a>, a platform of APIs, SDKs and services that enable developers to build <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/usd/">Universal Scene Description (OpenUSD)</a>-based applications and workflows.</p>
<p><a target="_blank" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a> has hundreds of scientists and engineers worldwide, with teams focused on topics including AI, computer graphics, computer vision, self-driving cars and robotics. Learn more about <a target="_blank" href="https://www.nvidia.com/en-us/events/cvpr/">NVIDIA Research at CVPR</a>.</p>
]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2024/06/JeDi-cow-sculpture.mp4" length="4129498" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/cat-astronaut.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/cat-astronaut-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Seamless in Seattle: NVIDIA Research Showcases Advancements in Visual Generative AI at CVPR]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Research Wins CVPR Autonomous Grand Challenge for End-to-End Driving</title>
		<link>https://blogs.nvidia.com/blog/auto-research-cvpr-2024/</link>
		
		<dc:creator><![CDATA[Danny Shapiro]]></dc:creator>
		<pubDate>Mon, 17 Jun 2024 13:00:10 +0000</pubDate>
				<category><![CDATA[Driving]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Robotics]]></category>
		<category><![CDATA[NVIDIA DGX]]></category>
		<category><![CDATA[NVIDIA DRIVE]]></category>
		<category><![CDATA[NVIDIA Research]]></category>
		<category><![CDATA[Omniverse]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=72147</guid>

					<description><![CDATA[Making moves to accelerate self-driving car development, NVIDIA was today named an Autonomous Grand Challenge winner at the Computer Vision and Pattern Recognition (CVPR) conference, running this week in Seattle. Building on last year’s win in 3D Occupancy Prediction, NVIDIA Research topped the leaderboard this year in the End-to-End Driving at Scale category with its	<a class="read-more" href="https://blogs.nvidia.com/blog/auto-research-cvpr-2024/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Making moves to accelerate self-driving car development, NVIDIA was today named an Autonomous Grand Challenge winner at the <a href="https://www.nvidia.com/en-us/events/cvpr/" target="_blank" rel="noopener">Computer Vision and Pattern Recognition</a> (CVPR) conference, running this week in Seattle.</p>
<p>Building on last year’s win in <a href="https://blogs.nvidia.com/blog/autonomous-driving-challenge-cvpr/" target="_blank" rel="noopener">3D Occupancy Prediction</a>, <a href="https://www.nvidia.com/en-us/research/" target="_blank" rel="noopener">NVIDIA Research</a> topped the leaderboard this year in the <a href="https://opendrivelab.com/challenge2024/#end_to_end_driving_at_scale" target="_blank" rel="noopener">End-to-End Driving at Scale category</a> with its Hydra-MDP model, outperforming more than 400 entries worldwide.</p>
<p>This milestone shows the importance of <a href="https://www.nvidia.com/en-us/glossary/generative-ai/" target="_blank" rel="noopener">generative AI</a> in building applications for physical AI deployments in autonomous vehicle (AV) development. The technology can also be applied to industrial environments, healthcare, robotics and other areas.</p>
<p>The winning submission received CVPR’s Innovation Award as well, recognizing NVIDIA’s approach to improving “any end-to-end driving model using learned open-loop proxy metrics.”</p>
<p>In addition, NVIDIA announced <a href="https://nvidianews.nvidia.com/news/omniverse-microservices-physical-ai" target="_blank" rel="noopener">NVIDIA Omniverse Cloud Sensor RTX</a>, a set of microservices that enable physically accurate sensor simulation to accelerate the development of fully autonomous machines of every kind.</p>
<h2><b>How End-to-End Driving Works</b></h2>
<p>The race to develop self-driving cars isn’t a sprint but more a never-ending triathlon, with three distinct yet crucial parts operating simultaneously: AI training, simulation and autonomous driving. Each requires its own accelerated computing platform, and together, the full-stack systems purpose-built for these steps form a powerful triad that enables continuous development cycles, always improving in performance and safety.</p>
<p>To accomplish this, a model is first trained on an AI supercomputer such as <a href="https://www.nvidia.com/en-us/data-center/dgx-platform/" target="_blank" rel="noopener">NVIDIA DGX</a>. It’s then tested and validated in simulation — using the <a href="https://www.nvidia.com/en-us/omniverse/" target="_blank" rel="noopener">NVIDIA Omniverse</a> platform and running on an <a target="_blank" href="https://www.nvidia.com/en-us/data-center/products/ovx/">NVIDIA OVX</a> system — before entering the vehicle, where, lastly, the <a href="https://www.nvidia.com/en-us/deep-learning-ai/products/agx-systems/" target="_blank" rel="noopener">NVIDIA DRIVE AGX</a> platform processes sensor data through the model in real time.</p>
<p>Building an autonomous system to navigate safely in the complex physical world is extremely challenging. The system needs to perceive and understand its surrounding environment holistically, then make correct, safe decisions in a fraction of a second. This requires human-like situational awareness to handle potentially dangerous or rare scenarios.</p>
<p>AV software development has traditionally been based on a modular approach, with separate components for object detection and tracking, trajectory prediction, and path planning and control.</p>
<p>End-to-end autonomous driving systems streamline this process using a unified model to take in sensor input and produce vehicle trajectories, helping avoid overcomplicated pipelines and providing a more holistic, data-driven approach to handle real-world scenarios.</p>
<p>Watch a video about the Hydra-MDP model, winner of the CVPR Autonomous Grand Challenge for End-to-End Driving:</p>
<p><iframe loading="lazy" title="NVIDIA Research Wins CVPR Autonomous Grand Challenge for End-to-End Driving" width="500" height="281" src="https://www.youtube.com/embed/wfpLLSz5iWY?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Navigating the Grand Challenge </b></h2>
<p>This year’s CVPR challenge asked participants to develop an end-to-end AV model, trained using the nuPlan dataset, to generate driving trajectory based on sensor data.</p>
<p>The models were submitted for testing inside the open-source NAVSIM simulator and were tasked with navigating thousands of scenarios they hadn’t experienced yet. Model performance was scored based on metrics for safety, passenger comfort and deviation from the original recorded trajectory.</p>
<p>NVIDIA Research’s winning end-to-end model ingests camera and lidar data, as well as the vehicle’s trajectory history, to generate a safe, optimal vehicle path for five seconds post-sensor input.</p>
<p>The workflow NVIDIA researchers used to win the competition can be replicated in high-fidelity simulated environments with NVIDIA Omniverse. This means AV simulation developers can recreate the workflow in a physically accurate environment before testing their AVs in the real world. NVIDIA Omniverse Cloud Sensor RTX microservices will be available later this year. <a href="https://developer.nvidia.com/omniverse/join" target="_blank" rel="noopener">Sign up</a> for early access.</p>
<p>In addition, NVIDIA ranked second for its submission to the CVPR Autonomous Grand Challenge for <a href="https://opendrivelab.com/challenge2024/#driving_with_language" target="_blank" rel="noopener">Driving with Language</a>. NVIDIA’s approach connects vision language models and autonomous driving systems, integrating the power of <a href="https://www.nvidia.com/en-us/glossary/large-language-models/" target="_blank" rel="noopener">large language models</a> to help make decisions and achieve generalizable, explainable driving behavior.</p>
<h2><b>Learn More at CVPR </b></h2>
<p>More than <a href="https://www.nvidia.com/en-us/events/cvpr/" target="_blank" rel="noopener">50 NVIDIA papers</a> were accepted to this year’s CVPR, on topics spanning automotive, healthcare, robotics and more. Over a dozen papers will cover NVIDIA automotive-related research, including:</p>
<ul>
<li><a href="https://arxiv.org/html/2406.06978v1" target="_blank" rel="noopener">Hydra-MDP: End-to-End Multimodal Planning With Multi-Target Hydra-Distillation</a>
<ul>
<li><a target="_blank" href="https://arxiv.org/html/2406.06978v1">Winner of CVPR’s </a><a target="_blank" href="https://opendrivelab.com/challenge2024/#end_to_end_driving_at_scale">End-to-End Driving at Scale challenge</a></li>
<li>Read the <a href="https://developer.nvidia.com/blog/end-to-end-driving-at-scale-with-hydra-mdp/" target="_blank" rel="noopener">NVIDIA technical blog</a></li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/2403.16439" target="_blank" rel="noopener">Producing and Leveraging Online Map Uncertainty in Trajectory Prediction</a>
<ul>
<li>CVPR best paper award finalist</li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/2402.05932" target="_blank" rel="noopener">Driving Everywhere With Large Language Model Policy Adaptation</a>
<ul>
<li>See <a href="https://www.youtube.com/watch?v=fQ3HJbEkP4U" target="_blank" rel="noopener">DRIVE Labs: LLM-Based Road Rules Guide Simplifies Driving</a></li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/2312.03031" target="_blank" rel="noopener">Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?</a></li>
<li><a href="https://arxiv.org/abs/2403.09230" target="_blank" rel="noopener">Improving Distant 3D Object Detection Using 2D Box Supervision</a></li>
<li><a href="https://arxiv.org/abs/2312.05247" target="_blank" rel="noopener">Dynamic LiDAR Resimulation Using Compositional Neural Fields</a></li>
<li><a href="https://arxiv.org/abs/2312.01696" target="_blank" rel="noopener">BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection</a></li>
<li><a href="https://xinshuoweng.github.io/paradrive/" target="_blank" rel="noopener">PARA-Drive: Parallelized Architecture for Real-Time Autonomous Driving</a></li>
</ul>
<p>Sanja Fidler, vice president of AI research at NVIDIA, will speak on vision language models at the CVPR <a href="https://cvpr2024.wad.vision" target="_blank" rel="noopener">Workshop on Autonomous Driving</a>.</p>
<p><i>Learn more about </i><a href="https://www.nvidia.com/en-us/research/" target="_blank" rel="noopener"><i>NVIDIA Research</i></a><i>, a global team of hundreds of scientists and engineers focused on topics including AI, computer graphics, computer vision, self-driving cars and robotics.</i></p>
<p><em>See </em><a target="_blank" href="https://www.nvidia.com/en-us/about-nvidia/legal-info/"><em>notice</em></a><em> regarding software product information.</em></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/cvpr-auto.gif"
			type="image/gif"
			width="1200"
			height="700"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/06/cvpr-auto-842x450.gif"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Research Wins CVPR Autonomous Grand Challenge for End-to-End Driving]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
	</channel>
</rss>
