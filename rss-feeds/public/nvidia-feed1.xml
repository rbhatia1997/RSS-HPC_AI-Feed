<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:media="http://search.yahoo.com/mrss/">

<channel>
	<title>NVIDIA Blog</title>
	<atom:link href="https://blogs.nvidia.com/feed/" rel="self" type="application/rss+xml" />
	<link>https://blogs.nvidia.com/</link>
	<description></description>
	<lastBuildDate>Wed, 07 Aug 2024 00:01:55 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.5.5</generator>
	<item>
		<title>Meet the Maker: High School Student Develops Robot Guide Dogs With NVIDIA Jetson</title>
		<link>https://blogs.nvidia.com/blog/selin-ornek-robot-guide-dogs/</link>
		
		<dc:creator><![CDATA[Andy Bui]]></dc:creator>
		<pubDate>Tue, 06 Aug 2024 16:00:30 +0000</pubDate>
				<category><![CDATA[Inner Geek]]></category>
		<category><![CDATA[Robotics]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Deep Learning Institute]]></category>
		<category><![CDATA[Education]]></category>
		<category><![CDATA[NVIDIA Jetson]]></category>
		<category><![CDATA[Social Impact]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73518</guid>

					<description><![CDATA[High school student Selin Alara Ornek is looking ahead ‚Äî using machine learning and the NVIDIA Jetson platform for edge AI and robotics to create robot guide dogs for the visually impaired. The project, called IC4U, is one of seven robots Ornek has created to date, including a school aid robot, named BB4All, that can	<a class="read-more" href="https://blogs.nvidia.com/blog/selin-ornek-robot-guide-dogs/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>High school student Selin Alara Ornek is looking ahead ‚Äî using machine learning and the <a target="_blank" href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/">NVIDIA Jetson</a> platform for edge AI and robotics to create robot guide dogs for the visually impaired.</p>
<p>The project, called <a target="_blank" href="https://selinoid.com/projects/ic4uver1/">IC4U</a>, is one of seven robots Ornek has created to date, including a school aid robot, named <a target="_blank" href="https://selinoid.com/projects/bb4all/">BB4All</a>, that can help prevent bullying with real-time notification and health-monitoring capabilities.</p>
<h2><b>About the Maker</b></h2>
<p>A high school senior from Istanbul, Turkey, Ornek has always had a passion for the intersection of AI, social good and robotics. She‚Äôs a self-taught robotics developer ‚Äî in building IC4U, she used the Jetson Developer Kit as a sandbox to explore and experiment.</p>
<p>She is a member of <a target="_blank" href="https://ai-4-all.org/">AI4ALL</a>, a nonprofit program with the mission to make AI more diverse and inclusive, and the New York Academy of Science. A global presence in the robotics scene, she‚Äôs been recognized at the European Youth Awards and Women in Tech Global Awards events. She placed first in the 2021 Istanbul Bosphorus Robot Cup and third at the 2023 OpenCV AI Competition.</p>
<h2><b>Her Inspiration</b></h2>
<p>Ornek‚Äôs inspiration for creating IC4U came from a trip to France, where she saw a guide dog assisting its owner. Her late dog, Korsan, was also a key source of inspiration.</p>
<p>‚ÄúI started to think about if a visually impaired person lost their dog, not only would they lose their best friend, but their eyes,‚Äù Ornek said.</p>
<p><img fetchpriority="high" decoding="async" class="alignnone wp-image-73519 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/08/selinornekproject-401x500.jpg" alt="" width="401" height="500" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/08/selinornekproject-401x500.jpg 401w, https://blogs.nvidia.com/wp-content/uploads/2024/08/selinornekproject-321x400.jpg 321w, https://blogs.nvidia.com/wp-content/uploads/2024/08/selinornekproject-768x957.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/08/selinornekproject-1233x1536.jpg 1233w, https://blogs.nvidia.com/wp-content/uploads/2024/08/selinornekproject-361x450.jpg 361w, https://blogs.nvidia.com/wp-content/uploads/2024/08/selinornekproject-173x215.jpg 173w, https://blogs.nvidia.com/wp-content/uploads/2024/08/selinornekproject-80x100.jpg 80w, https://blogs.nvidia.com/wp-content/uploads/2024/08/selinornekproject-1280x1594.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/08/selinornekproject.jpg 1605w" sizes="(max-width: 401px) 100vw, 401px" /></p>
<p>The project was built to offer the visually impaired a companion not limited by aging and health.</p>
<h2><b>Her Jetson Project</b></h2>
<p>Ornek initially used ultrasonic sensors located in IC4U‚Äôs eyes to detect obstacles. But after attending the 2021 World Summit AI as a panelist, she decided to develop new AI applications for the robot dog that‚Äôd enable it to mimic a real one.</p>
<p>The ultrasonic sensors only offered object detection from directly in front of IC4U, and Ornek wanted to expand detection to the robot‚Äôs entire surroundings.</p>
<p>The solution was using sound sensors located in the robot‚Äôs ears. IC4U can turn toward a sound and process visual information gathered by an integrated ZED 2i Wide-Angle 3D AI camera, which captures a wider range of visual data and helps detect information such as the size and speed of an object.</p>
<p><iframe title="IC4U3 Shorts" width="500" height="281" src="https://www.youtube.com/embed/videoseries?list=PL4b78FeKKZAjzz9D5VbUhbeUwWonnAsK0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>‚ÄúTo power the ZED 2i camera and for high-quality image processing, I used an <a target="_blank" href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-nano/product-development/">NVIDIA Jetson Nano</a> developer kit,‚Äù Ornek said. ‚ÄúI was so impressed with the ZED 2i camera‚Äôs performance that I didn‚Äôt want to limit its use to a simple object-recognition task.‚Äù</p>
<p>She began to think of other ways that IC4U could assist a visually impaired person. IC4U‚Äôs improved data processing from high-resolution sensors, powered by Jetson, enables it to detect city objects such as stop signs, traffic light colors and the denomination of paper money.</p>
<p><iframe title="iC4U3 Shorts: Demo 10 Traffic Lights Color Detection" width="500" height="281" src="https://www.youtube.com/embed/0KgxXRvYVTQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>In addition, Ornek used the Jetson Nano to add a shopping feature to IC4U via web scraping from publicly available resources, aiming to one day expand it by partnering with online retail stores.</p>
<h2><b>Back to School</b></h2>
<p>In the long run, Ornek hopes to deploy IC4U for use in smart cities and spaces ‚Äî continuing her exploration of AI applications with next-generation platforms like Jetson Orin.</p>
<p>This fall, she‚Äôll begin studying computer science at the University of British Columbia on a full scholarship, as a recipient of the Karen McKellin International Leader of Tomorrow Award. She strives to encourage other youth, especially girls, that technology is fun.</p>
<p>Students and educators with a valid accredited university or education-related email address can sign up to purchase the Jetson Orin Nano or Jetson AGX Orin Developer Kit at a discounted rate. U.S.-based students and educators can visit <a target="_blank" href="https://www.sparkfun.com/news/11197">Sparkfun</a> to sign up for their discount ‚Äî residents of other countries should <a target="_blank" href="https://store.nvidia.com/jetson/edu/">check</a> their eligibility (login required).</p>
<p><i>Learn more about the </i><a target="_blank" href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/"><i>NVIDIA Jetson platform</i></a><i> and </i><a target="_blank" href="https://developer.nvidia.com/embedded/learn/jetson-ai-certification-programs"><i>NVIDIA Deep Learning Institute Jetson AI courses and certifications</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1.jpg"
			type="image/jpeg"
			width="1999"
			height="1252"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Meet the Maker: High School Student Develops Robot Guide Dogs With NVIDIA Jetson]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Editor‚Äôs Paradise: NVIDIA RTX-Powered Video Software CyberLink PowerDirector Gains High-Efficiency Video Coding Upgrades</title>
		<link>https://blogs.nvidia.com/blog/studio-hevc-rtx-ai-august-driver/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Tue, 06 Aug 2024 14:45:48 +0000</pubDate>
				<category><![CDATA[Pro Graphics]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[In the NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[NVIDIA Studio]]></category>
		<category><![CDATA[NVIDIA Studio Driver]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Rendering]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73484</guid>

					<description><![CDATA[RTX-powered video editing app CyberLink PowerDirector now has a setting for high-efficiency video encoding (HEVC).]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><i>Editor‚Äôs note: This post is part of our </i><a href="https://blogs.nvidia.com/blog/tag/in-the-nvidia-studio/"><i>In the NVIDIA Studio</i></a><i> series, which celebrates featured artists, offers creative tips and tricks, and demonstrates how </i><a target="_blank" href="https://www.nvidia.com/en-us/studio/"><i>NVIDIA Studio</i></a><i> technology improves creative workflows. We‚Äôre also deep diving on new </i><a target="_blank" href="https://www.nvidia.com/en-us/geforce/rtx/"><i>GeForce RTX GPU</i></a><i> features, technologies and resources, and how they dramatically accelerate content creation.</i></p>
<p>Every month brings new creative app updates and optimizations powered by the <a target="_blank" href="https://www.nvidia.com/en-us/studio/">NVIDIA Studio</a> platform ‚Äî supercharging creative processes with <a target="_blank" href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX</a> and AI.</p>
<p>RTX-powered video editing app CyberLink PowerDirector now has a setting for high-efficiency video encoding (HEVC). 3D artists can access new features and faster workflows in Adobe Substance 3D Modeler and SideFX: Houdini. And content creators using Topaz Video AI Pro can now scale their photo and video touchups faster with <a target="_blank" href="https://developer.nvidia.com/tensorrt-getting-started">NVIDIA TensorRT acceleration</a>.</p>
<p>The August Studio Driver is ready to install via the <a target="_blank" href="https://www.nvidia.com/en-us/software/nvidia-app/">NVIDIA app</a> beta ‚Äî the essential companion for creators and gamers ‚Äî to keep <a target="_blank" href="https://www.nvidia.com/en-us/geforce/rtx/">GeForce RTX</a> PCs up to date with the latest NVIDIA drivers and technology.</p>
<p>And this week‚Äôs featured<i> In the NVIDIA Studio </i>artist Stavros Liaskos is creating physically accurate 3D digital replicas of Greek Orthodox churches, holy temples, monasteries and other buildings using the <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> platform for building and connecting Universal Scene Description (OpenUSD) apps.</p>
<p><i>Discover the latest breakthroughs in graphics and generative AI by watching the replay of NVIDIA founder and CEO Jensen Huang‚Äôs firechat chats with </i><a target="_blank" href="https://www.youtube.com/watch?v=H0WxJ7caZQU"><i>Lauren Goode, senior writer at WIRED</i></a><i>, and </i><a target="_blank" href="https://youtu.be/w-cmMcMZoZ4"><i>Meta founder and CEO Mark Zuckerberg</i></a><i> at SIGGRAPH.¬†</i></p>
<p><iframe loading="lazy" title="AI and The Next Computing Platforms With Jensen Huang and Mark Zuckerberg" width="500" height="281" src="https://www.youtube.com/embed/w-cmMcMZoZ4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>There‚Äôs a Creative App for That</b></h2>
<p>The <a target="_blank" href="https://developer.nvidia.com/video-codec-sdk">NVIDIA NVENC</a> video encoder is built into every RTX graphics card, offloading the compute-intensive task of video encoding from the CPU to a dedicated part of the GPU.</p>
<p>CyberLink PowerDirector, a popular video editing program that recently added support for <a href="https://blogs.nvidia.com/blog/rtx-ai-pc-studio-computex/">RTX Video HDR</a>, now has a setting to increase HEVC with NVIDIA NVENC HEVC Ultra-High-Quality mode.</p>
<p>The new functionality reduces bit rates and improves encoding efficiency by 10%, significantly boosting video quality. Using the custom setting, content creators can offer audiences superior viewing experiences.</p>
<figure id="attachment_73488" aria-describedby="caption-attachment-73488" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-cyberlink-powerdirector-nv-hevc-uhq-mode-1280w.png"><img loading="lazy" decoding="async" class="wp-image-73488 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-cyberlink-powerdirector-nv-hevc-uhq-mode-1280w-672x362.png" alt="" width="672" height="362" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-cyberlink-powerdirector-nv-hevc-uhq-mode-1280w-672x362.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-cyberlink-powerdirector-nv-hevc-uhq-mode-1280w-400x216.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-cyberlink-powerdirector-nv-hevc-uhq-mode-1280w-768x414.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-cyberlink-powerdirector-nv-hevc-uhq-mode-1280w-835x450.png 835w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-cyberlink-powerdirector-nv-hevc-uhq-mode-1280w-399x215.png 399w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-cyberlink-powerdirector-nv-hevc-uhq-mode-1280w-186x100.png 186w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-cyberlink-powerdirector-nv-hevc-uhq-mode-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73488" class="wp-caption-text">Encoding efficiency jumps by 55% with just a few clicks.</figcaption></figure>
<p>Alpha exporting allows users to add overlay effects to videos by exporting HEVC video with an alpha channel. This technique can be used to create transparent backgrounds and rapidly process animated overlays, making it ideal for creating social media content.</p>
<p>With an alpha channel, users can export HEVC videos up to 8x faster compared with run-length encoding supported by other processors, and with a 100x reduction in file size.</p>
<p>Adobe Substance 3D Modeler, a multisurface 3D sculpting tool for artists, virtual effects specialists and designers, released Block to Stock, an AI-powered, geometry-based feature for accelerating the prototyping of complex shapes.</p>
<p>It allows rough 3D shapes to be quickly replaced with pre-existing, similarly shaped 3D models that have greater detail. The result is a highly detailed shape crafted in no time.</p>
<p>The recently released version 20.5 of SideFX: Houdini, a 3D procedural software for modeling, animation and lighting, introduced <a target="_blank" href="https://raytracing-docs.nvidia.com/optix8/index.html">NVIDIA OptiX 8</a> and NVIDIA‚Äôs <a target="_blank" href="https://developer.nvidia.com/blog/improve-shader-performance-and-in-game-frame-rates-with-shader-execution-reordering/">Shader Execution Reordering</a> feature to its Karma XPU renderer ‚Äî exclusively on NVIDIA RTX GPUs.</p>
<p>With these additions, computationally intensive tasks can now be executed up to 4x faster on RTX GPUs.</p>
<p>Topaz Video AI Pro, a photo and video enhancement software for noise reduction, sharpening and upscaling, added <a target="_blank" href="https://developer.nvidia.com/tensorrt-getting-started">TensorRT acceleration</a> for multi-GPU configurations, enabling parallelization across multiple GPUs for supercharged rendering speeds ‚Äî up to 2x faster with two GPUs over a single GPU system, with further acceleration in systems with additional GPUs.</p>
<p><iframe loading="lazy" title="Introduction to NVIDIA TensorRT for High Performance Deep Learning Inference" width="500" height="281" src="https://www.youtube.com/embed/rK-jxPPY9V4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Virtual Cultural Sites to G(r)eek Out About</b></h2>
<p>Anyone can now explore over 30 Greek cultural sites in virtual reality, thanks to the immersive work of Stavros Liaskos, managing director of visual communications company <a target="_blank" href="https://reyelise.co.uk/">Reyelise</a>.</p>
<p>‚ÄúMany historical and religious sites are at risk due to environmental conditions, neglect and socio-political issues,‚Äù he said. ‚ÄúBy creating detailed 3D replicas, we‚Äôre helping to ensure their architectural splendor is preserved digitally for future generations.‚Äù</p>
<p>Liaskos dedicated the project to his father, who passed away last year.</p>
<p>‚ÄúHe taught me the value of patience and instilled in me the belief that nothing is unattainable,‚Äù he said. ‚ÄúHis wisdom and guidance continue to inspire me every day.‚Äù</p>
<p>Churches are architecturally complex structures. To create physically accurate 3D models of them, Liaskos used the advanced real-time rendering capabilities of Omniverse, connected with a slew of content-creation apps.</p>
<p><iframe loading="lazy" title="Church of Saint Konstantinos(3D Video)" width="500" height="281" src="https://www.youtube.com/embed/eqTv9FTiIh8?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>The OpenUSD framework enabled a seamless workflow across the various apps Liaskos used. For example, after using Trimble X7 for highly accurate 3D scanning of structures, Liaskos easily moved to Autodesk 3ds Max and Blender for modeling and animation.</p>
<p>Then, with ZBrush, he sculpted intricate architectural details on the models and refined textures with Adobe Photoshop and Substance 3D. It was all brought together in Omniverse for real-time lighting and rendering.</p>
<figure id="attachment_73497" aria-describedby="caption-attachment-73497" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-panagia-xrysospiliotissa-interior-1280w.png"><img loading="lazy" decoding="async" class="wp-image-73497 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-panagia-xrysospiliotissa-interior-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-panagia-xrysospiliotissa-interior-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-panagia-xrysospiliotissa-interior-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-panagia-xrysospiliotissa-interior-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-panagia-xrysospiliotissa-interior-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-panagia-xrysospiliotissa-interior-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-panagia-xrysospiliotissa-interior-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-panagia-xrysospiliotissa-interior-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73497" class="wp-caption-text">Interior rendering of the Panagia Xrysospiliotissa Church in Nicosia, Cyprus.</figcaption></figure>
<p>For post-production work, like adding visual effects and compiling rendered scenes, Liaskos used OpenUSD to transfer his projects to Adobe After Effects, where he finalized the video output. Nearly every element of his creative workflow was accelerated by his <a target="_blank" href="https://www.nvidia.com/en-us/design-visualization/rtx-a4500/">NVIDIA RTX A4500 GPU.¬†</a></p>
<figure id="attachment_73500" aria-describedby="caption-attachment-73500" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-aghios-basileios-metsovou-interior-1280w.png"><img loading="lazy" decoding="async" class="wp-image-73500 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-aghios-basileios-metsovou-interior-1280w-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-aghios-basileios-metsovou-interior-1280w-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-aghios-basileios-metsovou-interior-1280w-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-aghios-basileios-metsovou-interior-1280w-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-aghios-basileios-metsovou-interior-1280w-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-aghios-basileios-metsovou-interior-1280w-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-aghios-basileios-metsovou-interior-1280w-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-aghios-basileios-metsovou-interior-1280w.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73500" class="wp-caption-text">Interior scene of the Church of Saint Basil on Metsovou Street in Athens.</figcaption></figure>
<p>Liaskos also explored developing <a href="https://blogs.nvidia.com/blog/what-is-extended-reality/">extended reality</a> (XR) applications that allow users to navigate his 3D projects in real time in virtual reality (VR).</p>
<div style="width: 1280px;" class="wp-video"><!--[if lt IE 9]><script>document.createElement('video');</script><![endif]-->
<video class="wp-video-shortcode" id="video-73484-1" width="1280" height="740" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-agios-eleftherios-2-1280w.mp4?_=1" /><a href="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-agios-eleftherios-2-1280w.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-agios-eleftherios-2-1280w.mp4</a></video></div>
<p>&nbsp;</p>
<p>First, he used laser scanning and <a href="https://blogs.nvidia.com/blog/what-is-photogrammetry/">photogrammetry</a> to capture the detailed geometries and textures of the churches.</p>
<div style="width: 1280px;" class="wp-video"><video class="wp-video-shortcode" id="video-73484-2" width="1280" height="740" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-agios-eleftherios-1-1280w.mp4?_=2" /><a href="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-agios-eleftherios-1-1280w.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-agios-eleftherios-1-1280w.mp4</a></video></div>
<p>&nbsp;</p>
<p>Then, he tapped Autodesk 3ds Max and Maxon ZBrush for retopology, ensuring the models were optimized for real-time rendering without compromising detail.</p>
<p>After importing them into NVIDIA Omniverse with OpenUSD, Liaskos packaged the XR scenes so they could be streamed to VR headsets¬† using either the <a target="_blank" href="https://docs.omniverse.nvidia.com/create-xr/latest/index.html">NVIDIA Omniverse Create XR</a> spatial computing app or Unity Engine, enabling immersive viewing experiences.</p>
<p>‚ÄúThis approach will even more strikingly showcase the architectural beauty and cultural significance of these sites,‚Äù Liaskos said. ‚ÄúThe simulation must be as good as possible to recreate the overwhelming, impactful feeling of calm and safety that comes with visiting a deeply spiritual space.‚Äù</p>
<figure id="attachment_73529" aria-describedby="caption-attachment-73529" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-73529" src="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-featured-setup-1280w-new_720-672x246.png" alt="" width="672" height="246" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-featured-setup-1280w-new_720-672x246.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-featured-setup-1280w-new_720-400x146.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-featured-setup-1280w-new_720-406x149.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-featured-setup-1280w-new_720-188x69.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-featured-setup-1280w-new_720.png 719w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73529" class="wp-caption-text">Creator Stavros Liaskos.</figcaption></figure>
<p><i>Follow NVIDIA Studio on </i><a target="_blank" href="https://www.instagram.com/nvidiastudio/"><i>Instagram</i></a><i>, </i><a target="_blank" href="https://twitter.com/NVIDIAStudio"><i>X</i></a><i> and </i><a target="_blank" href="https://www.facebook.com/NVIDIAStudio/"><i>Facebook</i></a><i>. Access tutorials on the </i><a target="_blank" href="https://www.youtube.com/channel/UCDeQdW6Lt6nhq3mLM4oLGWw"><i>Studio YouTube channel</i></a><i> and get updates directly in your inbox by subscribing to the </i><a target="_blank" href="https://www.nvidia.com/en-us/studio/?nvmid=subscribe-creators-mail-icon"><i>Studio newsletter</i></a><i>.¬†</i></p>
<p><i>Stay up to date on NVIDIA Omniverse with </i><a target="_blank" href="https://www.instagram.com/nvidiaomniverse/"><i>Instagram</i></a><i>, </i><a target="_blank" href="https://medium.com/@nvidiaomniverse"><i>Medium</i></a><i> and </i><a target="_blank" href="https://twitter.com/nvidiaomniverse"><i>X</i></a><i>. For more, join the </i><a target="_blank" href="https://www.nvidia.com/en-us/omniverse/community/"><i>Omniverse community</i></a><i> and check out the Omniverse </i><a target="_blank" href="https://forums.developer.nvidia.com/c/omniverse/300"><i>forums</i></a><i>, </i><a target="_blank" href="https://discord.com/invite/XWQNJDNuaC"><i>Discord server</i></a><i>, </i><a target="_blank" href="https://www.twitch.tv/nvidiaomniverse"><i>Twitch</i></a><i> and </i><a target="_blank" href="https://www.youtube.com/channel/UCSKUoczbGAcMld7HjpCR8OA"><i>YouTube</i></a><i> channels.¬†</i></p>
]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-agios-eleftherios-2-1280w.mp4" length="4759687" type="video/mp4" />
<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2024/08/studio-itns-stavros-liaskos-wk121-agios-eleftherios-1-1280w.mp4" length="4994434" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/08/greece-nv-blog-header-preview-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/08/greece-nv-blog-header-preview-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Editor‚Äôs Paradise: NVIDIA RTX-Powered Video Software CyberLink PowerDirector Gains High-Efficiency Video Coding Upgrades]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>August Adventures Await: 18 New Games Coming to GeForce NOW</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-new-games-august-2024/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 01 Aug 2024 13:00:27 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73464</guid>

					<description><![CDATA[Members can choose their own adventure with GeForce NOW bringing 18 new games to the cloud in August ‚Äî including Square Enix‚Äôs fantasy role-playing game Visions of Mana when it launches on PC Thursday, Aug. 29. From cozy games to thrilling battles, there‚Äôs something for everyone. Dive into the latest titles and experience powerful performance	<a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-new-games-august-2024/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Members can choose their own adventure with <a target="_blank" href="http://geforcenow.com">GeForce NOW</a> bringing 18 new games to the cloud in August ‚Äî including Square Enix‚Äôs fantasy role-playing game <i>Visions of Mana</i> when it launches on PC Thursday, Aug. 29.</p>
<p>From cozy games to thrilling battles, there‚Äôs something for everyone. Dive into the latest titles and experience powerful performance across all devices ‚Äî start with the six games available to stream this week.</p>
<p>Plus, the limited-time <a href="https://blogs.nvidia.com/blog/geforce-now-thursday-summer-sale-2024/">GeForce NOW Summer Sale</a> continues, offering a 50% discount on new one-month and six-month Ultimate and Priority memberships. Check it out before the deal ends on Sunday, Aug. 18.</p>
<h2><b>Awesome August</b></h2>
<figure id="attachment_73468" aria-describedby="caption-attachment-73468" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-73468" src="https://blogs.nvidia.com/wp-content/uploads/2024/08/gfn-spotlight-stormgate-tw-li-2048x1024-1-672x336.jpg" alt="Stormgate on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/08/gfn-spotlight-stormgate-tw-li-2048x1024-1-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/08/gfn-spotlight-stormgate-tw-li-2048x1024-1-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/08/gfn-spotlight-stormgate-tw-li-2048x1024-1-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/08/gfn-spotlight-stormgate-tw-li-2048x1024-1-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/08/gfn-spotlight-stormgate-tw-li-2048x1024-1-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/08/gfn-spotlight-stormgate-tw-li-2048x1024-1-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/08/gfn-spotlight-stormgate-tw-li-2048x1024-1-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/08/gfn-spotlight-stormgate-tw-li-2048x1024-1-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/08/gfn-spotlight-stormgate-tw-li-2048x1024-1.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73468" class="wp-caption-text"><em>Fight for the future.</em></figcaption></figure>
<p>Plunge into the heart of battle with <i>Stormgate, </i>a newly released real-time strategy game from Frost Giant Studios, which is renowned for its work on popular games <i>StarCraft II </i>and <i>Warcraft III.</i> In single-player or multiplayer mode, fight demonic invaders, build bases and command armies to save humanity. Get immersed in a rich storyline, explore diverse factions and experience a blend of new and classic real-time strategy mechanics.</p>
<p>Members can check out the following new additions this week:</p>
<ul>
<li><i>Stormgate Early Access </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2012510?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 30)</li>
<li><i>Space for Sale </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1624060?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 30)</li>
<li><i>Cyber Knights: Flashpoint </i>(<a target="_blank" href="https://store.steampowered.com/app/1021210?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Dark and Darker </i>(<a target="_blank" href="https://store.steampowered.com/app/2016590?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Kunitsu-Gami: Path of the Goddess</i> (<a target="_blank" href="https://www.xbox.com/games/store/kunitsu-gami-path-of-the-goddess/9PCWXX0MS15S?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Kunitsu-Gami: Path of the Goddess Demo</i> (<a target="_blank" href="https://store.steampowered.com/app/2842890?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a target="_blank" href="https://www.xbox.com/games/store/kunitsu-gami-path-of-the-goddess/9pnctxgzg02x?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>)</li>
</ul>
<p>And here‚Äôs a preview of what‚Äôs coming later this month:</p>
<ul>
<li><i>Warhammer 40,000: Speed Freeks </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2078450?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Aug. 6)</li>
<li><i>Ratten Reich</i> (New release on <a target="_blank" href="https://store.steampowered.com/app/1717250?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Aug. 9)</li>
<li><i>Level Zero Extraction </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1456940?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Aug. 13)</li>
<li><i>shapez 2</i> (New release on <a target="_blank" href="https://store.steampowered.com/app/2162800?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Aug. 15)</li>
<li><i>Akimbot </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1843540?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Aug. 29)</li>
<li><i>Gori: Cuddly Carnage </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1299690?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Aug. 29)</li>
<li><i>MEMORIAPOLIS </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2228280?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Aug. 29)</li>
<li><i>Visions of Mana </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2490990?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Aug. 29)</li>
<li><i>Breachway </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2118810?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, Aug. 30)</li>
<li><i>Star Wars Outlaws</i> (New release on <a target="_blank" href="https://store.ubi.com/645ba713a9ce0448bffa4c12.html#ucid=AFL-ID_152062&amp;maltcode=geforcenow_convst_AFL_geforcenow_vg__STORE____&amp;addinfo=">Ubisoft</a>, Aug. 30)</li>
<li><i>Avatar: Frontiers of Pandora</i> (<a target="_blank" href="https://store.steampowered.com/app/2840770?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Heading Out </i>(<a target="_blank" href="https://store.steampowered.com/app/1640630?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Nine Sols </i>(<a target="_blank" href="https://store.steampowered.com/app/1809540?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>Saturnalia </i>(<a target="_blank" href="https://store.steampowered.com/app/916350?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>We Were Here Too </i>(<a target="_blank" href="https://store.steampowered.com/app/677160?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
</ul>
<h2><b>Jammin‚Äô July</b></h2>
<p>In addition to the 22 games announced last month, six more joined the <a target="_blank" href="http://play.geforcenow.com">GeForce NOW library</a>:<i></i></p>
<ul>
<li><i>Cricket 24: The Official Game of the Ashes </i>(New release on <a target="_blank" href="https://www.xbox.com/games/store/cricket-24-the-official-game-of-the-ashes/9nkf2sz630zh?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a> and available on PC Game Pass, July 9)</li>
<li><i>The Elder Scrolls V: Skyrim </i>(<a target="_blank" href="https://store.steampowered.com/app/72850?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>The Elder Scrolls V: Skyrim Special Edition</i> (<a target="_blank" href="https://store.steampowered.com/app/489830?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a target="_blank" href="https://www.epicgames.com/store/p/skyrim?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a> and <a target="_blank" href="https://www.xbox.com/games/store/the-elder-scrolls-v-skyrim-special-edition-pc/9p03jgq4s1gc?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Kunitsu-Gami: Path of the Goddess</i> (New release on <a target="_blank" href="https://store.steampowered.com/app/2510710?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 18)</li>
<li><i>Nobody Wants to Die</i> (New release on <a target="_blank" href="https://store.steampowered.com/app/1939970?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 17)</li>
<li><i>The Settlers: New Allies </i>(<a target="_blank" href="https://store.steampowered.com/app/2750080?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>HAWKED </i>and <i>Flintlock: The Siege of Dawn </i>(Xbox) were included in the July games list ‚Äî <i>HAWKED</i> will no longer be added to GeForce NOW, while <i>Flintlock: The Siege of Dawn</i> will be added at another time. Stay tuned to GFN Thursday for more updates.</li>
</ul>
<p>Starting in November, GeForce NOW will transition away from updating the GeForce NOW Windows and macOS apps for <a target="_blank" href="https://nvidia.custhelp.com/app/answers/detail/a_id/5566">legacy operating systems</a>, including Windows 7, Windows 8.1 and macOS 10.11-10.14. Members on these systems can still enjoy streaming on <a target="_blank" href="http://play.geforcenow.com">play.geforcenow.com</a> via supported web browsers.</p>
<p>What are you planning to play this weekend? Let us know on <a target="_blank" href="https://www.twitter.com/nvidiagfn">X</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="en" dir="ltr">Which video game(s) do you have over 50 hours on? <img src="https://s.w.org/images/core/emoji/15.0.3/72x72/23f1.png" alt="‚è±" class="wp-smiley" style="height: 1em; max-height: 1em;" /><img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f3ae.png" alt="üéÆ" class="wp-smiley" style="height: 1em; max-height: 1em;" /></p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f329.png" alt="üå©" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a target="_blank" href="https://twitter.com/NVIDIAGFN/status/1818678004270215544?ref_src=twsrc%5Etfw">July 31, 2024</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/08/gfn-thursday-8-1-nv-blog-1280x680-no-cta.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/08/gfn-thursday-8-1-nv-blog-1280x680-no-cta-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[August Adventures Await: 18 New Games Coming to GeForce NOW]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA and Zoox Pave the Way for Autonomous Ride-Hailing</title>
		<link>https://blogs.nvidia.com/blog/nvidia-zoox-autonomous-ride-hailing/</link>
		
		<dc:creator><![CDATA[Jessica Soares]]></dc:creator>
		<pubDate>Wed, 31 Jul 2024 19:16:18 +0000</pubDate>
				<category><![CDATA[Driving]]></category>
		<category><![CDATA[Robotics]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73429</guid>

					<description><![CDATA[In celebration of Zoox‚Äôs 10th anniversary, NVIDIA founder and CEO Jensen Huang recently joined the robotaxi company‚Äôs CEO, Aicha Evans, and its cofounder and CTO, Jesse Levinson, to discuss the latest in autonomous vehicle (AV) innovation and experience a ride in the Zoox robotaxi. In a fireside chat at Zoox‚Äôs headquarters in Foster City, Calif.,	<a class="read-more" href="https://blogs.nvidia.com/blog/nvidia-zoox-autonomous-ride-hailing/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>In celebration of <a href="https://zoox.com/" target="_blank" rel="noopener">Zoox</a>‚Äôs 10th anniversary, NVIDIA founder and CEO Jensen Huang recently joined the robotaxi company‚Äôs CEO, Aicha Evans, and its cofounder and CTO, Jesse Levinson, to discuss the latest in autonomous vehicle (AV) innovation and experience a ride in the Zoox robotaxi.</p>
<p>In a fireside chat at Zoox‚Äôs headquarters in Foster City, Calif., the trio reflected on the two companies‚Äô decade of collaboration. Evans and Levinson highlighted how Zoox pioneered the concept of a robotaxi purpose-built for ride-hailing and created groundbreaking innovations along the way, using NVIDIA technology.</p>
<p>‚ÄúThe world has never seen a robotics company like this before,‚Äù said Huang. ‚ÄúZoox started out solely as a sustainable robotics company that delivers robots into the world as a fleet.‚Äù</p>
<p>Since 2014, Zoox has been on a mission to create fully autonomous, bidirectional vehicles purpose-built for ride-hailing services. This sets it apart in an industry largely focused on retrofitting existing cars with self-driving technology.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-1.jpg"><img loading="lazy" decoding="async" class="alignnone size-large wp-image-73440" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-1-672x448.jpg" alt="" width="672" height="448" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-1-672x448.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-1-400x267.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-1-768x512.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-1-675x450.jpg 675w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-1-323x215.jpg 323w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-1-150x100.jpg 150w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-1.jpg 1020w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>A decade later, the company is operating <a href="https://zoox.com/vehicle" target="_blank" rel="noopener">its robotaxi</a>, powered by NVIDIA GPUs, on public roads.</p>
<h2><b>Computing at the Core</b></h2>
<p>Zoox robotaxis are, at their core, supercomputers on wheels. They‚Äôre built on multiple NVIDIA GPUs dedicated to processing the enormous amounts of data generated in real time by their sensors.</p>
<p>The sensor array includes cameras, lidar, radar, long-wave infrared sensors and microphones. The onboard computing system rapidly processes the raw sensor data collected and fuses it to provide a coherent understanding of the vehicle‚Äôs surroundings.</p>
<p>The processed data then flows through a <a href="https://zoox.com/journal/perception/" target="_blank" rel="noopener">perception engine</a> and <a href="https://www.amazon.science/latest-news/how-the-zoox-robotaxi-predicts-everything-everywhere-all-at-once" target="_blank" rel="noopener">prediction module</a> to <a href="https://zoox.com/journal/planner" target="_blank" rel="noopener">planning</a> and control systems, enabling the vehicle to navigate complex urban environments safely.</p>
<p>NVIDIA GPUs deliver the immense computing power required for the Zoox robotaxis‚Äô autonomous capabilities and continuous learning from new experiences.</p>
<h2><b>Using Simulation as a Virtual Proving Ground</b></h2>
<p>Key to Zoox‚Äôs AV development process is its extensive use of simulation. The company uses NVIDIA GPUs and software tools to run a wide array of simulations, testing its autonomous systems in virtual environments before real-world deployment.</p>
<p>These simulations range from synthetic scenarios to replays of real-world scenarios created using data collected from test vehicles. Zoox uses retrofitted Toyota Highlanders equipped with the same sensor and compute packages as its robotaxis to gather driving data and validate its autonomous technology.</p>
<p>This data is then fed back into simulation environments, where it can be used to create countless variations and replays of scenarios and agent interactions.</p>
<p>Zoox also uses what it calls ‚Äúadversarial simulations,‚Äù carefully crafted scenarios designed to test the limits of the autonomous systems and uncover potential edge cases.</p>
<p>The company‚Äôs comprehensive approach to simulation allows it to rapidly iterate and improve its autonomous driving software, bolstering AV safety and performance.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1.png"><img loading="lazy" decoding="async" class="alignnone wp-image-73443 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1-672x368.png" alt="" width="672" height="368" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1-672x368.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1-400x219.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1-768x420.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1-822x450.png 822w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1-393x215.png 393w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1-183x100.png 183w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image1-1.png 1242w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>‚ÄúWe‚Äôve been using NVIDIA hardware since the very start,‚Äù said Levinson. ‚ÄúIt‚Äôs a huge part of our simulator, and we rely on NVIDIA GPUs in the vehicle to process everything around us in real time.‚Äù</p>
<h2><b>A Neat Way to Seat</b></h2>
<p>Zoox‚Äôs robotaxi, with its unique bidirectional design and <a href="https://zoox.com/journal/robotaxi-carriage-seating" target="_blank" rel="noopener">carriage-style seating</a>, is optimized for autonomous operation and passenger comfort, eliminating traditional concepts of a car‚Äôs ‚Äúfront‚Äù and ‚Äúback‚Äù and providing equal comfort and safety for all occupants.</p>
<p>‚ÄúI came to visit you when you were zero years old, and the vision was compelling,‚Äù Huang said, reflecting on Zoox‚Äôs evolution over the years. ‚ÄúThe challenge was incredible. The technology, the talent ‚Äî it is all world-class.‚Äù</p>
<p>Using NVIDIA GPUs and tools, Zoox is poised to redefine urban mobility, pioneering a future of safe, efficient and sustainable autonomous transportation for all.</p>
<h2><b>From Testing Miles to Market Projections</b></h2>
<p>As the AV industry gains momentum, recent projections highlight the potential for explosive growth in the robotaxi market. <a href="https://guidehouseinsights.com/reports/what-is-the-pathway-to-sustainability-for-as-a-service-business-models-in-transportation" target="_blank" rel="noopener">Guidehouse Insights</a> forecasts over 5 million robotaxi deployments by 2030, with numbers expected to surge to almost 34 million by 2035.</p>
<p>The regulatory landscape reflects this progress, with 38 companies currently holding valid permits to test AVs with safety drivers in California. Zoox is currently one of only six companies permitted to test AVs without safety drivers in the state.</p>
<p>As the industry advances, Zoox has created a next-generation robotaxi by combining cutting-edge onboard computing with extensive simulation and development.</p>
<p><i>In the image at top, NVIDIA founder and CEO Jensen Huang stands with Zoox CEO Aicha Evans and Zoox cofounder and CTO Jesse Levinson in front of a Zoox robotaxi.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/zooxfiresidechathighres.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/zooxfiresidechathighres-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA and Zoox Pave the Way for Autonomous Ride-Hailing]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Researchers Harness Real-Time Gen AI to Build Immersive Desert World</title>
		<link>https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Wed, 31 Jul 2024 17:00:31 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Research]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[Rendering]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<category><![CDATA[Synthetic Data Generation]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73428</guid>

					<description><![CDATA[NVIDIA researchers used NVIDIA Edify, a multimodal architecture for visual generative AI, to build a detailed 3D desert landscape within a few minutes in a live demo at SIGGRAPH‚Äôs Real-Time Live event on Tuesday. During the event ‚Äî one of the prestigious graphics conference‚Äôs top sessions ‚Äî NVIDIA researchers showed how, with the support of	<a class="read-more" href="https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>NVIDIA researchers used <a target="_blank" href="https://www.nvidia.com/en-us/gpu-cloud/picasso/">NVIDIA Edify</a>, a multimodal architecture for visual generative AI, to build a detailed 3D desert landscape within a few minutes in a live demo at SIGGRAPH‚Äôs <a target="_blank" href="https://s2024.siggraph.org/program/real-time-live/">Real-Time Live</a> event on Tuesday.</p>
<p>During the event ‚Äî one of the prestigious graphics conference‚Äôs top sessions ‚Äî NVIDIA researchers showed how, with the support of an AI agent, they could build and edit a desert landscape from scratch within five minutes. The live demo highlighted how generative AI can act as an assistant to artists by accelerating ideation and generating custom secondary assets that would otherwise have been sourced from a repository.</p>
<p>By drastically decreasing ideation time, these AI technologies will empower 3D artists to be more productive and creative ‚Äî giving them the tools to explore concepts faster and expedite parts of their workflows. They could, for example, generate the background assets or 360 HDRi environments that the scene needs in minutes, instead of spending hours finding or creating them.</p>
<h2><b>From Idea to 3D Scene in Three Minutes</b></h2>
<p>Creating a full 3D scene is a complex, time-consuming task. Artists must support their hero asset with plenty of background objects to create a rich scene, then find an appropriate background and an environment map to light it. Due to time constraints, they‚Äôve often had to make a trade-off between rapid results and creative exploration.</p>
<p>With the support of AI agents, creative teams can achieve both goals: quickly bring concepts to life and continue iterating to achieve the right look.</p>
<p>In the Real-Time Live demo, the researchers used an AI agent to instruct an NVIDIA Edify-powered model to generate dozens of 3D assets, including cacti, rocks and the skull of a bull ‚Äî with previews produced in just seconds.</p>
<p>They next directed the agent to harness other models to create potential backgrounds and a layout of how the objects would be placed in the scene ‚Äî and showcased how the agent could adapt to last-minute changes in creative direction by quickly swapping the rocks for gold nuggets.</p>
<p>With a design plan in place, they prompted the agent to create full-quality assets and render the scene as a photorealistic image in <a target="_blank" href="https://docs.omniverse.nvidia.com/composer/latest/index.html">NVIDIA Omniverse USD Composer</a>, an app for virtual world-building.</p>
<p><iframe loading="lazy" title="Immersive Desert World -- From Idea to 3D Scene in Three Minutes | NVIDIA Research" width="500" height="281" src="https://www.youtube.com/embed/AJWTUvXA0Wc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>NVIDIA Edify Accelerates Environment Generation¬†</b></h2>
<p>NVIDIA Edify models can help creators focus on hero assets while accelerating the creation of background environments and objects using AI-powered scene generation tools. The Real-Time Live demo showcased two Edify models:¬†<b></b></p>
<ul>
<li aria-level="1"><b>Edify 3D</b> generates ready-to-edit 3D meshes from text or image prompts. Within seconds, the model can generate previews, including rotating animations of each object, to help creators rapidly prototype before committing to a specific design.</li>
<li aria-level="1"><b>Edify 360 HDRi</b> uses text or image prompts to generate up to 16K high-dynamic range images (HDRi) of nature landscapes, which can be used as backgrounds and to light scenes.</li>
</ul>
<p>During the demo, the researchers also showcased an AI agent powered by a large language model, and <b>USD Layout</b>, an AI model that generates scene layouts using OpenUSD, a platform for 3D workflows.</p>
<p>At SIGGRAPH, NVIDIA also announced that two leading creative content companies are giving designers and artists new ways to boost productivity with generative AI using <a href="https://blogs.nvidia.com/blog/edify-shutterstock-getty-images/">tools powered by NVIDIA Edify</a>.</p>
<p>Shutterstock has launched in commercial beta its <a target="_blank" href="https://www.shutterstock.com/discover/generative-ai-3d">Generative 3D</a> service, which lets creators quickly prototype and generate 3D assets using text or image prompts. Its 360 HDRi generator based on Edify also entered early access.</p>
<p>Getty Images updated its <a target="_blank" href="https://www.gettyimages.com/ai/generation/about">Generative AI by Getty Images</a> service with the latest version of NVIDIA Edify. Users can now create images twice as fast, with improved output quality and prompt adherence, and advanced controls and fine-tuning.</p>
<h2><b>Harnessing Universal Scene Description in NVIDIA Omniverse</b></h2>
<p>The 3D objects, environment maps and layouts generated using Edify models are structured with USD, a standard format for describing and composing 3D worlds. This compatibility allows artists to immediately import Edify-powered creations into Omniverse USD Composer.</p>
<p>Within Composer, they can use popular digital content creation tools to further modify the scene by, for example, changing the position of objects, modifying their appearance or adjusting lighting.</p>
<p>Real-Time Live is one of the most anticipated events at SIGGRAPH, featuring about a dozen real-time applications including generative AI, virtual reality and live performance capture technology. Watch the replay below.</p>
<p><iframe loading="lazy" title="SIGGRAPH 2024 Real-Time Live!" width="500" height="281" src="https://www.youtube.com/embed/Gm1B5DT8kE0?start=1752&#038;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>&nbsp;</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/Edify_3D_Generative_AI_Real_Time_Live.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/Edify_3D_Generative_AI_Real_Time_Live-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Researchers Harness Real-Time Gen AI to Build Immersive Desert World]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Oracle Cloud Infrastructure Expands NVIDIA GPU-Accelerated Instances for AI, Digital Twins and More</title>
		<link>https://blogs.nvidia.com/blog/oracle-cloud-infrastructure-ai-gpu-digital-twins/</link>
		
		<dc:creator><![CDATA[Rohil Bhargava]]></dc:creator>
		<pubDate>Wed, 31 Jul 2024 16:15:14 +0000</pubDate>
				<category><![CDATA[Data Center]]></category>
		<category><![CDATA[Hardware]]></category>
		<category><![CDATA[Networking]]></category>
		<category><![CDATA[Software]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[GPU]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73309</guid>

					<description><![CDATA[Enterprises are rapidly adopting generative AI, large language models (LLMs), advanced graphics and digital twins to increase operational efficiencies, reduce costs and drive innovation. However, to adopt these technologies effectively, enterprises need access to state-of-the-art, full-stack accelerated computing platforms. To meet this demand, Oracle Cloud Infrastructure (OCI) today announced NVIDIA L40S GPU bare-metal instances available	<a class="read-more" href="https://blogs.nvidia.com/blog/oracle-cloud-infrastructure-ai-gpu-digital-twins/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Enterprises are rapidly adopting <a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a>, large language models (<a target="_blank" href="https://www.nvidia.com/en-us/glossary/large-language-models/">LLMs</a>), advanced graphics and <a href="https://blogs.nvidia.com/blog/what-is-a-digital-twin/">digital twins</a> to increase operational efficiencies, reduce costs and drive innovation.</p>
<p>However, to adopt these technologies effectively, enterprises need access to state-of-the-art, full-stack accelerated computing platforms. To meet this demand, Oracle Cloud Infrastructure (OCI) today announced <a target="_blank" href="https://www.nvidia.com/en-us/data-center/l40s/">NVIDIA L40S GPU</a> bare-metal instances available to order and the upcoming availability of a new virtual machine accelerated by a single <a target="_blank" href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPU</a>. This new VM expands OCI‚Äôs existing H100 portfolio, which includes an NVIDIA HGX H100 8-GPU bare-metal instance.</p>
<p>Paired with NVIDIA networking and running the NVIDIA software stack, these platforms deliver powerful performance and efficiency, enabling enterprises to advance generative AI.</p>
<h2><b>NVIDIA L40S Now Available to Order on OCI</b></h2>
<p>The NVIDIA L40S is a universal data center GPU designed to deliver breakthrough multi-workload acceleration for generative AI, graphics and video applications. Equipped with fourth-generation Tensor Cores and support for the FP8 data format, the L40S GPU excels in training and fine-tuning small- to mid-size LLMs and in inference across a wide range of generative AI use cases.</p>
<p>For example, a single L40S GPU (FP8) can generate up to <a target="_blank" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md">1.4x more tokens per second</a> than a single <a target="_blank" href="https://www.nvidia.com/en-us/data-center/a100/">NVIDIA A100 Tensor Core GPU</a> (FP16) for Llama 3 8B with <a target="_blank" href="https://developer.nvidia.com/tensorrt">NVIDIA TensorRT-LLM</a> at an input and output sequence length of 128.</p>
<p>The L40S GPU also has best-in-class graphics and media acceleration. Its third-generation NVIDIA Ray Tracing Cores (RT Cores) and multiple encode/decode engines make it ideal for advanced visualization and digital twin applications.</p>
<p>The L40S GPU delivers up to 3.8x the real-time ray-tracing performance of its predecessor, and supports NVIDIA DLSS 3 for faster rendering and smoother frame rates. This makes the GPU ideal for developing applications on the <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> platform, enabling real-time, photorealistic 3D simulations and AI-enabled digital twins. With Omniverse on the L40S GPU, enterprises can develop advanced 3D applications and workflows for industrial digitalization that will allow them to design, simulate and optimize products, processes and facilities in real time before going into production.</p>
<p>OCI will offer the L40S GPU in its BM.GPU.L40S.4 bare-metal compute shape, featuring four NVIDIA L40S GPUs, each with 48GB of GDDR6 memory. This shape includes local NVMe drives with 7.38TB capacity, 4th Generation Intel Xeon CPUs with 112 cores and 1TB of system memory.</p>
<p>These shapes eliminate the overhead of any virtualization for high-throughput and latency-sensitive AI or machine learning workloads with OCI‚Äôs bare-metal compute architecture. The accelerated compute shape features the <a target="_blank" href="https://www.nvidia.com/en-us/networking/products/data-processing-unit/">NVIDIA BlueField-3 DPU</a> for improved server efficiency, offloading data center tasks from CPUs to accelerate networking, storage and security workloads. The use of BlueField-3 DPUs furthers OCI‚Äôs strategy of off-box virtualization across its entire fleet.</p>
<p><a target="_blank" href="https://www.oracle.com/ai-infrastructure/">OCI Supercluster</a> with NVIDIA L40S enables ultra-high performance with 800Gbps of internode bandwidth and low latency for up to 3,840 GPUs. OCI‚Äôs cluster network uses <a target="_blank" href="https://www.nvidia.com/en-us/networking/ethernet-adapters/">NVIDIA ConnectX-7 NICs</a> over RoCE v2 to support high-throughput and latency-sensitive workloads, including AI training.</p>
<p>‚ÄúWe chose OCI AI infrastructure with bare-metal instances and NVIDIA L40S GPUs for 30% more efficient video encoding,‚Äù said Sharon Carmel, CEO of Beamr Cloud. ‚ÄúVideos processed with Beamr Cloud on OCI will have up to 50% reduced storage and network bandwidth consumption, speeding up file transfers by 2x and increasing productivity for end users. Beamr will provide OCI customers video AI workflows, preparing them for the future of video.‚Äù</p>
<h2><b>Single-GPU H100 VMs Coming Soon on OCI¬†</b></h2>
<p>The VM.GPU.H100.1 compute virtual machine shape, accelerated by a single <a target="_blank" href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPU</a>, is coming soon to OCI. This will provide cost-effective, on-demand access for enterprises looking to use the power of NVIDIA H100 GPUs for their generative AI and HPC workloads.</p>
<p>A single H100 provides a good platform for smaller workloads and LLM inference. For example, one H100 GPU can generate more than <a target="_blank" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/performance/perf-overview.md">27,000 tokens per second</a> for Llama 3 8B (up to 4x more throughput than a single A100 GPU at FP16 precision) with <a target="_blank" href="https://developer.nvidia.com/tensorrt">NVIDIA TensorRT-LLM</a> at an input and output sequence length of 128 and FP8 precision.</p>
<p>The VM.GPU.H100.1 shape includes 2&#215;3.4TB of NVMe drive capacity, 13 cores of 4th Gen Intel Xeon processors and 246GB of system memory, making it well-suited for a range of AI tasks.</p>
<p>‚ÄúOracle Cloud‚Äôs bare-metal compute with NVIDIA H100 and A100 GPUs, low-latency Supercluster and high-performance storage delivers up to 20% better price-performance for Altair‚Äôs computational fluid dynamics and structural mechanics solvers,‚Äù said Yeshwant Mummaneni, chief engineer of data management analytics at Altair. ‚ÄúWe look forward to leveraging these GPUs with virtual machines for the Altair Unlimited virtual appliance.‚Äù</p>
<h2><b>GH200 Bare-Metal Instances Available for Validation</b></h2>
<p>OCI has also made available the BM.GPU.GH200 compute shape for customer testing. It features the <a target="_blank" href="https://www.nvidia.com/en-us/data-center/grace-hopper-superchip/">NVIDIA Grace Hopper Superchip</a> and <a target="_blank" href="https://www.nvidia.com/en-us/data-center/nvlink-c2c/">NVLink-C2C</a>, a high-bandwidth, cache-coherent 900GB/s connection between the <a target="_blank" href="https://www.nvidia.com/en-us/data-center/grace-cpu/">NVIDIA Grace CPU</a> and <a target="_blank" href="https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/">NVIDIA Hopper </a>GPU. This provides over 600GB of accessible memory, enabling up to 10x higher performance for applications running terabytes of data compared to the NVIDIA A100 GPU.</p>
<h2><b>Optimized Software for Enterprise AI¬†</b></h2>
<p>Enterprises have a wide variety of NVIDIA GPUs to accelerate their AI, HPC and data analytics workloads on OCI. However, maximizing the full potential of these GPU-accelerated compute instances requires an optimized software layer.</p>
<p><a target="_blank" href="https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/">NVIDIA NIM</a>, part of the <a target="_blank" href="https://cloudmarketplace.oracle.com/marketplace/en_US/listing/155314141">NVIDIA AI Enterprise software platform available on the OCI Marketplace</a>, is a set of easy-to-use microservices designed for secure, reliable deployment of high-performance AI model inference to deploy world-class generative AI applications.</p>
<p>Optimized for NVIDIA GPUs, NIM pre-built containers offer developers improved cost of ownership, faster time to market and security. NIM microservices for popular community models, found on the <a target="_blank" href="https://build.nvidia.com/explore/discover">NVIDIA API Catalog</a>, can be deployed easily on OCI.</p>
<p>Performance will continue to improve over time with upcoming GPU-accelerated instances, including NVIDIA H200 Tensor Core GPUs and NVIDIA Blackwell GPUs.</p>
<p>Order the L40S GPU and test the GH200 Superchip by <a target="_blank" href="https://www.oracle.com/cloud/contact-form.html">reaching out to OCI</a>. To learn more, join <a target="_blank" href="https://go.oracle.com/LP=143193">Oracle and NVIDIA at SIGGRAPH</a>, the world‚Äôs premier graphics conference, running through Aug. 1.</p>
<p><i>See </i><a target="_blank" href="https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fabout-nvidia%2Flegal-info%2F&amp;data=05%7C02%7Clpham%40nvidia.com%7Cd59f2f66f51e4deaac8008dc94b3ef0f%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638548747745016311%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&amp;sdata=dm8Os%2B4LtHW2ehZrPaxn38bsutMQBDeUdQuxrIa2y1Y%3D&amp;reserved=0"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2022/10/Copy-of-oracle-blog-logo-lockup-promo-package-2508744-1260x680-r3.png"
			type="image/png"
			width="1260"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2022/10/Copy-of-oracle-blog-logo-lockup-promo-package-2508744-1260x680-r3-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Oracle Cloud Infrastructure Expands NVIDIA GPU-Accelerated Instances for AI, Digital Twins and More]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Taking AI to Warp Speed: Decoding How NVIDIA‚Äôs Latest RTX-Powered Tools and Apps Help Developers Accelerate AI on PCs and Workstations</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-siggraph-chat-rtx-update/</link>
		
		<dc:creator><![CDATA[Jesse Clayton]]></dc:creator>
		<pubDate>Wed, 31 Jul 2024 13:00:24 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73418</guid>

					<description><![CDATA[NVIDIA is spotlighting the latest NVIDIA RTX-powered tools and apps at SIGGRAPH, an annual trade show at the intersection of graphics and AI.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><i>Editor‚Äôs note: This post is part of the </i><a href="https://blogs.nvidia.com/blog/tag/ai-decoded/"><i>AI Decoded series</i></a><i>, which demystifies AI by making the technology more accessible, and showcases new hardware, software, tools and accelerations for RTX PC users.</i></p>
<p>NVIDIA is spotlighting the latest <a target="_blank" href="https://www.nvidia.com/en-us/design-visualization/desktop-graphics/">NVIDIA RTX</a>-powered tools and apps at <a target="_blank" href="https://www.siggraph.org/">SIGGRAPH</a>, an annual trade show at the intersection of graphics and AI.</p>
<p>These AI technologies provide advanced ray-tracing and rendering techniques, enabling highly realistic graphics and immersive experiences in gaming, <a href="https://blogs.nvidia.com/blog/what-is-extended-reality/">virtual reality, animation and</a> cinematic special effects. RTX AI PCs and workstations are helping drive the future of interactive digital media, content creation, productivity and development.</p>
<h2><b>ACE‚Äôs AI Magic</b></h2>
<p>During a SIGGRAPH <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">fireside chat</a>, NVIDIA founder and CEO Jensen Huang introduced ‚Äú<a href="https://blogs.nvidia.com/blog/digital-humans-siggraph-2024/">James</a>‚Äù ‚Äî an interactive digital human built on <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> microservices ‚Äî that showcases the potential of AI-driven customer interactions.</p>
<p>Using <a target="_blank" href="https://developer.nvidia.com/ace">NVIDIA ACE</a> technology and based on a customer-service workflow, James is a virtual assistant that can connect with people using emotions, humor and contextually accurate responses. Soon, users will be able to interact with James in real time at <a target="_blank" href="http://ai.nvidia.com">ai.nvidia.com</a>.</p>
<figure id="attachment_73425" aria-describedby="caption-attachment-73425" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-73425" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-ace-james-virtual-assistant-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-ace-james-virtual-assistant-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-ace-james-virtual-assistant-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-ace-james-virtual-assistant-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-ace-james-virtual-assistant-1536x864.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-ace-james-virtual-assistant-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-ace-james-virtual-assistant-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-ace-james-virtual-assistant-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-ace-james-virtual-assistant-1280x720.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-ace-james-virtual-assistant.png 1775w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73425" class="wp-caption-text">James is a virtual assistant in NVIDIA ACE.</figcaption></figure>
<p>NVIDIA also introduced the latest advancements in the <a target="_blank" href="https://developer.nvidia.com/maxine">NVIDIA Maxine</a> AI platform for telepresence, as well as companies adopting NVIDIA ACE, a suite of technologies for bringing digital humans to life with <a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a>. These technologies enable digital human development with AI models for speech and translation, vision, intelligence, realistic animation and behavior, and lifelike appearance.</p>
<p>Maxine features two AI technologies that enhance the digital human experience in telepresence scenarios: Maxine 3D and Audio2Face-2D.</p>
<p>Developers can harness Maxine and ACE technologies to drive more engaging and natural interactions for people using digital interfaces across customer service, gaming and other interactive experiences.</p>
<p>Tapping advanced AI, NVIDIA ACE technologies allow developers to design avatars that can respond to users in real time with lifelike animations, speech and emotions. RTX GPUs provide the necessary computational power and graphical fidelity to render ACE avatars with stunning detail and fluidity.</p>
<p>With ongoing advancements and increasing adoption, ACE is setting new benchmarks for building virtual worlds and sparking innovation across industries. Developers tapping into the power of ACE with RTX GPUs can build more immersive applications and advanced, AI-based, interactive digital media experiences.</p>
<h2><b>RTX Updates Unleash AI-rtistry for Creators</b></h2>
<p><a target="_blank" href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/">NVIDIA GeForce RTX</a> PCs and NVIDIA RTX workstations are getting an upgrade with GPU accelerations that provide users with enhanced AI content-creation experiences.</p>
<p>For video editors, <a href="https://blogs.nvidia.com/blog/rtx-video-hdr-remix-studio-driver/">RTX Video HDR</a> is now available through <a href="https://blogs.nvidia.com/blog/studio-wondershare-filmora-rtx-ai-july-driver/">Wondershare Filmora</a> and DaVinci Resolve. With this technology, users can transform any content into high dynamic range video with richer colors and greater detail in light and dark scenes ‚Äî making it ideal for gaming videos, travel vlogs or event filmmaking. Combining RTX Video HDR with <a href="https://blogs.nvidia.com/blog/rtx-video-super-resolution/">RTX Video Super Resolution</a> further improves visual quality by removing encoding artifacts and enhancing details.</p>
<p><iframe loading="lazy" title="Filmora x NVIDIA Partnership" width="500" height="281" src="https://www.youtube.com/embed/I2ZUGB8JZKc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>RTX Video HDR requires an RTX GPU connected to an HDR10-compatible monitor or TV. Users with an RTX GPU-powered PC can send files to the Filmora desktop app and continue to edit with local RTX acceleration, doubling the speed of the export process with dual encoders on GeForce RTX 4070 Ti or above GPUs. Popular media player VLC in June added support for RTX Video Super Resolution and RTX Video HDR, adding AI-enhanced video playback.</p>
<p>Read <a href="https://blogs.nvidia.com/blog/studio-wondershare-filmora-rtx-ai-july-driver/">this blog on RTX-powered video editing</a> and the <a target="_blank" href="https://nvidia.custhelp.com/app/answers/detail/a_id/5448/~/rtx-video-super-resolution-faq">RTX Video FAQ</a> for more information. Learn more about <a target="_blank" href="https://filmora.wondershare.net/ai-features.html?gad_source=1&amp;gclid=CjwKCAjwnK60BhA9EiwAmpHZw62RpoOFbNoF1rHGVNbssHFgXqQygDzHwp_isBqCFRmYHx-0xE5gwxoCsUQQAvD_BwE">Wondershare Filmora‚Äôs AI-powered features</a>.</p>
<p>In addition, 3D artists are gaining more AI applications and tools that simplify and enhance workflows, including Replikant, Adobe, Topaz and Getty Images.</p>
<p><a target="_blank" href="https://www.replikant.com/">Replikant</a>, an AI-assisted 3D animation platform, is integrating <a target="_blank" href="https://www.nvidia.com/en-us/ai-data-science/audio2face/">NVIDIA Audio2Face</a>, an ACE technology, to enable improved lip sync and facial animation. By taking advantage of NVIDIA-accelerated generative models, users can enjoy real-time visuals enhanced by RTX and <a target="_blank" href="https://www.nvidia.com/en-us/geforce/technologies/dlss/">NVIDIA DLSS</a> technology. Replikant is now available on Steam.</p>
<p>Adobe Substance 3D Modeler has added Search Asset Library by Shape, an AI-powered feature designed to streamline the replacement and enhancement of complex shapes using existing 3D models. This new capability significantly accelerates prototyping and enhances design workflows.</p>
<p>New AI features in Adobe Substance 3D integrate advanced generative AI capabilities, enhancing its texturing and material-creation tools. Adobe has launched the first integration of its Firefly generative AI capabilities into Substance 3D Sampler and Stager, making 3D workflows more seamless and productive for industrial designers, game developers and visual effects professionals.</p>
<p>For tasks like text-to-texture generation and prompt descriptions, Substance 3D users can generate photorealistic or stylized textures. These textures can then be applied directly to 3D models. The new Text to Texture and Generative Background features significantly accelerate traditionally time-consuming and intricate 3D texturing and staging tasks.</p>
<p>Powered by NVIDIA RTX Tensor Cores, Substance 3D can significantly accelerate computations and allows for more intuitive and creative design processes. This development builds on Adobe‚Äôs innovation with Firefly-powered Creative Cloud upgrades in Substance 3D workflows.</p>
<p>Topaz AI has added <a target="_blank" href="https://developer.nvidia.com/tensorrt">NVIDIA TensorRT</a> acceleration for multi-GPU workflows, enabling parallelization across multiple GPUs for supercharged rendering speeds ‚Äî up to 2x faster with two GPUs over a single GPU system, and scaling further with additional GPUs.</p>
<p>Getty Images has updated its Generative AI by iStock service with new features to enhance image generation and quality. Powered by NVIDIA Edify models, the latest enhancement delivers generation speeds set to reach around six seconds for four images, doubling the performance of the previous model, with speeds at the forefront of the industry. The improved Text-2-Image and Image-2-Image functionalities provide higher-quality results and greater adherence to user prompts.</p>
<p>Generative AI by iStock users can now also designate camera settings such as focal length (narrow, standard or wide) and depth of field (near or far). Improvements to generative AI super-resolution enhances image quality by using AI to create new pixels, significantly improving resolution without over-sharpening the image.</p>
<h2><b>LLM-azing AI</b></h2>
<p>ChatRTX ‚Äî a tech demo that connects a <a target="_blank" href="https://www.nvidia.com/en-us/glossary/large-language-models/">large language model</a> (LLM), like Meta‚Äôs Llama, to a user‚Äôs data for quickly querying notes, documents or images ‚Äî is getting a user interface (UI) makeover, offering a cleaner, more polished experience.</p>
<p>ChatRTX also serves as an open-source reference project that shows developers how to build powerful, local, <a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">retrieval-augmented applications</a> (RAG) applications accelerated by RTX.</p>
<figure id="attachment_73419" aria-describedby="caption-attachment-73419" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-73419" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-672x442.png" alt="" width="672" height="442" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-672x442.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-400x263.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-768x506.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-1536x1011.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-684x450.png 684w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-327x215.png 327w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-152x100.png 152w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3-1280x843.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image3.png 1999w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73419" class="wp-caption-text">ChatRTX is getting a interface (UI) makeover.</figcaption></figure>
<p>The latest version of ChatRTX, released today, uses the Electron + Material UI framework, which lets developers more easily add their own UI elements or extend the technology‚Äôs functionality. The update also includes a new architecture that simplifies the integration of different UIs and streamlines the building of new chat and RAG applications on top of the ChatRTX backend application programming interface.</p>
<p>End users can download the latest version of ChatRTX from the <a target="_blank" href="https://www.nvidia.com/en-us/ai-on-rtx/chatrtx/">ChatRTX web page</a>. Developers can find the source code for the new release on the <a target="_blank" href="https://github.com/NVIDIA/ChatRTX">ChatRTX GitHub repository</a>.</p>
<p><a target="_blank" href="https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/">Meta Llama 3.1-8B models</a> are now optimized for inference on NVIDIA GeForce RTX PCs and NVIDIA RTX workstations. These models are natively supported with <a target="_blank" href="https://github.com/NVIDIA/TensorRT-LLM">NVIDIA TensorRT-LLM</a>, open-source software that accelerates LLM inference performance.</p>
<h2><b>Dell‚Äôs AI Chatbots: Harnessing RTX Rocket Fuel</b></h2>
<p>Dell is presenting how enterprises can boost AI development with an optimized RAG chatbot using NVIDIA AI Workbench and an NVIDIA NIM microservice for Llama 3. Using the <a target="_blank" href="https://github.com/NVIDIA/workbench-example-hybrid-rag">NVIDIA AI Workbench Hybrid RAG Project</a>, Dell is demonstrating how the chatbot can be used to converse with enterprise data that‚Äôs embedded in a local vector database, with inference running in one of three ways:</p>
<ul>
<li>Locally on a Hugging Face TGI server</li>
<li>In the cloud using NVIDIA inference endpoints</li>
<li>On self-hosted NVIDIA NIM microservices</li>
</ul>
<p>Learn more about the <a target="_blank" href="https://developer.nvidia.com/blog/optimize-ai-model-performance-and-maintain-data-privacy-with-hybrid-rag/">AI Workbench Hybrid RAG Project</a>. SIGGRAPH attendees can experience this technology firsthand at Dell Technologies‚Äô booth 301.</p>
<h2><b>HP AI Studio: Innovate Faster With CUDA-X and Galileo</b></h2>
<p>At SIGGRAPH, HP is presenting the Z by HP AI Studio, a centralized data science platform. Announced in October 2023, AI Studio has now been enhanced with the latest NVIDIA CUDA-X libraries as well as HP‚Äôs recent partnership with Galileo, a generative AI trust-layer company. Key benefits include:</p>
<ul>
<li>Deploy projects faster: Configure, connect and share local and remote projects quickly.</li>
<li>Collaborate with ease: Access and share data, templates and experiments effortlessly.</li>
<li>Work your way: Choose where to work on your data, easily switching between online and offline modes.</li>
</ul>
<p>Designed to enhance productivity and streamline AI development, AI Studio allows data science teams to focus on innovation. Visit HP‚Äôs booth 501 to see how AI Studio with <a target="_blank" href="https://developer.nvidia.com/blog/accelerated-data-analytics-speed-up-data-exploration-with-rapids-cudf/">RAPIDS cuDF</a> can boost data preprocessing to accelerate AI pipelines. Apply for <a target="_blank" href="https://reinvent.hp.com/LP=10935">early access to AI Studio</a>.</p>
<h2><b>An RTX Speed Surge for Stable Diffusion</b></h2>
<p>Stable Diffusion 3.0, the latest model from Stability AI, has been optimized with TensorRT to provide a 60% speedup.</p>
<p><iframe loading="lazy" title="Accelerate Stable Diffusion with RTX GPUs" width="500" height="281" src="https://www.youtube.com/embed/KA8kOsooDcw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>A NIM microservice for Stable Diffusion 3 with optimized performance is available for preview on <a target="_blank" href="http://ai.nvidia.com">ai.nvidia.com</a>.</p>
<p>There‚Äôs still time to <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">join NVIDIA at SIGGRAPH</a> to see how RTX AI is transforming the future of content creation and visual media experiences. The conference runs through Aug. 1.</p>
<p><i>Generative AI is transforming graphics and interactive experiences of all kinds. Make sense of what‚Äôs new and what‚Äôs next by subscribing to the </i><a target="_blank" href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><i>AI Decoded newsletter</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/siggraph-ai-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/siggraph-ai-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Taking AI to Warp Speed: Decoding How NVIDIA‚Äôs Latest RTX-Powered Tools and Apps Help Developers Accelerate AI on PCs and Workstations]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Creators to Have Personalized AI Assistants, Meta CEO Mark Zuckerberg Tells NVIDIA CEO Jensen Huang</title>
		<link>https://blogs.nvidia.com/blog/zuckerberg-huang/</link>
		
		<dc:creator><![CDATA[Brian Caulfield]]></dc:creator>
		<pubDate>Tue, 30 Jul 2024 00:30:28 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73130</guid>

					<description><![CDATA[In a highly anticipated fireside chat at SIGGRAPH 2024, NVIDIA founder and CEO Jensen Huang and Meta founder and CEO Mark Zuckerberg discussed the transformative potential of open source AI and AI assistants. Zuckerberg kicked off the discussion by announcing the launch of AI Studio, a new platform that allows users to create, share and	<a class="read-more" href="https://blogs.nvidia.com/blog/zuckerberg-huang/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>In a highly anticipated fireside chat at SIGGRAPH 2024, NVIDIA founder and CEO Jensen Huang and Meta founder and CEO Mark Zuckerberg discussed the transformative potential of open source AI and AI assistants.</p>
<p>Zuckerberg kicked off the discussion by announcing the launch of <a target="_blank" href="https://about.fb.com/news/2024/07/create-your-own-custom-ai-with-ai-studio/">AI Studio</a>, a new platform that allows users to create, share and discover AI characters, making AI more accessible to millions of creators and small businesses.</p>
<p>‚ÄúEvery single restaurant, every single website will probably, in the future, have these AIs ‚Ä¶‚Äù Huang said.</p>
<p>‚Äú&#8230;just like every business has an email address and a website and a social media account, I think, in the future, every business is going to have an AI,‚Äù Zuckerberg responded.</p>
<p>Zuckerberg has gotten it right before. Huang credited Zuckerberg and Meta with being leaders in AI, even if only some have noticed until recently.</p>
<p>‚ÄúYou guys have done amazing AI work,‚Äù Huang said, citing advancements from Meta in computer vision, language models, real-time translation. ‚ÄúWe all use Pytorch, that comes out of Meta.‚Äù</p>
<p><iframe loading="lazy" title="YouTube video player" src="https://www.youtube.com/embed/w-cmMcMZoZ4?si=n6m8GTto3u69RW1_" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe><br />
<b>The Importance of Open Source in Advancing AI</b><b><br />
</b><br />
Zuckerberg highlighted the importance of open source in advancing AI ‚Äî with the two business leaders emphasizing the importance of open platforms for innovation. <b><br />
</b><br />
Meta has rapidly emerged as a leader in AI, putting it to work throughout its businesses ‚Äî most notably with Meta AI, which is used across Facebook, Instagram and WhatsApp ‚Äî and advancing open-source AI throughout the industry, most recently with the release of Llama 3.1.</p>
<p>The open-source model represents a significant investment of time and training resources. The largest version of Llama boasts 405 billion parameters and was trained on over 16,000 NVIDIA H100 GPUs.</p>
<p>‚ÄúOne of the things that drives quality improvements is it used to be that you have a different model for each type of content,‚Äù Zuckerberg explained.</p>
<p>‚ÄúAs the models get bigger and more general, that gets better and better. So, I kind of dream of one day like you can almost imagine all of Facebook or Instagram being like a single AI model that has unified all these different content types and systems together,‚Äù he added.</p>
<p>Zuckerberg sees collaboration as key to more advancements. In a blog post released last week, Zuckerberg wrote that the release of Llama 3.1 promises to be an ‚Äúinflection point‚Äù in adopting open source in AI.</p>
<p>These advancements promise more tools to foster engagement, create compelling and personalized content ‚Äî such as digital avatars ‚Äî and build virtual worlds.</p>
<p>More broadly, the advancement of AI across a broad ecosystem promises to supercharge human productivity, for example, by giving every human on earth a digital assistant ‚Äî or assistants ‚Äî allowing people to live richer lives that they can interact with quickly and fluidly.</p>
<p>‚ÄúI feel like I‚Äôm collaborating with WhatsApp,‚Äù Huang said. ‚ÄúImagine I‚Äôm sitting here typing, and it‚Äôs generating the images as I‚Äôm going. I go back and change my words, and it‚Äôs generating other images.‚Äù</p>
<p><b>Vision for the Future</b></p>
<p>Looking ahead, both CEOs shared their visions for the future.</p>
<p>Zuckerberg expressed optimism about bringing AI together with the real world through eyeglasses ‚Äî noting his company‚Äôs collaboration with eyewear maker Luxottica ‚Äî that can be used to help transform education, entertainment and work.</p>
<p>Huang emphasized how interacting with AIs is becoming more fluid, moving beyond just text-based interactions.</p>
<p>‚ÄúToday‚Äôs AI is kind of turn-based. You say something, it says something back to you,‚Äù Huang said. In the future, AI could contemplate multiple options, or come up with a tree of options and simulate outcomes, making it much more powerful.‚Äù</p>
<p>Throughout the conversation, the two leaders playfully bantered about everything from fashion to steak sandwiches, ending the discussion by exchanging leather jackets.</p>
<p>Zuckerberg gave Huang a black leather shearling jacket with an enormous hood.</p>
<p>Huang gave Zuckerberg his own leather jacket, which he got from his wife, Lori, just for SIGGRAPH, quipping that it was just ‚Äútwo hours old.‚Äù</p>
<p>‚ÄúWell this one‚Äôs yours,‚Äù Zuckerberg said with a smile. ‚ÄúThis is worth more because it‚Äôs used.‚Äù</p>
<p><em><strong>Dive deeper into <a target="_blank" href="https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms/">Llama 3.1 in this technical blog¬†</a>and watch how accelerated computing and generative AI are transforming industries and creating new opportunities for innovation and growth in NVIDIA founder and CEO Jensen Huang‚Äôs two¬†<a href="https://www.nvidia.com/en-us/events/siggraph/" target="_blank" rel="noopener">fireside chats</a> at SIGGRAPH.</strong></em></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B1081-blog-press-1280x680-1.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B1081-blog-press-1280x680-1-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Creators to Have Personalized AI Assistants, Meta CEO Mark Zuckerberg Tells NVIDIA CEO Jensen Huang]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>‚ÄòEverybody Will Have an AI Assistant,‚Äô NVIDIA CEO Tells SIGGRAPH Audience</title>
		<link>https://blogs.nvidia.com/blog/nvidia-ceo-siggraph/</link>
		
		<dc:creator><![CDATA[Brian Caulfield]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 22:43:08 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73382</guid>

					<description><![CDATA[The generative AI revolution ‚Äî with deep roots in visual computing ‚Äî is amplifying human creativity even as accelerated computing promises significant gains in energy efficiency, NVIDIA founder and CEO Jensen Huang said Monday. That makes this week&#8217;s SIGGRAPH professional graphics conference, in Denver, the logical venue to discuss what‚Äôs next. ‚ÄúEverybody will have an	<a class="read-more" href="https://blogs.nvidia.com/blog/nvidia-ceo-siggraph/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>The generative AI revolution ‚Äî with deep roots in visual computing ‚Äî is amplifying human creativity even as accelerated computing promises significant gains in energy efficiency, NVIDIA founder and CEO Jensen Huang said Monday.</p>
<p>That makes this week&#8217;s SIGGRAPH professional graphics conference, in Denver, the logical venue to discuss what‚Äôs next.</p>
<p>‚ÄúEverybody will have an AI assistant,‚Äù Huang said. ‚ÄúEvery single company, every single job within the company, will have AI assistance.‚Äù</p>
<p>But even as generative AI promises to amplify human productivity, Huang said the accelerated computing technology that underpins it promises to make computing more energy efficient.</p>
<p>‚ÄúAccelerated computing helps you save so much energy, 20 times, 50 times, and doing the same processing,‚Äù Huang said. ‚ÄúThe first thing we have to do, as a society, is accelerate every application we can: this reduces the amount of energy being used all over the world.‚Äù</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1.png"><img loading="lazy" decoding="async" class="alignnone size-large wp-image-73389" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_A0151-blog-press-1280x680-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>The conversation follows a spate of announcements from NVIDIA today.</p>
<p><a href="https://nvidianews.nvidia.com/news/nvidia-announces-generative-ai-models-and-nim-microservices-for-openusd" target="_blank" rel="noopener">NVIDIA introduced a new suite of NIM microservices</a> tailored for diverse workflows, including OpenUSD, 3D modeling, physics, materials, robotics, industrial digital twins and physical AI.<br />
These advancements aim to enhance developer capabilities, particularly with the <a href="https://blogs.nvidia.com/blog/hugging-face-inference-nim-microservices-dgx-cloud/" target="_blank" rel="noopener">integration of Hugging Face Inference-as-a-Service on DGX Cloud</a>.</p>
<p>In addition, <a href="https://blogs.nvidia.com/blog/edify-shutterstock-getty-images/" target="_blank" rel="noopener">Shutterstock has launched a Generative 3D Service</a>, while <a href="https://blogs.nvidia.com/blog/edify-shutterstock-getty-images/" target="_blank" rel="noopener">Getty Images has upgraded its offerings using NVIDIA Edify technology</a>.</p>
<p>In the realm of AI and graphics, NVIDIA has revealed new <a href="https://blogs.nvidia.com/blog/generative-physical-ai-nim-microservices/" target="_blank" rel="noopener">OpenUSD NIM microservices and reference workflows</a> designed for generative physical AI applications.</p>
<p>This includes a <a href="https://nvidianews.nvidia.com/news/nvidia-accelerates-worldwide-humanoid-robotics-development" target="_blank" rel="noopener">program for accelerating humanoid robotics development</a> through new NIM microservices for robotics simulation and more.</p>
<p>Finally, WPP, the world&#8217;s largest advertising agency, is using <a href="https://blogs.nvidia.com/blog/coca-cola-wpp-omniverse-generative-ai/" target="_blank" rel="noopener">Omniverse-driven generative AI for The Coca-Cola Company</a>, helping drive brand authenticity, and showcasing the practical applications of NVIDIA‚Äôs advancements in AI technology across various industries.</p>
<p>Huang and Goode started their conversation by exploring how visual computing gave rise to everything from computer games to digital animation to GPU-accelerated computing and, most recently, generative AI powered by industrial-scale AI factories.</p>
<p>All these advancements build on one another. Robotics, for example, requires advanced AI and photorealistic virtual worlds where AI can be trained before being deployed into next-generation humanoid robots.</p>
<p>Huang explained that robotics requires three computers: one to train the AI, one to test the AI in a physically accurate simulation, and one within the robot itself.</p>
<p>‚ÄúJust about every industry is going to be affected by this, whether it‚Äôs scientific computing trying to do a better job predicting the weather with a lot less energy, to augmenting and collaborating with creators to generate images, or generating virtual scenes for industrial visualization,‚Äù Huang said. ‚ÄúRobotic self-driving cars are all going to be transformed by generative AI.‚Äù</p>
<p>Likewise, <a href="https://www.nvidia.com/en-us/omniverse/" target="_blank" rel="noopener">NVIDIA Omniverse systems</a> ‚Äî built around the OpenUSD standard ‚Äî will also be key to harnessing generative AI to create assets that the world‚Äôs largest brands can use.</p>
<p>By pulling from brand assets that live in Omniverse, which can capture brand assets, these systems can capture and replicate carefully curated brand magic.</p>
<p><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1.png"><img loading="lazy" decoding="async" class="alignnone size-large wp-image-73392" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/PJ_B0494-blog-press-1280x680-1.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a></p>
<p>Finally, all these systems ‚Äî visual computing, simulation and large-language models ‚Äî will come together to create digital humans who can help people interact with digital systems of all kinds.</p>
<p>‚ÄúOne of the things that we‚Äôre announcing here this week is the concept of digital agents, digital AIs that will augment every single job in the company,‚Äù Huang said.</p>
<p>‚ÄúAnd so one of the most important use cases that people are discovering is customer service,‚Äù Huang said. ‚ÄúIn the future, my guess is that it‚Äôs going to be human still, but AI in the loop.‚Äù</p>
<p>All of this, like any new tool, promises to amplify human productivity and creativity. ‚ÄúImagine the stories that you‚Äôre going to be able to tell with these tools,‚Äù Huang said.</p>
<p><iframe loading="lazy" title="What‚Äôs Next in AI: NVIDIA‚Äôs Jensen Huang Talks With WIRED‚Äôs Lauren Goode and Meta‚Äôs Mark Zuckerberg" width="500" height="281" src="https://www.youtube.com/embed/H0WxJ7caZQU?start=957&#038;feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/764A7888-blog-press-1280x680-2.png"
			type="image/png"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/764A7888-blog-press-1280x680-2-842x450.png"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[‚ÄòEverybody Will Have an AI Assistant,‚Äô NVIDIA CEO Tells SIGGRAPH Audience]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Recipe for Magic: WPP and NVIDIA Omniverse Help The Coca-Cola Company Scale Generative AI Content That Pops With Brand Authenticity</title>
		<link>https://blogs.nvidia.com/blog/coca-cola-wpp-omniverse-generative-ai/</link>
		
		<dc:creator><![CDATA[James Mills]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:49 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73259</guid>

					<description><![CDATA[When The Coca-Cola Company produces thirst-quenching marketing, the creative elements of campaigns aren‚Äôt just left to chance ‚Äî there‚Äôs a recipe for the magic. Now, the beverage company, through its partnership with WPP Open X, is beginning to scale its global campaigns with generative AI from NVIDIA Omniverse and NVIDIA NIM microservices. ‚ÄúWith NVIDIA, we	<a class="read-more" href="https://blogs.nvidia.com/blog/coca-cola-wpp-omniverse-generative-ai/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>When The Coca-Cola Company produces thirst-quenching marketing, the creative elements of campaigns aren‚Äôt just left to chance ‚Äî there‚Äôs a recipe for the magic. Now, the beverage company, through its partnership with WPP Open X, is beginning to scale its global campaigns with generative AI from <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> and <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> microservices.</p>
<p>‚ÄúWith NVIDIA, we can personalize and customize Coke and meals imagery across 100-plus markets, delivering on hyperlocal relevance with speed and at global scale,‚Äù said Samir Bhutada, global vice president of StudioX Digital Transformation at The Coca-Cola Company.</p>
<p>Coca-Cola has been working with WPP to develop <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/">digital twin</a> tools and roll out Prod X ‚Äî a custom production studio experience created specifically for the beverage maker to use globally.</p>
<p><a target="_blank" href="https://www.wpp.com/news/2024/07/wpp-to-create-generative-3d-worlds-in-collaboration-with-nvidia">WPP announced</a> today at SIGGRAPH that The Coca-Cola Company will be an early adopter for integrating the <a target="_blank" href="https://nvidianews.nvidia.com/news/nvidia-announces-generative-ai-models-and-nim-microservices-for-openusd">new NVIDIA NIM microservices</a> for Universal Scene Description (aka OpenUSD) into its Prod X roadmap. <a target="_blank" href="https://aousd.org/blog/explainer-series-what-is-openusd/">OpenUSD</a> is a 3D framework that enables interoperability between software tools and data types for building virtual worlds. NIM inference microservices provide models as optimized containers.</p>
<p>The USD Search NIM allows WPP to tap into a large archive of models to create on-brand assets, and the USD Code NIM can be used to assemble them into scenes.</p>
<p>These NIM microservices will enable Prod X users to create 3D advertising assets that contain culturally relevant elements on a global scale, using prompt engineering to quickly make adjustments to AI-generated images so that brands can better target their products at local markets.</p>
<p><iframe loading="lazy" title="BTS: how WPP is creating generative 3D worlds using NVIDIA NIM microservices" width="500" height="281" src="https://www.youtube.com/embed/n-vsX-NzHzw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Tapping Into NVIDIA NIM Microservices to Deploy Generative AI¬†</b></h2>
<p><a target="_blank" href="https://www.nvidia.com/en-us/industries/media-and-entertainment/wpp/">WPP</a> said that the NVIDIA NIM microservices will have a lasting impact on the 3D engineering and art world.</p>
<p>The <a target="_blank" href="https://build.nvidia.com/nvidia/usdsearch">USD Search NIM</a> can make WPP‚Äôs massive visual asset libraries quickly available via written prompts. The <a target="_blank" href="https://build.nvidia.com/nvidia/usdcode-llama3-70b-instruct">USD Code NIM</a> allows developers to enter prompts and get Python code to create novel 3D worlds.</p>
<p>‚ÄúThe beauty of the solution is that it compresses multiple phases of the production process into a single interface and process,‚Äù said Perry Nightingale, senior vice president of creative AI at WPP, of the new NIM microservices. ‚ÄúIt empowers artists to get more out of the technology and create better work.‚Äù</p>
<h2><b>Redefining Content Production With Production Studio</b></h2>
<p>WPP recently announced the release of <a target="_blank" href="https://www.wpp.com/news/2024/06/wpp-unveils-ai-powered-production-studio">Production Studio</a> on WPP Open, the company‚Äôs intelligent marketing operating system powered by AI. Co-developed with its production company, Hogarth, Production Studio taps into the Omniverse development platform and OpenUSD for its generative AI-enabled <a target="_blank" href="https://www.nvidia.com/en-us/use-cases/3d-product-configurator/">product configurator</a> workflows.</p>
<p>Production Studio can streamline and automate multilingual text, image and video creation, simplifying content creation for advertisers and marketers, and directly addresses the challenges advertisers continue to face in producing brand-compliant and product-accurate content at scale.</p>
<p>‚ÄúOur groundbreaking research with NVIDIA Omniverse for the past few years, and the research and development associated with having built our own core USD pipeline and decades of experience in 3D workflows, is what made it possible for us to stand up a tailored experience like this for The Coca-Cola Company,‚Äù said Priti Mhatre, managing director for strategic consulting and AI at Hogarth.</p>
<p><a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/"><i>SIGGRAPH</i></a><i> attendees can hear more about WPP‚Äôs efforts by joining the company‚Äôs session on ‚Äú</i><a target="_blank" href="https://s2024.conference-program.org/presentation/?id=ind_120&amp;sess=sess421"><i>Robotics, Generative AI, and OpenUSD: How WPP Is Building the Future of Creativity</i></a><i>.‚Äù</i></p>
<p><i>NVIDIA founder and CEO Jensen Huang will also be featured at the event in fireside chats with Meta founder and CEO Mark Zuckerberg and WIRED Senior Writer Lauren Goode. Watch the talks and other sessions from </i><a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/"><i>NVIDIA at SIGGRAPH 2024 on demand</i></a><i>.</i></p>
<p><em>Photo credit: WPP, The Coca-Cola Company</em></p>
<p><i>See </i><a target="_blank" href="https://www.nvidia.com/en-eu/about-nvidia/terms-of-service/"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/TCCC_master_08LATAM_taco_v001_1-scaled.jpg"
			type="image/jpeg"
			width="2048"
			height="1152"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/TCCC_master_08LATAM_taco_v001_1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Recipe for Magic: WPP and NVIDIA Omniverse Help The Coca-Cola Company Scale Generative AI Content That Pops With Brand Authenticity]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Reality Reimagined: NVIDIA Introduces fVDB to Build Bigger Digital Models of the World</title>
		<link>https://blogs.nvidia.com/blog/fvdb-bigger-digital-models/</link>
		
		<dc:creator><![CDATA[Ken Museth]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:47 +0000</pubDate>
				<category><![CDATA[Software]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<category><![CDATA[Universal Scene Description]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73236</guid>

					<description><![CDATA[NVIDIA announced at SIGGRAPH fVDB, a new deep-learning framework for generating AI-ready virtual representations of the real world. fVDB is built on top of OpenVDB, the industry-standard library for simulating and rendering sparse volumetric data such as water, fire, smoke and clouds. Generative physical AI, such as autonomous vehicles and robots that inhabit the real	<a class="read-more" href="https://blogs.nvidia.com/blog/fvdb-bigger-digital-models/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>NVIDIA announced at SIGGRAPH <a target="_blank" href="https://developer.nvidia.com/fvdb">fVDB</a>, a new deep-learning framework for generating AI-ready virtual representations of the real world.</p>
<p>fVDB is built on top of <a target="_blank" href="https://www.openvdb.org/">OpenVDB</a>, the industry-standard library for simulating and rendering sparse volumetric data such as water, fire, smoke and clouds.</p>
<p><a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-physical-ai/">Generative physical AI</a>, such as autonomous vehicles and robots that inhabit the real world, need to have ‚Äúspatial intelligence‚Äù ‚Äî the ability to understand and operate in 3D space.</p>
<p>Capturing the large scale and super-fine details of the world around us is essential. But converting reality into a virtual representation to train AI is hard.</p>
<p>Raw data for real-world environments can be collected through many different techniques, like neural radiance fields (<a href="https://blogs.nvidia.com/blog/neural-radiance-fields-3d-models/">NeRFs</a>) and lidar. fVDB translates this data into massive, AI-ready environments rendered in real time.</p>
<p><iframe loading="lazy" title="Train Physical AI With Real-World Simulations Using fVDB." width="500" height="281" src="https://www.youtube.com/embed/rx55wDWqSrw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>Building on a decade of innovation in the OpenVDB standard, the introduction of<a target="_blank" href="https://arxiv.org/abs/2407.01781"> fVDB</a> at SIGGRAPH represents a significant leap forward in how industries can benefit from <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/">digital twins</a> of the real world.</p>
<p>Reality-scale virtual environments are used for training autonomous agents. City-scale 3D models are captured by drones for climate science and disaster planning. Today, 3D generative AI is even used to plan urban spaces and smart cities.</p>
<p>fVDB enables industries to tap into spatial intelligence on a larger scale and with higher resolution than ever before, making physical AI even smarter.</p>
<p>The framework builds NVIDIA-accelerated AI operators on top of <a target="_blank" href="https://developer.nvidia.com/nanovdb">NanoVDB</a>, a GPU-accelerated data structure for efficient 3D simulations. These operators include convolution, pooling, attention and meshing, all of which are designed for high-performance 3D deep learning applications.</p>
<p>AI operators allow businesses to build complex neural networks for spatial intelligence, like large-scale point cloud reconstruction and 3D generative modeling.</p>
<p>fVDB is the result of a long-running effort by NVIDIA‚Äôs research team and is already used to support <a target="_blank" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>, <a target="_blank" href="https://developer.nvidia.com/drive">NVIDIA DRIVE</a> and <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/">NVIDIA Omniverse</a> projects that require high-fidelity models of large, complex real-world spaces.</p>
<h2><b>Key Advantages of fVDB</b></h2>
<ul>
<li style="font-weight: 300;" aria-level="1">Larger: 4x larger spatial scale than prior frameworks</li>
<li style="font-weight: 300;" aria-level="1">Faster: 3.5x faster than prior frameworks</li>
<li style="font-weight: 300;" aria-level="1">Interoperable: Businesses can fully tap into massive real-world datasets. fVDB reads VDB datasets into full-sized 3D environments. AI-ready and real-time rendered for building physical AI with spatial intelligence.</li>
<li style="font-weight: 300;" aria-level="1">More powerful: 10x more operators than prior frameworks. fVDB simplifies processes by combining functionalities that previously required multiple deep-learning libraries.</li>
</ul>
<p>fVDB will soon be available as <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> inference microservices. A trio of the microservices will enable businesses to incorporate fVDB into <a target="_blank" href="https://nvidianews.nvidia.com/news/nvidia-expands-openusd-to-generative-ai-for-robotics-industrial-digitalization-markets">OpenUSD workflows</a>, generating AI-ready OpenUSD geometry in NVIDIA Omniverse, a development platform for industrial digitalization and generative physical AI applications. They are:</p>
<ul>
<li style="font-weight: 300;" aria-level="1">fVDB Mesh Generation NIM ‚Äî Generates digital 3D environments of the real world</li>
<li style="font-weight: 300;" aria-level="1">fVDB NeRF-XL NIM ‚Äî Generates large-scale NeRFs in OpenUSD using Omniverse Cloud APIs</li>
<li style="font-weight: 300;" aria-level="1">fVDB Physics Super-Res NIM ‚Äî Performs super-resolution to generate an OpenUSD-based, high-resolution physics simulation</li>
</ul>
<p>Over the past decade, <a target="_blank" href="https://www.openvdb.org/about/">OpenVDB</a>, housed at the <a target="_blank" href="https://www.aswf.io/blog/fvdb/">Academy Software Foundation</a>, has earned multiple <a href="https://blogs.nvidia.com/blog/academy-awards-vfx-openusd/">Academy Awards</a> as a core technology used throughout the visual-effects industry. It has since grown beyond entertainment to industrial and scientific uses, like industrial design and robotics.</p>
<p>NVIDIA continues to enhance the open-source OpenVDB library. Four years ago, the company introduced <a target="_blank" href="https://developer.nvidia.com/nanovdb">NanoVDB</a>, which added GPU support to OpenVDB. This delivered an order-of-magnitude speed-up, enabling faster performance and easier development, and opening the door to real-time simulation and rendering.</p>
<p>Two years ago, NVIDIA introduced <a target="_blank" href="https://developer.nvidia.com/rendering-technologies/neuralvdb">NeuralVDB</a>, which builds machine learning on top of NanoVDB to compress the memory footprint of VDB volumes up to 100x, allowing creators, developers and researchers to interact with extremely large and complex datasets.</p>
<p>fVDB builds AI operators on top of NanoVDB to unlock spatial intelligence at the scale of reality. Apply to the <a target="_blank" href="https://developer.nvidia.com/fvdb/early-access-form">early-access program</a> for the fVDB PyTorch extension. fVDB will also be available as part of the <a target="_blank" href="https://github.com/AcademySoftwareFoundation/openvdb">OpenVDB GitHub repository</a>.</p>
<p><strong>Dive deeper into <a target="_blank" href="https://developer.nvidia.com/blog/building-spatial-intelligence-from-real-world-3d-data-using-deep-learning-framework-fvdb/">fVDB in this technical blog</a> and watch how accelerated computing and generative AI are transforming industries and creating new opportunities for innovation and growth in NVIDIA founder and CEO Jensen Huang‚Äôs two <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">fireside chats</a> at SIGGRAPH.</strong></p>
<p><i>See </i><a target="_blank" href="https://www.nvidia.com/en-eu/about-nvidia/terms-of-service/"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/DisasterRelief0.jpg"
			type="image/jpeg"
			width="1920"
			height="1080"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/DisasterRelief0-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Reality Reimagined: NVIDIA Introduces fVDB to Build Bigger Digital Models of the World]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>NVIDIA Supercharges Digital Marketing With Greater Control Over Generative AI</title>
		<link>https://blogs.nvidia.com/blog/nvidia-supercharges-marketing-agencies-generative-ai/</link>
		
		<dc:creator><![CDATA[James Mills]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:46 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<category><![CDATA[Universal Scene Description]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73289</guid>

					<description><![CDATA[The world‚Äôs brands and agencies are using generative AI to create advertising and marketing content, but it doesn‚Äôt always provide the desired outputs. NVIDIA offers a comprehensive set of technologies ‚Äî bringing together generative AI, NVIDIA NIM microservices, NVIDIA Omniverse and Universal Scene Description (OpenUSD) ‚Äî to allow developers to build applications and workflows that	<a class="read-more" href="https://blogs.nvidia.com/blog/nvidia-supercharges-marketing-agencies-generative-ai/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>The world‚Äôs brands and agencies are using generative AI to create advertising and marketing content, but it doesn‚Äôt always provide the desired outputs.</p>
<p>NVIDIA offers a comprehensive set of technologies ‚Äî bringing together generative AI, NVIDIA NIM microservices, NVIDIA Omniverse and <a target="_blank" href="https://aousd.org/blog/explainer-series-what-is-openusd/">Universal Scene Description</a> (OpenUSD) ‚Äî to allow developers to build applications and workflows that enable brand-accurate, targeted and efficient advertising at scale.</p>
<p>Developers can use the <a target="_blank" href="https://build.stg.ngc.nvidia.com/nvidia/usdsearch">USD Search NIM microservice to provide artists access to a vast archive of </a>OpenUSD-based, brand-approved assets ‚Äî such as products, props and environments ‚Äî <a target="_blank" href="https://build.stg.ngc.nvidia.com/nvidia/usdsearch">and</a> when integrated with the <a target="_blank" href="https://build.stg.ngc.nvidia.com/nvidia/usdcode-llama3-70b-instruct">USD Code NIM</a> microservice, assembly of these scenes can be accelerated. Teams can also use the <a target="_blank" href="https://www.nvidia.com/en-us/gpu-cloud/picasso/">NVIDIA Edify</a>-powered <a href="https://blogs.nvidia.com/blog/edify-shutterstock-getty-images/">Shutterstock Generative 3D</a> service to rapidly generate 3D new assets using AI.</p>
<p>The scenes, once constructed, can be rendered to a 2D image and used as input to direct an AI-powered image generator to create precise, brand-accurate visuals.</p>
<p>Global agencies, developers and production studios are tapping these technologies to revolutionize every aspect of the advertising process, from creative production and content supply chain to dynamic creative optimization.</p>
<p><a target="_blank" href="https://www.wpp.com/news/2024/07/wpp-to-create-generative-3d-worlds-in-collaboration-with-nvidia">WPP announced at SIGGRAPH</a> its adoption of the technologies, <a href="https://blogs.nvidia.com/blog/coca-cola-wpp-omniverse-generative-ai">naming The Coca-Cola Company</a> the first brand to embrace generative AI with Omniverse and NVIDIA NIM microservices.</p>
<p><iframe loading="lazy" title="BTS: how WPP is creating generative 3D worlds using NVIDIA NIM microservices" width="500" height="281" src="https://www.youtube.com/embed/n-vsX-NzHzw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Agencies and Service Providers Increase Adoption of Omniverse</b></h2>
<p>The NVIDIA Omniverse development platform has seen widespread adoption for its ability to build accurate <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/">digital twins</a> of products. These virtual replicas allow brands and agencies to create ultra-photorealistic and physically accurate 3D <a target="_blank" href="https://www.nvidia.com/en-us/use-cases/3d-product-configurator/">product configurators</a>, helping to increase personalization, customer engagement and loyalty, and average selling prices, and reducing return rates.</p>
<p>Digital twins can also serve many purposes and be updated to meet shifting consumer preferences with minimal time, cost and effort, helping flexibly scale content production.</p>
<h2><b>Agencies and Service Providers Increase Adoption of Omniverse</b></h2>
<p>The NVIDIA Omniverse development platform has seen widespread adoption for its ability to build accurate <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/">digital twins</a> of products. These virtual replicas allow brands and agencies to create ultra-photorealistic and physically accurate 3D <a target="_blank" href="https://www.nvidia.com/en-us/use-cases/3d-product-configurator/">product configurators</a>, helping to increase personalization, customer engagement and loyalty, and average selling prices, and reducing return rates.</p>
<p>Digital twins can also serve many purposes and be updated to meet shifting consumer preferences with minimal time, cost and effort, helping flexibly scale content production.</p>
<p>&nbsp;</p>
<figure id="attachment_73362" aria-describedby="caption-attachment-73362" style="width: 500px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="size-full wp-image-73362" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/monks-hatch.gif" alt="" width="500" height="500" /><figcaption id="caption-attachment-73362" class="wp-caption-text">Image courtesy of Monks, Hatch.</figcaption></figure>
<p>Global marketing and technology services company Monks developed Monks.Flow, an AI-centric professional managed service that uses the Omniverse platform to help brands virtually explore different customizable product designs and unlock scale and hyper-personalization across any customer journey.</p>
<p>‚ÄúNVIDIA Omniverse and OpenUSD‚Äôs interoperability accelerates connectivity between marketing, technology and product development,‚Äù said Lewis Smithingham, executive vice president of strategic industries at Monks. ‚ÄúCombining Omniverse with Monks‚Äô streamlined marketing and technology services, we infuse AI throughout the product development pipeline and help accelerate technological and creative possibilities for clients.‚Äù</p>
<p>Collective World, a creative and technology company, is an early adopter of real-time 3D, OpenUSD and NVIDIA Omniverse, using them to create high-quality digital campaigns for customers like Unilever and EE. The technologies allow Collective to develop digital twins, delivering consistent, high-quality product content at scale to streamline advertising and marketing campaigns.</p>
<p>Building on its use of NVIDIA technologies, Collective World announced at SIGGRAPH that it has joined the <a target="_blank" href="https://www.nvidia.com/en-us/about-nvidia/partners/">NVIDIA Partner Network</a>.</p>
<figure id="attachment_73356" aria-describedby="caption-attachment-73356" style="width: 672px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="wp-image-73356 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-672x378.jpg" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse-1280x720.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/collective-tool-built-on-omniverse.jpg 1999w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73356" class="wp-caption-text">Product digital twin configurator and content generation tool built by Collective on NVIDIA Omniverse.</figcaption></figure>
<p>INDG is using Omniverse to introduce new capabilities into Grip, its popular software tool. Grip uses OpenUSD and generative AI to streamline and enhance the creation process, delivering stunning, high-fidelity marketing content faster than ever.</p>
<p>‚ÄúThis integration helps bring significant efficiencies to every brand by delivering seamless interoperability and enabling real-time visualization,‚Äù said Frans Vriesendorp, CEO of INDG. ‚ÄúHarnessing the potential of USD to eliminate the lock-in to proprietary formats, the combination of Grip and Omniverse is helping set new standards in the realm of digital content creation.‚Äù</p>
<figure id="attachment_73359" aria-describedby="caption-attachment-73359" style="width: 672px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="wp-image-73359 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-1536x864.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image-1280x720.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/grip-beirsdorf-image.png 1999w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73359" class="wp-caption-text">Image generated with Grip, copyright Beiersdorf</figcaption></figure>
<p>To get started building applications and services using OpenUSD, Omniverse and NVIDIA AI, check out the <a target="_blank" href="https://www.nvidia.com/en-us/use-cases/3d-product-configurator/">product configurator developer resources</a> and the <a target="_blank" href="https://resources.nvidia.com/en-us-omniverse-product-configurator/ov-genai-workflow-reference-architecture?xs=674987&amp;ncid=no-ncid">generative AI workflow for content creation reference architecture</a>, or submit a <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/contact/">contact form</a> to learn more or connect with NVIDIA‚Äôs ecosystem of service providers.</p>
<p><i>Watch NVIDIA founder and CEO Jensen Huang‚Äôs fireside chats, as well as other on-demand sessions from </i><a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/"><i>NVIDIA at SIGGRAPH</i></a><i>.</i></p>
<p><i>Stay up to date by subscribing to our </i><a target="_blank" href="https://nvda.ws/3u5KPv1"><i>newsletter</i></a><i>, and following NVIDIA Omniverse on </i><a target="_blank" href="https://www.instagram.com/nvidiaomniverse/"><i>Instagram</i></a><i>, </i><a target="_blank" href="https://www.linkedin.com/showcase/nvidia-omniverse/"><i>LinkedIn</i></a><i>, </i><a target="_blank" href="https://medium.com/@nvidiaomniverse"><i>Medium</i></a><i> and </i><a target="_blank" href="https://twitter.com/nvidiaomniverse"><i>X</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/digital-marketing-agencies-featured-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/digital-marketing-agencies-featured-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[NVIDIA Supercharges Digital Marketing With Greater Control Over Generative AI]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>New NVIDIA Digital Human Technologies Enhance Customer Interactions Across Industries</title>
		<link>https://blogs.nvidia.com/blog/digital-humans-siggraph-2024/</link>
		
		<dc:creator><![CDATA[Ike Nnoli]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:41 +0000</pubDate>
				<category><![CDATA[Corporate]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Maxine]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73273</guid>

					<description><![CDATA[Generative AI is unlocking new ways for enterprises to engage customers through digital human avatars. At SIGGRAPH, NVIDIA previewed &#8220;James,&#8221; an interactive digital human that can connect with people using emotions, humor and more. James is based on a customer-service workflow using NVIDIA ACE, a reference design for creating custom, hyperrealistic, interactive avatars. Users can	<a class="read-more" href="https://blogs.nvidia.com/blog/digital-humans-siggraph-2024/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-ai/">Generative AI</a> is unlocking new ways for enterprises to engage customers through digital human avatars.</p>
<p>At <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">SIGGRAPH</a>, NVIDIA previewed &#8220;James,&#8221; an interactive digital human that can connect with people using emotions, humor and more. James is based on a customer-service workflow using <a target="_blank" href="https://developer.nvidia.com/ace">NVIDIA ACE</a>, a reference design for creating custom, hyperrealistic, interactive avatars.</p>
<p><span>Users can interact with James in real time at </span><a target="_blank" href="https://build.nvidia.com/nvidia/digital-humans-virtual-assistant"><span>ai.nvidia.com</span></a><span>.¬†</span></p>
<p>NVIDIA also showcased at the computer graphics conference the latest advancements to the <a target="_blank" href="https://developer.nvidia.com/maxine">NVIDIA Maxine AI platform</a>, including Maxine 3D and Audio2Face-2D for an immersive telepresence experience.</p>
<p>Developers can use Maxine and NVIDIA ACE digital human technologies to make customer interactions with digital interfaces more engaging and natural. ACE technologies enable digital human development with AI models for speech and translation, vision, intelligence, lifelike animation and behavior, and realistic appearance.</p>
<p>Companies across industries are using Maxine and ACE to deliver immersive virtual customer experiences.</p>
<h2><b>Meet James, a Digital Brand Ambassador</b></h2>
<div style="width: 1280px;" class="wp-video"><video class="wp-video-shortcode" id="video-73273-3" width="1280" height="720" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/AceCut-2.mp4?_=3" /><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/AceCut-2.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/07/AceCut-2.mp4</a></video></div>
<p>Built on top of <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> microservices, James is a virtual assistant that can provide contextually accurate responses.</p>
<p>Using retrieval-augmented generation (<a href="https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/">RAG</a>), James can accurately tell users about the latest NVIDIA technologies. ACE allows developers to use their own data to create domain-specific avatars that can communicate relevant information to customers.</p>
<p>James is powered by the latest <a target="_blank" href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX</a> rendering technologies for advanced, lifelike animations. His natural-sounding voice is powered by <a target="_blank" href="https://elevenlabs.io/blog/nvidia-ace-at-computex">ElevenLabs</a>. NVIDIA ACE lets developers customize animation, voice and language when building avatars tailored for different use cases.</p>
<h2><b>NVIDIA Maxine Enhances Digital Humans in Telepresence</b></h2>
<p>Maxine, a platform for deploying cutting-edge AI features that enhance the audio and video quality of digital humans, enables the use of real-time, photorealistic 2D and 3D avatars with video-conferencing devices.</p>
<p>Maxine 3D converts 2D video portrait inputs into 3D avatars, allowing the integration of highly realistic digital humans in video conferencing and other two-way communication applications. The technology will soon be available in early access.</p>
<p>Audio2Face-2D, currently in early access, animates static portraits based on audio input, creating dynamic, speaking digital humans from a single image. Try the technology at <a target="_blank" href="https://build.nvidia.com/nvidia/audio2face-2d">ai.nvidia.com</a>.</p>
<h2><b>Companies Embracing Digital Human Applications</b></h2>
<p>HTC, Looking Glass, Reply and UneeQ are among the latest companies using NVIDIA ACE and Maxine across a broad range of use cases, including customer service agents, and telepresence experiences in entertainment, retail and hospitality.</p>
<p>At SIGGRAPH, digital human technology developer <a target="_blank" href="http://www.digitalhumans.com/blog/uneeq-joining-nvidia-to-showcase-next-generation-digital-humans-at-siggraph-2024">UneeQ</a> is showcasing two new demos.</p>
<p>The first spotlights cloud-rendered digital humans powered by NVIDIA GPUs with local, in-browser computer vision for enhanced scalability and privacy, and animated using the Audio2Face-3D NVIDIA NIM microservice. UneeQ‚Äôs Synapse technology processes anonymized user data and feeds it to a <a target="_blank" href="https://www.nvidia.com/en-us/glossary/large-language-models/">large language model</a> (LLM) for more accurate, responsive interactions.</p>
<p>The second demo runs on a single NVIDIA RTX GPU-powered laptop, featuring an advanced digital human powered by Gemma 7B LLM, RAG and the NVIDIA Audio2Face-3D NIM microservice.</p>
<p>Both demos showcase UneeQ‚Äôs NVIDIA-powered efforts to develop digital humans that can react to users‚Äô facial expressions and actions, pushing the boundaries of realism in virtual customer service experiences.</p>
<p><iframe loading="lazy" title="UneeQ and NVIDIA showcase next-generation digital humans driven by Synanim and Audio2Face" width="500" height="281" src="https://www.youtube.com/embed/LD4ak_O0oLk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p><a target="_blank" href="https://blog.vive.com/us/htc-and-nvidia-partner-at-siggraph-elevating-viverse-avatar-interactions-with-audio2face-technology">HTC Viverse</a> has integrated the Audio2Face-3D NVIDIA NIM microservice into its VIVERSE AI agent for dynamic facial animation and lip sync, allowing for more natural and immersive user interactions.</p>
<p>Hologram technology company <a target="_blank" href="https://blog.lookingglassfactory.com/p/b3e1da47-f231-4fb6-8ee0-1fef3818ff2f/">Looking Glass</a>‚Äô Magic Mirror demo at SIGGRAPH uses a simple camera setup and Maxine‚Äôs advanced 3D AI capabilities to generate a real-time holographic feed of users‚Äô faces on its newly launched, group-viewable Looking Glass 16-inch and 32-inch Spatial Displays.</p>
<p><a target="_blank" href="https://www.reply.com/en">Reply</a> is unveiling an enhanced version of <a target="_blank" href="https://www.reply.com/en/artificial-intelligence/futura-the-ai-driven-tour-expert">Futura</a>, its cutting-edge digital human developed for Costa Crociere‚Äôs Costa Smeralda cruise ship. Powered by <a target="_blank" href="https://build.nvidia.com/nvidia/audio2face">Audio2Face-3D NVIDIA NIM</a> and <a target="_blank" href="https://build.nvidia.com/nvidia/parakeet-ctc-riva">Riva ASR NIM</a> microservices, Futura‚Äôs speech-synthesis capabilities tap advanced technologies including GPT-4o, LlamaIndex for RAG and Microsoft Azure text-to-speech services.</p>
<p>Futura also incorporates Reply‚Äôs proprietary <a target="_blank" href="https://www.reply.com/en/artificial-intelligence/affective-computing">affective computing technology</a>, alongside Hume AI and MorphCast, for comprehensive emotion recognition. Built using Unreal Engine 5.4.3 and MetaHuman Creator with NVIDIA ACE-powered facial animation, Futura supports six languages. The intelligent assistant can help plan personalized port visits, suggest tailored itineraries and facilitate tour bookings.</p>
<p>In addition, Futura refines recommendations based on guest feedback and uses a specially created knowledge base to provide informative city presentations, enhancing tourist itineraries. Futura aims to enhance customer service and offer immersive interactions in real-world scenarios, leading to streamlined operations and driving business growth.</p>
<p><i>Learn more about </i><a target="_blank" href="https://developer.nvidia.com/ace"><i>NVIDIA ACE</i></a><i> and </i><a target="_blank" href="https://developer.nvidia.com/maxine"><i>NVIDIA Maxine</i></a><i>.¬†</i></p>
<p><i>Discover how accelerated computing and generative AI are transforming industries and creating new opportunities for innovation by watching NVIDIA founder and CEO Jensen Huang‚Äôs </i><a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/"><i>fireside chats</i></a><i> at SIGGRAPH.</i></p>
<p><i>See </i><a target="_blank" href="https://www.nvidia.com/en-eu/about-nvidia/terms-of-service/"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2024/07/AceCut-2.mp4" length="400181" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/sigg24-social-ace-kv-1200x628-1.jpg"
			type="image/jpeg"
			width="1200"
			height="628"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/sigg24-social-ace-kv-1200x628-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[New NVIDIA Digital Human Technologies Enhance Customer Interactions Across Industries]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Hugging Face Offers Developers Inference-as-a-Service Powered by NVIDIA NIM</title>
		<link>https://blogs.nvidia.com/blog/hugging-face-inference-nim-microservices-dgx-cloud/</link>
		
		<dc:creator><![CDATA[Alexis Bjorlin]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:41 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[DGX Cloud]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73334</guid>

					<description><![CDATA[One of the world‚Äôs largest AI communities ‚Äî comprising 4 million developers on the Hugging Face platform ‚Äî is gaining easy access to NVIDIA-accelerated inference on some of the most popular AI models. New inference-as-a-service capabilities will enable developers to rapidly deploy leading large language models such as the Llama 3 family and Mistral AI	<a class="read-more" href="https://blogs.nvidia.com/blog/hugging-face-inference-nim-microservices-dgx-cloud/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>One of the world‚Äôs largest AI communities ‚Äî comprising 4 million developers on the Hugging Face platform ‚Äî is gaining easy access to NVIDIA-accelerated inference on some of the most popular AI models.</p>
<p>New inference-as-a-service capabilities will enable developers to rapidly deploy leading <a target="_blank" href="https://www.nvidia.com/en-us/glossary/large-language-models/">large language models</a> such as the Llama 3 family and Mistral AI models with optimization from <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> microservices running on <a target="_blank" href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a>.</p>
<p>Announced today at the <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">SIGGRAPH</a> conference, the service will help developers quickly prototype with open-source AI models hosted on the Hugging Face Hub and deploy them in production. Enterprise Hub users can tap serverless inference for increased flexibility, minimal infrastructure overhead and optimized performance with NVIDIA NIM.</p>
<p>The inference service complements <a target="_blank" href="https://nvidianews.nvidia.com/news/nvidia-and-hugging-face-to-connect-millions-of-developers-to-generative-ai-supercomputing">Train on DGX Cloud</a>, an AI training service already available on Hugging Face.</p>
<p>Developers facing a growing number of open-source models can benefit from a hub where they can easily compare options. These training and inference tools give Hugging Face developers new ways to experiment with, test and deploy cutting-edge models on NVIDIA-accelerated infrastructure. They‚Äôre made easily accessible using the ‚ÄúTrain‚Äù and ‚ÄúDeploy‚Äù drop-down menus on Hugging Face model cards, letting users get started with just a few clicks.</p>
<p>Get started with <a href="https://huggingface.co/blog/inference-dgx-cloud" target="_blank" rel="noopener">inference-as-a-service powered by NVIDIA NIM</a>.</p>
<p><b>Beyond a Token Gesture ‚Äî NVIDIA NIM Brings Big Benefits</b></p>
<p><a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a> is a collection of AI microservices ‚Äî including NVIDIA AI foundation models and open-source community models ‚Äî optimized for inference using industry-standard application programming interfaces, or APIs.</p>
<p>NIM offers users higher efficiency in processing tokens ‚Äî the units of data used and generated by a language model. The optimized microservices also improve the efficiency of the underlying NVIDIA DGX Cloud infrastructure, which can increase the speed of critical AI applications.</p>
<p>This means developers see faster, more robust results from an AI model accessed as a NIM compared with other versions of the model. The 70-billion-parameter version of Llama 3, for example, delivers up to 5x higher throughput when accessed as a NIM compared with off-the-shelf deployment on <a target="_blank" href="https://www.nvidia.com/en-us/data-center/h100/">NVIDIA H100 Tensor Core GPU-powered systems</a>.</p>
<p><b>Near-Instant Access to DGX Cloud Provides Accessible AI Acceleration</b></p>
<p>The NVIDIA DGX Cloud platform is purpose-built for <a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a>, offering developers easy access to reliable accelerated computing infrastructure that can help them bring production-ready applications to market faster.</p>
<p>The platform provides scalable GPU resources that support every step of AI development, from prototype to production, without requiring developers to make long-term AI infrastructure commitments.</p>
<p>Hugging Face inference-as-a-service on NVIDIA DGX Cloud powered by NIM microservices offers easy access to compute resources that are optimized for AI deployment, enabling users to experiment with the latest AI models in an enterprise-grade environment.</p>
<p><b>More on NVIDIA NIM at SIGGRAPH¬†</b></p>
<p>At SIGGRAPH, NVIDIA also introduced <a target="_blank" href="https://nvidianews.nvidia.com/news/nvidia-announces-generative-ai-models-and-nim-microservices-for-openusd">generative AI models and NIM microservices for the OpenUSD framework</a> to accelerate developers‚Äô abilities to build highly accurate virtual worlds for the next evolution of AI.</p>
<p>To experience more than 100 NVIDIA NIM microservices with applications across industries, visit <a target="_blank" href="http://ai.nvidia.com/">ai.nvidia.com</a>.</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-hugging-face-logos.jpg"
			type="image/jpeg"
			width="1920"
			height="1080"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/nvidia-hugging-face-logos-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Hugging Face Offers Developers Inference-as-a-Service Powered by NVIDIA NIM]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>AI Gets Physical: New NVIDIA NIM Microservices Bring Generative AI to Digital Environments</title>
		<link>https://blogs.nvidia.com/blog/generative-physical-ai-nim-microservices/</link>
		
		<dc:creator><![CDATA[Adam Scraba]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:23 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[Robotics]]></category>
		<category><![CDATA[Computer Vision]]></category>
		<category><![CDATA[Digital Twin]]></category>
		<category><![CDATA[Industrial and Manufacturing]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<category><![CDATA[Omniverse]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<category><![CDATA[Smart Spaces]]></category>
		<category><![CDATA[Synthetic Data Generation]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73301</guid>

					<description><![CDATA[Millions of people already use generative AI to assist in writing and learning. Now, the technology can also help them more effectively navigate the physical world. NVIDIA announced at SIGGRAPH generative physical AI advancements including the NVIDIA Metropolis reference workflow for building interactive visual AI agents and new NVIDIA NIM microservices that will help developers	<a class="read-more" href="https://blogs.nvidia.com/blog/generative-physical-ai-nim-microservices/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Millions of people already use <a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-ai/">generative AI</a> to assist in writing and learning. Now, the technology can also help them more effectively navigate the physical world.</p>
<p>NVIDIA announced at SIGGRAPH <a target="_blank" href="https://www.nvidia.com/en-us/glossary/generative-physical-ai">generative physical AI</a> advancements including the <a target="_blank" href="https://www.nvidia.com/en-us/use-cases/visual-ai-agents/">NVIDIA Metropolis reference workflow</a> for building interactive visual AI agents and new NVIDIA NIM microservices that will help developers train physical machines and improve how they handle complex tasks.</p>
<p>These include <a href="https://blogs.nvidia.com/blog/fvdb-bigger-digital-models/">three fVDB NIM microservices</a> that support NVIDIA‚Äôs new deep learning framework for 3D worlds, as well as the <a target="_blank" href="https://nvidianews.nvidia.com/news/nvidia-expands-openusd-to-generative-ai-for-robotics-industrial-digitalization-markets">USD Code, USD Search and USD Validate NIM microservices</a> for working with Universal Scene Description (aka <a target="_blank" href="https://aousd.org/blog/explainer-series-what-is-openusd/">OpenUSD</a>).</p>
<p>The NVIDIA OpenUSD NIM microservices work together with the world‚Äôs first generative AI models for OpenUSD development ‚Äî also developed by NVIDIA ‚Äî to enable developers to <a target="_blank" href="https://developer.nvidia.com/blog/integrate-generative-ai-into-openusd-workflows-using-new-nvidia-omniverse-developer-tools/">incorporate generative AI copilots and agents into USD workflows</a> and broaden the possibilities of 3D worlds.</p>
<h2><b>NVIDIA NIM Microservices Transform Physical AI Landscapes</b></h2>
<p>Physical AI uses advanced simulations and learning methods to help robots and other industrial automation more effectively perceive, reason and navigate their surroundings. The technology is transforming industries like manufacturing and healthcare, and advancing smart spaces with robots, factory and warehouse technologies, surgical AI agents and cars that can operate more autonomously and precisely.</p>
<p>NVIDIA offers a broad range of <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NIM microservices</a> customized for specific models and industry domains. NVIDIA‚Äôs suite of NIM microservices tailored for physical AI supports capabilities for speech and translation, vision and intelligence, and realistic animation and behavior.</p>
<h2><b>Turning Visual AI Agents Into Visionaries With NVIDIA NIM</b></h2>
<p><a target="_blank" href="https://www.nvidia.com/en-us/use-cases/visual-ai-agents/">Visual AI agents</a> use computer vision capabilities to perceive and interact with the physical world and perform reasoning tasks.</p>
<p>Highly perceptive and interactive visual AI agents are powered by a new class of generative AI models called <a target="_blank" href="https://build.nvidia.com/explore/vision">vision language models (VLMs)</a>, which bridge digital perception and real-world interaction in physical AI workloads to enable enhanced decision-making, accuracy, interactivity and performance. With VLMs, developers can build vision AI agents that can more effectively handle challenging tasks, even in complex environments.</p>
<p>Generative AI-powered visual AI agents are rapidly being deployed across hospitals, factories, warehouses, retail stores, airports, traffic intersections and more.</p>
<p>To help physical AI developers more easily build high-performing, custom visual AI agents, NVIDIA offers NIM microservices and reference workflows for physical AI. The NVIDIA Metropolis reference workflow provides a simple, structured approach for customizing, building and deploying visual AI agents, as detailed in <a target="_blank" href="https://developer.nvidia.com/blog/build-vlm-powered-visual-ai-agents-using-nvidia-nim-and-nvidia-via-microservices/">the blog</a>.</p>
<h2><b>NVIDIA NIM Helps</b><b> K2K </b><b>Make Palermo More Efficient, Safe and Secure</b></h2>
<p>City traffic managers in Palermo, Italy, deployed visual AI agents using NVIDIA NIM to uncover physical insights that help them better manage roadways.</p>
<p>K2K, an <a target="_blank" href="https://nvidia.com/en-us/autonomous-machines/intelligent-video-analytics-platform/">NVIDIA Metropolis</a> partner, is leading the effort, integrating NVIDIA NIM microservices and VLMs into AI agents that analyze the city‚Äôs live traffic cameras in real time. City officials can ask the agents questions in natural language and receive fast, accurate insights on street activity and suggestions on how to improve the city‚Äôs operations, like adjusting traffic light timing.</p>
<p><img loading="lazy" decoding="async" class="alignnone wp-image-73295 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-1536x864.png 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-800x450.png 800w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-382x215.png 382w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui-1280x720.png 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceui.png 1600w" sizes="(max-width: 672px) 100vw, 672px" /></p>
<p>Leading global electronics giants <a href="https://blogs.nvidia.com/blog/foxconn-digital-twin-ai/">Foxconn</a> and <a href="https://blogs.nvidia.com/blog/computex-metropolis-nim/">Pegatron</a> have adopted physical AI, NIM microservices and Metropolis reference workflows to more efficiently design and run their massive manufacturing operations.</p>
<p>The companies are building <a target="_blank" href="https://www.nvidia.com/en-us/use-cases/ai-for-virtual-factory-solutions/">virtual factories</a> in simulation to save significant time and costs. They‚Äôre also running more thorough tests and refinements for their physical AI ‚Äî including AI multi-camera and visual AI agents ‚Äî in digital twins before real-world deployment, improving worker safety and leading to operational efficiencies.</p>
<h2><b>Bridging the Simulation-to-Reality Gap With Synthetic Data Generation</b></h2>
<p>Many AI-driven businesses are now adopting a ‚Äúsimulation-first‚Äù approach for generative physical AI projects involving real-world industrial automation.</p>
<p>Manufacturing, factory logistics and robotics companies need to manage intricate human-worker interactions, advanced facilities and expensive equipment. NVIDIA physical AI software, tools and platforms ‚Äî including physical AI and VLM NIM microservices, reference workflows and <a target="_blank" href="https://developer.nvidia.com/fVDB">fVDB</a> ‚Äî can help them streamline the highly complex engineering required to create digital representations or virtual environments that accurately mimic real-world conditions.</p>
<p>VLMs are seeing widespread adoption across industries because of their ability to generate highly realistic imagery. However, these models can be challenging to train because of the immense volume of data required to create an accurate physical AI model.</p>
<p><a href="https://blogs.nvidia.com/blog/what-is-synthetic-data/">Synthetic data</a> generated from <a target="_blank" href="https://www.nvidia.com/en-us/omniverse/solutions/digital-twins/">digital twins</a> using computer simulations offers a powerful alternative to real-world datasets, which can be expensive ‚Äî and sometimes impossible ‚Äî to acquire for model training, depending on the use case.</p>
<p>Tools like NVIDIA NIM microservices and <a target="_blank" href="https://docs.omniverse.nvidia.com/extensions/latest/ext_replicator.html">Omniverse Replicator</a> let developers <a target="_blank" href="https://developer.nvidia.com/blog/how-to-build-a-generative-ai-enabled-synthetic-data-pipeline-with-openusd/">build generative AI-enabled synthetic data pipelines</a> to accelerate the creation of robust, diverse datasets for training physical AI. This enhances the adaptability and performance of models such as VLMs, enabling them to generalize more effectively across industries and use cases.</p>
<h2><b>Availability</b></h2>
<p>Developers can access state-of-the-art, open and NVIDIA-built foundation AI models and NIM microservices at <a target="_blank" href="http://ai.nvidia.com">ai.nvidia.com</a>. The Metropolis NIM reference workflow is available in the <a target="_blank" href="https://github.com/NVIDIA/metropolis-nim-workflows/tree/main">GitHub repository</a>, and Metropolis VIA microservices are available for download in <a target="_blank" href="https://developer.nvidia.com/visual-insight-agent-early-access">developer preview</a>.</p>
<p>OpenUSD NIM microservices are available in preview through the <a target="_blank" href="http://ai.nvidia.com">NVIDIA API catalog</a>.</p>
<p>Watch how accelerated computing and generative AI are transforming industries and creating new opportunities for innovation and growth in NVIDIA founder and CEO Jensen Huang‚Äôs <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">fireside chats</a> at SIGGRAPH.</p>
<p><i>See </i><a target="_blank" href="https://nam11.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fabout-nvidia%2Flegal-info%2F&amp;data=05%7C02%7Clpham%40nvidia.com%7Cd59f2f66f51e4deaac8008dc94b3ef0f%7C43083d15727340c1b7db39efd9ccc17a%7C0%7C0%7C638548747745016311%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&amp;sdata=dm8Os%2B4LtHW2ehZrPaxn38bsutMQBDeUdQuxrIa2y1Y%3D&amp;reserved=0"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceblog.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/physicalaimicroserviceblog-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[AI Gets Physical: New NVIDIA NIM Microservices Bring Generative AI to Digital Environments]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>For Your Edification: Shutterstock Releases Generative 3D, Getty Images Upgrades Service Powered by NVIDIA</title>
		<link>https://blogs.nvidia.com/blog/edify-shutterstock-getty-images/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Mon, 29 Jul 2024 20:30:04 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[Customer Stories]]></category>
		<category><![CDATA[DGX Cloud]]></category>
		<category><![CDATA[Events]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[NVIDIA NIM]]></category>
		<category><![CDATA[SIGGRAPH 2024]]></category>
		<category><![CDATA[Simulation and Design]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73113</guid>

					<description><![CDATA[Designers and artists have new and improved ways to boost their productivity with generative AI trained on licensed data. Shutterstock, a leading platform for creative content, launched its Generative 3D service in commercial beta. It lets creators quickly prototype 3D assets and generate 360 HDRi backgrounds that light scenes, using just text or image prompts.	<a class="read-more" href="https://blogs.nvidia.com/blog/edify-shutterstock-getty-images/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>Designers and artists have new and improved ways to boost their productivity with generative AI trained on licensed data.</p>
<p>Shutterstock, a leading platform for creative content, launched its <a target="_blank" href="https://www.shutterstock.com/discover/generative-ai-3d">Generative 3D service</a> in commercial beta. It lets creators quickly prototype 3D assets and generate 360 HDRi backgrounds that light scenes, using just text or image prompts.</p>
<p>Getty Images, a premier visual content creator and marketplace, turbocharged its <a target="_blank" href="https://www.gettyimages.com/ai/generation/about">Generative AI by Getty Images</a> service so it creates images twice as fast, improves output quality, brings advanced controls and enables fine-tuning.</p>
<p>The services are built with NVIDIA‚Äôs visual AI foundry using <a target="_blank" href="https://www.nvidia.com/en-us/gpu-cloud/picasso/">NVIDIA Edify</a>, a multimodal generative AI architecture. The AI models are then optimized and packaged for maximum performance with <a target="_blank" href="https://www.nvidia.com/en-us/ai/">NVIDIA NIM</a>, a set of accelerated microservices for AI inference.</p>
<p>Edify enables service providers to train responsible generative models on their licensed data and scale them quickly with <a target="_blank" href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a>, the cloud-first way to get the best of NVIDIA AI.</p>
<h2><b>Generative AI Speeds 3D Modeling</b></h2>
<p>Available now for enterprises in commercial beta, Shutterstock‚Äôs service lets designers and artists quickly create 3D objects that help them prototype or populate virtual environments. For example, tapping generative AI, they can quickly create the silverware and plates on a dining room table so they can focus on designing the characters around it.</p>
<p>The 3D assets the service generates are ready to edit using digital content creation tools, and available in a variety of popular file formats. Their clean geometry and layout gives artists an advanced starting point for adding their own flair.</p>
<div class="mceTemp"></div>
<figure id="attachment_73330" aria-describedby="caption-attachment-73330" style="width: 1080px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="wp-image-73330 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/chairmesh.gif" alt="" width="1080" height="608" /><figcaption id="caption-attachment-73330" class="wp-caption-text">An example of a 3D mesh from Shutterstock Generative 3D.</figcaption></figure>
<p>The AI model first delivers a preview of a single asset in as little as 10 seconds. If users like it, the preview can be turned into a higher-quality 3D asset, complete with physically based rendering materials like concrete, wood or leather.</p>
<p>At this year‚Äôs <a target="_blank" href="https://s2024.siggraph.org/">SIGGRAPH</a> computer graphics conference, designers will see just how fast they can make their ideas come to life.</p>
<p>Shutterstock will demo a workflow in Blender that lets artists generate objects directly within their 3D environment. In the Shutterstock booth at SIGGRAPH, HP will show 3D prints and physical prototypes of the kinds of assets attendees can design on the show floor using Generative 3D.</p>
<p>Shutterstock is also working with global marketing and communications services company <a target="_blank" href="https://www.nvidia.com/en-us/industries/media-and-entertainment/wpp/">WPP</a> to bring ideas to life with Edify 3D generation for virtual production (see video below).</p>
<p><iframe loading="lazy" title="NVIDIA Shutterstock Gen3D" src="https://player.vimeo.com/video/983328464?h=2a6e6255af&amp;dnt=1&amp;app_id=122963" width="500" height="281" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write"></iframe></p>
<p>Explore Generative 3D by Shutterstock on the company‚Äôs <a target="_blank" href="https://www.shutterstock.com/discover/generative-ai-3d">website</a>, or test-drive the application programming interface (API) at <a target="_blank" href="https://build.nvidia.com/shutterstock/edify-3d">build.nvidia.com</a>.</p>
<h2><b>Virtual Lighting Gets Real</b></h2>
<p>Lighting a virtual scene with accurate reflections can be a complicated task. Creatives need to operate expensive 360-degree camera rigs and go on set to create backgrounds from scratch, or search vast libraries for something that approximates what they want.</p>
<p>With Shutterstock‚Äôs Generative 3D service, users can now simply describe the exact environment they need in text or with an image, and out comes a high-dynamic-range panoramic image, aka 360 HDRi, in brilliant 16K resolution. (See video below.)</p>
<div style="width: 960px;" class="wp-video"><video class="wp-video-shortcode" id="video-73113-4" width="960" height="540" loop="1" autoplay="1" preload="metadata" controls="controls"><source type="video/mp4" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/3_HDRi_SD_30fps_fast.mp4?_=4" /><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/3_HDRi_SD_30fps_fast.mp4">https://blogs.nvidia.com/wp-content/uploads/2024/07/3_HDRi_SD_30fps_fast.mp4</a></video></div>
<p>Want that beautiful new sports car shown in a desert, a tropical beach or maybe on a winding mountain road? With generative AI, designers can shift gears fast.</p>
<p>Three companies plan to integrate Shutterstock‚Äôs 360 HDRi APIs directly into their workflows ‚Äî WPP, CGI studio Katana and Dassault Syst√®mes, developer of the 3DEXCITE applications for creating high-end visualizations and 3D content for virtual worlds.</p>
<figure id="attachment_73327" aria-describedby="caption-attachment-73327" style="width: 1400px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="size-full wp-image-73327" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/genaibygettyimages.gif" alt="" width="1400" height="700" /><figcaption id="caption-attachment-73327" class="wp-caption-text">Examples from Generative AI by Getty Images.</figcaption></figure>
<h2><b>Great Images Get a Custom Fit</b></h2>
<p>Generative AI by Getty Images has upgraded to a more powerful Edify AI model with a portfolio of new features that let artists control image composition and style.</p>
<p>Want a red beach ball floating above that perfect shot of a coral reef in Fiji? Getty Images‚Äô service can get it done in a snap.</p>
<p>The new model is twice as fast, boosts image quality and prompt accuracy, and lets users control camera settings like the depth of field or focal length of a shot. Users can generate four images in about six seconds and scale them up to 4K resolution.</p>
<figure id="attachment_73324" aria-describedby="caption-attachment-73324" style="width: 1400px" class="wp-caption alignnone"><img loading="lazy" decoding="async" class="size-full wp-image-73324" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls.jpg" alt="An example of the camera controls in Generative AI by Getty Images." width="1400" height="700" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls.jpg 1400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Gen-AI-by-Getty-Images-2-camera-controls-1280x640.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /><figcaption id="caption-attachment-73324" class="wp-caption-text">An example of the camera controls in Generative AI by Getty Images.</figcaption></figure>
<p>In addition, the commercially safe <a href="https://blogs.nvidia.com/blog/what-are-foundation-models/">foundational model</a> now serves as the basis for a fine-tuning capability that lets companies customize the AI with their own data. That lets them generate images tailored to the creative style of their specific brands.</p>
<p>New controls in the service support the use of a sketch or depth map to guide the composition or structure of an image.</p>
<p>Creatives at Omnicom, a global leader in marketing and sales solutions, are using Getty Images‚Äô service to streamline advertising workflows and safely create on-brand content. The collaboration with Getty Images is part of Omnicom‚Äôs strategy to infuse generative AI into every facet of its business, helping teams move from ideas to outcomes faster.</p>
<p>Generative AI by Getty Images is available through the <a target="_blank" href="https://www.gettyimages.com/">Getty Images </a>and <a target="_blank" href="https://www.istockphoto.com/">iStock</a> websites, and <a target="_blank" href="https://www.gettyimages.com/enterprise/contact-sales?form=GI_NVIDIAStrategicParnership">via an API</a>.</p>
<p>For more about NVIDIA‚Äôs offerings, read about the <a target="_blank" href="https://www.nvidia.com/en-us/gpu-cloud/picasso/">AI foundry</a> for visual generative AI built on <a target="_blank" href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">NVIDIA DGX Cloud</a>, and try it on <a target="_blank" href="https://build.nvidia.com/explore/visual-design">ai.nvidia.com</a>.</p>
<p>To get the big picture, listen to NVIDIA founder and CEO Jensen Huang in two <a target="_blank" href="https://www.nvidia.com/en-us/events/siggraph/">fireside chats</a> at SIGGRAPH.</p>
<p><i>See </i><a target="_blank" href="https://www.nvidia.com/en-eu/about-nvidia/terms-of-service/"><i>notice</i></a><i> regarding software product information.</i></p>
]]></content:encoded>
					
		
		<enclosure url="https://blogs.nvidia.com/wp-content/uploads/2024/07/3_HDRi_SD_30fps_fast.mp4" length="9811620" type="video/mp4" />

		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/Key-visual-of-Shutterstock-Getty-Images-Gen-AI-servcies.jpg"
			type="image/jpeg"
			width="1920"
			height="1028"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/Key-visual-of-Shutterstock-Getty-Images-Gen-AI-servcies-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[For Your Edification: Shutterstock Releases Generative 3D, Getty Images Upgrades Service Powered by NVIDIA]]></media:title>
			<media:description type="html">Image of generative AI services from Shutterstock and Getty Images</media:description>
			</media:content>
			</item>
		<item>
		<title>Unleash the Dragonborn: ‚ÄòElder Scrolls V: Skyrim Special Edition‚Äô Joins GeForce NOW</title>
		<link>https://blogs.nvidia.com/blog/geforce-now-thursday-elder-scrolls-skyrim/</link>
		
		<dc:creator><![CDATA[GeForce NOW Community]]></dc:creator>
		<pubDate>Thu, 25 Jul 2024 13:00:02 +0000</pubDate>
				<category><![CDATA[Gaming]]></category>
		<category><![CDATA[Cloud Gaming]]></category>
		<category><![CDATA[GeForce NOW]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73239</guid>

					<description><![CDATA[‚ÄúHey, you. You‚Äôre finally awake.‚Äù It‚Äôs the summer of Elder Scrolls ‚Äî whether a seasoned Dragonborn or a new adventurer, dive into the legendary world of Tamriel this GFN Thursday as The Elder Scrolls V: Skyrim Special Edition joins the cloud. Epic adventures await, along with nine new games joining the GeForce NOW library this	<a class="read-more" href="https://blogs.nvidia.com/blog/geforce-now-thursday-elder-scrolls-skyrim/">
		Read Article		<span data-icon="y"></span>
	</a>
	]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p>‚ÄúHey, you. You‚Äôre finally awake.‚Äù</p>
<p>It‚Äôs the summer of <i>Elder Scrolls</i> ‚Äî whether a seasoned Dragonborn or a new adventurer, dive into the legendary world of Tamriel this GFN Thursday as <i>The Elder Scrolls V: Skyrim Special Edition</i> joins the cloud.</p>
<p>Epic adventures await, along with nine new games joining the <a target="_blank" href="https://www.nvidia.com/en-us/geforce-now/games/">GeForce NOW library</a> this week.</p>
<p>Plus make sure to catch the <a href="https://blogs.nvidia.com/blog/geforce-now-thursday-summer-sale-2024">GeForce NOW Summer Sale</a> for 50% off new Ultimate and Priority memberships.</p>
<h2><b>Unleash the Dragonborn</b></h2>
<figure id="attachment_73243" aria-describedby="caption-attachment-73243" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-73243" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-672x378.jpg" alt="Skyrim on GeForce NOW" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-672x378.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-400x225.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-768x432.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-1536x864.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-scaled.jpg 2048w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-800x450.jpg 800w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-382x215.jpg 382w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-178x100.jpg 178w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Skyrim-Screenshots_3840x2160-02-1280x720.jpg 1280w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73243" class="wp-caption-text"><em>Taking an arrow to the knee won‚Äôt stop gamers from questing in the cloud.</em></figcaption></figure>
<p>Experience the legendary adventures, breathtaking landscapes and immersive storytelling of the iconic role-playing game <i>The Elder Scrolls V: Skyrim Special Edition</i> from Bethesda Game Studios ‚Äî now accessible on any device from the cloud. Become the Dragonborn and defeat Alduin the World-Eater, a dragon prophesied to destroy the world.<i>¬†</i></p>
<p>Explore a vast landscape, complete quests and improve skills to develop characters in the open world of <i>Skyrim</i>. The <i>Special Edition</i> includes add-ons with all-new features, including remastered art and effects. It also brings the adventure of Bethesda Game Studios creations, including new quests, environments, characters, dialogue, armor and weapons.</p>
<p>Get ready to embark on unforgettable quests, battle fearsome foes and uncover the rich lore of the <i>Elder Scrolls</i> universe, all with the power and convenience of <a target="_blank" href="https://www.nvidia.com/en-us/geforce-now/">GeForce NOW</a>. ‚ÄúFus Ro Dah‚Äù with an <a target="_blank" href="https://www.nvidia.com/en-us/geforce-now/memberships/">Ultimate membership</a> to stream at up to 4K resolution and 120 frames per second with up to eight-hour gaming sessions for the ultimate immersive experience throughout the realms of Tamriel.</p>
<h2><b>All Hands on Deck</b></h2>
<figure id="attachment_73249" aria-describedby="caption-attachment-73249" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-73249" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-672x336.jpg" alt="World of Warships members rewards on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-World_of_Warships_Members_Rewards.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73249" class="wp-caption-text"><em>Get those sea legs ready for a reward.</em></figcaption></figure>
<p>Wargaming is bringing back an in-game event exclusively for GeForce NOW members this week.</p>
<p>Through Tuesday, July 30, members who complete the quest while streaming <i>World of Warships</i> can earn up to five GeForce NOW one-day Priority codes ‚Äî one for each day of the challenge. Aspiring admirals can learn more on the <i>World of Warships</i> <a target="_blank" href="https://worldofwarships.com/en/news/">blog</a> and <a target="_blank" href="https://x.com/WorldofWarships">social channels</a>.</p>
<h2><b>Shiny and New</b></h2>
<figure id="attachment_73246" aria-describedby="caption-attachment-73246" style="width: 672px" class="wp-caption aligncenter"><img loading="lazy" decoding="async" class="size-large wp-image-73246" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-672x336.jpg" alt="Conscript on GeForce NOW" width="672" height="336" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-672x336.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-400x200.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-768x384.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-1536x768.jpg 1536w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-842x421.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-406x203.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-188x94.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT-1280x640.jpg 1280w, https://blogs.nvidia.com/wp-content/uploads/2024/07/GFN_Thursday-CONSCRIPT.jpg 2048w" sizes="(max-width: 672px) 100vw, 672px" /><figcaption id="caption-attachment-73246" class="wp-caption-text"><em>Rendezvous with death.</em></figcaption></figure>
<p>Take on classic survival horror in <i>CONSCRIPT</i> from Jordan Mochi and Team17. Inspired by legendary games in the genre, the game is set in 1916 during the Great War. <i>CONSCRIPT </i>blends all the punishing mechanics of older horror games into a cohesive, tense and unique experience. Play as a French soldier searching for his missing-in-action brother during the Battle of Verdun. Search through twisted trenches, navigate overrun forts and cross no-man‚Äôs-land to find him.</p>
<p>Here‚Äôs the full list of new games this week:</p>
<ul>
<li><i>Cataclismo </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1422440?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 22</li>
<li><i>CONSCRIPT </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/1286990?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 23)</li>
<li><i>F1 Manager 2024 </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2591280?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 23)</li>
<li><i>EARTH DEFENSE FORCE 6 </i>(New release on <a target="_blank" href="https://store.steampowered.com/app/2291060?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, July 25)</li>
<li><i>The Elder Scrolls V: Skyrim </i>(<a target="_blank" href="https://store.steampowered.com/app/72850?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>The Elder Scrolls V: Skyrim Special Edition</i> (<a target="_blank" href="https://store.steampowered.com/app/489830?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>, <a target="_blank" href="https://www.epicgames.com/store/p/skyrim?utm_source=nvidia&amp;utm_campaign=geforce_now">Epic Games Store</a> and <a target="_blank" href="https://www.xbox.com/games/store/the-elder-scrolls-v-skyrim-special-edition-pc/9p03jgq4s1gc?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Gang Beasts </i>(<a target="_blank" href="https://store.steampowered.com/app/285900?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a> and <a target="_blank" href="https://www.xbox.com/games/store/gang-beasts/BPQZT43FWD49?utm_source=nvidia&amp;utm_campaign=geforce_now">Xbox</a>, available on PC Game Pass)</li>
<li><i>Kingdoms and Castles </i>(<a target="_blank" href="https://store.steampowered.com/app/569480?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
<li><i>The Settlers: New Allies </i>(<a target="_blank" href="https://store.steampowered.com/app/2750080?utm_source=nvidia&amp;utm_campaign=geforce_now">Steam</a>)</li>
</ul>
<p>What are you planning to play this weekend? Let us know on <a target="_blank" href="https://www.twitter.com/nvidiagfn">X</a> or in the comments below.</p>
<blockquote class="twitter-tweet" data-width="500" data-dnt="true">
<p lang="und" dir="ltr"><img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f4c8.png" alt="üìà" class="wp-smiley" style="height: 1em; max-height: 1em;" /><img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f199.png" alt="üÜô" class="wp-smiley" style="height: 1em; max-height: 1em;" />  è·¥è·¥ú Ä ·¥Ñ ü·¥è·¥ú·¥Ö …¢·¥Ä·¥ç…™…¥…¢ Íú±·¥ã…™ ü ü …™…¥·¥Ñ Ä·¥á·¥ÄÍú±·¥á·¥Ö</p>
<p>&mdash; <img src="https://s.w.org/images/core/emoji/15.0.3/72x72/1f329.png" alt="üå©" class="wp-smiley" style="height: 1em; max-height: 1em;" /> NVIDIA GeForce NOW (@NVIDIAGFN) <a target="_blank" href="https://twitter.com/NVIDIAGFN/status/1816141289769033834?ref_src=twsrc%5Etfw">July 24, 2024</a></p></blockquote>
<p><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<p>&nbsp;</p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/gfn-thursday-7-11-nv-blog-1280x680-no-copy-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/gfn-thursday-7-11-nv-blog-1280x680-no-copy-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Unleash the Dragonborn: ‚ÄòElder Scrolls V: Skyrim Special Edition‚Äô Joins GeForce NOW]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
		<item>
		<title>Demystifying AI-Assisted Artistry With Adobe Apps Using NVIDIA RTX</title>
		<link>https://blogs.nvidia.com/blog/ai-decoded-adobe-firefly-creative-cloud-rtx/</link>
		
		<dc:creator><![CDATA[Gerardo Delgado]]></dc:creator>
		<pubDate>Wed, 24 Jul 2024 13:00:56 +0000</pubDate>
				<category><![CDATA[Generative AI]]></category>
		<category><![CDATA[3D]]></category>
		<category><![CDATA[AI Decoded]]></category>
		<category><![CDATA[Art]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Creators]]></category>
		<category><![CDATA[GeForce]]></category>
		<category><![CDATA[NVIDIA RTX]]></category>
		<category><![CDATA[Rendering]]></category>
		<guid isPermaLink="false">https://blogs.nvidia.com/?p=73159</guid>

					<description><![CDATA[Adobe Creative Cloud applications, which tap NVIDIA RTX GPUs, are designed to enhance the creativity of users, empowering them to work faster and focus on their craft.]]></description>
										<content:encoded><![CDATA[<div id="bsf_rt_marker"></div><p><i>Editor‚Äôs note: This post is part of the </i><a href="https://blogs.nvidia.com/blog/tag/ai-decoded/"><i>AI Decoded series</i></a><i>, which demystifies AI by making the technology more accessible, and showcases new hardware, software, tools and accelerations for RTX PC users.</i></p>
<p>Adobe Creative Cloud applications, which tap <a target="_blank" href="https://www.nvidia.com/en-us/design-visualization/technologies/rtx/">NVIDIA RTX GPUs</a>, are designed to enhance the creativity of users, empowering them to work faster and focus on their craft.</p>
<p>These tools seamlessly integrate into existing creator workflows, enabling greater productivity and delivering power and precision.</p>
<h2><b>Look to the Light</b></h2>
<p><a target="_blank" href="https://helpx.adobe.com/creative-cloud/generative-ai-overview.html#generative-ai-with-adobe-creative-cloud">Generative AI</a> creates new data in forms such as images or text by learning from existing data. It effectively visualizes and generates content to match what a user describes and helps open up fresh avenues for creativity.</p>
<p>Adobe Firefly is Adobe‚Äôs family of creative generative AI models that offer new ways to ideate and create while assisting creative workflows using generative AI. They‚Äôre designed to be safe for commercial use and were trained, using NVIDIA GPUs, on licensed content, like Adobe Stock Images, and public domain content where copyright has expired.</p>
<p><iframe loading="lazy" title="Adobe Firefly: A New Era of Creativity | Adobe" width="500" height="281" src="https://www.youtube.com/embed/f_2KsIwoV4Q?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>Firefly features are integrated in Adobe‚Äôs most popular creative apps.</p>
<p><iframe loading="lazy" title="Bring Illustrations to Life with Adobe AI Effects powered by NVIDIA GPUs on PC &amp; in the Cloud" width="500" height="281" src="https://www.youtube.com/embed/7ELrpfVY4P4?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>Adobe Photoshop features the <a target="_blank" href="https://www.adobe.com/products/photoshop/generative-fill.html">Generative Fill tool,</a> which uses simple description prompts to easily add content from images. With the latest Reference Image feature currently in beta, users can also upload a sample image to get image results closer to their desired output.</p>
<figure id="attachment_73163" aria-describedby="caption-attachment-73163" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees.png"><img loading="lazy" decoding="async" class="wp-image-73163 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-672x259.png" alt="" width="672" height="259" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-672x259.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-400x154.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-768x296.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-842x325.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-406x157.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees-188x73.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-ai-bees.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73163" class="wp-caption-text">Use Generative Fill to add content and Reference Image to refine it.</figcaption></figure>
<p>Generative Expand allows artists to extend the border of their image with the Crop tool, filling in bigger canvases with new content that automatically blends in with the existing image.</p>
<figure id="attachment_73166" aria-describedby="caption-attachment-73166" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill.png"><img loading="lazy" decoding="async" class="wp-image-73166 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-672x357.png" alt="" width="672" height="357" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-672x357.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-400x213.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-768x408.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-842x447.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-406x215.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill-188x100.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/gen-background-fill.png 1280w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73166" class="wp-caption-text">Bigger canvas? Not a problem.</figcaption></figure>
<p>RTX-accelerated Neural Filters, such as Photo Restoration, enable complex adjustments such as colorizing black-and-white photos and performing style transfers using AI. The Smart Portrait filter, which allows non-destructive editing with filters, is based on work from <a target="_blank" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>.</p>
<p><iframe loading="lazy" title="Accelerating AI in Photoshop Neural Filters with RTX A2000" width="500" height="281" src="https://www.youtube.com/embed/WwCe9Woy1jw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>The brand-new Generative Shape Fill (beta) in Adobe Illustrator, powered by the latest Adobe Firefly Vector Model, allows users to accelerate design workflows by quickly filling shapes with detail and color in their own styles. With Generative Shape Fill, designers can easily match the style and color of their own artwork to create a wide variety of editable and scalable vector graphic options.</p>
<figure id="attachment_73170" aria-describedby="caption-attachment-73170" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe.jpg"><img loading="lazy" decoding="async" class="wp-image-73170 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-672x294.jpg" alt="" width="672" height="294" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-672x294.jpg 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-400x175.jpg 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-768x336.jpg 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-842x369.jpg 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-406x178.jpg 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe-188x82.jpg 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/AI-Adobe.jpg 1000w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73170" class="wp-caption-text">Generative AI.</figcaption></figure>
<p><a target="_blank" href="https://www.adobe.com/products/illustrator/generative-recolor.html">Adobe Illustrator‚Äôs Generative Recolor</a> feature lets creators type in a text prompt to explore custom color palettes and themes for their vector artwork in seconds.</p>
<figure id="attachment_73173" aria-describedby="caption-attachment-73173" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color.png"><img loading="lazy" decoding="async" class="wp-image-73173 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-672x327.png" alt="" width="672" height="327" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-672x327.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-400x195.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-768x374.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-842x410.png 842w, https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-406x198.png 406w, https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color-188x92.png 188w, https://blogs.nvidia.com/wp-content/uploads/2024/07/illustrator-color.png 1109w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73173" class="wp-caption-text">Color us impressed.</figcaption></figure>
<p>NVIDIA will continue working with Adobe to support advanced generative AI models, with a focus on deep integration into the apps the world‚Äôs leading creators use.</p>
<h2><b>Making Moves on Video</b></h2>
<p>Adobe Premiere Pro is one of the most popular and powerful video editing solutions.</p>
<p>Its Enhance Speech tool, accelerated by RTX, uses AI to remove unwanted noise and improve the quality of dialogue clips so they sound professionally recorded. It‚Äôs up to 4.5x faster on RTX PCs.</p>
<figure id="attachment_73176" aria-describedby="caption-attachment-73176" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/enhance-speech.png"><img loading="lazy" decoding="async" class="wp-image-73176 size-full" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/enhance-speech.png" alt="" width="672" height="445" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/enhance-speech.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/enhance-speech-400x265.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/enhance-speech-325x215.png 325w, https://blogs.nvidia.com/wp-content/uploads/2024/07/enhance-speech-151x100.png 151w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73176" class="wp-caption-text">Adobe Premiere Pro‚Äôs AI-powered Enhance Speech tool removes unwanted noise and improves dialogue quality.</figcaption></figure>
<p><a target="_blank" href="https://helpx.adobe.com/premiere-pro/using/auto-reframe.html">Auto Reframe</a>, another Adobe Premiere feature, uses GPU acceleration to identify and track the most relevant elements in a video, and intelligently reframes video content for different aspect ratios. Scene Edit Detection automatically finds the original edit points in a video, a necessary step before the video editing stage begins.</p>
<p><iframe loading="lazy" title="How to detect a cut with Scene Edit Detection in Premiere Pro" width="500" height="281" src="https://www.youtube.com/embed/DYor3a5DKBA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Visual Effects</b></h2>
<p>Separating a foreground object from a background is a crucial step in many visual effects and compositing workflows.</p>
<p>Adobe After Effects has a new feature that uses a matte to isolate an object, enabling capabilities including background replacement and the selective application of effects to the foreground.</p>
<p>Using the <a target="_blank" href="https://helpx.adobe.com/after-effects/using/roto-brush-refine-matte.html">Roto Brush</a> tool, artists can draw strokes on representative areas of the foreground and background elements. After Effects uses that information to create a segmentation boundary between the foreground and background elements, delivering cleaner cutouts with fewer clicks.</p>
<p><iframe loading="lazy" title="How to use Next-Gen Rotobrush 3 in Adobe After Effects" width="500" height="281" src="https://www.youtube.com/embed/VJ3cNt0yEhQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Creating 3D Product Shots</b></h2>
<p>The Substance 3D Collection is Adobe‚Äôs solution for 3D material authoring, texturing and rendering, enabling users to rapidly create stunningly photorealistic 3D content, including models, materials and lighting.</p>
<p>Visualizing products and designs in the context of a space is compelling, but it can be time-consuming to find the right environment for the objects to live in. Substance 3D Stager‚Äôs<a target="_blank" href="https://helpx.adobe.com/substance-3d-stager/features/generative-background.html"> Generative Background</a> feature, powered by Adobe Firefly, solves this issue by letting artists quickly explore generated backgrounds to composite 3D models.</p>
<p>Once an environment is selected, Stager can automatically match the perspective and lighting to the generated background.</p>
<p><iframe loading="lazy" title="Generate Background Images Fast in Substance 3D Stager &amp; Firefly | Adobe Substance 3D" width="500" height="281" src="https://www.youtube.com/embed/woUUPrSiVPc?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<h2><b>Material Authoring With AI</b></h2>
<p>Adobe Substance 3D Sampler, also part of the Substance 3D Collection, is designed to transform images of surfaces and objects into photorealistic physically based rendering (PBR) materials, 3D models and high-dynamic range environment lights. With the recent introduction of<a target="_blank" href="https://helpx.adobe.com/substance-3d-sampler/release-notes/version-4-4---substance-3d-sampler.html"> new generative workflows powered by Adobe Firefly</a>, Sampler is making it easier than ever for artists to explore variations when creating materials for everything from product visualization projects to the latest AAA games.</p>
<p>Sampler‚Äôs Text-to-Texture feature allows users to generate tiled images from detailed text prompts. These generated images can then be edited and transformed into photorealistic PBR materials using the machine learning-powered Image-to-Material feature or any Sampler filter.</p>
<p><iframe loading="lazy" title="How to Use Generative Tools in Sampler for Fast Iterations | Adobe Substance 3D" width="500" height="281" src="https://www.youtube.com/embed/YD3tPlO5h3g?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>Image-to-Texture similarly enables the creation of tiled textures from reference images, providing an alternate way to prompt and generate variations from existing visual content.</p>
<figure id="attachment_73179" aria-describedby="caption-attachment-73179" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture.png"><img loading="lazy" decoding="async" class="wp-image-73179 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-672x378.png" alt="" width="672" height="378" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-672x378.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-400x225.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-768x432.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-801x450.png 801w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-383x215.png 383w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture-178x100.png 178w, https://blogs.nvidia.com/wp-content/uploads/2024/07/image-to-texture.png 975w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73179" class="wp-caption-text">Adobe 3D Sampler‚Äôs Image-to-Texture feature.</figcaption></figure>
<p>Sampler‚Äôs Text-to-Pattern feature uses text prompts to generate tiling patterns, which can be used as base colors or inputs for various filters, such as the Cloth Weave filter for creating original fabric materials.</p>
<p>All of these generative AI features in the Substance 3D Collection, supercharged with RTX GPUs, are designed to help 3D creators ideate and create faster.</p>
<h2><b>Photo-tastic Features</b></h2>
<p>Adobe Lightroom‚Äôs AI-powered <a target="_blank" href="https://helpx.adobe.com/lightroom-cc/using/enhance-details.html">Raw Details</a> feature produces crisp detail and more accurate renditions of edges, improves color rendering and reduces artifacts, enhancing the image without changing its original resolution. This feature is handy for large displays and prints, where fine details are visible.</p>
<figure id="attachment_73182" aria-describedby="caption-attachment-73182" style="width: 672px" class="wp-caption aligncenter"><a href="https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance.png"><img loading="lazy" decoding="async" class="wp-image-73182 size-large" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-672x412.png" alt="" width="672" height="412" srcset="https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-672x412.png 672w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-400x245.png 400w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-768x471.png 768w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-734x450.png 734w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-351x215.png 351w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance-163x100.png 163w, https://blogs.nvidia.com/wp-content/uploads/2024/07/Enhance.png 926w" sizes="(max-width: 672px) 100vw, 672px" /></a><figcaption id="caption-attachment-73182" class="wp-caption-text">Enhance, enhance, enhance.</figcaption></figure>
<p><a target="_blank" href="https://helpx.adobe.com/lightroom-cc/using/enhance-details.html">Super Resolution</a> helps create an enhanced image with similar results as Raw Details but with 2x the linear resolution. This means that the enhanced image will have 2x the width and height of the original image ‚Äî or 4x the total pixel count. This is especially useful for increasing the resolution of cropped imagery.</p>
<p><iframe loading="lazy" title="NVIDIA RTX Accelerates AI Super Resolution in Adobe Photoshop" width="500" height="281" src="https://www.youtube.com/embed/UURFgE43PUg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe></p>
<p>For faster editing, AI-powered, RTX-accelerated masking tools like Select Subject, which isolates people from an image, and Select Sky, which captures skies, enable users to create complex masks with the click of a button.</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-73188" src="https://blogs.nvidia.com/wp-content/uploads/2024/07/magic.mask_.gif" alt="" width="640" height="360" /></p>
<p>Visit Adobe‚Äôs <a target="_blank" href="https://www.adobe.com/ai/overview/features.html">AI features page</a> for a complete list of AI features using RTX.</p>
<p><i>Looking for more AI-powered content creation apps? Consider </i><a target="_blank" href="https://www.nvidia.com/en-us/geforce/broadcasting/broadcast-app/"><i>NVIDIA Broadcast</i></a><i>, which transforms any room into a home studio, free for RTX GPU owners.¬†</i></p>
<p><i>Generative AI is transforming gaming, videoconferencing and interactive experiences of all kinds. Make sense of what‚Äôs new and what‚Äôs next by subscribing to the </i><a target="_blank" href="https://www.nvidia.com/en-us/ai-on-rtx/?modal=subscribe-ai"><i>AI Decoded newsletter</i></a><i>.</i></p>
]]></content:encoded>
					
		
		
		
			<media:content
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/adobe-apps-nv-blog-1280x680-1.jpg"
			type="image/jpeg"
			width="1280"
			height="680"
			>
			<media:thumbnail
			url="https://blogs.nvidia.com/wp-content/uploads/2024/07/adobe-apps-nv-blog-1280x680-1-842x450.jpg"
			width="842"
			height="450"
			/>
			<media:title type="html"><![CDATA[Demystifying AI-Assisted Artistry With Adobe Apps Using NVIDIA RTX]]></media:title>
			<media:description type="html"></media:description>
			</media:content>
			</item>
	</channel>
</rss>
