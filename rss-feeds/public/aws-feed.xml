<?xml version="1.0" encoding="UTF-8" standalone="no"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" version="2.0">

<channel>
	<title>AWS HPC Blog</title>
	<atom:link href="https://aws.amazon.com/blogs/hpc/feed/" rel="self" type="application/rss+xml"/>
	<link>https://aws.amazon.com/blogs/hpc/</link>
	<description>Just another Amazon Web Services site</description>
	<lastBuildDate>Wed, 19 Jun 2024 14:47:06 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>An agent-based simulation of Amazon’s inbound supply chain</title>
		<link>https://aws.amazon.com/blogs/hpc/an-agent-based-simulation-of-amazons-inbound-supply-chain/</link>
		
		<dc:creator><![CDATA[Siva Veluchamy]]></dc:creator>
		<pubDate>Wed, 19 Jun 2024 14:47:06 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">897cb050a52ce3f07883cdad8b87aedea2e5ba9c</guid>

					<description>Hundreds of millions of products, the entire *first-mile* of distribution - learn how Amazon simulated their massive US supply chain, end-to-end, with help from a company called Simudyne.</description>
										<content:encoded>&lt;p&gt;&lt;img class="alignright wp-image-3762 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/30/An-agent-based-simulation-of-Amazons-inbound-supply-chain-2.png" alt="An agent-based simulation of Amazon's inbound supply chain" width="380" height="212"&gt;&lt;/p&gt; 
&lt;p&gt;Amazon has a complex inbound supply chain network for our retail operations in the United States, and we’re always seeking innovative ways to make it more efficient.&lt;/p&gt; 
&lt;p&gt;Recently, we embarked on an ambitious project: simulating our entire US inbound supply chain. To put this into perspective, this is the entire “first-mile” of distribution, tracking the movement of hundreds of millions of individual products through the network. This was a significant first step towards enhancing the network’s efficiency, and was made possible by using &lt;a href="https://docs.simudyne.com/overview"&gt;Simudyne’s Software Development Kit&lt;/a&gt; and the compute power of AWS.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk you through the story of our supply chain, and how we came to pull of what we think is quite an amazing feat. The supply chain toolkit used to build this is available on AWS Marketplace, so if you’re reading this – and interested in supply chain simulation – you can just dive right in.&lt;/p&gt; 
&lt;h2&gt;Amazon’s inbound network: a complex system&lt;/h2&gt; 
&lt;p&gt;Amazon’s inbound network in the US is a multi-stage, complex, and dynamic system where goods flow from a myriad of suppliers to strategically-located cross-dock facilities (we call them IXDs), and ultimately to Fulfillment Centers.&lt;/p&gt; 
&lt;p&gt;Suppliers range from local startups to international conglomerates, each feeding into the diverse Amazon inventory. The IXDs are vital in this system, acting as high-efficiency transit hubs where products are sorted and prepped for their next leg to the Fulfillment Centers.&lt;/p&gt; 
&lt;p&gt;Fulfillment Centers are large facilities, outfitted with advanced technologies like artificial intelligence (AI) systems and robotics. Items are stored, cataloged, and eventually dispatched to customers from these locations. This segment of the supply chain exemplifies the need for balance in optimization – a blend of efficiency, cost management, and technological innovation.&lt;/p&gt; 
&lt;p&gt;Optimizing a network like this is challenging. We need to balance cost efficiency and customer satisfaction, align operations with environmental sustainability, ensure adaptability to future market shifts, and deal manage technological advancements.&lt;/p&gt; 
&lt;p&gt;There must also be an understanding of the interface between the &lt;em&gt;micro&lt;/em&gt; aspects of fulfillment centers (i.e., what’s happening within individual centers/facilities) and the &lt;em&gt;macro&lt;/em&gt; aspects of a supply chain network (i.e., phenomena that emerge when examining the web of relationships between facilities).&lt;/p&gt; 
&lt;p&gt;One way to approach this optimization problem is by building and executing computer simulations, or mathematical models that aim to mimic systems, like supply chain networks, so the simulations can be used to study how those systems work. Building a supply chain simulation that can represent this variety of scales needs an intricate understanding of the supply chain’s dynamics – from warehousing logistics to the last-mile delivery intricacies.&lt;/p&gt; 
&lt;h2&gt;Solving for complexity: the power of agent-based models&lt;/h2&gt; 
&lt;p&gt;Simudyne’s Agent-Based Modeling (ABM) technology and SDK turned out to be a game-changer in our optimization toolkit. This technology allows us to simulate the Amazon supply chain with high fidelity, with each agent representing a distinct component like a warehouse or delivery vehicle. By mimicking real-world behavior and decision-making processes, we can analyze and optimize various aspects of supply chains like topology, inventory placement, resource allocation, and so forth.&lt;/p&gt; 
&lt;p&gt;Simudyne’s SDK is designed to strike a balance between handling the multi-scale system representation problem, the effort needed to build and implement simulations, and the ability of simulations to take advantage of large-scale computing that can speed up the business decision-making process.&lt;/p&gt; 
&lt;p&gt;Key to this balance is Simudyne’s advanced approach to agent-based modeling, which leverages a graph-based approach designed for more accurate and efficient simulations. Unlike traditional agent-based modeling tools that might use graph representation primarily for data visualization, Simudyne’s method involves using the graph as the core computational structure.&lt;/p&gt; 
&lt;p&gt;In this setup, each agent, whether a facility, vehicle, or another entity, is a node with connections to other nodes, representing their interactions and relationships. This structure allows agents to make independent observations and decisions based on their interactions, leading to a more dynamic and realistic simulation of the real world.&lt;/p&gt; 
&lt;h2&gt;Sumdyne’s SDK&lt;/h2&gt; 
&lt;p&gt;Simudyne uses a &lt;a href="https://docs.simudyne.com/features/graph-computation"&gt;Pregel-like framework for processing large-scale graphs&lt;/a&gt;. Pregel supports parallelized processing of massive graphs and graph computations by distributing the workload across multiple cores, for instance across large Amazon EC2 instances like the r6a.32xlarge, with 128 vCPUs. This technique is highly efficient for handling the complex, large-scale supply chain simulations that Amazon requires.&lt;/p&gt; 
&lt;p&gt;Simudyne’s Java-based SDK employs structures like &lt;em&gt;Parallel Streams&lt;/em&gt; and &lt;em&gt;Concurrent Lists&lt;/em&gt; that unlock additional intra-simulation parallelization potential. The SDK is designed for scalability from small-scale tests to large-scale production without needing significant code rewrites, ensuring that our simulation models are not only accurate, but flexible and adaptable.&lt;/p&gt; 
&lt;p&gt;Simudyne recently released a &lt;a href="https://docs.simudyne.com/supply_chain_toolkit"&gt;Supply Chain Toolkit&lt;/a&gt; that specifically enhances the SDK with specialized tools for modeling supply chain networks. The new toolkit includes some useful classes for representing the network:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Facility&lt;/code&gt; agent&lt;strong&gt; &amp;nbsp;&lt;/strong&gt;– a class which &lt;a href="https://docs.simudyne.com/supply_chain_toolkit/facility"&gt;users can extend&lt;/a&gt; to represent various types of facilities within a supply chain. This class includes essential functions for managing the flow of products, and users can add multiple &lt;code&gt;LoadingBay&lt;/code&gt; classes for different types of docks.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;TransportMessages&lt;/code&gt; – a class that &lt;a href="https://docs.simudyne.com/supply_chain_toolkit/transport_message"&gt;facilitates the movement&lt;/a&gt; of inventory between facilities, with options for cargo to be represented either as &lt;code&gt;Product&lt;/code&gt; class or as a string for memory efficiency.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;LoadingBay&lt;/code&gt; – a class that &lt;a href="https://docs.simudyne.com/supply_chain_toolkit/loading_bay"&gt;represents loading docks&lt;/a&gt; in the simulation, with a transport queue and a specified capacity, allowing for various types of docks and custom priority handling.&lt;/p&gt; 
&lt;p&gt;For visualizing supply chain networks, the &lt;a href="https://docs.simudyne.com/supply_chain_toolkit/python_visualization"&gt;Simudyne SDK integrates with Python tools like Plotly&lt;/a&gt;, so we can create interactive maps to display facility information. We can customize this visualization to display additional data points and network connections.&lt;/p&gt; 
&lt;p&gt;Overall, the toolkit is designed to provide a comprehensive suite for simulating and optimizing supply chain networks, offering operational researchers and supply chain analysts a powerful tool for modeling complex dynamics and managing high-throughput scenarios.&lt;/p&gt; 
&lt;div id="attachment_3794" style="width: 1396px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3794" loading="lazy" class="size-full wp-image-3794" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/07/IMG-2024-06-07-14.51.33.png" alt="Fig 1: Sample Dashboard Connecting Simudyne SDK to Custom Visualization Layer" width="1386" height="581"&gt;
 &lt;p id="caption-attachment-3794" class="wp-caption-text"&gt;Fig 1: Sample Dashboard Connecting Simudyne SDK to Custom Visualization Layer&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Running supply chain simulations on AWS&lt;/p&gt; 
&lt;p&gt;Using the Simudyne SDK and the Supply Chain Toolkit, Amazon’s Worldwide Design Engineering team, Simudyne, and AWS’s Emerging Technologies &amp;amp; Workloads team worked together to build a simulation of Amazon’s US inbound supply chain on AWS.&lt;/p&gt; 
&lt;p&gt;We’ve shown the reference architecture in Figure 2. The simulation code connects to an Amazon Redshift data warehouse to pull relevant supply chain data that’s used in the simulations.&lt;/p&gt; 
&lt;p&gt;Typically, a full-scale simulation of the US inbound supply chain can be executed on a r6a.32xlarge EC2 instance, which not only has 128 vCPUs, but also comes with over 1 TiB of RAM. Agent-based simulations are often memory-intensive applications and more performant when the state of agents during simulations can be saved and operated in memory, making high-memory instances found on AWS very handy. On an r6a.32xlarge instance, three (3) simulation weeks of the full US inbound supply chain ran in about an hour.&lt;/p&gt; 
&lt;div id="attachment_3755" style="width: 1004px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3755" loading="lazy" class="size-full wp-image-3755" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/30/IMG-2024-05-30-10.36.03.png" alt="Figure 2: AWS reference architecture used to run simulations of the Amazon US inbound supply chain." width="994" height="599"&gt;
 &lt;p id="caption-attachment-3755" class="wp-caption-text"&gt;Figure 2: AWS reference architecture used to run simulations of the Amazon US inbound supply chain.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Using Simudyne’s technology to simulate our supply chain has opened new avenues for us. We think It can provide insights into cost reduction, service improvement, environmental impact, and strategic adaptability.&lt;/p&gt; 
&lt;p&gt;This project is not just a testament to Amazon’s commitment to innovation but also an example for people exploring the frontiers of supply chain optimization. We’ll continue to refine our models and strategies because the potential for groundbreaking improvements in e-commerce logistics remains vast and exciting.&lt;/p&gt; 
&lt;p&gt;Now that we’ve successfully modeled our inbound supply chain network helping with the Inbound regionalization initiative as outlined in our CEO’s letter to shareholders, our focus is now shifting to simulating the outbound network – the critical journey of products from our Fulfillment Centers to customers. This new phase of modeling will let us explore and experiment with various strategies – including intermodal transport – within the simulated environment.&lt;/p&gt; 
&lt;p&gt;This allows us to refine and validate our strategies before we implement them in the real world. In our upcoming posts, we plan to share insights from our experience extending our simulations and using AI to further optimize the network, offering a glimpse into the cutting-edge methods driving supply chain efficiency.&lt;/p&gt; 
&lt;p&gt;Simudyne currently offers a 30-day free trial of its SDK, which you can find in &lt;a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-2hr6jozikusxs"&gt;AWS Marketplace&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Call for participation: HPC tutorial series from the HPCIC</title>
		<link>https://aws.amazon.com/blogs/hpc/call-for-participation-hpc-tutorial-series-from-the-hpcic/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Tue, 11 Jun 2024 17:09:26 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Public Sector]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[visualization]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">7634a0bf6ed82010fb48611fdb16217bc1cbb185</guid>

					<description>Interested in getting hands-on experience with cutting-edge HPC tools? Check out this blog post on an upcoming virtual training series from @LLNL and @AWSCloud. Learn emerging technologies from the experts this August.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3849" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-16.56.36.png" alt="Call for participation: HPC tutorial series from the HPCIC" width="380" height="161"&gt;Lawrence Livermore National Laboratory (LLNL) and AWS are joining forces to provide a training opportunity for emerging HPC tools and application. This takes the form of a series of tutorials over the summer by the HPC Innovation Center (HPCIC) at the Lab, which focuses on a broad suite of open-source software projects originating from LLNL. The series aims to give attendees experience with these cutting-edge technologies.&lt;/p&gt; 
&lt;p&gt;This &lt;a href="https://hpcic.llnl.gov/tutorials/2024-hpc-tutorials"&gt;virtual series&lt;/a&gt; consists of nine distinct tutorials through the month of August. Each tutorial focuses on a specific package from the HPCIC. You’ll get hands-on experience with each project and learn more about running HPC on AWS. Each event will be hosted on AWS resources and use &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; to provide the environment for attendees.&lt;/p&gt; 
&lt;p&gt;LLNL has a long history of delivering some of the world’s largest supercomputers and the supporting software to effectively use them. This collaboration between LLNL and AWS brings these projects and industry best practices to a broader community. Together we are democratizing access to next generation computational research tools and HPC resources.&lt;/p&gt; 
&lt;h2&gt;How to Participate&lt;/h2&gt; 
&lt;p&gt;Come join us in learning how these projects can help advance your HPC research and improve efficiency. These tutorials provide a great opportunity to learn modern techniques and tools in an adaptive cloud environment.&lt;/p&gt; 
&lt;p&gt;To register your interest, use the &lt;a href="https://llnlfed.webex.com/webappng/sites/llnlfed/webinar/webinarSeries/register/f0f129eba81946dc8a30552fc657ee94"&gt;signup form&lt;/a&gt;. Details of each event and registration deadlines are listed in the following table.&lt;/p&gt; 
&lt;table style="width: 90%"&gt; 
 &lt;thead&gt; 
  &lt;tr style="background-color: #000000;height: 20px"&gt; 
   &lt;th style="width: 20%"&gt;&lt;span style="color: #ffffff"&gt;Date&lt;/span&gt;&lt;/th&gt; 
   &lt;th style="width: 20%" width="72"&gt;&lt;span style="color: #ffffff"&gt;&lt;strong&gt;Time (PDT, GMT-7)&lt;/strong&gt;&lt;/span&gt;&lt;/th&gt; 
   &lt;td width="406"&gt;&lt;span style="color: #ffffff"&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu Aug 1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;BLT:&lt;/strong&gt;&amp;nbsp;build, link, and test large-scale applications&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tue Aug 6&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;8–11:30am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Spack (part 1 of 2):&lt;/strong&gt;&amp;nbsp;learn to install your software quickly&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Wed Aug 7&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;8–11:30am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Spack (part 2 of 2)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu Aug 8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–12pm&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Caliper:&lt;/strong&gt;&amp;nbsp;integrate performance profiling capabilities into your applications&lt;br&gt; &lt;strong&gt;Hatchet:&lt;/strong&gt;&amp;nbsp;analyze hierarchical performance data&lt;br&gt; &lt;strong&gt;Thicket:&lt;/strong&gt;&amp;nbsp;optimize application performance on supercomputers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tue Aug 13&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9– 11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;RAJA:&lt;/strong&gt;&amp;nbsp;run and port codes across different GPUs&lt;br&gt; &lt;strong&gt;Umpire:&lt;/strong&gt;&amp;nbsp;discover, provision, and manage HPC memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu Aug 15&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Axom:&lt;/strong&gt;&amp;nbsp;leverage robust, flexible software components for scientific applications&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tue Aug 20&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Ascent:&lt;/strong&gt;&amp;nbsp;visualize and analyze your simulations in situ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu Aug 22&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;MFEM:&lt;/strong&gt;&amp;nbsp;use scalable finite element discretizations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tue Aug 27&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;WEAVE:&lt;/strong&gt;&amp;nbsp;analyze runs of your code&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu 29&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Flux:&lt;/strong&gt;&amp;nbsp;run thousands of jobs in a workflow&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;We’re excited about this opportunity to give back to the HPC community alongside LLNL and look forward to meeting you all over the duration of the event.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Integrating Research and Engineering Studio with AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/research-and-engineering-studio-integration-with-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Doug Morand]]></dc:creator>
		<pubDate>Tue, 11 Jun 2024 13:38:50 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[DCV]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[VDI]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">975dc9ecff3f91bdf9cbec7ea6b4ab9cc4b3a431</guid>

					<description>Researchers, engineers &amp;amp; scientists - learn how to leverage AWS ParallelCluster with Research &amp;amp; Engineering Studio for a full-featured cloud workspace. Read this post for details on this new integration.</description>
										<content:encoded>&lt;p&gt;&lt;a href="https://aws.amazon.com/hpc/res/"&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3787" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/06/Integrating-Research-and-Engineering-Studio-with-AWS-ParallelCluster-1.png" alt="Integrating Research and Engineering Studio with AWS ParallelCluster" width="380" height="213"&gt;Research and Engineering Studio on AWS (RES)&lt;/a&gt; is an easy-to-use self-service portal for researchers and engineers to access and manage their cloud-based workspaces with persistent storage and secure virtual desktops to access their data and run interactive applications.&lt;/p&gt; 
&lt;p&gt;The people who use RES are also frequently users of HPC clusters. RES today does not ship with direct integration to other AWS solutions for HPC, like &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;So today, we’re happy to announce a new HPC recipe which creates a RES-compatible &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/login-nodes-v3.html"&gt;ParallelCluster login node&lt;/a&gt; software stack. This solution uses new features added to RES 2024.04 including &lt;a href="https://docs.aws.amazon.com/res/latest/ug/res-ready-ami.html"&gt;RES-ready AMIs&lt;/a&gt; and project &lt;a href="https://docs.aws.amazon.com/res/latest/ug/projects.html#project-launch-template"&gt;launch templates&lt;/a&gt;. The RES-ready AMI allows you to pre-install RES dependencies for your virtual desktop instances (VDIs) to pre-bake software and configuration into your images, and improve boot times for your end users.&lt;/p&gt; 
&lt;p&gt;These AMIs can be registered as a RES &lt;a href="https://docs.aws.amazon.com/res/latest/ug/evdi.html#software-stacks"&gt;software stack&lt;/a&gt; for your end users to create a VDI, which joins a ParallelCluster to become their own personal login node. Using this recipe, you’ll see how to create customized software stacks that can be used as conduits to access other native AWS cloud services.&lt;/p&gt; 
&lt;h2&gt;ParallelCluster login node for RES&lt;/h2&gt; 
&lt;p&gt;Starting from version 3.7.0, ParallelCluster provides login nodes for users to access the Slurm-based environment in the cloud. Both RES and ParallelCluster support multi-user access using Active Directory (AD) integration. With that mechanism in place, shared storage and user authentication becomes easier to manage.&lt;/p&gt; 
&lt;p&gt;We start with an existing ParallelCluster that’s integrated with the same AD and using the same &lt;code&gt;ldap_id_mapping&lt;/code&gt; setting. We take a snapshot of a login node of the cluster, turn it into an Amazon Machine Image (AMI), and use as the Base image within an EC2 Image Builder recipe to create a RES-ready AMI. The &lt;code&gt;ldap_ip_mapping&lt;/code&gt; setting is an installation-level setting used by System Security Services Daemon (SSSD). This setting (when enabled) allows SSSD to generate a UID/GID.&lt;/p&gt; 
&lt;p&gt;This setting is used in both RES and ParallelCluster. In RES, it’s enabled through the &lt;code&gt;EnableLdapIDMapping&lt;/code&gt; AWS CloudFormation parameter which you use to &lt;a href="https://docs.aws.amazon.com/res/latest/ug/launch-the-product.html"&gt;launch the product&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In ParallelCluster, it’s configured in the &lt;code&gt;DirectoryService&lt;/code&gt; section of the configuration file, as part of &lt;code&gt;AdditionalSssdConfigs&lt;/code&gt; (more on this &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/DirectoryService-v3.html#yaml-DirectoryService-AdditionalSssdConfigs"&gt;in our docs&lt;/a&gt;), like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;DirectoryService:
…
  AdditionalSssdConfigs:
    ldap_id_mapping: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The RES-compatible login node AMI is created through an &lt;a href="https://aws.amazon.com/systems-manager/"&gt;AWS Systems Manager&lt;/a&gt; (SSM) automation document. SSM allows you to gather insights and automate tasks across your AWS accounts.&lt;/p&gt; 
&lt;p&gt;This automation process performs two tasks:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It creates an Amazon Machine Image (AMI) from the login node&lt;/li&gt; 
 &lt;li&gt;Updates the ParallelCluster &lt;em&gt;head node security group&lt;/em&gt; by adding ingress rules to allow connections from RES virtual desktops. Slurm (6819-6829) and NFS (2049) ingress connections are allowed via the &lt;code&gt;RESPCLoginNodeSG&lt;/code&gt; security group. This group will be used later to associate to RES project(s) when creating login node VDI instances.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;After the RES-compatible login node AMI has been created, we can create the RES-ready AMI using &lt;a href="https://aws.amazon.com/image-builder/"&gt;EC2 Image Builder&lt;/a&gt;. EC2 Image Builder simplifies the build process of building and maintaining “golden images”. Our SSM automation process will create an Image Builder recipe that includes the base AMI, and an additional component for the RES login node to configure the image to work as a RES VDI. The resulting AMI from the Image Builder pipeline will be used to create a RES software stack.&lt;/p&gt; 
&lt;p&gt;Once the RES-ready AMI has been created by Image Builder, a RES administrator can login to create the Project and Software Stack by in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Update a RES Project to modify the Launch template (or create a project if it’s their first time using RES). The &lt;code&gt;RESPCLoginNodeSG&lt;/code&gt; must be added to any project that requires access to the ParallelCluster. This can be added in the RES project &lt;em&gt;Resource Configurations -&amp;gt; Advanced Options -&amp;gt; Add Security Groups&lt;/em&gt; configuration section.&lt;/li&gt; 
 &lt;li&gt;Create a new Software Stack using the RES-ready AMI created from the Image Builder pipeline.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Once the software stack has been created, end-users that have access to it as part of a Project can create their own, dedicated, login node virtual desktops.&lt;/p&gt; 
&lt;h2&gt;Virtual desktop login node in action&lt;/h2&gt; 
&lt;p&gt;End-users will access the login node virtual desktop in the same way they access other virtual desktops.&lt;/p&gt; 
&lt;div id="attachment_3776" style="width: 957px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3776" loading="lazy" class="size-full wp-image-3776" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/06/IMG-2024-06-06-13.46.01.png" alt="Figure 1 – The Virtual Desktops contain a virtual desktop (PC-LoginNode372) which is a based on a LoginNode instance compatible with ParallelCluster." width="947" height="550"&gt;
 &lt;p id="caption-attachment-3776" class="wp-caption-text"&gt;Figure 1 – The Virtual Desktops contain a virtual desktop (PC-LoginNode372) which is a based on a LoginNode instance compatible with ParallelCluster.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Once end users have access to the login node VDI they can interact with ParallelCluster in the same way they’re accustomed. They’ll have access to the same shared storage in the ParallelCluster Login Node &lt;em&gt;and&lt;/em&gt; in the RES VDI. The next couple of screenshots show examples of shared storage accessed from both the Login Node and RES VDI.&lt;/p&gt; 
&lt;div id="attachment_3777" style="width: 998px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3777" loading="lazy" class="size-full wp-image-3777" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/06/IMG-2024-06-06-13.46.34.png" alt="Figure 2 – A terminal session on a ParallelCluster Login Node showing the user shared storage directory listing" width="988" height="257"&gt;
 &lt;p id="caption-attachment-3777" class="wp-caption-text"&gt;Figure 2 – A terminal session on a ParallelCluster Login Node showing the user shared storage directory listing&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3778" style="width: 996px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3778" loading="lazy" class="size-full wp-image-3778" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/06/IMG-2024-06-06-13.46.54.png" alt="Figure 3 – A RES virtual desktop session showing the same shared storage directory accessible from the ParallelCluster Login Node" width="986" height="428"&gt;
 &lt;p id="caption-attachment-3778" class="wp-caption-text"&gt;Figure 3 – A RES virtual desktop session showing the same shared storage directory accessible from the ParallelCluster Login Node&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;End users can now interact with a ParallelCluster (PC) from a RES VDI. This VDI acts similarly to a ParallelCluster login node with the added benefit that end-users in the RES environment can launch their own login node, with VDI, any time they need one.&lt;/p&gt; 
&lt;div id="attachment_3791" style="width: 1448px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3791" loading="lazy" class="size-full wp-image-3791" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/07/res-loginnode-screenvideo.gif" alt="Figure 4 – A virtual desktop session showing examples of Slurm commands demonstrating the integration with ParallelCluster." width="1438" height="544"&gt;
 &lt;p id="caption-attachment-3791" class="wp-caption-text"&gt;Figure 4 – A virtual desktop session showing examples of Slurm commands demonstrating the integration with ParallelCluster.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Getting started with RES-compatible ParallelCluster login nodes&lt;/h2&gt; 
&lt;p&gt;You can follow the steps to create a RES-compatible login node for your ParallelCluster by heading to the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/login_node_for_res"&gt;Login Node for Research and Engineering Studio&lt;/a&gt; recipe that’s part of the &lt;a href="https://hpc.news/recipes"&gt;HPC Recipes Library&lt;/a&gt; (a great resource, if you’re unfamiliar with it until now).&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Integrating AWS ParallelCluster and Research and Engineering Studio unlocks the ability for end users using interactive desktops to process large amounts of data when HPC is necessary. It’s a great experience, because not only does this put large-scale computational power in the hands of scientists, it does so in a way that’s friendly to use, and accessible any time they need it.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Securing HPC on AWS: implementing STIGs in AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/securing-hpc-on-aws-implementing-stigs-in-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Alex Domijan]]></dc:creator>
		<pubDate>Tue, 04 Jun 2024 11:35:47 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Security]]></category>
		<category><![CDATA[Security, Identity, & Compliance]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">a5572dfa24f9456e8fb3e01a8a7cc20766ce2442</guid>

					<description>Want to accelerate creating compliant Amazon EC2 images? Learn how HPC users can leverage cloud-native methods for applying STIG security standards.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3698" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/Securing-HPC.png" alt="" width="380" height="212"&gt;&lt;/em&gt;Today, we’ll discuss cloud-native methods that HPC customers can use to accelerate their process for creating Amazon Elastic Compute Cloud (Amazon EC2) images for AWS ParallelCluster that are compliant with Security Technical Implementation Guides (STIGs), a set of standards maintained by the US government.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk you through the process of applying STIGs to your ParallelCluster environment, help you identify the decisions you need to make on the way, and show you some of the tools you can use to make it all easier.&lt;/p&gt; 
&lt;h2&gt;What’s a STIG?&lt;/h2&gt; 
&lt;p&gt;STIGs are maintained by &lt;a href="https://public.cyber.mil/stigs/downloads/"&gt;a US government organization&lt;/a&gt;, and are simply a set of security standards that can be applied to different environments, like &lt;a href="https://aws.amazon.com/pm/ec2/?gclid=Cj0KCQjwiMmwBhDmARIsABeQ7xQS_X3FGRFdQeMzTWl84Gi1PUFg8U7-Y2A8g30P269Hh4BW5wxBXvIaAmfOEALw_wcB&amp;amp;trk=9cd376cd-1c18-46f2-9f75-0e1cdbca94c5&amp;amp;sc_channel=ps&amp;amp;ef_id=Cj0KCQjwiMmwBhDmARIsABeQ7xQS_X3FGRFdQeMzTWl84Gi1PUFg8U7-Y2A8g30P269Hh4BW5wxBXvIaAmfOEALw_wcB:G:s&amp;amp;s_kwcid=AL!4422!3!651751059309!e!!g!!amazon%20ec2!19852662176!145019189697"&gt;Amazon EC2&lt;/a&gt;. Think of STIGs as a checklist of items to apply to your EC2 instances where each checklist item has a corresponding severity level attached to it that says, “&lt;em&gt;the risk of not doing x is a low, medium, or high security risk&lt;/em&gt;”. You’ll also see these security levels referred to as Category Codes (CAT) where CAT 1 corresponds to a high security risk, CAT 2 to medium, and CAT 3 to low.&lt;/p&gt; 
&lt;p&gt;For example, one high security-risk STIG checklist item for Red Hat Enterprise Linux (RHEL) 8 is to not allow accounts configured with blank or null passwords. To resolve this, an administrator can login to the operating system and manually configure accounts to not have a blank or null password. With hundreds of checklist items it is easy to see why this can quickly become a burdensome task. The process described in this post automates up to 87% of the otherwise manual STIG remediation process.&lt;/p&gt; 
&lt;h2&gt;Why do customers want to implement STIGs?&lt;/h2&gt; 
&lt;p&gt;In short, some want to, and some need to. Customers such as the U.S. Department of Defense (DoD) must adhere to stringent compliance standards for operating system hardening. Other customers may prefer to use STIGs as a benchmark to improve their security posture.&lt;/p&gt; 
&lt;p&gt;Customers like the DoD often operate in AWS without any access to the Internet. Organizational policy dictates the reason for why, which is usually to reduce the risk of sensitive data going places it shouldn’t. We address how customers with these network restrictions can accelerate STIG hardening using AWS cloud native tools.&lt;/p&gt; 
&lt;p&gt;Once you’ve “&lt;em&gt;STIG’d&lt;/em&gt;” your ParallelCluster instances, how can you verify which checklist items you have crossed off? This is where&amp;nbsp;&lt;a href="https://www.open-scap.org/"&gt;OpenSCAP&lt;/a&gt;, an open-source security and compliance tool, comes into play. OpenSCAP automates continuous monitoring, vulnerability management, and reporting of security policy compliance data. While OpenSCAP is primarily designed to align with DoD security standards, it’s used to establish security baselines across many industries.&lt;/p&gt; 
&lt;p&gt;This post will focus on some supported ParallelCluster operating systems (OS): RHEL8, Amazon Linux 2 (AL2), and Ubuntu 20.04 (at the time of writing this, the &lt;a href="https://public.cyber.mil/stigs/downloads/"&gt;DISA STIG document library&lt;/a&gt; didn’t contain a benchmark for Ubuntu 22.04 – which is why it’s not mentioned).&lt;/p&gt; 
&lt;p&gt;We worked through the process defined in this post using the &lt;a href="https://aws.amazon.com/govcloud-us/?whats-new-ess.sort-by=item.additionalFields.postDateTime&amp;amp;whats-new-ess.sort-order=desc"&gt;AWS GovCloud West&lt;/a&gt; region, but you should be able to repeat it in other AWS regions.&lt;/p&gt; 
&lt;p&gt;For HPC customers completely new to AWS, we recommend reviewing this &lt;a href="https://aws.amazon.com/blogs/hpc/the-plumbing-best-practice-infrastructure-to-facilitate-hpc-on-aws/"&gt;blog post&lt;/a&gt; which speaks about best practices for setting up a foundation in AWS to build your HPC workloads on.&lt;/p&gt; 
&lt;h2&gt;AMIs for AWS ParallelCluster&lt;/h2&gt; 
&lt;p&gt;An &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"&gt;Amazon Machine Image&lt;/a&gt; (AMI) is a template that contains a software configuration (for example, an OS, an application server, and applications). From an AMI, you launch an&amp;nbsp;EC2 instance, which is a copy of the AMI running as a virtual server in the cloud. AMIs used for ParallelCluster are unique because they have software installed on them necessary for operating the cluster management tool.&lt;/p&gt; 
&lt;p&gt;Customers can optionally choose to create custom AMIs for ParallelCluster using &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/custom-ami-v3.html"&gt;two methods&lt;/a&gt;, both of which we can use for achieving STIG compliance, depending on factors like Internet connectivity and OS choice.&lt;/p&gt; 
&lt;p&gt;The first option is the build image configuration process which you can trigger from a ParallelCluster CLI &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcluster.build-image-v3.html"&gt;command&lt;/a&gt;: &lt;code&gt;pcluster build-image&lt;/code&gt;. This process uses &lt;a href="https://aws.amazon.com/image-builder/"&gt;Amazon EC2 Image Builder&lt;/a&gt; to launch a build instance, apply the &lt;a href="https://github.com/aws/aws-parallelcluster-cookbook"&gt;ParallelCluster cookbook&lt;/a&gt;, install the ParallelCluster software stack, and perform other necessary configuration tasks.&lt;/p&gt; 
&lt;p&gt;The second option involves taking a baseline ParallelCluster AMI (one produced by the ParallelCluster team themselves) and customizing it by performing manual modifications through &lt;a href="https://aws.amazon.com/systems-manager/"&gt;AWS Systems Manager&lt;/a&gt; (SSM).&lt;/p&gt; 
&lt;h2&gt;Process comparison&lt;/h2&gt; 
&lt;p&gt;Should you take a baseline ParallelCluster image and then apply STIGs, or take an image that already has STIGs applied (a “golden image”), and then install ParallelCluster on top? The end result is fundamentally similar, but there are some trade-offs depending on which route you choose.&lt;/p&gt; 
&lt;p&gt;The benefit of applying STIGs after a ParallelCluster image is created is that you can minimize permissions attached to the EC2 instance’s role. There &lt;em&gt;are&lt;/em&gt; additional &lt;a href="https://aws.amazon.com/iam/"&gt;AWS Identity and Access Management&lt;/a&gt; (IAM) permissions required to trigger the build image process and you can find them in &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/iam-roles-in-parallelcluster-v3.html#iam-roles-in-parallelcluster-v3-user-policy-build-image"&gt;our documentation&lt;/a&gt;. The tradeoff you’re making is that you would be standing up a new image build pipeline to accommodate security policy (STIG) enforcement starting from a baseline ParallelCluster image.&lt;/p&gt; 
&lt;p&gt;An advantage of taking a golden image and installing ParallelCluster is that you can maintain an already established image build pipeline that may accelerate internal compliance processes. However, this would require a wider permissions boundary in comparison to the previous example. There’s also a chance that installing new software could impact how STIG compliant your images are. For customers interested in trying this process on your own AMIs, you can follow along with any of the sections below depending on Internet connectivity and operating system requirements as the process is the same. In either case, we recommend performing compliance scans on your images.&lt;/p&gt; 
&lt;h2&gt;Accelerating RHEL8, AL2, and Ubuntu 20.04 STIG compliance&lt;/h2&gt; 
&lt;p&gt;Apart from the OS your use cases require, the process to achieve&amp;nbsp;STIG compliance is determined by whether your Amazon EC2 instances have Internet connectivity or not. If your compliance requirements allow you the flexibility to choose, then it’s easier with Internet connectivity.&lt;/p&gt; 
&lt;p&gt;For users &lt;strong&gt;with&lt;/strong&gt; Internet connectivity who want to use RHEL8 or AL2 operating systems, refer to the instructions in our &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/stig#rhel8-and-al2-instances-with-internet-connectivity"&gt;GitHub repo&lt;/a&gt; that’s part of the HPC Recipes Library which will guide you through the EC2 Image Builder process.&lt;/p&gt; 
&lt;p&gt;For users &lt;strong&gt;without&lt;/strong&gt; Internet connectivity who want to use RHEL8 or AL2 operating systems, refer to &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/stig#rhel8-and-al2-instances-without-internet-connectivity"&gt;these instructions&lt;/a&gt; in the same repository. This type of connectivity scenario is perhaps more common amongst customers with STIG requirements. These customers can take advantage of &lt;a href="https://aws.amazon.com/privatelink/"&gt;AWS PrivateLink&lt;/a&gt; which is a feature of Virtual Private Cloud (&lt;a href="https://aws.amazon.com/vpc/"&gt;VPC&lt;/a&gt;) and allows for private connectivity to AWS services. To take advantage of this technology for purposes of accelerating STIG compliance, ensure that you configure the &lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html"&gt;required VPC endpoints&lt;/a&gt; to allow connectivity from your private subnet to SSM. You’ll also need the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/network-configuration-v3.html#aws-parallelcluster-in-a-single-public-subnet-no-internet-v3"&gt;required VPC endpoints&lt;/a&gt; for ParallelCluster which will be used to launch your cluster with the resulting AMI.&lt;/p&gt; 
&lt;p&gt;The process for Ubuntu 20.04 includes an extra step compared to RHEL8 and AL2 operating systems because there are a couple of findings that Systems Manager cannot rectify during its run command. Due to this, we launch a baseline ParallelCluster Ubuntu 20.04 EC2 instance with a user data script that resolves findings &lt;a href="https://www.stigviewer.com/stig/canonical_ubuntu_18.04_lts/2022-08-25/finding/V-219166"&gt;V-219166&lt;/a&gt;, &lt;a href="https://www.stigviewer.com/stig/canonical_ubuntu_20.04_lts/2023-09-08/finding/V-238237"&gt;V-238237&lt;/a&gt;, and &lt;a href="https://www.stigviewer.com/stig/canonical_ubuntu_20.04_lts/2021-03-23/finding/V-238218"&gt;V-238218.&lt;/a&gt; As with RHEL8 and AL2 operating systems, customers without Internet connectivity should ensure they configure the required VPC endpoints to allow connectivity from your private subnet to SSM, and the required VPC endpoints for ParallelCluster. Instructions for Ubuntu 20.04 can be found in our &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/stig#ubuntu-2004-instances-with-or-without-internet-connectivity"&gt;HPC samples repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;As previously mentioned, there are corresponding severity levels (high, medium, low) associated with STIG checklist items. Customers can choose which security level they want to apply to their Amazon EC2 instances which is described in our &lt;a href="https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/awsec2-configure-stig.html"&gt;SSM documentation&lt;/a&gt;. We used the STIG High baseline which includes any vulnerability that can result in loss of confidentiality, availability, or integrity. Customers can optionally choose to make additional modifications to the AMIs after the STIG process of their choosing has been performed. In any event, we recommend testing AMI compatibility with your application prior to deploying to a production environment.&lt;/p&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;p&gt;Customers may be interested to find out what the effects of running the EC2 Image Builder STIG High component or Systems Manager STIG High document has on their respective operating systems.&lt;/p&gt; 
&lt;p&gt;We used OpenSCAP to perform compliance scanning to assess the security posture of our instances. It also uses the concept of profiles to determine which checks it will run and the profile can vary on mission requirements and OS.&lt;/p&gt; 
&lt;p&gt;For the purposes of maintaining a consistent benchmark for before and after assessments, we used the &lt;code&gt;xccdf_mil.disa.stig_profile_MAC-2_Sensitive&lt;/code&gt; profile for RHEL8 and Ubuntu 20.04 operating systems, and &lt;code&gt;stig-rhel7-disa&lt;/code&gt; on AL2.&lt;/p&gt; 
&lt;p&gt;Each of the ‘Baseline’ AMIs in the screenshots that follow refer to the baseline ParallelCluster AMI. In other words, these are the AMIs you would find by typing the CLI &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcluster.list-official-images-v3.html"&gt;command&lt;/a&gt;: &lt;code&gt;pcluster list-official-images&lt;/code&gt;. Note that the baseline and subsequent STIG high AMI results may change in future ParallelCluster releases.&lt;/p&gt; 
&lt;div id="attachment_3684" style="width: 1253px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3684" loading="lazy" class="size-full wp-image-3684" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.46.20.png" alt="Figure 1 - RHEL8 baseline ParallelCluster AMI OpenSCAP results from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 83 checks and failing 148 for a result of being 35.93% compliant with this profile." width="1243" height="451"&gt;
 &lt;p id="caption-attachment-3684" class="wp-caption-text"&gt;Figure 1 – RHEL8 baseline ParallelCluster AMI OpenSCAP results from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 83 checks and failing 148 for a result of being 35.93% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3685" style="width: 1270px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3685" loading="lazy" class="size-full wp-image-3685" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.46.41.png" alt="Figure 2 - RHEL8 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 201 checks and failing 30 for a result of being 87.01% compliant with this profile." width="1260" height="447"&gt;
 &lt;p id="caption-attachment-3685" class="wp-caption-text"&gt;Figure 2 – RHEL8 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 201 checks and failing 30 for a result of being 87.01% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3686" style="width: 1243px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3686" loading="lazy" class="size-full wp-image-3686" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.47.16.png" alt="Figure 3 - Amazon Linux 2 baseline ParallelCluster AMI OpenSCAP results from running the stig-rhel7-disa profile. This shows the EC2 instance as passing 54 checks and failing 160 for a result of being 58.88% compliant with this profile." width="1233" height="446"&gt;
 &lt;p id="caption-attachment-3686" class="wp-caption-text"&gt;Figure 3 – Amazon Linux 2 baseline ParallelCluster AMI OpenSCAP results from running the stig-rhel7-disa profile. This shows the EC2 instance as passing 54 checks and failing 160 for a result of being 58.88% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3687" style="width: 1263px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3687" loading="lazy" class="size-full wp-image-3687" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.47.33.png" alt="Figure 4 - Amazon Linux 2 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the stig-rhel7-disa profile. This shows the EC2 instance as passing 142 checks and failing 72 for a result of being 68.09% compliant with this profile." width="1253" height="449"&gt;
 &lt;p id="caption-attachment-3687" class="wp-caption-text"&gt;Figure 4 – Amazon Linux 2 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the stig-rhel7-disa profile. This shows the EC2 instance as passing 142 checks and failing 72 for a result of being 68.09% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3688" style="width: 1273px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3688" loading="lazy" class="size-full wp-image-3688" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.48.01.png" alt="Figure 5 - Ubuntu 20.04 baseline ParallelCluster AMI OpenSCAP results from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 17 checks and failing 92 for a result of being 15.6% compliant with this profile." width="1263" height="454"&gt;
 &lt;p id="caption-attachment-3688" class="wp-caption-text"&gt;Figure 5 – Ubuntu 20.04 baseline ParallelCluster AMI OpenSCAP results from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 17 checks and failing 92 for a result of being 15.6% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3689" style="width: 1259px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3689" loading="lazy" class="size-full wp-image-3689" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.48.17.png" alt="Figure 6 - Ubuntu 20.04 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 42 checks and failing 67 for a result of being 38.53% compliant with this profile." width="1249" height="446"&gt;
 &lt;p id="caption-attachment-3689" class="wp-caption-text"&gt;Figure 6 – Ubuntu 20.04 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 42 checks and failing 67 for a result of being 38.53% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Running your own OpenSCAP scans&lt;/h2&gt; 
&lt;p&gt;If you want to perform additional STIGs on ParallelCluster AMIs, you may want to run those images through the same OpenSCAP profiles used for this blog post.&lt;/p&gt; 
&lt;p&gt;We’ve stored the scripts for &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/blob/main/recipes/pcluster/stig/assets/EC2_RHEL8_SCAP_Assessment.sh"&gt;RHEL8&lt;/a&gt;, &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/blob/main/recipes/pcluster/stig/assets/EC2_AL2_SCAP_Assessment.sh"&gt;AL2&lt;/a&gt;, and &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/blob/main/recipes/pcluster/stig/assets/EC2_UBUNTU_2004_SCAP_Assessment.sh"&gt;Ubuntu 20.04&lt;/a&gt; in our GitHub repo. These scripts &lt;em&gt;do&lt;/em&gt; require Internet connectivity to run because they download a series of tools like the AWS CLI and OpenSCAP, and STIG benchmarks to the EC2 instance being evaluated.&lt;/p&gt; 
&lt;p&gt;You’ll need to &lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html"&gt;create an S3 bucket&lt;/a&gt;, and update the name of the bucket inside the script where it saves the results of the evaluations. The scripts use EC2 instance &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html"&gt;metadata&lt;/a&gt;&amp;nbsp;to dynamically name the output files in Amazon S3 after the instance, so they’re not overwritten as new instances are tested.&lt;/p&gt; 
&lt;p&gt;To run these scripts with minimal effort, you can run them as a user-data script upon launch and have the HTML results automatically sent to your S3 bucket. Inputting a user-data script follows the same logic as described under step 3 of the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/stig#ubuntu-2004-instances-with-or-without-internet-connectivity"&gt;Ubuntu 20.04 section&lt;/a&gt;. For RHEL8 and Ubuntu 20.04 operating systems, it takes approximately 10 minutes from instance launch to see the results uploaded to your Amazon S3 bucket. AL2 takes approximately 20-25 minutes.&lt;/p&gt; 
&lt;h2&gt;Using the resulting images&lt;/h2&gt; 
&lt;p&gt;The STIG’d AMIs can be found in the EC2 section of the Management Console and referenced in a ParallelCluster configuration file. You can create clusters using the ParallelCluster &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-configuring.html"&gt;CLI&lt;/a&gt; or the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/configure-create-pcui-v3.html"&gt;UI&lt;/a&gt;. For purposes of this post, we’ll show an example of placing the STIG’d AMI ID into the ParallelCluster configuration file for a cluster in GovCloud West.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Region: us-gov-west-1
Image:
  Os: rhel8
HeadNode:
  InstanceType: c5a.4xlarge
  Networking:
    SubnetId: {your-subnet-id}
  Ssh:
    KeyName: {your-keypair}
  Image:
    CustomAmi: {your-AMI-id}
SharedStorage:
  - MountDir: /fsx  
    Name: FSxExtData
    StorageType: FsxLustre
    FsxLustreSettings:
      StorageCapacity: 1200
      DeploymentType: PERSISTENT_1
      PerUnitStorageThroughput: 50
      DeletionPolicy: Delete
Scheduling:
  Scheduler: slurm
  SlurmSettings:
    QueueUpdateStrategy: DRAIN
  SlurmQueues:
  - Name: queue1
    ComputeResources:
    - Name: compute
      Instances:
      - InstanceType: hpc7a.48xlarge
      MinCount: 1
      MaxCount: 10
      Efa:
       Enabled: true
    Networking:
      SubnetIds:
      - {your-subnet-id}
      PlacementGroup:
        Enabled: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should edit the Items enclosed in the {} to include your identifiers.&lt;/p&gt; 
&lt;p&gt;Once you’ve created the file you can launch the cluster using the command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster --cluster-name &amp;lt;name&amp;gt; --cluster-configuration &amp;lt;file-name&amp;gt;.yml&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see validation warning messages because you are using a custom AMI, however these messages can be ignored and will not impact the creation of the cluster. You can track the cluster creation status through the &lt;a href="https://aws.amazon.com/cloudformation/"&gt;AWS CloudFormation&lt;/a&gt; console or by using the ParallelCluster CLI command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster list-clusters&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Verifying levels of compliance for compute resources is a requirement in some industries, and desired in others. Throughout this post, we’ve discussed several different cloud-native methods HPC customers with compliance requirements can choose from to accelerate their STIG process in AWS ParallelCluster depending on their Internet connectivity (or lack thereof) and operating system choice.&lt;/p&gt; 
&lt;p&gt;We recommend validating that your output images work with your application in a development environment prior to running in production.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Large scale training with NVIDIA NeMo Megatron on AWS ParallelCluster using P5 instances</title>
		<link>https://aws.amazon.com/blogs/hpc/large-scale-training-with-nemo-megatron-on-aws-parallelcluster-using-p5-instances/</link>
		
		<dc:creator><![CDATA[Aman Shanbhag]]></dc:creator>
		<pubDate>Wed, 29 May 2024 13:52:05 +0000</pubDate>
				<category><![CDATA[Amazon Machine Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[FEA]]></category>
		<category><![CDATA[GPU]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<guid isPermaLink="false">9f2f940ec2831d85025f4b4f1d755f1419ed79d4</guid>

					<description>Launching distributed GPT training? See how AWS ParallelCluster sets up a fast shared filesystem, SSH keys, host files, and more between nodes. Our guide has the details for creating a Slurm-managed cluster to train NeMo Megatron at scale.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Akshit Arora (NVIDIA), Peter Dykas (NVIDIA), Aman Shanbhag (AWS), Sean Smith (AWS), Pierre-Yves (AWS)&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Today we’ll take you on a step-by-step guide to help you to create a cluster of p5.48xlarge instances, using &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html"&gt;AWS ParallelCluster&lt;/a&gt; to launch GPT training through the NVIDIA NeMo Megatron framework, using Slurm. We’ve put detailed information about this &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher"&gt;in our GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We use ParallelCluster to execute NVIDIA NeMo Megatron across multiple nodes, because it takes care of mounting a fast shared filesystem between the nodes, synchronizing the SSH keys, creating a host file, and all the other overheads that make job submission possible.&lt;/p&gt; 
&lt;p&gt;&lt;a class="c-link" href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html" target="_blank" rel="noopener noreferrer" data-stringify-link="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html" data-sk="tooltip_parent"&gt;AWS ParallelCluster&lt;/a&gt; is a supported, open-source cluster management tool that makes it easy to create, scale, and manage clusters of accelerated&amp;nbsp;instances based on the open-source Slurm scheduler. It uses a YAML configuration file to stand up a head node,&amp;nbsp;accelerated&amp;nbsp;compute nodes, and a file system. Users can login and submit jobs to pre-provisioned nodes, or dynamically spin-up Amazon Elastic Compute Cloud (Amazon EC2) instances using On-Demand or Spot. ParallelCluster also offers a&amp;nbsp;&lt;a class="c-link" href="https://aws.amazon.com/blogs/hpc/large-scale-training-with-nemo-megatron-on-aws-parallelcluster-using-p5-instances/ParallelCluster%20also%20offers%20a%20web-based%20user%20interface%20that%20serves%20as%20a%20dashboard%20for%20creating,%20monitoring,%20and%20managing%20clusters." target="_blank" rel="noopener noreferrer" data-stringify-link="https://aws.amazon.com/blogs/hpc/large-scale-training-with-nemo-megatron-on-aws-parallelcluster-using-p5-instances/ParallelCluster%20also%20offers%20a%20web-based%20user%20interface%20that%20serves%20as%20a%20dashboard%20for%20creating,%20monitoring,%20and%20managing%20clusters." data-sk="tooltip_parent"&gt;web-based user interface&lt;/a&gt;&amp;nbsp;that serves as a dashboard for creating, monitoring, and managing clusters.&lt;/p&gt; 
&lt;h2&gt;Introducing NVIDIA NeMO Framework&lt;/h2&gt; 
&lt;p&gt;The NVIDIA NeMo Framework (or just &lt;em&gt;NeMo FW&lt;/em&gt; for the rest of this post) focuses on foundation model-training for generative AI models. Large language model (LLM) pre-training typically needs a lot of compute and model parallelism to efficiently scale training. NeMo FW’s model training scales to thousands of NVIDIA GPUs and can be used for training LLMs on trillions of tokens.&lt;/p&gt; 
&lt;p&gt;The &lt;em&gt;&lt;a href="https://github.com/NVIDIA/NeMo-Megatron-Launcher"&gt;NVIDIA NeMo Megatron Launcher&lt;/a&gt;&lt;/em&gt; &lt;em&gt;(NeMo Launcher)&lt;/em&gt; is a cloud-native tool for launching end-to-end NeMo FW training jobs. The Launcher is designed to be a simple and easy-to-use tool for launching NeMo FW training jobs on CSPs or on-prem clusters.&lt;/p&gt; 
&lt;p&gt;The launcher is typically used from a head node and only requires a minimal python installation. Launcher will generate (and launch) the submission scripts needed by the cluster scheduler and will also organize and store the job results. Launcher includes tested configuration files, but anything in a configuration file can be modified by the user. Launcher supports many functionalities, from cluster setup and configuration, data downloading, curating and model training setup, evaluation and deployment.&lt;/p&gt; 
&lt;h2&gt;Steps to create cluster and launch jobs&lt;/h2&gt; 
&lt;p&gt;This guide assumes that Amazon EC2 P5 instances are available in us-east-2 for you, but this may vary depending on your account / capacity reservation and the region you plan to use.&lt;/p&gt; 
&lt;h3&gt;Step 0: Install ParallelCluster CLI&lt;/h3&gt; 
&lt;p&gt;Installing CLI Instructions: &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-virtual-environment.html"&gt;https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-virtual-environment.html&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If &lt;code&gt;virtualenv&lt;/code&gt; is not installed, install &lt;code&gt;virtualenv&lt;/code&gt; using &lt;code&gt;pip3&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;python3 -m pip install --upgrade pip
python3 -m pip install --user --upgrade virtualenv
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Create a virtual environment, name it, and activate it.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;python3 -m virtualenv ~/apc-ve
source ~/apc-ve/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Install ParallelCluster into your virtual environment.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;(apc-ve)~$ python3 -m pip install "aws-parallelcluster" --upgrade --user&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Install Node Version Manager (&lt;code&gt;nvm&lt;/code&gt;) and the latest Long-Term Support (LTS) &lt;code&gt;Node.js&lt;/code&gt; version.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash
$ chmod ug+x ~/.nvm/nvm.sh
$ source ~/.nvm/nvm.sh
$ nvm install --lts
$ node --version
$ export PATH=$PATH:~/.local/bin
$ pcluster version
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 1: Create a VPC and Security Groups&lt;/h3&gt; 
&lt;div id="attachment_3704" style="width: 931px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3704" loading="lazy" class="size-full wp-image-3704" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/22/IMG-2024-05-22-13.06.54.png" alt="Figure 1 – A VPC configuration in a new account with one public subnet and one private subnet in the target region. The P5 instance topology is defined to have 32 ENI cards of 100Gbps each." width="921" height="433"&gt;
 &lt;p id="caption-attachment-3704" class="wp-caption-text"&gt;Figure 1 – A VPC configuration in a new account with one public subnet and one private subnet in the target region. The P5 instance topology is defined to have 32 ENI cards of 100Gbps each.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;If you’re using a new AWS account, your VPC configuration will consist of one public subnet and a private subnet in the target region. We &lt;a href="https://github.com/aws/aws-ofi-nccl/blob/master/topology/p5.48xl-topo.xml"&gt;define the P5 instance topology&lt;/a&gt; to have a total of 32 Elastic Network Interfaces (ENI) cards of 100Gbps each. To handle 32 ENIs, compute instances need to be placed into a private subnet, otherwise your cluster will fail in creation because a public IP is not automatically assigned on instances with multiple NICs. You can find more information about deploying a VPC for ParallelCluster &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/1.architectures/1.vpc_network#vpc-cloudformation-stacks"&gt;in our GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unless you’re comfortable deploying a private subnet and setting the routes and security groups, we recommend that you deploy a custom VPC using the CloudFormation template called ML-VPC. This template is region-agnostic and enables you to create a VPC with the required network architecture to run your workloads.&lt;/p&gt; 
&lt;p&gt;You can follow the steps to deploy your new VPC:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Deploy this &lt;a href="https://console.aws.amazon.com/cloudformation/home?#/stacks/quickcreate?templateURL=https%3A%2F%2Fawsome-distributed-training.s3.amazonaws.com%2Ftemplates%2F1.vpc-multi-az.yaml&amp;amp;stackName=ML-VPC"&gt;CloudFormation template&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;You’ll see a list of parameters: 
  &lt;ul&gt; 
   &lt;li&gt;In &lt;strong&gt;Name of your VPC&lt;/strong&gt;, you can leave it as default LargeScaleVPC.&lt;/li&gt; 
   &lt;li&gt;For &lt;strong&gt;Availability zones&lt;/strong&gt; (AZ’s), select your desired AZ. This will deploy a public and private subnet in that AZ. &lt;em&gt;If you’re using a capacity reservation (CR), use the AZ specific to the CR.&lt;/em&gt;&lt;/li&gt; 
   &lt;li&gt;Keep the &lt;strong&gt;S3 Endpoint&lt;/strong&gt;, &lt;strong&gt;Public Subnet&lt;/strong&gt; and &lt;strong&gt;DynamoDB Endpoint&lt;/strong&gt; as true.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Check the acknowledgement box in the &lt;strong&gt;Capabilities&lt;/strong&gt; section and create the stack.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;It’ll take a few minutes to deploy your network architecture. The stack outputs tab will contain IDs of your security groups and subnets. You’ll need to keep this information handy for the next step.&lt;/p&gt; 
&lt;h3&gt;Step 2: Build ParallelCluster custom AMI&lt;/h3&gt; 
&lt;p&gt;We used the following configuration (which we saved as image_build.yaml), for adding ParallelCluster dependencies on top of the &lt;a href="https://aws.amazon.com/releasenotes/aws-deep-learning-base-gpu-ami-ubuntu-22-04/"&gt;AWS Deep Learning Base GPU AMI&lt;/a&gt;. This Deep Learning AMI page also contains a command for retrieving the AMI ID (search for “Query AMI-ID with AWSCLI”). You can specify which AMI to use as base depending on your requirements, and then install ParallelCluster dependencies on top of it following &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/building-custom-ami-v3.html"&gt;the tutorial in our service documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Now ensure that the &lt;code&gt;SubnetId&lt;/code&gt;, &lt;code&gt;ParentImage&lt;/code&gt;, and &lt;code&gt;SecurityGroupIds&lt;/code&gt; are set to the values exported when deploying your network architecture in Step 1. Save the configuration to the file &lt;code&gt;image_build.yaml&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Build:
 InstanceType: p5.48xlarge
 SubnetId: subnet-xxxxxx
 ParentImage: ami-xxxxxx
 SecurityGroupIds:
 - sg-xxxxxx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We specify security groups and subnet specific to private subnet (in the required AZ) created as a result of Step 1.&lt;/p&gt; 
&lt;p&gt;Now launch the ParallelCluster custom AMI creation job like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster build-image --image-id p5-pcluster-dlgpu-baseami --image-configuration image_build.yaml --region us-east-2&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This step takes about 30-45 minutes to complete.&lt;/p&gt; 
&lt;h3&gt;Step 3: Launch ParallelCluster&lt;/h3&gt; 
&lt;p&gt;Once the AMI is ready, it’s time to launch your cluster.&lt;/p&gt; 
&lt;p&gt;Here’s a reference configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Image:
 Os: ubuntu2204
HeadNode:
 InstanceType: m5.8xlarge
 LocalStorage:
   RootVolume:
     Size: 200
     DeleteOnTermination: true
 Networking:
   SubnetId: subnet-xxxxxx
 Ssh:
   KeyName: &amp;lt;key-name&amp;gt;
 Iam:
   S3Access:
     - BucketName: &amp;lt;s3-bucket-name&amp;gt;
 CustomActions:
   OnNodeConfigured:
     Script: https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/authentication-credentials/multi-runner/postinstall.sh
     Args:
       - https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/authentication-credentials/pyxis/postinstall.sh
       - -/fsx
       - https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh
Scheduling:
 Scheduler: slurm
 SlurmQueues:
 - Name: compute
   ComputeSettings:
     LocalStorage:
       RootVolume:
         Size: 200
   ComputeResources:
   - Name: compute
     InstanceType: p5.48xlarge
     MinCount: 2
     MaxCount: 2
     CapacityReservationTarget:
           CapacityReservationId: cr-xxxxxx
     Efa:
       Enabled: true
   Networking:
     PlacementGroup:
       Enabled: true
     SubnetIds:
       - subnet-xxxxxx
   CustomActions:
     OnNodeConfigured:
       Script: https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/authentication-credentials/multi-runner/postinstall.sh
       Args:
         - https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/authentication-credentials/pyxis/postinstall.sh
         - -/fsx
   Image:
     CustomAmi: ami-xxxxxx
SharedStorage:
 - MountDir: /fsx
   Name: FSxDataMount
   StorageType: FsxLustre
   FsxLustreSettings:
     StorageCapacity: 1200
     DeploymentType: PERSISTENT_2
Monitoring:
 DetailedMonitoring: true
 Logs:
   CloudWatch:
     Enabled: true # good for debug
 Dashboards:
   CloudWatch:
     Enabled: false # provide basic dashboards
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, you should:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Update the &lt;code&gt;Region&lt;/code&gt; to your intended region&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;Networking:SubnetId&lt;/code&gt; (for the head node) to the &lt;strong&gt;public subnet&lt;/strong&gt; you created in Step 1.&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;Ssh:KeyName&lt;/code&gt;, to your specific key.&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;MinCount&lt;/code&gt; and &lt;code&gt;MaxCount&lt;/code&gt; to the desired numbers of instances you’d like in the cluster.&lt;/li&gt; 
 &lt;li&gt;Set the &lt;code&gt;CapacityReservationId&lt;/code&gt;, if any.&lt;/li&gt; 
 &lt;li&gt;Update the compute node &lt;code&gt;Networking:SubnetId&lt;/code&gt; to private subnet you created in Step 1.&lt;/li&gt; 
 &lt;li&gt;Optionally: 
  &lt;ol&gt; 
   &lt;li&gt;Set the &lt;code&gt;Iam:S3Access:BucketName&lt;/code&gt;, if you’d like the compute instances to be able to access an Amazon Simple Storage Service (Amazon S3) bucket.&lt;/li&gt; 
   &lt;li&gt;Update the &lt;code&gt;ImportPath&lt;/code&gt; within &lt;code&gt;SharedStorage&lt;/code&gt; to an Amazon S3 bucket URI, if you’d like to initialize your Lustre storage with data from an S3 bucket.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There’s more information about postinstall scripts and a library of especially useful ones &lt;a href="https://github.com/aws-samples/aws-parallelcluster-post-install-scripts/tree/main"&gt;in our GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can launch the cluster-creation process using a command like this once you’ve chosen a name for your cluster:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster --cluster-name &amp;lt;cluster-name&amp;gt; --cluster-configuration pcluster_config.yaml --region us-east-2  --rollback-on-failure False&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Some additional commands you’ll need later:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To destroy a cluster: &lt;code&gt;pcluster delete-cluster -n &amp;lt;cluster-name&amp;gt; -r us-east-2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;To SSH to the head node: &lt;code&gt;pcluster ssh -n &amp;lt;cluster-name&amp;gt; -r us-east-2 -i ssh_key.pem&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;You can use &lt;code&gt;sinfo&lt;/code&gt; on the head node to validate the cluster.&lt;/li&gt; 
 &lt;li&gt;You can get cluster status, too: &lt;code&gt;pcluster describe-cluster -n &amp;lt;cluster-name&amp;gt; -r us-east-2&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once your cluster is launched, you can validate some important elements by checking package versions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Elastic fabric adapter (EFA) – this is the custom-built, high-speed network interface into the Amazon EC2 fabric for running HPC and distributed machine-learning codes:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ fi_info --version
fi_info: 1.18.2amzn1.0
libfabric: 1.18.2amzn1.0
libfabric api: 1.18
 
$ cat /opt/amazon/efa_installed_packages
# EFA installer version: 1.24.1
# Debug packages installed: no
# Packages installed:
efa-config_1.15_all efa-profile_1.5_all libfabric-aws-bin_1.18.1_amd64 libfabric-aws-dev_1.18.1_amd64 libfabric1-aws_1.18.1_amd64 openmpi40-aws_4.1.5-1_amd64 ibacm_46.0-1_amd64 ibverbs-providers_46.0-1_amd64 ibverbs-utils_46.0-1_amd64 infiniband-diags_46.0-1_amd64 libibmad-dev_46.0-1_amd64 libibmad5_46.0-1_amd64 libibnetdisc-dev_46.0-1_amd64 libibnetdisc5_46.0-1_amd64 libibumad-dev_46.0-1_amd64 libibumad3_46.0-1_amd64 libibverbs-dev_46.0-1_amd64 libibverbs1_46.0-1_amd64 librdmacm-dev_46.0-1_amd64 librdmacm1_46.0-1_amd64 rdma-core_46.0-1_amd64 rdmacm-utils_46.0-1_amd64 efa_2.5.0-1.amzn1_amd64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We have a &lt;a href="https://github.com/aws/aws-ofi-nccl/blob/master/doc/efa-env-var.md"&gt;document&lt;/a&gt; to help streamline EFA environment variables in your Docker image and scripts, along with some additional guidance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Message-passing interface (MPI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ mpirun --version
mpirun (Open MPI) 4.1.6&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;AWS-OFI-NCCL plugin&lt;/li&gt; 
 &lt;li style="list-style-type: none"&gt; 
  &lt;ol&gt; 
   &lt;li&gt;NCCL is the NVIDIA Collective Communications Library; it provides inter-GPU communication primitives. &lt;a href="https://github.com/aws/aws-ofi-nccl"&gt;AWS-OFI-NCCL&lt;/a&gt; is a plug-in that enables developers on AWS to use libfabric as a network provider while running NCCL based applications.&lt;/li&gt; 
   &lt;li&gt;The plugin is the easiest way to get the version is by running a NCCL (NVIDIA Collective Communications Library) test. You can build the tests using &lt;a href="https://github.com/NVIDIA/nccl-tests"&gt;these instructions&lt;/a&gt;, and look for logs reporting the version. You should see something like: &lt;code&gt;Initializing aws-ofi-nccl 1.7.4-aws&lt;/code&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;NCCL version – you should also find &lt;code&gt;NCCL version 2.18.5+cuda12.2&lt;/code&gt; in the logs of the NCCL test.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To resolve issues regarding Cluster Creation, please refer &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/troubleshooting-v3-cluster-deployment.html"&gt;to our troubleshooting documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 4: Cluster validation&lt;/h3&gt; 
&lt;p&gt;NeMo Launcher offers a &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/cloudserviceproviders.html#cluster-validation"&gt;cluster validation script&lt;/a&gt; which runs NVIDIA &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cloud-native/containers/dcgm"&gt;DCGM&lt;/a&gt; (Data Center GPU Manager) tests and NCCL tests. The DCGM is a suite of tools for managing and monitoring NVIDIA GPUs in cluster environments. All DCGM functionality is available via the dcgmi, which is the DCGM command-line utility.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd /path/to/NeMoMegatronLauncher/csp_tools/aws &amp;amp;&amp;amp; bash cluster_validation.sh --nodes=2&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To resolve issues regarding Cluster Validation, please refer to the “Troubleshooting” section later.&lt;/p&gt; 
&lt;h3&gt;Step 5: Launch GPT training job&lt;/h3&gt; 
&lt;p&gt;Use &lt;code&gt;enroot&lt;/code&gt; to import the container to local:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;enroot import --output nemo_megatron_training.sqsh dockerd://nvcr.io/ea-bignlp/nemofw-training:23.07-py3&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, download the vocab and merges files:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json
wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here is the &lt;code&gt;config.yaml&lt;/code&gt; for a GPT 20B training job. Make a copy of this file and make changes as we describe next:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;defaults:
  - _self_
  - cluster: bcm  # Leave it as bcm even if using bcp. It will be ignored for bcp.
  - data_preparation: gpt3/download_gpt3_pile
  - training: gpt3/20b
  - conversion: gpt3/convert_gpt3
  - fine_tuning: null
  - prompt_learning: null
  - adapter_learning: null
  - ia3_learning: null
  - evaluation: gpt3/evaluate_all
  - export: gpt3/export_gpt3
  - override hydra/job_logging: stdout

hydra:
  run:
    dir: .
  output_subdir: null

debug: False

stages:
  - training
  # - conversion
  # - evaluation
  # - export

cluster_type: bcm  # bcm or bcp. If bcm, it must match - cluster above.
launcher_scripts_path: /home/ubuntu/NeMo-Megatron-Launcher/launcher_scripts  # Path to NeMo Megatron Launch scripts, should ends with /launcher_scripts
data_dir: /fsx/gpt3_dataset  # Location to store and read the data.
base_results_dir: /fsx/gpt3_dataset/results  # Location to store the results, checkpoints and logs.
container_mounts:
  - /home/ubuntu/NeMo-Megatron-Launcher/csp_tools/aws/:/nccl
container: /home/ubuntu/NeMo-Megatron-Launcher/nemo_megatron_training.sqsh

wandb_api_key_file: null  # File where the w&amp;amp;B api key is stored. Key must be on the first line.

env_vars:
  NCCL_TOPO_FILE: /nccl/topo.xml # Should be a path to an XML file describing the topology
  UCX_IB_PCI_RELAXED_ORDERING: null # Needed to improve Azure performance
  NCCL_IB_PCI_RELAXED_ORDERING: null # Needed to improve Azure performance
  NCCL_IB_TIMEOUT: null # InfiniBand Verbs Timeout. Set to 22 for Azure
  NCCL_DEBUG: INFO # Logging level for NCCL. Set to "INFO" for debug information
  NCCL_PROTO: simple # Protocol NCCL will use. Set to "simple" for AWS
  TRANSFORMERS_OFFLINE: 1
  NCCL_AVOID_RECORD_STREAMS: 1

# GPU Mapping
numa_mapping:
  enable: True  # Set to False to disable all mapping (performance will suffer).
  mode: unique_contiguous  # One of: all, single, single_unique, unique_interleaved or unique_contiguous.
  scope: node  # Either node or socket.
  cores: all_logical  # Either all_logical or single_logical.
  balanced: True  # Whether to assing an equal number of physical cores to each process.
  min_cores: 1  # Minimum number of physical cores per process.
  max_cores: 8  # Maximum number of physical cores per process. Can be null to use all available cores.

# Do not modify below, use the values above instead.
data_preparation_config: ${hydra:runtime.choices.data_preparation}
training_config: ${hydra:runtime.choices.training}
fine_tuning_config: ${hydra:runtime.choices.fine_tuning}
prompt_learning_config: ${hydra:runtime.choices.prompt_learning}
adapter_learning_config: ${hydra:runtime.choices.adapter_learning}
ia3_learning_config: ${hydra:runtime.choices.ia3_learning}
evaluation_config: ${hydra:runtime.choices.evaluation}
conversion_config: ${hydra:runtime.choices.conversion}
export_config: ${hydra:runtime.choices.export}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this &lt;code&gt;config.yaml&lt;/code&gt; file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Update the &lt;code&gt;launcher_scripts_path&lt;/code&gt; to the absolute path for NeMo Launcher’s launcher_scripts&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;data_dir&lt;/code&gt; to wherever the data is residing.&lt;/li&gt; 
 &lt;li&gt;Update container with path to &lt;code&gt;sqsh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;base_results_dir&lt;/code&gt; to point to the directory where you’d like to organize the results.&lt;/li&gt; 
 &lt;li&gt;Update &lt;code&gt;NCCL_TOPO_FILE&lt;/code&gt; to point to a xml &lt;a href="https://github.com/aws/aws-ofi-nccl/blob/master/topology/p5.48xl-topo.xml"&gt;specific to P5&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Optionally update &lt;code&gt;container_mounts&lt;/code&gt; to mount a specific directory from host into container.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can find some example configuration files &lt;a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/config.yaml"&gt;in our GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To launch the job:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd /path/to/NeMoMegatronLauncher/launcher_scripts &amp;amp;&amp;amp; python main.py &amp;amp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once you launch this job, you can look at the &lt;code&gt;.log&lt;/code&gt; file (of format &lt;code&gt;log-nemo-megatron-&amp;lt;model_name&amp;gt;_&amp;lt;date&amp;gt;.log&lt;/code&gt;) to track the logs of the training job. Additionally, you can use a &lt;code&gt;.err&lt;/code&gt; file (of format &lt;code&gt;log-nemo-megatron-&amp;lt;model_name&amp;gt;_&amp;lt;date&amp;gt;.err&lt;/code&gt;) to track the errors and warnings, if any, of your training job. If you set up TensorBoard, you can also check the events file (of format &lt;code&gt;events.out.tfevents.&amp;lt;compute_details&amp;gt;&lt;/code&gt;) to look over the loss curves, learning rates and other parameters that NeMo tracks. For more information on this, refer to &lt;a href="https://www.tensorflow.org/tensorboard/get_started"&gt;the TensorBoard documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note: The files mentioned above are located in the directory specified by you in the &lt;code&gt;base_results_dir&lt;/code&gt; field in the &lt;code&gt;config.yaml&lt;/code&gt; file above.&lt;/p&gt; 
&lt;p&gt;This is what an example &lt;code&gt;.log&lt;/code&gt; file looks like. Note, this is only a part of the entire log file (“…” below entails omitted parts of the output), until step 3 of the training job (the actual logs contain logs until step 60000000):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;    ************** Experiment configuration ***********
…
[NeMo I 2024-01-18 00:20:43 exp_manager:394] Experiments will be logged at /shared/backup120820231021/gpt_results/gpt3_126m_8_fp8_01172024_1619/results
[NeMo I 2024-01-18 00:20:43 exp_manager:835] TensorboardLogger has been set up
…
[NeMo I 2024-01-18 00:21:13 lr_scheduler:910] Scheduler "&amp;lt;nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fe75b3387f0&amp;gt;" 
    will be used during training (effective maximum steps = 60000000) - 
    Parameters : 
…
Sanity Checking DataLoader 0:   0%|                | 0/2 [00:00&amp;lt;?, ?it/s]
Sanity Checking DataLoader 0: 100%|████████| 2/2 [00:07&amp;lt;00:00,  3.92s/it]
…                                                      
Epoch 0: :   0%| | 1/60000000 [00:35&amp;lt;583621:51:56, v_num=, reduced_train_
Epoch 0: :   0%| | 2/60000000 [00:36&amp;lt;303128:00:21, v_num=, reduced_train_
Epoch 0: :   0%| | 3/60000000 [00:36&amp;lt;202786:11:11, v_num=, reduced_train_
…
&lt;/code&gt;&lt;/pre&gt; 
&lt;div id="attachment_3705" style="width: 949px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3705" loading="lazy" class="size-full wp-image-3705" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/22/IMG-2024-05-22-13.25.45.png" alt="Figure 2 – Sample output graphs from our run using TensorBoard." width="939" height="301"&gt;
 &lt;p id="caption-attachment-3705" class="wp-caption-text"&gt;Figure 2 – Sample output graphs from our run using TensorBoard.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Cluster creation failed&lt;/h3&gt; 
&lt;p&gt;Bringing up a cluster can fail for many reasons. The easiest way to debug is to create a cluster with &lt;code&gt;--rollback-on-failure False&lt;/code&gt;. Then you can see information in the AWS CloudFormation console detailing why the cluster creation failed. Even more detailed information will be in the logs on the head node which you can find in: &lt;code&gt;/var/log/cfn-init.log /var/log/cloud-init.log /var/log/cloud-init-output.log&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The most common reason for cluster failure is that you may not have access to the target EC2 capacity. You’ll see this in the &lt;code&gt;/var/log/parallelcluster/clustermgtd&lt;/code&gt; log on the head node, or in CloudFormation.&lt;/p&gt; 
&lt;h3&gt;Cluster Validation Issues&lt;/h3&gt; 
&lt;h3&gt;1 – DCGMI output&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;Error: Unable to complete diagnostic for group 2147483647. Return: (-21) Host engine connection invalid/disconnected.
srun: error: compute-st-compute-1: task 0: Exited with exit code 235
Error: Unable to complete diagnostic for group 2147483647. Return: (-21) Host engine connection invalid/disconnected.
srun: error: compute-st-compute-2: task 1: Exited with exit code 235
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Resolution&lt;/strong&gt;: The DCGM container may not be accessible from &lt;a href="https://docs.nvidia.com/ngc/index.html"&gt;NGC&lt;/a&gt;. Try converting the DCGM container to a local &lt;code&gt;.sqsh&lt;/code&gt; file using &lt;code&gt;enroot&lt;/code&gt; and pointing the validation script (&lt;code&gt;csp_tools/aws/dcgmi_diag.sh&lt;/code&gt;) to this local file, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;enroot import --output dcgm.sqsh 'docker://$oauthtoken@nvcr.io#nvidia/cloud-native/dcgm:2.3.5-1-ub
i8' 
srun --container-image=dcgm.sqsh bash -c "dcgmi diag -r 3"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2 – PMIX Error in NCCL Logs&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;[compute-st-compute-2:201457] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Resolution&lt;/strong&gt;: This is a non-fatal error. Try adding export &lt;code&gt;PMIX_MCA_gds=^ds12&lt;/code&gt; to the &lt;code&gt;csp_tools/aws/nccl.sh&lt;/code&gt; script.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog post, we’ve shown you how to leverage the AWS ParallelCluster and the NVIDIA NeMo Megatron Framework to enable large-scale Large Language Model (LLM) training on AWS P5 instances. Together, AWS ParallelCluster and the NVIDIA NeMo Megatron Framework can empower researchers and developers to train LLMs on trillions of tokens, scaling to thousands of GPUs, which means accelerating time-to-market for cutting-edge natural language processing (NLP) applications.&lt;/p&gt; 
&lt;p&gt;To learn more about training GPT3 NeMo Megatron on Slurm, refer to &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher"&gt;AWS Sample&lt;/a&gt;s. To learn more about ParallelCluster and Nemo Megatron, check out the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html"&gt;ParallelCluster User Guide&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA/NeMo-Megatron-Launcher"&gt;NeMo Megatron Launcher&lt;/a&gt;, and &lt;a href="https://www.mlworkshops.com/01-getting-started/05-pcui-connect.html"&gt;Parallel Cluster UI&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Building an AI simulation assistant with agentic workflows</title>
		<link>https://aws.amazon.com/blogs/hpc/building-an-ai-simulation-assistant-with-agentic-workflows/</link>
		
		<dc:creator><![CDATA[Sam Bydlon]]></dc:creator>
		<pubDate>Tue, 28 May 2024 14:57:10 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">a1c68c7ee69e75af5f1ce9d202a580c62bf691db</guid>

					<description>Simulations provide critical insights but running them takes specialized people, which can slow everyone down. We show how a Simulation Assistant can use LLMs and agents to start these workflows via chat so you can get results sooner.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3673" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/20/building-an-AI-simulation-assistant.png" alt="" width="380" height="212"&gt;Simulations have become indispensable tools which enable organizations to predict outcomes, evaluate risks, and make informed decisions. Simulations provide valuable insights that drive strategic decision-making – running the gamut from supply chain optimization to exploration of design alternatives for products and processes.&lt;/p&gt; 
&lt;p&gt;But running and analyzing simulations can be a time-consuming task because it requires specialized teams of data scientists, analysts, and subject matter experts. In the manufacturing sector, these experts are in high demand to model and optimize complex production processes. This regularly leads to backlogs and delays in obtaining critical insights. In the healthcare industry, specialized teams of epidemiologists and statisticians run the simulations for infectious disease modeling that public health officials need to make decisions. The limited bandwidth of these specialists creates bottlenecks and inefficiencies that impact the ability to rapidly respond to emerging health crises in a data-driven manner.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll examine an generative AI-based “Simulation Assistant” demo application built using &lt;a href="https://python.langchain.com/v0.1/docs/modules/agents/"&gt;LangChain Agents&lt;/a&gt; and &lt;a href="https://www.anthropic.com/"&gt;Anthropic’s&lt;/a&gt; recently released Claude V3 large language model (LLM) on Amazon Bedrock.&lt;/p&gt; 
&lt;p&gt;By leveraging the latest advancements in LLMs and AWS, we’ll show you how to streamline and democratize your simulation workflows using a scalable and serverless architecture for an application with a chatbot-style interface. This will allow users to launch and interact with simulations using natural language prompts.&lt;/p&gt; 
&lt;h2&gt;How does this help experts?&lt;/h2&gt; 
&lt;p&gt;The Simulation Assistant demo offers a blueprint for providing significant value to organizations in two key ways. It:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Democratizes simulation-driven problem solving: &lt;/strong&gt;While regulated industries may still require certified personnel for final sign-off, this solution demonstrates a way to democratize simulation use beyond specialist teams. By enabling knowledgeable personnel across functions, such as analysts, managers, and decision-makers, to launch and analyze simulations under the guidance of experts, organizations can increase the utilization of simulation capabilities and free up the bandwidth of their simulation experts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhances efficiency for simulation experts: &lt;/strong&gt;Allowing a wider user-base to run routine simulations lets experts focus on high value tasks like performance tuning or building new simulations. Streamlined, automated workflows accessible through a single chatbot interface improves productivity, standardizes processes, enables knowledge sharing, and increases result reliability.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Whether you’re a business analyst, product manager, researcher, or a simulation expert, this demonstration offers an intuitive and efficient way to harness the power of simulations by leveraging the capabilities of generative AI through Amazon Bedrock and the scalability of AWS – driving innovation and operational excellence across diverse industries.&lt;/p&gt; 
&lt;h2&gt;Solution overview&lt;/h2&gt; 
&lt;div id="attachment_3742" style="width: 1511px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3742" loading="lazy" class="size-full wp-image-3742" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/Sim_Assist_Demo_May24update-AWS-Architecture.drawio.png" alt="Figure 1 – Architectural diagram for Simulation Assistant application. A containerized Streamlit web app is deployed via a load-balanced AWS Fargate service. The web app instantiates an LLM-based agent with access to seven tools. The retriever tool provides RAG capabilities using Amazon Kendra. Others tools enable the agent to perform a custom mathematical transform, invoke a simple inflation simulation executed using AWS Lambda, invoke a set of containerized investment portfolio simulations using AWS Batch that store results in an Amazon DynamoDB table, and analyze a batch of simulation results by generating plots." width="1501" height="1163"&gt;
 &lt;p id="caption-attachment-3742" class="wp-caption-text"&gt;Figure 1 – Architectural diagram for Simulation Assistant application. A containerized Streamlit web app is deployed via a load-balanced AWS Fargate service. The web app instantiates an LLM-based agent with access to seven tools. The retriever tool provides RAG capabilities using Amazon Kendra. Others tools enable the agent to perform a custom mathematical transform, invoke a simple inflation simulation executed using AWS Lambda, invoke a set of containerized investment portfolio simulations using AWS Batch that store results in an Amazon DynamoDB table, and analyze a batch of simulation results by generating plots.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 1 depicts the architecture of the Simulation Assistant application. A web application, built using &lt;a href="https://streamlit.io/"&gt;Streamlit&lt;/a&gt;, serves as the user interface. Streamlit is an open-source Python library that allows you to create interactive web applications for machine learning and data science use cases. We’ve containerized this app using Docker and stored it in an &lt;a href="https://aws.amazon.com/ecr/"&gt;Amazon Elastic Container Registry (ECR)&lt;/a&gt; repository.&lt;/p&gt; 
&lt;p&gt;The containerized web application is deployed as a load-balanced AWS Fargate Service within an &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon Elastic Container Service (ECS)&lt;/a&gt; cluster. &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt; is a serverless compute engine that allows you to run containers without managing servers or clusters. By using Fargate, the Simulation Assistant application can scale its compute resources up or down automatically based on the incoming traffic, ensuring optimal performance and cost-efficiency.&lt;/p&gt; 
&lt;p&gt;The web application is fronted by an &lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html"&gt;Application Load Balancer (ALB)&lt;/a&gt;. The ALB distributes incoming traffic across multiple targets, like Fargate tasks, in a balanced manner. This load balancing mechanism ensures that user requests are efficiently handled, even during periods of high traffic, by dynamically routing requests to available container instances.&lt;/p&gt; 
&lt;h2&gt;Life cycle of a request&lt;/h2&gt; 
&lt;p&gt;When a user accesses the Simulation Assistant application, their request is received by the ALB, which then forwards the request to one of the healthy Fargate tasks running the Streamlit web application. This serverless deployment approach, combined with the load-balancing capabilities of the ALB, provides a highly available and scalable architecture for the Simulation Assistant, allowing it to handle varying levels of user traffic without the need for manually provisioning and managing servers.&lt;/p&gt; 
&lt;p&gt;The Streamlit web application acts as the central hub, orchestrating the interaction between different AWS services to enable seamless simulation capabilities for users. Within the Streamlit app, we’ve used &lt;a href="https://aws.amazon.com/bedrock/"&gt;Amazon Bedrock&lt;/a&gt; to process user queries by leveraging state-of-the-art language models. Bedrock is a fully-managed service that makes these art foundation models from both Amazon and leading AI startups available via a unified API, while abstracting away complex model management.&lt;/p&gt; 
&lt;p&gt;For simple simulations, like price inflation scenarios, the Streamlit app integrates with &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; functions. These serverless functions can encapsulate lightweight simulation logic, allowing for efficient execution and scalability without the need for provisioning and managing dedicated servers.&lt;/p&gt; 
&lt;p&gt;Additionally, we’re also leveraging &lt;a href="https://aws.amazon.com/kendra/"&gt;Amazon Kendra&lt;/a&gt;, an intelligent search service, to enable retrieval augmented generation (RAG). Amazon Kendra indexes and searches through documents stored in an &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon S3&lt;/a&gt; bucket, acting as a source repository. This integration empowers the application to provide relevant information from existing documents, enhancing the simulation capabilities and enabling more informed decision-making.&lt;/p&gt; 
&lt;p&gt;For more computationally intensive simulations, like running sets of investment portfolio simulations in a Monte Carlo-style manner, the Simulation Assistant uses &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;. AWS Batch is a fully managed batch processing service that efficiently runs batch computing workloads across AWS resources. The Simulation Assistant submits jobs to AWS Batch, which then dynamically provisions the compute resources needed to run them in parallel, enabling faster execution times and scalability.&lt;/p&gt; 
&lt;p&gt;Once the simulations are complete, the results are stored in an &lt;a href="https://aws.amazon.com/dynamodb/"&gt;Amazon DynamoDB&lt;/a&gt; database, a fully managed NoSQL database service. DynamoDB provides fast and predictable performance with seamless scalability, making it well-suited for storing and retrieving simulation data efficiently. Furthermore, the application integrates with &lt;a href="https://aws.amazon.com/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt;, a serverless event bus service. When a simulation batch is finished, EventBridge triggers a notification, which is sent to the user via email using &lt;a href="https://aws.amazon.com/sns/"&gt;Amazon Simple Notification Service (SNS)&lt;/a&gt;. This notification system keeps users informed about the completion of their simulation requests, allowing them to promptly access and analyze the results.&lt;/p&gt; 
&lt;h2&gt;LLM-based “agents” with “tools”&lt;/h2&gt; 
&lt;p&gt;The Streamlit web application houses the logic of the key technological concept underlying the Simulation Assistant application, the enablement of what is called “&lt;em&gt;agentic behavior&lt;/em&gt;” of an LLM. The precise definition of an LLM agent is elusive, because the field is relatively new and rapidly evolving. But the general idea is to augment the capabilities of LLMs by enabling the models to break down tasks into individual steps, make plans, and take actions including the use of tools to solve specific tasks, and even work together as a team of multiple agents that can collaborate and influence each other.&lt;/p&gt; 
&lt;p&gt;One design pattern for enabling agentic behavior of is called “&lt;a href="https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/?ref=dl-staging-website.ghost.io"&gt;Tool Use&lt;/a&gt;”, in which an LLM is taught (through prompt engineering or fine-tuning) how to trigger pieces of additional software, wrapped in a standardized form like a function call. These additional pieces of software are called “tools”. The Simulation Assistant employs tools to augment the behavior of an underlying foundation model. In our demo, the underlying foundation model is &lt;a href="https://www.anthropic.com/news/claude-3-family"&gt;Claude V3 Sonnet&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Tools help LLMs solve problems that are not reliably solved by direct generation using the underlying transformer network. LLMs are demonstrating incredible ability at solving problems and performing mathematical reasoning – try asking Claude V3 Sonnet to “simulate the price of milk over the next 20 years with a dynamic inflation rate where the mean is 4% and standard deviation 2%”. But their ability to simulate more complex systems like financial markets or the spread of wildfires is still (for now) best left to trusted simulation codes. By introducing tools that can execute those trusted codes and instructing the LLM on how and when tools should be used, LLMs can gain profound new abilities.&lt;/p&gt; 
&lt;p&gt;There are many possibilities for what tools can &lt;em&gt;be&lt;/em&gt; and &lt;em&gt;do&lt;/em&gt;, and the application of tools and agentic behavior in general is a concept that will have wide ranging uses cases far beyond simulation. Tools can help LLMs to perform web searches, query a database, or schedule a meeting using your calendar system, just to name a few options.&lt;/p&gt; 
&lt;h2&gt;How we applied agents and tools&lt;/h2&gt; 
&lt;p&gt;The Simulation Assistant instantiates a LangChain agent, based on Claude V3 Sonnet. &lt;a href="https://aws.amazon.com/what-is/langchain/"&gt;LangChain&lt;/a&gt; is an open-source framework for building applications with LLMs. With LangChain &lt;a href="https://python.langchain.com/v0.1/docs/modules/agents/"&gt;Agents&lt;/a&gt; and &lt;a href="https://python.langchain.com/v0.1/docs/modules/tools/"&gt;Tools&lt;/a&gt;, developers can create powerful generative AI-based applications that leverage LLM agentic behavior and integrate with existing systems and workflows.&lt;/p&gt; 
&lt;p&gt;There are &lt;a href="https://python.langchain.com/docs/modules/agents/agent_types/"&gt;several kinds of agents&lt;/a&gt; that can be constructed with LangChain, but not all agent types support tools with multiple inputs. Since simulations often require users to specify an array of input parameters to define the system mechanics to be simulated, we’ll want to choose an agent type that supports multi-input tools and can be used in concert with Amazon Bedrock. The Simulation Assistant demo uses a &lt;a href="https://python.langchain.com/docs/modules/agents/agent_types/structured_chat/"&gt;&lt;em&gt;structured chat agent&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;which satisfies both of these requirements.&lt;/p&gt; 
&lt;p&gt;Building the Simulation Assistant involved addressing several key technical challenges:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Designing and implementing an agent-tool architecture:&lt;/strong&gt; To create an effective agent-tool system, careful design of the tool interfaces and integration with the LangChain agent framework was required. We handled multi-input tools by defining structured input schemas. We enabled communication between the LLM and the tools using LangChain’s &lt;em&gt;tool abstraction layer&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt engineering for agentic behavior:&lt;/strong&gt; Crafting prompts that guide the LLM to exhibit desired agentic behavior was an iterative process. The approach involved exploring the capabilities and limitations of Claude V3 Sonnet, designing prompts that promoted appropriate tool selection and usage, and refining the prompts based on performance in test scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scaling and managing simulation workloads:&lt;/strong&gt; To handle computationally intensive simulations, we designed a scalable architecture using AWS Batch for running simulation jobs. This allowed us to efficiently manage compute resources and reliably execute simulations with varying workloads.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interpreting and visualizing simulation results:&lt;/strong&gt; To help users interpret and visualize simulation outputs, we developed tools that process and summarize simulation data, and integrated visualizations within Simulation Assistant.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We gave the Simulation Assistant agent access to seven tools, including ones designed to perform a custom mathematical transform defined within the Streamlit application, perform simple inflation simulations housed in an AWS Lambda function, launch Monte Carlo-style simulation ensembles via AWS Batch to mimic an investment portfolio and visualize results, and to perform RAG over an internal database of documents.&lt;/p&gt; 
&lt;p&gt;These are simple examples, showing how tools can be built to handle some common elements of simulation workflows. But tools can do a lot more: they can also be designed to prepare configuration files, trigger pre- or post-processing jobs on related data – or even call other specialized LLMs that are able to interpret and summarize the results of simulations.&lt;/p&gt; 
&lt;h2&gt;A sample workflow&lt;/h2&gt; 
&lt;p&gt;When a user enters a natural language query into Simulation Assistant like, “&lt;em&gt;run a set of 100 investment simulations starting at $10,000, with cash flow of $250, for 50 steps&lt;/em&gt;,” we pass the query to Amazon Bedrock inside a prompt &lt;em&gt;specifically engineered&lt;/em&gt; to work well in an agent-tool setting. You can find examples of polished prompts you can use at the &lt;a href="https://smith.langchain.com/hub"&gt;LangChain Hub&lt;/a&gt; – including prompts that &lt;a href="https://smith.langchain.com/hub/hwchase17/structured-chat-agent?organizationId=6e7cb68e-d5eb-56c1-8a8a-5a32467e2996"&gt;work well with structured chat agents&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Behind the scenes, the agentic LLM breaks down the request into steps, decides whether each step can be completed with “bare hands” (i.e. without tools), or whether one of its tools can be used to complete the step. For our example request, the agentic LLM determines that it needs to use a tool that allows it to run batches of investment portfolio simulations, performs the entity extraction of the key parameters like cash flow, and uses these parameters to trigger an AWS Batch job to run the simulations. It does this completely independently.&lt;/p&gt; 
&lt;p&gt;The agent then responds to the user telling them they’ll receive an email when the simulations are complete and provides some helpful information that can be used to analyze the simulation results.&lt;/p&gt; 
&lt;p&gt;Figure 2 is a graphical depiction of this process.&lt;/p&gt; 
&lt;div id="attachment_3671" style="width: 1198px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3671" loading="lazy" class="size-full wp-image-3671" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/20/IMG-2024-05-20-09.11.00.png" alt="Figure 2 – Workflow of an LLM-based agent with tools for batch simulation execution. User requests are formatted into prompts by the application and sent to the LLM, which decomposes the request, selects and triggers the appropriate tool with extracted parameters (run_batch_simulations tool with simulation inputs). Tool results are returned to the LLM for generating a final response displayed to the user, including information for accessing and analyzing simulation outputs." width="1188" height="627"&gt;
 &lt;p id="caption-attachment-3671" class="wp-caption-text"&gt;Figure 2 – Workflow of an LLM-based agent with tools for batch simulation execution. User requests are formatted into prompts by the application and sent to the LLM, which decomposes the request, selects and triggers the appropriate tool with extracted parameters (run_batch_simulations tool with simulation inputs). Tool results are returned to the LLM for generating a final response displayed to the user, including information for accessing and analyzing simulation outputs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For a deeper look into how we used these tools and to see the Simulation Assistant demo in action (including the batch simulation query described above) you can check out the video demo below. This will walk you through some potential interactions with the application, and shows the LLM-based agent interacting with various tools representing different aspects of a simulation workflow. You’ll see the agent list the available tools and provide instructions on how to use them when prompted.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 3 â&#128;&#147; A video showing the Simulation Assistant demo in action. Click to play." width="500" height="281" src="https://www.youtube-nocookie.com/embed/chmq52YX84A?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 3 – A video showing the Simulation Assistant demo in action. Click to play.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The core portion of the demo involves the user asking the agent to run a set of 100 investment portfolio simulations with specific parameters like starting amount, cash flow, and number of steps. The agent interprets this natural language query, extracts the necessary parameters, and triggers an AWS Batch job to execute the simulations in parallel. Once the simulations complete, the agent retrieves the results from a database and visualizes them using another tool.&lt;/p&gt; 
&lt;p&gt;Additionally, the video contrasts the agent’s capabilities with a standard LLM’s response to the same query, so you can see the enhanced abilities provided by the agent-tool architecture. You’ll also notice the agent breaking down a complex problem into steps and leveraging multiple tools in different orders to solve it, demonstrating the flexibility of the approach.&lt;/p&gt; 
&lt;h2&gt;Future Work&lt;/h2&gt; 
&lt;p&gt;While the Simulation Assistant demo achieves a lot, we want to extend the demo in the future to tackle some more technical challenges:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Integrating with existing simulation codebases:&lt;/strong&gt; A key goal of ours is to integrate existing simulation codebases as tools within the agent-tool architecture. This will require a deep understanding of the codebases and, in some cases, modifying them to fit the tool interface requirements. For example, to integrate &lt;a href="https://www.openfoam.com/"&gt;OpenFOAM&lt;/a&gt; (a popular open-source computational fluid dynamics software) as a tool, the approach would involve wrapping OpenFOAM’s solver and utility executables as Python functions, defining input and output schemas, and potentially modifying the codebase to enable programmatic execution and data exchange.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ensuring simulation reproducibility and traceability:&lt;/strong&gt; We’d like to enable comprehensive reproducibility and traceability of the simulation runs by implementing logging mechanisms that track input parameters, intermediate steps, and provide detailed documentation for each simulation execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Establishing guardrails:&lt;/strong&gt; Guardrails and safeguards are crucial to ensure the secure and responsible use of the simulation framework. This may involve setting limits on compute resources, enforcing access controls, and implementing validation checks to prevent potential misuse or unintended consequences. Additionally, ethical considerations should be considered, like ensuring privacy and data protection, avoiding biases, and promoting transparency in the simulation processes.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;This post introduces an AWS-native Simulation Assistant demo that we hope will provide inspiration, and a blueprint, for organizations looking to leverage generative AI and other cutting-edge techniques like agentic LLM behavior to help their businesses.&lt;/p&gt; 
&lt;p&gt;The demo shows the potential of these technologies for revolutionizing simulation workflows across various industries. Using LangChain Agents, Amazon Bedrock, and the scalability of AWS services, the Simulation Assistant demo can offer a glimpse into a future where simulations might be more accessible and interactive. We hope this will let organizations unlock new insights and drive better decision-making.&lt;/p&gt; 
&lt;p&gt;The application of agentic LLM frameworks extend far beyond simulations, and the concept of tools can be applied to a countless number of domains or workflows. By enabling LLMs to interact with external systems, perform computations, and trigger actions, organizations can augment their existing processes in this way, fostering innovation and operational excellence.&lt;/p&gt; 
&lt;p&gt;Solutions like this can also pave the way for new paradigms in human-machine collaboration, amplifying human capabilities and accelerating the pace of discovery.&lt;/p&gt; 
&lt;p&gt;If your organization is interested in exploring how to implement these techniques in concert with your workflows, we encourage you to reach out to your AWS account team, or send an email to &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Announcing: Seqera Containers for the bioinformatics community</title>
		<link>https://aws.amazon.com/blogs/hpc/announcing-seqera-containers-for-the-bioinformatics-community/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Thu, 23 May 2024 15:35:06 +0000</pubDate>
				<category><![CDATA[Featured]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Life Sciences]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Bioinformatics]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[Genomics]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<guid isPermaLink="false">771f9023418ab35b6435bfa6a8543c7119414297</guid>

					<description>Genomics community: rejoice! Seqera and AWS have teamed up to announce Seqera Containers, an open-source, no cost, reliable way to generate containers.</description>
										<content:encoded>&lt;div id="attachment_3815" style="width: 390px" class="wp-caption alignright"&gt;
 &lt;a href="https://youtu.be/SE6ZCvh1ThQ" target="_new" rel="noopener"&gt;&lt;img aria-describedby="caption-attachment-3815" loading="lazy" class="wp-image-3815 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/YT-Containers-just-got-easier.png" alt="Announcing: Seqera Containers for the bioinformatics community" width="380" height="213"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-3815" class="wp-caption-text"&gt;You can watch a demo led by Phil Ewels on our Tech Shorts channel on YouTube.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;This post was contributed by Brendan Bouffler, Head of Developer Relations, HPC Engineering at AWS, Phil Ewels, Snr Product Manager for Open Source at Seqera, and Paolo Di Tommaso, the CTO &amp;amp; co-founder of Seqera.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Life sciences is a rapidly-moving area of research, and you don’t need to look too far for amazing examples where the practitioners of this field have adopted new tools and techniques to solve ever harder problems. Within this domain, bioinformatics has emerged as a crucial discipline, bridging the gap between biology and computer science. Many of the features in AWS Batch have been driven by this community, leading to a close relationship between our teams and the people working at the front lines of research.&lt;/p&gt; 
&lt;p&gt;But while biological data has become more complex and massive, it has also become increasingly personal, and inevitably regulated. The world needed researchers across borders and geographies to share insights and techniques without necessarily moving the data, or sharing the same infrastructure. The conditions were set for another uptake of new technology.&lt;/p&gt; 
&lt;p&gt;Adopting containers for packaging applications turned out to be pivotal for this shift: it revolutionized the way bioinformatics workflows are &lt;a href="https://seqera.io/blog/nextflow-and-aws-batch-inside-the-integration-part-1-of-3/"&gt;developed&lt;/a&gt;, &lt;a href="https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-1-of-2/"&gt;deployed&lt;/a&gt;, &lt;em&gt;and &lt;/em&gt;&lt;a href="https://nf-co.re/"&gt;&lt;em&gt;shared&lt;/em&gt;&lt;/a&gt; – increasing the reproducibility of an analysis. But while they’ve unquestionably simplified the life of bioinformaticians, their creation and usage is not without some friction.&lt;/p&gt; 
&lt;p&gt;Today, in conjunction with our friends at &lt;a href="https://seqera.io/about/"&gt;Seqera&lt;/a&gt;, we’re announcing a project we’re supporting called &lt;strong&gt;Seqera Containers. &lt;/strong&gt;This is a freely-available resource for the entire bioinformatics community (in the cloud or not) which simplifies the container experience – allowing researchers to generate a container for any combination of Conda and PyPI packages at the click of a button. Best of all, Seqera Containers are publicly accessible to everyone, at no-cost.&lt;/p&gt; 
&lt;h2&gt;Building containers on-demand – powered by Wave and AWS&lt;/h2&gt; 
&lt;p&gt;Seqera Containers is not a traditional container registry: users are not expected to browse existing container images or push local images to a remote. Instead, Seqera Containers provides a simple form to choose a combination of packages from Conda or the Python Package Index (PyPI). Clicking “Get Container” returns a Docker or Singularity image URI which can be used immediately.&lt;/p&gt; 
&lt;div id="attachment_3716" style="width: 1328px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3716" loading="lazy" class="size-full wp-image-3716" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/23/IMG-2024-05-23-07.44.30-1.png" alt="Figure 1 - the community wave registry is easy to access, and even easier to use." width="1318" height="942"&gt;
 &lt;p id="caption-attachment-3716" class="wp-caption-text"&gt;Figure 1 – the community wave registry is easy to access, and even easier to use.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Wave containerization service&lt;/h2&gt; 
&lt;p&gt;The beating heart of Seqera Containers is &lt;a href="https://seqera.io/wave/"&gt;Wave&lt;/a&gt; – an &lt;a href="https://github.com/seqeralabs/wave"&gt;open-source&lt;/a&gt; technology built by Seqera for next-generation container provisioning that aims to simplify the usage of containers. Instead of writing Dockerfile scripts to build images, a developer or end user can request a just-in-time container image tailored for the target execution platform using only package names and version numbers from popular software packaging tools &lt;a href="https://conda.io/"&gt;Conda&lt;/a&gt; (including &lt;a href="https://bioconda.github.io/"&gt;Bioconda&lt;/a&gt;&lt;u&gt;),&lt;/u&gt; and the &lt;a href="https://pypi.org/"&gt;Python Package Index&lt;/a&gt; and later, &lt;a href="https://packages.spack.io/"&gt;Spack&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Wave can also compile images on the fly to match your compute infrastructure – including x86 and Arm64-based processors (where upstream packages are available).&lt;/p&gt; 
&lt;p&gt;Wave supports both Docker and Singularity images – and thus any other container technologies that use Docker images, like Podman, Shifter, Sarus, or Charliecloud. Seqera Containers provides an OCI compliant registry for native Singularity image builds and even allows direct .sif image download as a flat file, without needing Singularity installed locally.&lt;/p&gt; 
&lt;p&gt;As part of the build process, Wave conducts a vulnerability scan using the &lt;a href="https://trivy.dev/"&gt;Trivy&lt;/a&gt; security scanner and is able to generate &lt;a href="https://en.wikipedia.org/wiki/Software_supply_chain"&gt;SBOM manifests&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Reproducible container URIs&lt;/h2&gt; 
&lt;p&gt;Building containers on demand is convenient, but for the sake of performance, reproducibility, and provenance, it’s important to use the exact same image for every run. To do this, Wave generates a checksum for the build and can push the built image to a traditional OCI registry. Subsequent requests with the same checksum will be returned directly from the registry.&lt;/p&gt; 
&lt;p&gt;So, when you request an image through the Seqera Containers web interface, most of the time the images will come from the cache – meaning virtually instant access and consistent reuse by you, or anyone in the community. That image cache doesn’t expire, so those images will still be there when you need to reproduce that analysis in a few years’ time.&lt;/p&gt; 
&lt;h2&gt;Built-in support for Nextflow&lt;/h2&gt; 
&lt;p&gt;We hope that this service and these containers will be of use to the entire bioinformatics community. However, the experience of using Seqera Containers will be particularly good for Nextflow users.&lt;/p&gt; 
&lt;p&gt;Using the &lt;a href="https://nextflow.io/docs/latest/wave.html"&gt;Nextflow wave plugin&lt;/a&gt;, pipeline developers can avoid specifying container URIs in their pipeline code entirely. Instead, just naming the software packages in the conda (or, later, spack) declarations and then setting wave.enabled = true and wave.freeze = true is all you need. This instructs Nextflow to request an image from Wave for these packages, store it in the Seqera Containers public registry, and then use this at run time.&lt;/p&gt; 
&lt;p&gt;Wave isn’t restricted to working with Nextflow, there is also a &lt;a href="https://github.com/seqeralabs/wave-cli"&gt;Wave CLI&lt;/a&gt; which anyone can use to generate containers as well as an &lt;a href="https://docs.seqera.io/wave/api"&gt;API&lt;/a&gt;, both of which have all the same functionality as the Nextflow plugin and Seqera Containers web interface.&lt;/p&gt; 
&lt;h2&gt;It’s available now&lt;/h2&gt; 
&lt;div id="attachment_3818" style="width: 390px" class="wp-caption alignright"&gt;
 &lt;a href="https://youtu.be/HjNwpJPMvWQ" target="new" rel="noopener"&gt;&lt;img aria-describedby="caption-attachment-3818" loading="lazy" class="size-full wp-image-3818" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/YT-on-demand-containers-for-bioinformatics.png" alt="Paolo Di Tommaso talks about Nextflow and the revolution it started." width="380" height="213"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-3818" class="wp-caption-text"&gt;Paolo Di Tommaso talks about Nextflow and the revolution it started.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The impact of container technology on the progress of life sciences research just can’t be overstated.&lt;/p&gt; 
&lt;p&gt;It addresses big challenges in bioinformatics, like reproducibility, scalability, and collaboration. It eases software management, and empowers researchers to focus on their core scientific endeavors. This has accelerated the pace of discoveries, fostered collaboration, and enhanced the overall quality and reproducibility of bioinformatics research.&lt;/p&gt; 
&lt;p&gt;We think today’s announcement pushes containers, and bioinformatics one step further, by making it dramatically easier to get your hands on containers in the right shape, size, and form-factor to meet the needs of your pipelines. Batch users will definitely enjoy using this. But, we’re pretty sure this will be a community resource everyone will love, and AWS is thrilled to be able to support the Seqera team to deliver this for the entire community.&lt;/p&gt; 
&lt;p&gt;If you still need convincing, try it out yourself now – head to &lt;a href="https://seqera.io/containers/"&gt;https://seqera.io/containers/&lt;/a&gt; and type in some of your favorite bioinformatics tool names before clicking “Build”. Pull your Docker image and get to work.&lt;/p&gt; 
&lt;p&gt;And if you have ideas for Seqera or AWS, don’t hesitate to reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Using machine learning to drive faster automotive design cycles</title>
		<link>https://aws.amazon.com/blogs/hpc/using-machine-learning-to-drive-faster-automotive-design-cycles/</link>
		
		<dc:creator><![CDATA[Steven Miller]]></dc:creator>
		<pubDate>Mon, 13 May 2024 14:40:10 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AI]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">2c1df43a0e4299fe6554a7b7f672836e27c25a46</guid>

					<description>Aerospace and automotive companies are speeding up their product design using AI. In this post we'll discuss how they're using machine learning to shift design cycles from hours to seconds using surrogate models.</description>
										<content:encoded>&lt;p&gt;The automotive product engineering process involves months or years of iterative design reviews and refinement, with back-and-forth feedback between stakeholders regularly to adjust designs and evaluate the impact of design changes on engineering metrics like the coefficient of drag. Between each design iteration, engineers wait hours or days for simulations to complete, which means they can only execute a handful of design decisions each week.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll show how automakers can reduce cycle times from hours to seconds by leveraging surrogate machine-learning (ML) models in place of HPC, physics-based simulations and create subtle design variations for non-parametric geometries.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;In short&lt;/strong&gt;: automotive engineers can use emerging ML methodologies to speed up the product engineering process.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;Typically, the automotive product design process involves conceptualization, computer-aided design (CAD) geometry creation, and engineering simulations for validating the designs to ensure aerodynamic efficiency and desired structural stability.&lt;/p&gt; 
&lt;p&gt;Recent advances in AI/ML and Generative AI technology including algorithms such as MeshGraphNets, U-Nets and Variational Autoencoders, have provided a path towards accelerating the design process through fast ML inferences which allow us to run fewer full-fidelity physics-based HPC simulations, which can take hours, while gathering insight into large design spaces.&lt;/p&gt; 
&lt;p&gt;In this work, we’ll focus on computational fluid dynamics (CFD) simulations for intelligent aerodynamic surface design, though there are often other design considerations like impact response, noise, vibration and harshness.&lt;/p&gt; 
&lt;p&gt;The goal of this work is &lt;em&gt;not&lt;/em&gt; to develop a comprehensive toolkit for aerodynamic design – we want to present a simple user experience to show how engineers can use scientific ML methods, together with AWS services to deliver business value. We’ll mainly use &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;Nice DCV&lt;/a&gt;, &lt;a href="https://aws.amazon.com/pm/serv-s3/"&gt;Amazon S3&lt;/a&gt; and &lt;a href="https://aws.amazon.com/pm/sagemaker/"&gt;Amazon SageMaker&lt;/a&gt;. We’ll also explain how to build a web application that uses ML and generative design to enhance these development processes.&lt;/p&gt; 
&lt;div id="attachment_3604" style="width: 857px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3604" loading="lazy" class="size-full wp-image-3604" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.33.24.png" alt="Figure 1 – Overview of Workflow Structure incorporating Web Application UI overview, AI/ML Methodology and AWS Implementation" width="847" height="433"&gt;
 &lt;p id="caption-attachment-3604" class="wp-caption-text"&gt;Figure 1 – Overview of Workflow Structure incorporating Web Application UI overview, AI/ML Methodology and AWS Implementation&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Roadmap for this post&lt;/h2&gt; 
&lt;p&gt;Much like the workflow chart in Figure 1, we’ll divide this post into three parts.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, we’ll start by explaining how to build a minimally viable web application including the end user experience and core components starting from an AWS architecture blueprint, highlighting some key components.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Next&lt;/strong&gt;, we’ll dive deep into the underlying ML models including the training methodology and inference deployment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Finally&lt;/strong&gt;, we’ll wrap up with a brief discussion about how we imagine automakers using workflows like this in practice to augment and accelerate the product design lifecycles.&lt;/p&gt; 
&lt;h2&gt;Cloud implementation and AWS architecture&lt;/h2&gt; 
&lt;p&gt;Let’s look at how to quickly and securely implement a minimally-viable web application. Using that, we’ll generate ground truth meshes, run OpenFOAM simulations, and train the machine learning models using AWS HPC services.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AWS architecture.&lt;/strong&gt; We don’t want to completely replace HPC simulations – these are necessary for verification and validation. We want to enable engineers to quickly build a web application inside their environment, access it securely, and store all their related artifacts. For this reason, we’ve chosen a minimalist architecture and we’ll leave it up to the automaker to decide how to integrate this into their existing environment.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 2: AWS Architecture diagram for deployment on a g5 instance using DCV, with endpoints to retrieve data or optionally submit compute jobs on self-managed ML endpoints (AWS Batch) or Amazon SageMaker&lt;/em&gt;&lt;/p&gt; 
&lt;div id="attachment_3605" style="width: 665px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3605" loading="lazy" class="size-full wp-image-3605" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.34.05.png" alt="Figure 2: AWS Architecture diagram for deployment on a g5 instance using DCV, with endpoints to retrieve data or optionally submit compute jobs on self-managed ML endpoints (AWS Batch) or Amazon SageMaker" width="655" height="345"&gt;
 &lt;p id="caption-attachment-3605" class="wp-caption-text"&gt;Figure 2: AWS Architecture diagram for deployment on a g5 instance using DCV, with endpoints to retrieve data or optionally submit compute jobs on self-managed ML endpoints (AWS Batch) or Amazon SageMaker&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We’re hosting the web application on a g5.12xlarge Amazon Elastic Compute Cloud (Amazon EC2) instance. The &lt;a href="https://aws.amazon.com/ec2/instance-types/g5/"&gt;G5&lt;/a&gt; has 4 x A10G NVIDIA GPUs to serve the machine learning models and we’re using the &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;NICE DCV&lt;/a&gt; streaming protocol to securely stream the desktop to the end.&lt;/p&gt; 
&lt;p&gt;We’re also keeping the EC2 instance secure inside of a VPC and we only allow TCP traffic to flow from the instance to a single end user.&lt;/p&gt; 
&lt;p&gt;We train drag prediction models using Amazon SageMaker, and we securely store the inputs and outputs (including model artifacts) in Amazon Simple Storage Service (Amazon S3).&lt;/p&gt; 
&lt;p&gt;We use AWS Batch to launch the CFD simulations to create the ground truth data.&lt;/p&gt; 
&lt;p&gt;To keep all communications secure, we use VPC endpoints for all key services.&lt;/p&gt; 
&lt;h2&gt;Application workflow and user experience&lt;/h2&gt; 
&lt;p&gt;Now we can build the minimally viable application to show this new workflow. The web application layer consists of five main modules: application, inference configuration, pretrained models, processed meshes, and utilities.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Application.&lt;/strong&gt; The core application, built on the open-source &lt;a href="https://streamlit.io/"&gt;Streamlit&lt;/a&gt; library, serves the user interface and all the orchestration to deliver the user experience. We used a 3D plotting library (&lt;a href="https://docs.pyvista.org/version/stable/"&gt;PyVista&lt;/a&gt;) to display 3D visualizations and enable user interaction with STL meshes (original and design targets) and ML and physics-based computational fluid dynamics results, including pressure cross-sections and 3D streamlines.&lt;/p&gt; 
&lt;p&gt;In this example, we’ll begin with a choice of a base car model (coupe or estate/station-wagon), and then provide the user with design modification options. In this case, some features of the car (like front bumper shape, windscreen angle, trunk depth, and cabin height) are provided, but we could easily extend this to other design features we’re interested in.&lt;/p&gt; 
&lt;p&gt;For each of these combinations, the design variations are produced through non-parametric “morphing” of the base meshes using &lt;em&gt;radial basis function (RBF) interpolation&lt;/em&gt;. If we wanted to explore parametric design, and the base CAD geometries were available, we could potentially incorporate them into the workflow. The user interface is shown in Figure 3.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure3 - Using machine learning to drive faster automotive design cycles." width="500" height="375" src="https://www.youtube-nocookie.com/embed/2Z06wjAPVkA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Radial basis function (RBF) – &lt;/strong&gt;RBF interpolation lets us distort meshes at specific parts of the car, while keeping other features constant – including the chassis platform and tire radius. RBF relies on the concept of a deformation map between the original mesh and the deformed mesh and we get to control the extent of deformation using the slider in the user interface.&lt;/p&gt; 
&lt;p&gt;To enable engineers to show specifically how the mesh has been deformed, we’ve provided a side-by-side comparison of the original mesh and the new mesh in the UI (which you can see in Figure 4) before we pass the deformed mesh along to the ML model for inference.&lt;/p&gt; 
&lt;div id="attachment_3607" style="width: 831px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3607" loading="lazy" class="size-full wp-image-3607" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.34.57.png" alt="Figure 4 – Deformed geometry (left) with a wireframe overlay of original mesh highlighting change and original mesh (right)" width="821" height="348"&gt;
 &lt;p id="caption-attachment-3607" class="wp-caption-text"&gt;Figure 4 – Deformed geometry (left) with a wireframe overlay of original mesh highlighting change and original mesh (right)&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Inference configuration.&lt;/strong&gt; We trained a hierarchical machine learning model for a CFD flow field and drag predictions – we’ll explain that more soon. At runtime, we hosted these on different GPUs which allowed us to parallelize the inference of individual sub-models. To get inferences from the ML model, we specify the key inputs and parameters using inference configurations. These inputs include the decimated surface meshes of the car and a geometry for volume calculations. Additional input parameters include the batch size for inference, number of workers and other neural network associated hyperparameters.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Processed mesh.&lt;/strong&gt; When a user requests an inference, the application generates a processed &lt;a href="https://www.hdfgroup.org/solutions/hdf5/"&gt;HDF5&lt;/a&gt; file which contains the new mesh and the corresponding flow fields. These HDF5 files are then exposed to the end user via an interactive 3D visualization. There are some extra components that support the application: a function to convert meshes to HDF5 files, another function to apply the RBF to morph meshes based on user inputs, and finally a function to run inferences and gather outputs.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deployment.&lt;/strong&gt; Since this application is meant for demonstration purposes only, we designed it to run on a single compute instance, and chose a g5.12xlarge that includes 4 x GPUs that’s able to run the entire application. To speed the application, we distribute individual ML sub-models across the GPUs. To keep it all secure, we isolate the instances from the public internet in a private VPC. And we used NICE DCV to access the instance for an easier remote desktop experience.&lt;/p&gt; 
&lt;p&gt;Geometric and ML models&lt;/p&gt; 
&lt;p&gt;To enable engineers to rapidly iterate, potentially even &lt;em&gt;during&lt;/em&gt; design conversations, our web application predicts the &lt;em&gt;coefficient of drag&lt;/em&gt; (Cd) and the flow fields on unseen geometries. We used a hierarchy of ML models for the flow fields and drag predictions.&lt;/p&gt; 
&lt;p&gt;In both cases, we trained our models using synthetic data that we generated by morphing the open source &lt;a href="https://www.epc.ed.tum.de/en/aer/research-groups/automotive/drivaer/"&gt;DrivAer&lt;/a&gt; data set, and then ran full-fidelity CFD simulations on all these geometries using AWS Batch with the open-source &lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Synthetic data generation&lt;/strong&gt;. To create training data for this project, we used the RBF to strategically morph the underlying mesh of both the coupe and the SUV. This method has broad applicability and can be generically applied to many parametric and non-parametric STL meshes.&lt;/p&gt; 
&lt;p&gt;For each area of the car, we specified points in an overlayed 1000-point (10x10x10) cubic lattice, bounded by the dimensions of the car. This allowed us to create 400 variations (100 for each of the area of the car) that we used as the basis for ground-truth simulations. Figure 5 shows how we used the RBF to deform the mesh &lt;em&gt;and&lt;/em&gt; create the training data for the ML model. Each time frame in the video corresponds to a different deformed mesh.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Using machine learning to drive faster automotive design cycles - Figure5" width="500" height="281" src="https://www.youtube-nocookie.com/embed/aBcO03OBTm0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p style="text-align: left"&gt;&lt;em&gt;Figure 5 – Synthetic mesh generation using morphing of individual features of a car shown in sequence&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;After we created 400 variations of the base meshes, we used OpenFOAM to create CFD simulations with steady-state RANS simulations using the k-ω steady-state turbulence model with a fixed inlet velocity of 20m/s.&lt;/p&gt; 
&lt;p&gt;We ran these simulations in parallel using AWS Batch on c5.24xlarge instances using MPI for parallelism – this took around 7 hours to run. We could have chosen larger mesh sizes, but we settled on a mesh size that allowed resolving fine geometry – in a reasonable compute time.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Coefficient of drag with MeshGraphNets (MGN) –&lt;/strong&gt; MGN is a deep-learning framework for learning mesh-based simulations which represents the mesh as a graph where information is propagated across the graph’s nodes and edges. MGN excels at efficiently capturing unstructured mesh topologies and it’s a great fit for predicting Cd values. To train the MGN model, we used the synthetic data that we generated using the RBF, and OpenFOAM. During training, we provided the underlying mesh and the Coefficient of Drag (Cd) metrics as inputs.&lt;/p&gt; 
&lt;p&gt;We then trained a model on 320 samples, with each mesh decimated while preserving topological features for computational efficiency, containing approximately 6,000 nodes. We used a p3.2xlarge instance for 500 epochs, with a total training time of 46 minutes.&lt;/p&gt; 
&lt;p&gt;The model achieved a MAPE (&lt;em&gt;mean absolute percentage error&lt;/em&gt;) of 2.5% in drag coefficient (Cd) when predicting on unseen data, with an average inference time of 0.028 seconds per sample. Figure 6 shows the predicted vs. actual drag coefficient (Cd) plot for train, validation, and unseen test data (car meshes).&lt;/p&gt; 
&lt;div id="attachment_3609" style="width: 718px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3609" loading="lazy" class="size-full wp-image-3609" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.36.06.png" alt="Figure 6: Predicted vs. actual drag coefficient (Cd) plot for train, validation, and unseen test data (i.e. car meshes)." width="708" height="426"&gt;
 &lt;p id="caption-attachment-3609" class="wp-caption-text"&gt;Figure 6: Predicted vs. actual drag coefficient (Cd) plot for train, validation, and unseen test data (i.e. car meshes).&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Although the model didn’t match the ground truth CFD simulations exactly, this level of agreement from the MGN surrogate model may prove acceptable during the initial design iterations leading up to a final, high-fidelity simulation. After that comes real-world wind tunnel testing, which has an inherent uncertainty in measurements.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Flow Fields using deep convolutional neural networks (CNNs).&lt;/strong&gt; To predict pressure distributions and velocity fields for full 3D flow, we used a CNN architecture called &lt;a href="https://github.com/wolny/pytorch-3dunet"&gt;U-Net&lt;/a&gt; which has shown promise in high-resolution volume segmentation and regression tasks. This architecture included encoding and decoding convolutional layers with skip connections. We trained individual U-Nets for each of the pressure and velocity primary variables.&lt;/p&gt; 
&lt;p&gt;We converted the 3D volume outputs from 400 x OpenFOAM simulations into the training (375) and validation (25) ground-truth datasets. We used a p5.48xlarge instance with 8 x H100 GPUs for approximately 28 hours. We saw a volume RSME (&lt;em&gt;root-mean-square-error&lt;/em&gt;) of around 3% for pressure, and 5% for combined velocity magnitudes, for unseen data not in the training or validation sets – although this is dependent on the degree of mesh warping away from the initial baseline mesh. Figure 7 shows the differences in velocity slices an unseen mesh.&lt;/p&gt; 
&lt;div id="attachment_3610" style="width: 680px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3610" loading="lazy" class="size-full wp-image-3610" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.36.52.png" alt="Figure 7: ML Predicted (Top) vs Ground Truth OpenFOAM (Bottom) X-Component Velocity" width="670" height="516"&gt;
 &lt;p id="caption-attachment-3610" class="wp-caption-text"&gt;Figure 7: ML Predicted (Top) vs Ground Truth OpenFOAM (Bottom) X-Component Velocity&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Business value creation&lt;/h2&gt; 
&lt;p&gt;We’ve now shown how to rapidly build a minimally viable web interface that engineers can use to take advantage of these ML models to speed up the product engineering process. But now we need to couple this with a new business process so automakers can realize actual business value.&lt;/p&gt; 
&lt;div id="attachment_3611" style="width: 855px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3611" loading="lazy" class="size-full wp-image-3611" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.37.12.png" alt="Figure 8: Overall product design iterations using machine learning surrogate models to accelerate the cycles" width="845" height="168"&gt;
 &lt;p id="caption-attachment-3611" class="wp-caption-text"&gt;Figure 8: Overall product design iterations using machine learning surrogate models to accelerate the cycles&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Using the speed of the ML surrogate models, it’s possible to explore hundreds of designs per week, and we can now make meetings more collaborative.&lt;/p&gt; 
&lt;p&gt;Instead of reviewing results from prior runs, engineers can solicit feedback from their peers real-time0, and get the results in 20-60 seconds, avoiding costly wait times. This will require engineers to think differently about product design reviews and meetings. Engineers will need to set expectations accordingly and be prepared to guide conversations. But this enables engineers to explore a greater design space than they otherwise could – especially given budget or time constraints. We’re confident this approach will lead to new innovations.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;With the right combination of machine-learning and process change, automakers can use the architecture explained here to build tailored solutions to speed up the product engineering process, reduce the cost of compute, and explore a greater number of design options in a shorter period of time. If you or your team want to discuss the business case, a recommended implementation path with our Professional Services team, or technical solution blueprint in more detail, you can reach out to us at ask-hpc@amazon.com.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Announcing the High Performance Software Foundation (HPSF)</title>
		<link>https://aws.amazon.com/blogs/hpc/announcing-the-high-performance-software-foundation/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Mon, 13 May 2024 05:55:13 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">e91b47faf0b755001ca4998fd17206fcbdaf12f6</guid>

					<description>We're excited to share how we’re involved in launching the High Performance Software Foundation to increase access to and adoption of HPC. By bringing together key players to collaborate, we can lower barriers and accelerate development of portable HPC software stacks.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="size-full wp-image-3644 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/09/AdobeStock_144582125-resized.png" alt="Announcing the High Performance Software Foundation" width="380" height="253"&gt;&lt;/em&gt;In high performance computing (HPC), where speed and efficiency are paramount, open-source software provides the load-bearing, structural support. Our community has long recognized the immense potential of collaborative development, sharing of resources, and the power of community-driven innovation.&lt;/p&gt; 
&lt;p&gt;So we’re excited to announce that we’re a Premier founding member of the new &lt;strong&gt;High Performance Software Foundation&lt;/strong&gt; (HPSF). The HPSF was launched today in Hamburg at &lt;a href="https://www.isc-hpc.com/"&gt;ISC’24&lt;/a&gt;, by the Linux Foundation, the nonprofit organization that enables mass innovation through open-source. The HPSF has strong support across the HPC landscape.&lt;/p&gt; 
&lt;p&gt;Through a series of technical projects, the HPSF aims to build, promote, and advance a portable core software stack for HPC to increase adoption by lowering barriers to contribution, supporting open-source development efforts, and making HPC more accessible to all of us. We’ve seen from our work with RIKEN to &lt;a href="https://aws.amazon.com/blogs/publicsector/why-fugaku-japans-fastest-supercomputer-went-virtual-on-aws/"&gt;extend Fugaku to the cloud&lt;/a&gt;&amp;nbsp;how impactful this can be.&lt;/p&gt; 
&lt;p&gt;Recent years have seen extraordinary demand for HPC across domains which are critical to all of us, from climate modeling and genomics to drug-discovery and engineering design. With the emergence of machine learning and AI, there’s never been a more important task than to reinforce the structures which have made all of this possible.&lt;/p&gt; 
&lt;h2&gt;How can HPSF help?&lt;/h2&gt; 
&lt;p&gt;The HPSF aims to make life easier for HPC developers through a number of focused initiatives, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Continuous Integration (CI) resources tailored for HPC projects&lt;/li&gt; 
 &lt;li&gt;Continuously built, turnkey software stacks&lt;/li&gt; 
 &lt;li&gt;Architecture support&lt;/li&gt; 
 &lt;li&gt;Performance regression testing and benchmarking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AWS has been supporting many of these efforts for the &lt;a href="https://spack.io/"&gt;Spack&lt;/a&gt; community since 2021, leading to the launch of the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-the-spack-rolling-binary-cache/"&gt;Spack Rolling Binary Cache&lt;/a&gt; two years ago. We’ve all learned a lot from this relationship, which we hope the wider HPC open-source community will be able to benefit from, through the foundation.&lt;/p&gt; 
&lt;h2&gt;First steps&lt;/h2&gt; 
&lt;p&gt;One of HPSF’s first jobs is to set up the technical advisory committee (TAC) which will manage working groups tackling a variety of HPC topics. Drawing from member organizations and community participants, the TAC will follow a governance model based on the &lt;a href="https://www.cncf.io/"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF).&lt;/p&gt; 
&lt;p&gt;The HPSF launches today with the following technical projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Spack&lt;/strong&gt;: the HPC package manager&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Kokkos&lt;/strong&gt;: a performance-portable programming model for writing modern C++ applications in a hardware-agnostic way&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Viskores (formerly VTK-m)&lt;/strong&gt;: a toolkit of scientific visualization algorithms for accelerator architectures&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HPCToolkit&lt;/strong&gt;: performance measurement and analysis tools for computers ranging from laptops to GPU-accelerated supercomputers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Apptainer&lt;/strong&gt;: Formerly known as Singularity, Apptainer is a Linux Foundation project providing a high performance, full featured HPC and computing optimized container subsystem&lt;/li&gt; 
 &lt;li&gt;&lt;b&gt;E4S: &lt;/b&gt;a curated, hardened distribution of scientific software packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting involved&lt;/h2&gt; 
&lt;p&gt;The HPSF welcomes organizations from across the HPC community to become involved and help drive innovation in open-source HPC solutions.&lt;/p&gt; 
&lt;p&gt;To learn more about the HPSF, to contribute, or to become a member, you can visit the &lt;a href="https://hpsf.io/"&gt;HPSF website&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The HPSF’s commitment to supporting and nurturing these vital software packages will enable the HPC community to have access to the tools and resources necessary to push the boundaries of scientific discovery. We’re thrilled to be part of it.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Best practices for running molecular dynamics simulations on AWS Graviton3E</title>
		<link>https://aws.amazon.com/blogs/hpc/best-practices-for-running-molecular-dynamics-simulations-on-aws-graviton3e/</link>
		
		<dc:creator><![CDATA[Nathaniel Ng]]></dc:creator>
		<pubDate>Tue, 07 May 2024 13:55:18 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[GROMACS]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">1e63bf1a121b398b36fb135821ca6835db681e7f</guid>

					<description>If you run molecular dynamics simulations, you need to read this. We walk through running benchmarks of popular apps like GROMACS and LAMMPS on new Hpc7g instances and Graviton3E processors. The results - up to 35% better vector performance versus Graviton3! Learn how to optimize your own workflows.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Nathaniel Ng, Shun Utsui, and James Chen from AWS Solution Architecture&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Last year, &lt;a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc7g-instances-powered-by-aws-graviton3e-processors-optimized-for-high-performance-computing-workloads/"&gt;we announced&lt;/a&gt; the general availability of Hpc7g instances, the instance type focused on HPC workloads powered by AWS Graviton3E. Graviton3E processors deliver up to 35% higher vector-instruction performance &lt;a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc7g-instances-powered-by-aws-graviton3e-processors-optimized-for-high-performance-computing-workloads/"&gt;compared to Graviton3&lt;/a&gt;, providing higher performance benefits for HPC applications.&lt;/p&gt; 
&lt;p&gt;Molecular dynamics (MD) is a domain that frequently leverages HPC resources. Previously, customers ran their MD workloads using predominantly x86 architectures, but we’ve heard that many are interested in understanding the performance they can get on Graviton3E.&lt;/p&gt; 
&lt;p&gt;So, in this post, we’ll show how you can run MD workloads on Hpc7g instances using AWS ParallelCluster, a supported open-source cluster management tool that allows you to deploy a scalable HPC environment on AWS in a matter of minutes. We’ll use &lt;a href="https://www.gromacs.org/"&gt;GROMACS&lt;/a&gt; and &lt;a href="https://www.lammps.org/"&gt;LAMMPS&lt;/a&gt; as examples – two very popular MD applications. And we’ll highlight the best practices for the tools – and the compiler flags – we used to achieve optimal performance.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Key to the architecture is AWS ParallelCluster. Customers can install the ParallelCluster CLI on their laptops using Python’s pip package manager, and use it to deploy a cluster in the cloud from their laptops by supplying a short configuration file.&lt;/p&gt; 
&lt;p&gt;Through the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcui-using-v3.html"&gt;ParallelCluster UI&lt;/a&gt;, you can use a wizard to configure a cluster without editing a configuration file or installing anything on your laptop. Details on how to set up a ParallelCluster environment can be found in &lt;a href="https://www.hpcworkshops.com/01-hpc-overview.html"&gt;this workshop&lt;/a&gt;. You can also find a &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/try_hpc7g"&gt;one-click launchable stack&lt;/a&gt; in the HPC Recipe Library (on GitHub), which will build an Hpc7g cluster for you after asking a minimum of questions.&lt;/p&gt; 
&lt;div id="attachment_3496" style="width: 831px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3496" loading="lazy" class="size-full wp-image-3496" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.40.13.png" alt=" Figure 1: Architecture using AWS ParallelCluster to run MD workloads with AWS Graviton 3E instances. " width="821" height="378"&gt;
 &lt;p id="caption-attachment-3496" class="wp-caption-text"&gt;&lt;br&gt;Figure 1: Architecture using AWS ParallelCluster to run MD workloads with AWS Graviton 3E instances.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;ParallelCluster uses Slurm for its job scheduler, to dynamically scale the number of Hpc7g.16xlarge compute instances, responding to the queue.&lt;/p&gt; 
&lt;p&gt;These instances are powered by custom-built AWS Graviton3E processors. They feature the latest DDR5 memory offering 50% more bandwidth compared to DDR4, and they carry 200Gbps network interfaces with the Elastic Fabric Adapter (EFA). Graviton3E processors implement Scalable Vector Extension (SVE) of the Neoverse V1 architecture and hence can deliver up to 2x better performance for floating point codes than Graviton2.&lt;/p&gt; 
&lt;p&gt;To achieve sufficient performance on storage I/O, we deployed 4.8 TB of Amazon FSx for Lustre – a fully-managed, high-performance Lustre file system. We selected the &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/using-fsx-lustre.html"&gt;PERSISTENT_2&lt;/a&gt; deployment type with a disk throughput of &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/performance.html"&gt;1000 MBps/TiB of storage provisioned&lt;/a&gt;, and backed it with an Amazon Simple Storage Service (Amazon S3) bucket.&lt;/p&gt; 
&lt;p&gt;We’ve documented all the details of our ParallelCluster configuration in our &lt;a href="https://github.com/aws-samples/aws-graviton-md-example"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Development tooling&lt;/h2&gt; 
&lt;p&gt;We tested several configurations and concluded that we recommend using the following compilers and libraries for most use cases. You can check our &lt;a href="https://github.com/aws-samples/aws-graviton-md-example"&gt;GitHub repository&lt;/a&gt; to find out best practices for compiler flags when building MD applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Compiler&lt;/strong&gt;: Arm compiler for Linux (ACfL) version 23.04 or later&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Library&lt;/strong&gt;: Arm performance libraries (ArmPL) version 23.04 or later, included in ACfL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MPI&lt;/strong&gt;: Open MPI version 4.1.5 or later&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It’s worth noting that Arm Compilers and Performance Libraries for HPC developers are &lt;a href="https://community.arm.com/arm-community-blogs/b/high-performance-computing-blog/posts/arm-compilers-and-libraries-for-hpc-now-free"&gt;now available for no cost&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In the rest of this post, we’ll explain why we preferred these tools, by diving deeper into the performance of GROMACS and LAMMPS with different compilers, compiler options, and input files.&lt;/p&gt; 
&lt;h2&gt;GROMACS&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.gromacs.org/about.html"&gt;GROMACS&lt;/a&gt; is an open-source software suite for high-performance molecular dynamics and output analysis. It’s widely adopted in the computer simulation fields not only for biochemical molecules but also for non-biological systems like polymers and fluid dynamics. The GROMACS community have optimized it to make great use of the SIMD capabilities of many modern HPC architectures.&lt;/p&gt; 
&lt;p&gt;For Arm architectures, the &lt;a href="https://manual.gromacs.org/current/install-guide/index.html#simd-support"&gt;code supports both NEON (ASIMD) and SVE instructions&lt;/a&gt;. In this post, we used GNU 12.2 and the Arm compiler for Linux (ACfL) 23.04 as the testing compiler suite, with Arm Performance Library (ArmPL) 23.04 for the math library, and Open MPI 4.1.5 linked with Libfabric to build different binary executables of GROMACS 2022.5.&lt;/p&gt; 
&lt;h3&gt;Build scripts&lt;/h3&gt; 
&lt;p&gt;Our main objective was to find out the best compiler suite – and SIMD support fit – for Graviton3E-based Hpc7g instances. We’ll discuss the build scripts, job submission scripts, and performance data here – and you can find all the scripts in our &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/tree/main/codes/GROMACS"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For GROMACS’s build script, there’s no difference in the CMake options for GNU and Arm compiler for Linux.&lt;/p&gt; 
&lt;p&gt;For GNU compilers, the Open MPI 4.1.5 environment is already installed in ParallelCluster’s machine images. We recommend anyone new to the cloud to use the system default Open MPI library. However, the system default Open MPI does &lt;em&gt;not&lt;/em&gt; support ACfL, so we’ve &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/setup/2-install-openmpi-with-acfl.sh"&gt;supplied a script&lt;/a&gt; which demonstrates how to compile and install Open MPI 4.1.5 with ACfL. Once this is installed, we can use the bash module environments to switch between the GNU and Arm compiler.&lt;/p&gt; 
&lt;p&gt;With this done, it’s now possible to build executables for GROMACS 2022.5 for both SVE or NEON/ASIMD instruction sets.&lt;/p&gt; 
&lt;p&gt;We’ve stored the procedures for this in yet another &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/tree/main/codes/GROMACS"&gt;reference script&lt;/a&gt;. We only need to change the parameter &lt;code&gt;GMX_SIMD&lt;/code&gt; in the configuration setup. For the SVE-enable binary, the parameter is –&lt;code&gt;DGMX_SIMD=ARM_SVE&lt;/code&gt;. For building the NEON/ASIMD-enabled binary, you’ll need to switch the parameter to &lt;code&gt;-DGMX_SIMD= ARM_NEON_ASIMD&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Test cases&lt;/h3&gt; 
&lt;p&gt;We applied three standard test cases for GROMACS from the &lt;a href="https://repository.prace-ri.eu/git/UEABS/ueabs"&gt;Unified European Application Benchmark Suite (UEABS)&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://repository.prace-ri.eu/ueabs/GROMACS/2.2/GROMACS_TestCaseA.tar.xz"&gt;Test Case A &lt;/a&gt;is the ion channel system of the membrane protein GluCl embedded in a DOPC membrane and solvated in TIP3P water. This system contains 142k atoms.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://repository.prace-ri.eu/ueabs/GROMACS/2.2/GROMACS_TestCaseB.tar.xz"&gt;Test Case B&lt;/a&gt; is a model of cellulose and lignocellulosic biomass in an aqueous solution. The system has 3.3 million atoms.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://repository.prace-ri.eu/ueabs/GROMACS/2.2/GROMACS_TestCaseC.tar.xz"&gt;Test Case C&lt;/a&gt; is the standard test case for NAMD benchmarks. The system is a 3 x 3 x 3 replica of the STMV (&lt;em&gt;Satellite Tobacco Mosaic Virus&lt;/em&gt;). The size of model is about 28 million atoms.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For all these cases, we set the total simulation steps to 10k.&lt;/p&gt; 
&lt;p&gt;To find out the best combination of compiler and SIMD setup, we started to test the application performance on a single Hpc7g.16xlarge instance. Figure 2 charts the performance we saw for test case A (142K atoms). ACfL with SVE-enabled generated the best performance. The binary this generated was about 9-10% faster than the one using NEON/ASIMD. The binary produced by the ACfL with SVE ran 6% faster than when we used the GNU compiler with SVE.&lt;/p&gt; 
&lt;div id="attachment_3497" style="width: 714px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3497" loading="lazy" class="size-full wp-image-3497" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.41.42.png" alt="Figure 2: Performance of GROMACS 2022.5 for GluCl Ion Channel system (142K atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enable binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs. " width="704" height="466"&gt;
 &lt;p id="caption-attachment-3497" class="wp-caption-text"&gt;Figure 2: Performance of GROMACS 2022.5 for GluCl Ion Channel system (142K atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enable binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 3 charts the performance we saw for test case B (3.3M atoms). The best setup we found – again – was to use the ACfL with SVE enabled. We measured a 28% improvement for the SVE-enabled binary, compared with the NEON/ASIMD-enable binary when we used this compiler.&lt;/p&gt; 
&lt;div id="attachment_3498" style="width: 717px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3498" loading="lazy" class="size-full wp-image-3498" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.42.04.png" alt="Figure 3: Performance of GROMACS 2022.5 for cellulose and lignocellulosic biomass (3.3M atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enabled binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs. " width="707" height="465"&gt;
 &lt;p id="caption-attachment-3498" class="wp-caption-text"&gt;Figure 3: Performance of GROMACS 2022.5 for cellulose and lignocellulosic biomass (3.3M atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enabled binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The performance results for test case C (28M atoms) are shown in Figure 4. Again, we saw a similar pattern to before: the ACfL with SVE enabled was the best option for GROMACS running on Hpc7g instance. In this case, the performance delta was 19% compared to NEON-ASIMD. And again, the binary generated by ACfL was 6% faster than the one built by the GNU compiler.&lt;/p&gt; 
&lt;div id="attachment_3499" style="width: 716px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3499" loading="lazy" class="size-full wp-image-3499" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.42.44.png" alt="Figure 4: Performance of GROMACS 2022.5 for Satellite Tobacco Mosaic Virus, STMV (28M atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enable binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs. " width="706" height="462"&gt;
 &lt;p id="caption-attachment-3499" class="wp-caption-text"&gt;Figure 4: Performance of GROMACS 2022.5 for Satellite Tobacco Mosaic Virus, STMV (28M atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enable binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Based on the results running on single Hpc7g instance, we concluded that ACfL with SVE-enabled SIMD, generated the best performance results.&lt;/p&gt; 
&lt;p&gt;By default, the configuration tool in GROMACS detected the CPU automatically, and selected the SIMD support correctly, too. GROMACS also detected the configuration of SIMD correctly for AWS Graviton3.&lt;/p&gt; 
&lt;p&gt;We chose test case C to highlight the scalability for performance on Hpc7g when we ran across multiple nodes. The result charted in Figure 5 confirms this – scalability of performance is near-linear with EFA enabled.&lt;/p&gt; 
&lt;div id="attachment_3500" style="width: 895px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3500" loading="lazy" class="size-full wp-image-3500" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.43.17.png" alt="Figure 5: Scalability of Test Case C running on the Hpc7g-based cluster with, and without, EFA. 200Gpbs EFA contributes the scalability in the cases running beyond 2 compute instances. The binary was compiled with ACfL with SVE-enabled. All the data points are based on the average of three individual runs. " width="885" height="579"&gt;
 &lt;p id="caption-attachment-3500" class="wp-caption-text"&gt;Figure 5: Scalability of Test Case C running on the Hpc7g-based cluster with, and without, EFA. 200Gpbs EFA contributes the scalability in the cases running beyond 2 compute instances. The binary was compiled with ACfL with SVE-enabled. All the data points are based on the average of three individual runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Conclusion for GROMACS&lt;/h3&gt; 
&lt;p&gt;Based on these performance results for three quite different, but well-known test cases, we found that SVE-enabled binary was faster than using ASIMD/Neon instructions. ACfL produced faster code than the GNU compiler. Specifically for Hpc7g, ACfL with SVE-enabled SIMD setting was the best configuration when paired with the latest ArmPL and Open MPI with EFA enabled.&lt;/p&gt; 
&lt;h2&gt;LAMMPS&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.lammps.org/"&gt;LAMMPS&lt;/a&gt; (Large-scale Atomic/Molecular Massively-Parallel Simulator) is a classical molecular dynamics simulator, used for particle-based modelling of materials. It was developed at Sandia National Laboratories, and is available as an open-source tool, distributed under GPLv2. You can download the source code from the &lt;a href="https://github.com/lammps/lammps/releases"&gt;LAMMPS GitHub repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In terms of algorithms, LAMMPS uses parallel spatial decomposition, as well as parallel FFTs for long-range Coloumbic interactions. The computational cost of scaling with the number of atoms is O(N) for short-range interactions, and O(N log N) when computations involve Coloumbic interactions with FFT-based methods.&lt;/p&gt; 
&lt;p&gt;To prepare for LAMMPS running on AWS, we can follow the same steps as before for GROMACS for the AWS ParallelCluster, GCC, ACfL, and Open MPI setup. The LAMMPS &lt;a href="https://docs.lammps.org/Install.html"&gt;install guide&lt;/a&gt; has several options for installation, but to use the latest version of LAMMPS, and more recent versions of GCC, ACfL, and Open MPI, LAMMPS must be compiled from source.&lt;/p&gt; 
&lt;p&gt;LAMMPS has specific Makefiles that are optimized for Arm architecture available: &lt;a href="https://github.com/lammps/lammps/blob/develop/src/MAKE/MACHINES/Makefile.aarch64_arm_openmpi_armpl"&gt;Makefile.aarch64_arm_openmpi_armpl&lt;/a&gt; and &lt;a href="https://github.com/lammps/lammps/blob/develop/src/MAKE/MACHINES/Makefile.aarch64_g++_openmpi_armpl"&gt;Makefile.aarch64_g++_openmpi_armpl&lt;/a&gt;, for use with ACfL and GCC compilers respectively. In the case of ACfL, we used &lt;code&gt;-march=armv8-a+sve&lt;/code&gt; &lt;a href="https://developer.arm.com/documentation/102476/0100/Programming-with-SVE/Auto-vectorization"&gt;to add the SVE instructions&lt;/a&gt;, and checked that switching between &lt;code&gt;-march=armv8-a+sve&lt;/code&gt; and &lt;code&gt;-march=armv8-a+simd&lt;/code&gt; did not impact performance. For &lt;em&gt;both&lt;/em&gt; ACfL and GCC Makefiles, we added &lt;code&gt;-fopenmp&lt;/code&gt; to &lt;code&gt;CCFLAGS&lt;/code&gt; and &lt;code&gt;LINKFLAGS&lt;/code&gt; to enable OpenMP. We’ve summarized the final settings in Table 1.&lt;/p&gt; 
&lt;div id="attachment_3501" style="width: 927px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3501" loading="lazy" class="wp-image-3501 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.44.36.png" alt="Table 1: Compiler settings for GCC and ACfL" width="917" height="247"&gt;
 &lt;p id="caption-attachment-3501" class="wp-caption-text"&gt;Table 1: Compiler settings for GCC and ACfL&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To compile LAMMPS, the final Makefiles available in our GitHub repo, were &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/LAMMPS/2a-compile-lammps-acfl-sve.sh"&gt;2a-compile-lammps-acfl-sve.sh&lt;/a&gt; for ACfL and &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/LAMMPS/2b-compile-lammps-gcc.sh"&gt;2b-compile-lammps-gcc.sh&lt;/a&gt; for GCC.&lt;/p&gt; 
&lt;p&gt;The key points to note in our LAMMPS compile script involve loading the correct environment modules – &lt;code&gt;libfabric-aws/1.17.1&lt;/code&gt; and &lt;code&gt;armpl/23.04.1&lt;/code&gt; for both compilers, &lt;code&gt;acfl/23.04.1&lt;/code&gt; for ACfL, and &lt;code&gt;gnu/12.2.0&lt;/code&gt; for GCC – and replacing the &lt;code&gt;CCFLAGS&lt;/code&gt; and &lt;code&gt;LINKFLAGS&lt;/code&gt; variables with those from Table 1. For ACfL, we set both &lt;code&gt;PATH&lt;/code&gt; and &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to the Open MPI installation folders.&lt;/p&gt; 
&lt;p&gt;We pulled the LAMMPS source code from the git repository into the install folder (&lt;code&gt;~/software&lt;/code&gt; in our case), and checked out the 23 June 2022 branch for compilation (&lt;code&gt;git checkout stable_23Jun2022_update4&lt;/code&gt;). You can check out other releases, too. If you want the default stable branch, use &lt;code&gt;git checkout stable&lt;/code&gt;. Use &lt;code&gt;make clean-all&lt;/code&gt; to remove all the intermediate object files and executable files, and &lt;code&gt;make no-all&lt;/code&gt; to turn off all options. Next, we ran &lt;code&gt;make yes-most&lt;/code&gt; to install most of the packages, which was enough for us to test all five LAMMPS benchmarks.&lt;/p&gt; 
&lt;p&gt;Once we compiled LAMMPS, we were able to submit jobs using Slurm. We’ve included sample job scripts &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/LAMMPS/3a-lammps-acfl-sve.sh"&gt;3a-lammps-acfl-sve.sh&lt;/a&gt; (for ACfL-compiled LAMMPS) and &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/LAMMPS/3b-lammps-gcc.sh"&gt;3b-lammps-gcc.sh&lt;/a&gt; (for GCC-compiled LAMMPS) in our repo. In the final command, you should execute LAMMPS using &lt;code&gt;mpirun&lt;/code&gt;. The &lt;code&gt;-var x&lt;/code&gt;, &lt;code&gt;-var y&lt;/code&gt;, and &lt;code&gt;-var z&lt;/code&gt; flags specify the &lt;code&gt;NX&lt;/code&gt;, &lt;code&gt;NY&lt;/code&gt;, and &lt;code&gt;NZ&lt;/code&gt; parameters passed to the LAMMPS input file, and the &lt;code&gt;-in&lt;/code&gt; parameter indicates the LAMMPS input file (&lt;code&gt;in.lj&lt;/code&gt; in our example).&lt;/p&gt; 
&lt;p&gt;For the LAMMPS test runs, we chose the five benchmark cases listed at the &lt;a href="https://www.lammps.org/bench.html"&gt;LAMMPS Benchmarks site&lt;/a&gt; as described in Table 2:&lt;/p&gt; 
&lt;div id="attachment_3502" style="width: 638px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3502" loading="lazy" class="size-full wp-image-3502" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.48.36.png" alt="Table 2: Description of the 5 LAMMPS benchmarks" width="628" height="353"&gt;
 &lt;p id="caption-attachment-3502" class="wp-caption-text"&gt;Table 2: Description of the 5 LAMMPS benchmarks&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We ran a comparison between LAMMPS compiled with the GCC, and LAMMPS compiled with ACfL for all of the 5 input files (&lt;code&gt;in.lj&lt;/code&gt;, &lt;code&gt;in.chain&lt;/code&gt;, &lt;code&gt;in.eam&lt;/code&gt;,&lt;code&gt; in.chute&lt;/code&gt;, and &lt;code&gt;in.rhodo&lt;/code&gt;), running on a single Hpc7g.16xlarge instance.&lt;/p&gt; 
&lt;p&gt;For each of the input files, we submitted three jobs with GCC and three jobs with ACfL, and we used the average of the runs to report the results. We took the speed from the &lt;code&gt;log.lammps&lt;/code&gt; output file, reported in tau/day for the &lt;code&gt;chain&lt;/code&gt;, &lt;code&gt;chute&lt;/code&gt;, and &lt;code&gt;lj&lt;/code&gt; test cases, and ns/day for the &lt;code&gt;eam&lt;/code&gt; and &lt;code&gt;rhodo&lt;/code&gt; test cases.&lt;/p&gt; 
&lt;p&gt;The LAMMPS benchmarks each contain 32,000 atoms (for &lt;code&gt;NX=NY=NZ=1&lt;/code&gt;). We chose &lt;code&gt;NX=NY=NZ=8&lt;/code&gt; as a balance between &lt;code&gt;NX=NY=NZ=1&lt;/code&gt; (with high variability in performance due to the short run time) and &lt;code&gt;NX=NY=NZ=32&lt;/code&gt; (resulting in out of memory errors).&lt;/p&gt; 
&lt;div id="attachment_3504" style="width: 896px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3504" loading="lazy" class="size-full wp-image-3504" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.51.47.png" alt="Figure 6. Performance of LAMMPS a single Hpc7g.16xlarge instance with NX=NY=NZ=8, when compiled using the GCC and ACfL compilers. Speed is in tau/day for the chain, chute, and lj test cases, and ns/day for the eam and rhodo test cases." width="886" height="562"&gt;
 &lt;p id="caption-attachment-3504" class="wp-caption-text"&gt;Figure 6. Performance of LAMMPS a single Hpc7g.16xlarge instance with NX=NY=NZ=8, when compiled using the GCC and ACfL compilers. Speed is in tau/day for the chain, chute, and lj test cases, and ns/day for the eam and rhodo test cases.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;ACfL consistently outperformed GCC for all the single-node runs, with improvements ranging from 2.3% to 46%.&lt;/p&gt; 
&lt;div id="attachment_3503" style="width: 885px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3503" loading="lazy" class="size-full wp-image-3503" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.50.35.png" alt="Table 3: Speedups with ACfL over GCC for the 5 benchmark cases on a single Hpc7g.16xlarge instance with NX=NY=NZ=8." width="875" height="124"&gt;
 &lt;p id="caption-attachment-3503" class="wp-caption-text"&gt;Table 3: Speedups with ACfL over GCC for the 5 benchmark cases on a single Hpc7g.16xlarge instance with NX=NY=NZ=8.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For the multi-node runs, we further increased &lt;code&gt;NX=NY=NZ&lt;/code&gt; to &lt;code&gt;32&lt;/code&gt;, to increase the runtime – and therefore reduce the variability – in the results. This brought the total number of atoms to 32,000 x 32 x 32 x 32 = ~ 1 billion atoms.&lt;/p&gt; 
&lt;p&gt;We chose to focus only on the Lennard Jones benchmark and tested both GCC- and ACfL- compiled LAMMPS with &lt;code&gt;OMP_NUM_THREADS=1,2,4&lt;/code&gt;. For 8 nodes (512 cores) and above, we observed better results with &lt;code&gt;OMP_NUM_THREADS=2&lt;/code&gt; for both compilers. &lt;em&gt;Unlike the single node runs&lt;/em&gt;, we observed that GCC-compiled LAMMPS outperformed its ACfL-compiled counterpart. We tested the GCC-compiled version up to 128 nodes.&lt;/p&gt; 
&lt;p&gt;We’ve plotted the results in Figure 7.&lt;/p&gt; 
&lt;div id="attachment_3505" style="width: 906px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3505" loading="lazy" class="size-full wp-image-3505" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.53.46.png" alt="Figure 7: Scalability of GCC-compiled LAMMPS for Lennard Jones benchmark case (1 billion atoms) using multiple Hpc7g.16xlarge instances. The line for linear scaling is plotted in grey." width="896" height="617"&gt;
 &lt;p id="caption-attachment-3505" class="wp-caption-text"&gt;Figure 7: Scalability of GCC-compiled LAMMPS for Lennard Jones benchmark case (1 billion atoms) using multiple Hpc7g.16xlarge instances. The line for linear scaling is plotted in grey.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we showed how you can run GROMACS and LAMMPS on Hpc7g-based instance. We discussed the ideal toolchain and SIMD setup based on the results we saw.&lt;/p&gt; 
&lt;p&gt;We didn’t modify the source code of GROMACS and LAMMPS, choosing instead to leverage “auto-vectorization” implemented in the compilers.&lt;/p&gt; 
&lt;p&gt;Arm’s compiler for Linux (ACfL) with SVE enabled SIMD boosted GROMACS performance up to 22% compared with the GNU compiler and NEON/ASIMD on Hpc7g instances. Arm compiler for Linux sped up LAMMPS simulation between 2.3% to 46%, depending on the case, compared to the GNU compiler on a single node basis. For LAMMPS, we also saw performance improvements at larger core counts with hybrid MPI + OpenMP parallelization.&lt;/p&gt; 
&lt;p&gt;We’ve made all our build scripts – and job submission scripts – available in our GitHub samples repo, and we’d encourage you to use this if you want to build on this work for your own research workloads. If you need to discuss any of this, feel free to reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Accelerate drug discovery with NVIDIA BioNeMo Framework on Amazon EKS</title>
		<link>https://aws.amazon.com/blogs/hpc/accelerate-drug-discovery-with-nvidia-bionemo-framework-on-amazon-eks/</link>
		
		<dc:creator><![CDATA[Doruk Ozturk]]></dc:creator>
		<pubDate>Wed, 01 May 2024 15:01:56 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">6ce72440eac8d6fda6dcdd01cb3b62a20165c5d0</guid>

					<description>This post was contributed by Doruk Ozturk and Ankur Srivastava at AWS, and Neel Patel at NVIDIA. Introduction Drug discovery is a long and expensive process. Pharmaceutical companies must sift through thousands of compound possibilities to find potential new drugs to treat diseases. This process takes multiple years and costs billions of dollars, with the […]</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Doruk Ozturk and Ankur Srivastava at AWS, and Neel Patel at NVIDIA.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Drug discovery is a long and expensive process. Pharmaceutical companies must sift through thousands of compound possibilities to find potential new drugs to treat diseases. This process takes multiple years and costs billions of dollars, with the majority of the candidates failing during clinical trials.&lt;/p&gt; 
&lt;p&gt;As generative artificial intelligence (generative AI) continues to transform industries, the life sciences sector is leveraging these advanced technologies to accelerate drug discovery. Generative AI tools powered by deep learning models make it possible to analyze massive datasets, identify patterns, and generate insights to aid the search for new drug compounds. However, running these generative AI workloads requires a full-stack approach that combines robust computing infrastructure with optimized domain-specific software that can accelerate time to solution.&lt;/p&gt; 
&lt;p&gt;In this blog post, we’ll show you how to leverage the NVIDIA BioNeMo platform on Amazon Elastic Kubernetes Service (Amazon EKS) to accelerate drug discovery by using generative AI and other machine learning technologies.&lt;/p&gt; 
&lt;h2&gt;NVIDIA BioNeMo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/clara/bionemo/"&gt;NVIDIA BioNeMo&lt;/a&gt; is a generative AI platform for drug discovery that simplifies and accelerates the training of models using your own data. BioNeMo provides researchers and developers a fast and easy way to build and integrate state-of-the-art generative AI applications across the entire drug discovery pipeline—from target identification to lead optimization—with AI workflows for 3D protein structure prediction, de novo design, virtual screening, docking, and property prediction.&lt;/p&gt; 
&lt;p&gt;The BioNeMo framework facilitates centralized model training, optimization, fine-tuning, and inferencing for protein and molecular design. Researchers can build and train foundation models from scratch at scale, or use pre-trained model checkpoints provided with the BioNeMo Framework for fine-tuning for downstream tasks. Currently, BioNeMo supports models such as ESM1nv, ESM2nv, ProtT5nv, DNABERT, OpenFold, EquiDock, DiffDock, and MegaMolBART. To read more about BioNeMo, visit &lt;a href="https://docs.nvidia.com/bionemo-framework/latest/"&gt;the documentation page&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_3527" style="width: 1689px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3527" loading="lazy" class="wp-image-3527 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/15/HPCBlog-281-fig1-1.png" alt="Figure 1: This image shows the workflow for developing models on NVIDIA BioNeMo. The process is divided into phases for model development and customization and then fine-tuning and deployment." width="1679" height="691"&gt;
 &lt;p id="caption-attachment-3527" class="wp-caption-text"&gt;Figure 1: This image shows the workflow for developing models on NVIDIA BioNeMo. The process is divided into phases for model development and customization and then fine-tuning and deployment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In this post, we’ll walk through how to deploy the NVIDIA BioNeMo Framework on Amazon Elastic Kubernetes Service (Amazon EKS). Amazon EKS provides a fully managed Kubernetes service, making it simpler to run distributed, containerized generative AI workloads at scale. The process will cover:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Setting up an EKS cluster with NVIDIA GPU nodes&lt;/li&gt; 
 &lt;li&gt;Leveraging Amazon FSx for Lustre for high-performance data storage and sharing&lt;/li&gt; 
 &lt;li&gt;Downloading and ingesting Uniref-50 data in a machine learning friendly format&lt;/li&gt; 
 &lt;li&gt;Running a distributed pre-training job to train the ESM-1nv model&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;div id="attachment_3528" style="width: 1145px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3528" loading="lazy" class="size-full wp-image-3528" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/15/HPCBlog-281-fig2-1.png" alt="Figure 2:&amp;nbsp; This architecture diagram shows Amazon EKS cluster with GPU nodes, Amazon FSx for Lustre filesystem, and BioNeMo containers. GPU nodes are optimized for machine learning workloads. The FSx filesystem enables fast access to data needed for distributed training. Amazon CloudWatch is used for logging and monitoring." width="1135" height="1003"&gt;
 &lt;p id="caption-attachment-3528" class="wp-caption-text"&gt;Figure 2: This architecture diagram shows Amazon EKS cluster with GPU nodes, Amazon FSx for Lustre filesystem, and BioNeMo containers. GPU nodes are optimized for machine learning workloads. The FSx filesystem enables fast access to data needed for distributed training. Amazon CloudWatch is used for logging and monitoring.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We leveraged the &lt;a href="https://github.com/aws-ia/terraform-aws-eks-blueprints"&gt;Amazon EKS Blueprints for Terraform &lt;/a&gt;and &lt;a href="https://github.com/awslabs/data-on-eks"&gt;Data on EKS&lt;/a&gt; open source projects to build a robust Kubernetes infrastructure on EKS using Infrastructure as Code best practices. These projects enabled us to quickly stand up an EKS cluster while meeting security and operational excellence requirements.&lt;/p&gt; 
&lt;p&gt;In particular, the Terraform blueprints let us create a production-grade EKS cluster with networking, security groups, node groups, and other critical components out-of-the-box. The Data on EKS repository provided examples for running data-intensive workloads such as Apache Spark and TensorFlow on EKS. Together, these tools allowed us to launch an EKS cluster purpose-built for large-scale data processing in a reproducible and automated fashion. We can easily scale the cluster up to hundreds of nodes to handle compute-intensive jobs. Adopting these community-built modules accelerated our delivery timelines while ensuring the infrastructure remained secure, observable, and operationally robust as it scaled. The flexibility to customize the modules as needed also enabled us to tailor the infrastructure to the specific needs of our data workloads.&lt;/p&gt; 
&lt;h1&gt;The NVIDIA BioNeMo on EKS Blueprint&lt;/h1&gt; 
&lt;p&gt;We published all of the templates we used to deploy BioNeMo on EKS as a new Data on EKS &lt;a href="https://github.com/awslabs/data-on-eks/tree/main/ai-ml/bionemo"&gt;blueprint&lt;/a&gt; on GitHub. We’ll continue to iterate and improve on this blueprint over time, so you should bookmark and refer to the &lt;a href="https://awslabs.github.io/data-on-eks/docs/gen-ai/training/bionemo"&gt;official documentation&lt;/a&gt; on the Data on EKS website moving forward. However, we will discuss key configuration details and technical details here for reference.&lt;/p&gt; 
&lt;p&gt;Step one, as always, is to prepare the input data as a machine learning friendly structure. Uniref50 contains over 50 million unique protein sequences clustered from UniProt at 50% identity. This comprehensive set provides a foundation for vital tasks such as gene annotation and protein family prediction.&lt;/p&gt; 
&lt;p&gt;To leverage Uniref50’s scale, we downloaded and organized the data into training, validation, and test partitions. This layout enhances downstream machine learning by enabling effective model building, evaluation, and monitoring of overfitting. An Amazon FSx for Lustre shared filesystem provided the high-throughput storage needed for data sharing across the compute cluster nodes.&lt;/p&gt; 
&lt;p&gt;After data preparation, we are ready to run the pretraining job. It is important to leverage all available NVIDIA GPU resources for maximum performance. P5 instances powered by NVIDIA H100 Tensor Core GPUs offer the best performance, however, in this example, we used two p3.16xlarge instances with eight GPUs each for demonstration purposes. To utilize all 16 GPUs, we configured the &lt;code&gt;PytorchJob&lt;/code&gt; &lt;a href="https://github.com/awslabs/data-on-eks/blob/main/ai-ml/bionemo/examples/training/esm1nv_pretrain-job.yaml"&gt;custom resource&lt;/a&gt; with one GPU per replica:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-yaml"&gt;resources:
  requests:
    nvidia.com/gpu: 1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;We set replicas to match the total number of available GPUs across the worker nodes:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-yaml"&gt;pytorchReplicaSpecs:
&amp;nbsp; Worker:
&amp;nbsp;&amp;nbsp;&amp;nbsp; replicas: 16&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;To allocate 8 processes per GPU node, we set:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-yaml"&gt;nprocPerNode: "8"&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;By default, workloads that don’t require GPUs won’t be scheduled on our GPU nodes. To make sure our BioNeMo pods were scheduled on the GPU nodes, we added tolerations:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-yaml"&gt;tolerations:
-  Key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;To utilize a larger cluster for BioNeMo analysis, change the above values to match the cluster size. For example, to fully leverage a cluster of 8 p3.8xlarge instances, each with 4 GPUs, you would need to change the following parameters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Set “&lt;code&gt;nprocPerNode&lt;/code&gt;” to 4 to indicate the number of GPUs available per node.&lt;/li&gt; 
 &lt;li&gt;Set “&lt;code&gt;replicas&lt;/code&gt;” to 32 to total the number of GPUs across the 8 nodes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As each p3.8xlarge instance contains 4 GPUs, 8 instances x 4 GPUs per instance = 32 GPUs in the cluster.&lt;/p&gt; 
&lt;p&gt;By properly configuring these parameters according to the resources provisioned, you can efficiently parallelize the training across the all-available GPUs in the cluster. For more information on configuring Pytorch and Kubeflow for your specific needs, read the &lt;a href="https://pytorch.org/docs/stable/distributed.html"&gt;Pytorch distributed module documentation&lt;/a&gt; and the &lt;a href="https://www.kubeflow.org/docs/components/training/"&gt;Kubeflow Training Operator’s documentation&lt;/a&gt;&lt;u&gt;,&lt;/u&gt; respectively.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In summary, leveraging technologies such as NVIDIA BioNeMo on Amazon EKS can accelerate AI-powered drug discovery by orders of magnitude. By combining the power of generative models with robust infrastructure for distributed training, researchers can rapidly analyze massive protein datasets to uncover new drug compound candidates. Automating infrastructure deployment using Terraform modules and best practices for maximum GPU utilization, storage, and monitoring enables workloads to benefit from security, scalability, and observability. Subject matter experts are a key resource for drug discover, and purpose-built generative AI platforms on cloud-native infrastructure can augment their creativity and intuition. As this technology continues maturing, we may see further breakthroughs in delivering life-saving treatments to patients faster.&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/15/nilkanthp-nvidia-profile.png" alt="Neel Patel" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Neel Patel&lt;/h3&gt; 
  &lt;p&gt;Neel Patel is a drug discovery scientist at NVIDIA, who focuses on cheminformatics and computational structural biology. Before joining NVIDIA, Patel was a computational chemist at Takeda Pharmaceuticals. He holds a Ph.D. from the University of Southern California. He lives in San Diego with his family and enjoys hiking and travelling.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Data, emerging technologies, and the circular economy: how Accenture and AWS are unlocking environmental and business impact</title>
		<link>https://aws.amazon.com/blogs/hpc/data-emerging-technologies-and-the-circular-economy-how-accenture-and-aws-are-unlocking-environmental-and-business-impact/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Tue, 30 Apr 2024 12:27:06 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">cfe27b15e171b02db52875e9424a371ce2457466</guid>

					<description>Realizing the $4.5 trillion circular economy opportunity requires accurate data, scalable HPC and agile tools. Read this post to discover how AWS and Accenture partner for real progress.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright wp-image-3572 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/Data-technology-and-the-circular-economy-how-Accenture-and-AWS-are-unlocking-environmental-and-business-impact.png" alt="Data, technology, and the circular economy: how Accenture and AWS are unlocking environmental and business impact" width="380" height="212"&gt;This post was contributed by Ilan Gleiser, Principal Specialist, Emerging Technologies at AWS, Joshua Curtis, Circular Intelligence Global Lead and Patrick Ford, Circular Intelligence North America Lead, Accenture Sustainability Services&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;It is well documented that the circular economy is an opportunity for positive impact on business and society. Accenture’s analysis presents an economic opportunity of $4.5tn value is at stake for the global economy to 2030 by departing from our current ‘&lt;em&gt;take-make-waste&lt;/em&gt;’ economic system [1]. Ellen MacArthur Foundation outlines the importance of circularity as a solution to climate change, with 45% of the required carbon emission reductions to achieve a 1.5-degree world coming from how we make and consume products [2].&lt;/p&gt; 
&lt;p&gt;It’s clear that resource use and circularity are critical to the creation of a sustainable, healthy economy. But how do we realize this value? How does a business identify where and how circular strategies can create financial and environmental impact?&lt;/p&gt; 
&lt;p&gt;In this post, we will explore the challenges to achieving accurate and actionable data on circular economy performance and impact, and the solutions that lie in emerging technologies. Join us as we explore opportunities for kicking off and accelerating the data transformation needed to drive authentic, impact-driven progress on circularity.&lt;/p&gt; 
&lt;h2&gt;Achieving data-driven circularity&lt;/h2&gt; 
&lt;p&gt;The importance of transitioning to circular business models is why the European Commission is, for the first time, making measurement and disclosure of resource use and circular economy impacts mandatory for companies. The newly-launched European Sustainability Reporting Standards (ESRS) include a requirement [3] for companies – where material – to report on circular economy metrics like:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;the percentage of material used for products and packaging that are renewable, recycled or re-used&lt;/li&gt; 
 &lt;li&gt;the volume of waste by stream that is recovered by destination&lt;/li&gt; 
 &lt;li&gt;the financial effects of material risks and opportunities arising from resource use&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;But how do we calculate these metrics? How do we collect, aggregate, and analyze the data in a way that doesn’t require significant time and resources year on year? How do we not only do this to understand where we are, but also to determine where we need to go?&lt;/p&gt; 
&lt;p&gt;While there is an increasing number of new sustainability measurement products being presented to the market, there is no one ‘tool’ to answer these questions. In fact, these questions themselves are not new — they are the essence of data-driven decision-making which is the foundation of any profitable business. The circular measurement challenge is a data challenge and must be approached as such.&lt;/p&gt; 
&lt;p&gt;What is new – and evolving – is the potential application of digital technologies to support the transformation of circular economy data management for companies. For example, the advancement of the Internet of Things (IoT) enables tracking of product movement and health through use phases; machine learning algorithms can help companies identify patterns in circular economy data, like trends in demand for certain recycled materials; and blockchain technology can be used to create a transparent and secure ledger of circular-related transactions, enabling stakeholders to track and verify the movement of materials and products through the value chain.&lt;/p&gt; 
&lt;p&gt;As we continue to tackle the circular measurement challenge, it is essential to approach it with a data-driven mindset. Digital technologies have the potential to revolutionize how we manage and analyze circular economy data, allowing us to create a more sustainable and efficient economy for the future. Accenture and AWS are collaborating to make these applications a reality.&lt;/p&gt; 
&lt;h2&gt;The challenges to circular data transformation&lt;/h2&gt; 
&lt;p&gt;To help ensure digital solutions are effective in managing circular economy performance, it’s crucial to design them to address specific challenges faced by businesses. Let’s begin by exploring these challenges.&lt;/p&gt; 
&lt;p&gt;First, selecting the right metrics themselves is not straightforward. We mentioned the European Commission’s regulations ESRS E5 on resource use and the Circular Economy. They provide headline metrics for business disclosure. The &lt;a href="https://pacecircular.org/sites/default/files/2023-01/CEIC_Circular%20Target%20Activation%20Guides_FINAL_01182023.pdf"&gt;Circular Target-Setting Guidance&lt;/a&gt; from the Circular Economy Indicators Coalition (CEIC), a partnership between The Platform for Accelerating the Circular Economy (PACE) and Circle Economy (supported by Accenture) provides an overview of leading measurement methodologies and approaches for business implementation.&lt;/p&gt; 
&lt;p&gt;These are important starting points for business across industries, but they don’t account for the specific value chains or functional priorities of businesses in different sectors. For example, the metrics to measure circular economy performance (and therefore the data required) vary significantly for a fashion retailer compared to an oil and gas major. Ultimately, selecting the right metrics must be led by each business, drawing on the wealth of supporting materials, best practices and market standards.&lt;/p&gt; 
&lt;p&gt;Next comes the hard part: identifying, collecting and transforming the data. Comprehensive circular measurement relies on data from across the value chain, often not tracked in existing enterprise systems. The foundational data itself is simple enough – what materials are being used and where do they come from; what waste is being produced and where is it going – are tangible examples. The challenge is collecting that data across product lines, business units, and geographies and then transforming the data to be usable. For example, when calculating your percentage of materials that are recycled, renewable or re-used (as ESRS E5 requires), materials data must be segmented in ways not currently built into enterprise data capture. Without technology, this requires line-by-line segmentation based on data that is available e.g. through supplier declarations. The bottom line is that collecting data to measure circular performance is an arduous process, requiring time and costs, and is hindered by data gaps. To do this at the business level, in a way that enables action, is not only helped by digital technologies, it depends upon them.&lt;/p&gt; 
&lt;p&gt;Finally, companies must transform this data into actionable insights to guide decision-making. Circularity is not an end, but a means to optimize planetary and business impact. To accelerate this impact, resource use data like the above example regarding materials that are recycled, renewable or re-used, must be connected with other internal and external data sets like sales data and emissions intensity factors. Companies must understand how different material choices impact carbon emissions, as well as procurement costs and business profitability. The true story of corporate circularity is of trade-offs and investment requirements to capture long-term value. Without a comprehensive approach to circular economy measurement and data transformation, understanding those trade-offs properly and making impact-driven decisions is impossible. This again adds complexity to data collection and analysis, with the only solution for ongoing insight generation being an automated, centralized approach, like a circular and/or sustainability data lake, which combines data sets and applies analytics solutions for calculation and visualization.&lt;/p&gt; 
&lt;div id="attachment_3564" style="width: 867px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3564" loading="lazy" class="size-full wp-image-3564" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-13.24.08.png" alt="Figure 1. The Circular Business Hub: Illustration of data visualization for circular economy performance management. Companies must overcome challenges to achieve measurement and visualization of circularity at each pillar of the business value chain." width="857" height="537"&gt;
 &lt;p id="caption-attachment-3564" class="wp-caption-text"&gt;Figure 1. The Circular Business Hub: Illustration of data visualization for circular economy performance management. Companies must overcome challenges to achieve measurement and visualization of circularity at each pillar of the business value chain.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;The role of emerging technologies&lt;/h2&gt; 
&lt;p&gt;Accenture and AWS are collaborating to bring the best of their combined data, technology and sustainability expertise to transform circular economy data management. AWS offers the broadest set of capabilities in artificial intelligence (AI), machine learning (ML), Internet of Things (IoT), big data analytics, and high-performance computing (HPC) in the market. Accenture is the world’s leading integrator of AWS solutions and technologies – they’ve completed over 1,100 projects with us over 15 years of partnership.&lt;/p&gt; 
&lt;p&gt;Teaming up on the circular economy, Accenture brings its 12+ years of client experience in circular economy strategy and implementation, and 100+ global circular economy experts, to guide the application of these digital solutions to unlock value for joint customers from data-driven circularity.&lt;/p&gt; 
&lt;p&gt;Accenture has developed core assets as part of a suite of circular intelligence solutions, powered by AWS. These include industry-specific KPI frameworks, a foundational data model and a proof-of-concept dashboard to act as a platform for client co-development and customization. Building upon these assets, a core priority of this collaboration is to enable the automation of circular data ingestion, transformation and analysis to enable ongoing performance measurement and generation of actionable insights for companies across the value chain. To do this, we’re leveraging &lt;a href="https://www.accenture.com/us-en/services/cloud/aws-business-group#block-velocity"&gt;Velocity&lt;/a&gt;, a co-funded and co-developed platform that adds new cloud innovations up to 50% faster, to develop the data architecture which pulls in resource use and circular economy-related datasets into a central circular data lake.&lt;/p&gt; 
&lt;p&gt;A circular data lake (Figure 2) works by taking raw data from siloed sources and bringing it into a unified, organized system. This process is made possible using the AWS Transfer family, which collects data and brings it into a raw layer.&lt;/p&gt; 
&lt;div id="attachment_3565" style="width: 835px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3565" loading="lazy" class="size-full wp-image-3565" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-13.24.48.png" alt="Figure 2. Reference Architecture of an automated circular data lake for managing circular economy measurement and insight generation" width="825" height="375"&gt;
 &lt;p id="caption-attachment-3565" class="wp-caption-text"&gt;Figure 2. Reference Architecture of an automated circular data lake for managing circular economy measurement and insight generation&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Once the data is in the raw layer, one or more &lt;em&gt;ETL&lt;/em&gt; (extract, transform, and load) workstreams are triggered. These ETL workstreams work together to clean, refine, and organize the data. To facilitate the ETL process, the reference architecture employs several AWS Glue jobs and AWS Lambda functions. These functions and jobs help to ensure that the ETL workflows are fully orchestrated and operate efficiently and effectively, to produce the refined data necessary for circular metrics and KPIs.&lt;/p&gt; 
&lt;p&gt;Once the data is fully curated and prepared, it’s ready for business intelligence and machine learning (AWS Athena, Amazon Redshift, and Amazon SageMaker). These services enable companies to derive value from the data by analyzing it, identifying patterns and trends. By doing so, it’s possible to transform siloed data from a wide range of sources into actionable insights that can drive circular business progress.&lt;/p&gt; 
&lt;p&gt;The circular data lake is designed to centralize all data related to a business’ resource use and subsequent impact. With this, it must be designed and implemented in connection with other environmental objectives like climate and nature-related data sources. Indeed, the circular data lake is designed to be a component of an organization’s overall sustainability data lake fabric, creating a single system of record for ESG data management.&lt;/p&gt; 
&lt;p&gt;Building upon the circular data architecture, created and tested with joint customers, Accenture and AWS are building solutions for companies focused on creating value from data in functional areas of circular transformation. This means going beyond just measuring circular economy performance (which we refer to as ‘foundational’ use cases) to applying digital solutions for value creation from circular insights (these are ‘advanced’ use cases).&lt;/p&gt; 
&lt;div id="attachment_3566" style="width: 836px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3566" loading="lazy" class="size-full wp-image-3566" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-13.25.35.png" alt="Figure 3. Overview of foundational and advanced circular intelligence use cases at each stage of an industry-agnostic value chain " width="826" height="385"&gt;
 &lt;p id="caption-attachment-3566" class="wp-caption-text"&gt;Figure 3. Overview of foundational and advanced circular intelligence use cases at each stage of an industry-agnostic value chain&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 3 shows examples of circular economy use cases split between foundational and advanced. Circular intelligence helps companies measure the impact of their investments and compare results over time. For more examples of how digital technologies enable a circular economy, please see &lt;a href="https://aws.amazon.com/blogs/hpc/how-to-make-digital-technologies-for-the-circular-economy-work-for-your-business/"&gt;this&lt;/a&gt; blog post.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 3. Overview of foundational and advanced circular intelligence use cases at each stage of an industry-agnostic value chain &lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;The circular intelligence maturity journey&lt;/h2&gt; 
&lt;p&gt;Let’s look at an example of how AWS services and solutions are being used to enable foundational and advanced use cases in the procurement stage of the product development lifecycle.&lt;/p&gt; 
&lt;h3&gt;Example: how to transition away from virgin, non-renewable materials to optimize for decarbonization, cost reduction and risk management?&lt;/h3&gt; 
&lt;p&gt;The materials that companies buy for products and services are a critical component of their environmental impact, while the reliance on virgin, finite materials also presents a growing supply chain risk for many industries.&lt;/p&gt; 
&lt;p&gt;To achieve foundational circular procurement decision-making, machine learning algorithms can analyze past procurement data and identify patterns and insights to make better purchasing decisions. For example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;With AWS’s &lt;a href="https://aws.amazon.com/solutions/ai-ml/intelligent-document-processing/"&gt;Intelligent Document Processing&lt;/a&gt; solutions, companies can streamline the ingestion of bills of materials across product lines. By automating the procurement data ingestion process, companies can measure their circular input footprint (share of circular materials per product and/or packaging type), as well as the cost and carbon impact of circular inputs. This allows for faster and more accurate decision-making when selecting sustainable alternatives.&lt;/li&gt; 
 &lt;li&gt;Another powerful tool is &lt;a href="https://aws.amazon.com/textract/"&gt;Amazon Textract&lt;/a&gt;, which can automatically ingest supplier declarations, saving time and increasing accuracy. By digitizing these declarations, companies can easily track progress toward their circular procurement goals and identify areas that require improvement.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These applications are a significant value add for companies in reducing the time required to assess circular performance in procurement, while enabling decisions that account for trade-offs, and can accelerate the highest value initiatives.&lt;/p&gt; 
&lt;p&gt;The value opportunity doesn’t end there, however.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;With generative AI services, like &lt;a href="https://aws.amazon.com/bedrock/"&gt;Amazon Bedrock&lt;/a&gt;, companies can build on this foundation for predictive analytics, real-time scenario modeling and advanced forecasting – AWS Partner &lt;a href="https://www.simudyne.com/"&gt;Simudyne&lt;/a&gt;, uses high-performance computing enabled simulations, coupled with LLM powered chatbots, to provide recommendations of how companies can decarbonize their supply chains, and therefore reduce their scope 3 emissions.&lt;/li&gt; 
 &lt;li&gt;By leveraging market data and research to identify opportunities for alternative materials and supply chain risk assessment, companies can gain an edge over the competition. For example, AWS partner &lt;a href="https://goodchemistry.com/"&gt;Good Chemistry&lt;/a&gt;, uses &lt;a href="https://aws.amazon.com/hpc/"&gt;HPC clusters&lt;/a&gt;, orchestrated by AWS Batch, to &lt;a href="https://aws.amazon.com/blogs/hpc/massively-scaling-quantum-chemistry-to-support-a-circular-economy/"&gt;design new materials&lt;/a&gt; and accelerate the identification of molecules to substitute or destroy toxic chemicals from the production process and environment.&lt;/li&gt; 
 &lt;li&gt;By using SageMaker for its &lt;a href="https://aws.amazon.com/blogs/machine-learning/remote-monitoring-of-raw-material-supply-chains-for-sustainability-with-amazon-sagemaker-geospatial-capabilities/"&gt;geospatial capabilities&lt;/a&gt;, companies are proactively addressing potential environmental risks. By monitoring deforestation in real-time with satellite data, IOT sensors and drones, coupled with machine learning algorithms, regulators, insurance companies and businesses can be aware of the risk of their current material footprint.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Overall, the use of AWS emerging technologies helps companies, governments and other stakeholders, transition towards circular materials in the short and long term, making sustainability a priority in their procurement process.&lt;/p&gt; 
&lt;h2&gt;Call to action&lt;/h2&gt; 
&lt;p&gt;Each company’s journey on circular economy measurement will look different, depending on industry, strategic priorities, and technical maturity. At Accenture and AWS, we believe solutions must be fit-for-purpose and co-designed for the specific business context. While there are growing options for ‘plug-and-play’ sustainability solutions, specifically in carbon management, developing the data foundation for automated baselining across the value chain and moving towards value-creating technology applications requires a level of customization and co-development using existing products and repeatable solutions where available.&lt;/p&gt; 
&lt;p&gt;The first step to get started is defining your circular economy blueprint with key metrics across each area of your business. This is the foundation for a data-driven strategy and many companies are working hard to collect data for reporting on circular performance on an annual basis. To then understand and plan for foundational and advanced circular measurement use cases, it is key to assess your current functional and technical maturity to measure and manage these metrics. Identifying where the raw data lives, how it is collected and stored, and who is responsible for it is critical to map the journey towards automation. From there, it is a case of prioritization of metrics and use cases through engaging with stakeholders across different business groups. The end-users of the data and insights must be involved in the full journey to ensure the applied solution serves the required needs.&lt;/p&gt; 
&lt;p&gt;The next step is to design, build and test. Cross-functional teams will need to work together to integrate solutions as part of a circular data lake, and develop the user interface for practical, day-to-day business decision making. Proof-of-Concepts (POCs) are valuable in building the foundational technical architecture, illustrating the value with a tangible output, and establishing buy-in from leadership and key stakeholders. This can then prime you for rapid deployment and scale.&lt;/p&gt; 
&lt;p&gt;From there, deployment must be thought of as a multi-generational journey. Companies should focus on their key requirements for a Minimum Viable Product (MVP) solution that supports their core business priorities. Be it cross-value chain performance management, or specific use case advancement, the MVP must also include solutions for data pipeline automation and a data governance strategy. Accenture and AWS are supporting companies across these phases of implementation, including the development of targeted POCs and MVPs to tackle the key challenges your business is facing on circular measurement and prove the value of emerging technologies applied to these challenges. This landscape is rapidly evolving and our joint assets are developing with every implementation; co-development with leading businesses is at the heart of this.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Circular economy measurement requires more than an off-the-shelf sustainability solution; it is a data and technology problem that requires a comprehensive approach. By leveraging the power of AWS and Accenture’s expertise, businesses can unlock the full potential of the circular economy. By capturing data into a purpose driven data lake and running advanced analytics, our joint approach provides businesses with actionable insights and recommendations for optimizing resource usage and reducing waste. By prioritizing sustainability and embracing greater intelligence, businesses can drive positive environmental and business impacts, paving the way for a more sustainable future.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;h4&gt;References&lt;/h4&gt; 
&lt;p&gt;[1] Lacy, Long and Spindler, The Circular Economy Handbook: Realizing the Circular Advantage (2019)&lt;br&gt; [2] Ellen MacArthur Foundation, Completing the Picture: How the Circular Economy Tackles Climate Change (2021)&lt;br&gt; [3]European Sustainability Reporting Standards in scope for companies with a turnover above €150 million in EU&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Optimizing MPI application performance on hpc7a by effectively using both EFA devices</title>
		<link>https://aws.amazon.com/blogs/hpc/optimizing-mpi-application-performance-on-hpc7a-by-effectively-using-both-efa-devices/</link>
		
		<dc:creator><![CDATA[Sai Sunku]]></dc:creator>
		<pubDate>Wed, 24 Apr 2024 15:04:39 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[FEA]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[MPI]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">08bac8f4eb9c45d176364cc57f8dc42002333566</guid>

					<description>Get the inside scoop on optimizing your MPI apps and configuration for AWS's powerful new Hpc7a instances. Dual rail gives these instances huge networking potential @ 300 Gb/s - if properly used. This post provides benchmarks, sample configs, and real speedup numbers to help you maximize network performance. Whether you run weather simulations, CFD, or other HPC workloads, you'll find practical tips for your codes.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Sai Sunku, Software Engineer, Annapurna Labs, Matt Koop, Principal Engineer and Karthik Raman, Principal Performance Engineer, HPC Engineering&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The hpc7a instance type is the latest generation AMD-based HPC instance type offered by AWS. It’s available in multiple sizes and offers superior performance compared to the previous generation hpc6a instance type. See our &lt;a href="https://aws.amazon.com/blogs/hpc/deep-dive-into-hpc7a-the-newest-amd-powered-member-of-the-hpc-instance-family/"&gt;previous post&lt;/a&gt; for a deep dive on the hpc7a instance itself and this &lt;a href="https://aws.amazon.com/blogs/hpc/instance-sizes-in-the-amazon-ec2-hpc7-family-a-different-experience/"&gt;other post&lt;/a&gt; that describes the different instance sizes.&lt;/p&gt; 
&lt;p&gt;If you’re an MPI user, you’ll want to know that Hpc7a’s network bandwidth of 300 Gbps is 3x higher than hpc6a. We’ve done this by using &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#network-cards"&gt;two network cards&lt;/a&gt;, where each card is exposed to the user software as a different PCIe device. Effectively using the two network cards is essential for achieving the best performance on hpc7a.&lt;/p&gt; 
&lt;p&gt;In this post we’ll show you how to configure your application and MPI to use both network cards so you can achieve the greatest performance for your codes. We’ll also discuss some benchmarks you can look at to verify that you’re &lt;em&gt;actually&lt;/em&gt; using both network cards. Finally, we’ll dig into some application results to give you a taste of the speedup you can get in your applications.&lt;/p&gt; 
&lt;h2&gt;Under the hood&lt;/h2&gt; 
&lt;p&gt;Let’s start with the PCIe topology. As &lt;a href="https://aws.amazon.com/blogs/hpc/deep-dive-into-hpc7a-the-newest-amd-powered-member-of-the-hpc-instance-family/"&gt;the post on instance sizes&lt;/a&gt; described, all hpc7a instance sizes use the same underlying hardware, but with different cores enabled or disabled to make it easy to get the best memory bandwidth per core for most codes. The hardware is based on the 4th generation AMD EPYC (Genoa) processor with 192 physical cores in a 2-socket configuration with 96 physical cores per socket.&lt;/p&gt; 
&lt;p&gt;In multi-domain NUMA systems, not all PCIe devices are created equal. Processes can be more efficient if they use the PCIe device closest to them and avoid communication across sockets. It’s no different for EFA devices. Using the network card closest to your MPI process allows for &lt;em&gt;much&lt;/em&gt; better performance. The topology for EFA devices is shown in Figure 1.&lt;/p&gt; 
&lt;div id="attachment_3554" style="width: 899px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3554" loading="lazy" class="size-full wp-image-3554" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-11.50.35.png" alt="Figure 1 – Topology of the EFA devices in hpc7a. Each green square represents a processor core and the orange rectangle the L3 cache shared by 8 cores. There are two NUMA domains with 96 cores per domain connected by a high speed interconnect. The two EFA devices are associated with one NUMA node each." width="889" height="395"&gt;
 &lt;p id="caption-attachment-3554" class="wp-caption-text"&gt;Figure 1 – Topology of the EFA devices in hpc7a. Each green square represents a processor core and the orange rectangle the L3 cache shared by 8 cores. There are two NUMA domains with 96 cores per domain connected by a high speed interconnect. The two EFA devices are associated with one NUMA node each.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Running MPI applications&lt;/h2&gt; 
&lt;p&gt;The good news is that the hpc7a instance is designed to work right out of the box with your MPI for a variety of expected uses.&lt;/p&gt; 
&lt;p&gt;Many MPIs refer to using multiple network cards as “multi-rail,” where each “rail” refers to a single network card, or in our case, an EFA device. In both Open MPI and Intel MPI, when using a multi-rail configuration with EFA, each rank will be assigned a single “rail.”&lt;/p&gt; 
&lt;p&gt;This means that in a two rail setup, half of the ranks on the instance will be assigned to one rail, and the other half to the other rail. This is generally desirable, because each rank is communicating separately – and for many applications that can lead to more efficient communication. Our tests have shown that real applications can be significantly faster when they use both EFA devices and you’ll see that in the data at the end of this post, too.&lt;/p&gt; 
&lt;p&gt;If you run your application with 192 ranks on a hpc7a.96xlarge instance with one rank per core, then each rank should automatically use the correct network card. But if you choose to run your application with fewer ranks per instance (maybe your application is memory intensive or you’re optimizing for licensing costs) then you need to consider which process is running on which core. You can configure the process placement through your scheduler or MPI. But this is also exactly why we offer smaller hpc7a instance sizes with some of the cores disabled. For many applications, using a smaller instance size will assign the MPI ranks to the correct cores and thus provide the best performance.&lt;/p&gt; 
&lt;p&gt;However, if you use a bandwidth benchmark that uses only &lt;em&gt;one process per instance&lt;/em&gt; (like the &lt;code&gt;osu_bw&lt;/code&gt; benchmark), you can only see the bandwidth associated with a single rail. Figure 2 shows the results of &lt;code&gt;osu_bw&lt;/code&gt; benchmark with 2 x hpc7a instances. The measured bandwidth is significantly smaller than the advertised rate of 300 Gbps. This lower bandwidth is because the benchmark is running a single rank per &lt;em&gt;node&lt;/em&gt;. Virtually all real applications run multiple ranks per node.&lt;/p&gt; 
&lt;h3&gt;How can you make sure that different ranks are assigned to different EFA devices?&lt;/h3&gt; 
&lt;p&gt;If you’re using Open MPI v 4.1.0 or later or the newly released Intel MPI 2021.12, &lt;strong&gt;you don’t have to do much&lt;/strong&gt;. Open MPI and the latest Intel MPI can detect the two EFA devices and use them both, exactly as you’d expect.&lt;/p&gt; 
&lt;p&gt;However, to get the same experience with Intel MPI versions 2021.11 &lt;em&gt;or earlier&lt;/em&gt;, you need to set an environment variable &lt;code&gt;I_MPI_MULTIRAL=1&lt;/code&gt; and pass that environment variable to the &lt;code&gt;mpirun&lt;/code&gt; or &lt;code&gt;mpiexec&lt;/code&gt; command. Typically, that means running your application like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;mpirun -genv I_MPI_MULTIRAIL=1 -n &amp;lt;number of processes&amp;gt; &amp;lt;your application&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Support for this environment variable was added to Intel MPI version 2021.6.&lt;/p&gt; 
&lt;div id="attachment_3555" style="width: 546px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3555" loading="lazy" class="size-full wp-image-3555" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-11.52.00.png" alt="Figure 2 – OSU single pair bandwidth benchmark for Intel MPI and Open MPI. The measured bandwidth is significantly smaller than the maximum value of 300 Gbps for hpc7a." width="536" height="377"&gt;
 &lt;p id="caption-attachment-3555" class="wp-caption-text"&gt;Figure 2 – OSU single pair bandwidth benchmark for Intel MPI and Open MPI. The measured bandwidth is significantly smaller than the maximum value of 300 Gbps for hpc7a.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Performance measurements&lt;/h2&gt; 
&lt;p&gt;To measure the bandwidth with both EFA devices in use, we need a multi-pair bandwidth benchmark similar to the osu_mbw_mr benchmark. In this benchmark, the MPI ranks are grouped into pairs and the ranks in each pair send data to each other.&lt;/p&gt; 
&lt;p&gt;Figure 3a shows the results of an internally developed multi-pair bandwidth benchmark similar to &lt;code&gt;osu_mbw_mr&lt;/code&gt; for two hpc7a instances with 192 ranks on each instance. With &lt;code&gt;I_MPI_MULTIRAIL=0&lt;/code&gt;, Intel MPI was unable to take advantage of both network cards and we see a bandwidth value close to 150 Gbps. With &lt;code&gt;I_MPI_MULTIRAIL=1&lt;/code&gt;, we see the expected value close to 300 Gbps. Open MPI shows a bandwidth close to 300 Gbps out of the box.&lt;/p&gt; 
&lt;p&gt;While bandwidth is important if your application is sending large messages, the number of messages sent, or the message rate, becomes more important for smaller messages.&lt;/p&gt; 
&lt;p&gt;Figure 3b shows the message rate for 2kiB messages. Using both EFA devices led to a significant improvement in the message rate at small message sizes as well. So whether your application sends a few large messages or many small messages, you can improve its performance by using both EFA devices.&lt;/p&gt; 
&lt;div id="attachment_3556" style="width: 891px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3556" loading="lazy" class="size-full wp-image-3556" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-11.52.42.png" alt="Figure 3 – Multi-pair bandwidth and message rate benchmarks for Intel MPI with I_MPI_MULTIRAIL=0, Intel MPI with I_MPI_MULTIRAIL=1 and Open MPI. The message rate is for messages 2kiB in size. Intel MPI with I_MPI_MULTIRAIL=0 is unable to take advantage of both EFA devices and shows a lower bandwidth and message rate.=" width="881" height="325"&gt;
 &lt;p id="caption-attachment-3556" class="wp-caption-text"&gt;Figure 3 – Multi-pair bandwidth and message rate benchmarks for Intel MPI with I_MPI_MULTIRAIL=0, Intel MPI with I_MPI_MULTIRAIL=1 and Open MPI. The message rate is for messages 2kiB in size. Intel MPI with I_MPI_MULTIRAIL=0 is unable to take advantage of both EFA devices and shows a lower bandwidth and message rate.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Finally, let’s look at some application benchmarks to see how setting &lt;code&gt;I_MPI_MULTIRAIL=1&lt;/code&gt; can speed up actual applications.&lt;/p&gt; 
&lt;p&gt;Figure 4 shows the results for the &lt;a href="https://www2.mmm.ucar.edu/wrf/src/conus2.5km.tar.gz"&gt;CONUS 2.5km benchmark&lt;/a&gt; performance using WRF v4.2.2. We used the Intel compiler version 2022.1.2 and Intel MPI 2021.9.0 to compile and run WRF and calculated the speedup based on the total compute time. For 32 instances, setting &lt;code&gt;I_MPI_MULTIRAIL=1&lt;/code&gt; led to a 10% increase in the speedup. For 192 instances, the increase was over 30%.&lt;/p&gt; 
&lt;div id="attachment_3557" style="width: 515px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3557" loading="lazy" class="size-full wp-image-3557" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-11.53.44.png" alt="Figure 4 –Speedup in runtime for WRF CONUS 2.5km benchmark. At large instance counts, we see that I_MPI_MULTIRAIL=1 shows a higher speedup." width="505" height="374"&gt;
 &lt;p id="caption-attachment-3557" class="wp-caption-text"&gt;Figure 4 –Speedup in runtime for WRF CONUS 2.5km benchmark. At large instance counts, we see that I_MPI_MULTIRAIL=1 shows a higher speedup.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Finally, figure 5 shows the results for the &lt;a href="https://www.ansys.com/en-in/it-solutions/benchmarks-overview/ansys-fluent-benchmarks/ansys-fluent-benchmarks-release-19/external-flow-over-a-formula-1-race-car"&gt;External flow over a Formula-1 race car&lt;/a&gt; test-case, running on ANSYS Fluent and Intel MPI 2021.9.0. We used the solver rating to compare performance. For 48 instances, setting &lt;code&gt;I_MPI_MULTIRAIL=1&lt;/code&gt; boosted the solver rating by &amp;gt; 15%.&lt;/p&gt; 
&lt;div id="attachment_3558" style="width: 600px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3558" loading="lazy" class="size-full wp-image-3558" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-11.54.12.png" alt="Figure 5 –Solver rating for f1_racecar_140M test case in ANSYS Fluent. At large instance counts, we see that I_MPI_MULTIRAIL=1 provides a significant boost in solver rating." width="590" height="409"&gt;
 &lt;p id="caption-attachment-3558" class="wp-caption-text"&gt;Figure 5 –Solver rating for &lt;code&gt;f1_racecar_140M&lt;/code&gt; test case in ANSYS Fluent. At large instance counts, we see that&lt;code&gt; I_MPI_MULTIRAIL=1&lt;/code&gt; provides a significant boost in solver rating.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we talked about the PCIe topology of hpc7a instances, how MPI applications use EFA devices and how you can make tuning changes to achieve best performance on hpc7a. If you’re using Open MPI, you can get the best performance right out of the box. If you’re using Intel MPI, we suggest upgrading to the latest 2021.12 release to achieve the same outcome, or set the environment variable &lt;code&gt;I_MPI_MULTIRAIL=1&lt;/code&gt; in your job scripts.&lt;/p&gt; 
&lt;p&gt;We hope you get great performance from the hpc7a instances for your HPC workloads. Reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt; with your thoughts and questions. Happy computing!&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Build and deploy a 1 TB/s file system in under an hour</title>
		<link>https://aws.amazon.com/blogs/hpc/build-and-deploy-a-1-tb-s-file-system-in-under-an-hour/</link>
		
		<dc:creator><![CDATA[Randy Seamans]]></dc:creator>
		<pubDate>Tue, 23 Apr 2024 14:26:36 +0000</pubDate>
				<category><![CDATA[Amazon FSx]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<guid isPermaLink="false">2f2bf7a37ba03c570aa2e58624876ea742c109fd</guid>

					<description>Want to set up a high-speed shared file system for your #HPC or #AI workloads in under an hour? Learn how with this new blog post.</description>
										<content:encoded>&lt;div id="attachment_3822" style="width: 390px" class="wp-caption alignright"&gt;
 &lt;a href="https://youtu.be/Nm3j5PWPFrY" target="new" rel="noopener"&gt;&lt;img aria-describedby="caption-attachment-3822" loading="lazy" class="size-full wp-image-3822" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/YT-Huge-tracts-of-Lustre.png" alt="" width="380" height="213"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-3822" class="wp-caption-text"&gt;You can see a demo with Randy on our HPC Tech Shorts channel on YouTube: https://youtu.be/Nm3j5PWPFrY&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;High throughput shared files systems are an essential part of any HPC or AI environment. Whether you want to train large language models (LLMs), find effective new drugs, or lead the search for new sources of energy, shared file systems are responsible for feeding compute and storing the hard-won results.&lt;/p&gt; 
&lt;p&gt;If you manage or use an on-premise environment, you know the complexity, value, and cost of providing a high throughput persistent data repository. You probably also know it typically takes weeks or months to plan and provision a large Lustre environment from scratch.&lt;/p&gt; 
&lt;p&gt;In this post, I’ll show how to build persistent shared file systems capable of &lt;em&gt;terabytes per second&lt;/em&gt; in AWS in under an hour.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;The dynamic nature of &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt; enables your organization to leverage the massive scale of AWS compute, network, and storage for your HPC and AI applications while only paying for actual usage. If your organization is trying to reduce both time and cost to reach a solution, FSx for Lustre combined with &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html"&gt;AWS Accelerated Instances&lt;/a&gt; gets you off to a great start without waiting for on-premise resources.&lt;/p&gt; 
&lt;p&gt;While live at the &lt;a href="https://sc23.supercomputing.org/"&gt;SuperComputing 2023&lt;/a&gt; AWS booth, I demonstrated a 1.1 PiB FSx for Lustre file system capable of over &lt;strong&gt;a terabyte per second&lt;/strong&gt; from a standard AWS account in the Northern Virginia region.&lt;/p&gt; 
&lt;p&gt;Prior to building the file system, your account quotas for FSx for Lustre must be increased. There is no charge for maintaining an increased quota. But most importantly, I did not have to architect or pre-provision storage, storage servers, interconnect, nor select the proper Lustre configuration to meet my performance goals.&lt;/p&gt; 
&lt;p&gt;As shown in figure 1, after completing a few questions, the FSx service started creating a high-throughput, persistent, SSD backed POSIX-compliant file system.&lt;/p&gt; 
&lt;div id="attachment_3540" style="width: 1006px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3540" loading="lazy" class="size-full wp-image-3540" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/16/CleanShot-2024-04-16-at-16.56.22.png" alt="Figure 1 – Screenshot of the ease of creating a Persistent FSx for Lustre 1 TB/s file system in progress" width="996" height="454"&gt;
 &lt;p id="caption-attachment-3540" class="wp-caption-text"&gt;Figure 1 – Screenshot of the ease of creating a Persistent FSx for Lustre 1 TB/s file system in progress&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;By relieving the heavy lift of deployment, not only is there a huge savings in time, but the human effort previously used to plan, evaluate, procure, rack, build, configure, and manage can be redirected to tasks that more directly impact the time to solution.&lt;/p&gt; 
&lt;h2&gt;How long does it take for the file system to become ready?&lt;/h2&gt; 
&lt;p&gt;When a FSx for Lustre file system is created, you can create an empty file system, or you can import existing metadata from data repositories. &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/overview-dra-data-repo.html"&gt;Data Repositories&lt;/a&gt; are simply &lt;a href="https://aws.amazon.com/s3/"&gt;S3 buckets&lt;/a&gt; or &lt;a href="https://repost.aws/knowledge-center/s3-prefix-nested-folders-difference"&gt;S3 Prefixes&lt;/a&gt; that exist in AWS.&lt;/p&gt; 
&lt;p&gt;When creating my 1 PiB file system at SC23, the build completed in far less than one hour – &lt;em&gt;including importing the metadata&lt;/em&gt;. You can specify one data repository association (DRA) on build, and you can add up to seven more (as needed) after initial build. Of course, the length of time to load a DRA is directly dependent upon the throughput level, and the volume of metadata.&lt;/p&gt; 
&lt;p&gt;What sort of environments does FSx for Lustre enable? At SC23, I used a cluster of over 120 i3en.24xlarge Amazon EC2 compute-optimized instances, each with a 100 Gbit/s &lt;a href="https://aws.amazon.com/hpc/efa/"&gt;Elastic Fabric Adapter&lt;/a&gt; for network connectivity.&lt;/p&gt; 
&lt;p&gt;Recently we introduced new &lt;a href="https://aws.amazon.com/ec2/instance-types/#HPC_Optimized"&gt;HPC optimized instances&lt;/a&gt; that support EFA bandwidth up to 300 Gb/s (Hpc7a instance). You can &lt;a href="https://www.youtube.com/watch?v=i2T7Hi2Yjoc"&gt;find out more about them&lt;/a&gt; from our HPC Tech Shorts channel on YouTube. If you need to scale your compute for heavy duty GPU workloads, FSx for Lustre also can connect to the &lt;a href="https://aws.amazon.com/ec2/ultraclusters/"&gt;EC2 UltraCluster&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The particular instances you choose for any given workload should be tailored to your application’s requirements to keep your costs low, and speed of execution high.&lt;/p&gt; 
&lt;p&gt;Now, once our build and/or load is complete, let’s move on to unit testing of our new file system.&lt;/p&gt; 
&lt;h2&gt;How fast can this go?&lt;/h2&gt; 
&lt;div id="attachment_3549" style="width: 813px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3549" loading="lazy" class="size-full wp-image-3549" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/16/CleanShot-2024-04-16-at-16.57.20-1.png" alt="Figure 2 – Screenshot of a single cluster node achieving 9.25 GByte/s of file system Read/Write throughput via fio" width="803" height="418"&gt;
 &lt;p id="caption-attachment-3549" class="wp-caption-text"&gt;Figure 2 – Screenshot of a single cluster node achieving 9.25 GByte/s of file system Read/Write throughput via fio&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In figure 2 above, you can see the standard file benchmark &lt;a href="https://fio.readthedocs.io/en/latest/fio_doc.html"&gt;fio&lt;/a&gt; was used with a 1 megabyte IO size to drive both read and write throughput from &lt;strong&gt;a single node&lt;/strong&gt;, using most of the 100 Gbps &lt;a href="https://aws.amazon.com/hpc/efa/"&gt;Elastic Fabric Adapter (EFA)&lt;/a&gt; throughput. Note that 9.25 Gigabyte/s was achieved with absolutely no tuning for optimal block size or network settings, to get close to line speed. The SC23 demonstration was not designed to show best-case benchmarked performance, but to demonstrate non-cached, sustained performance available without any tuning, using default settings. Stay tuned to this channel for more posts explaining how to optimize FSx for Lustre performance using popular Lustre benchmarking tools.&lt;/p&gt; 
&lt;p&gt;In the background of figure 2, you can see the &lt;a href="https://aws.amazon.com/cloudwatch/"&gt;AWS CloudWatch&lt;/a&gt; dashboard I created using a few clicks to monitor file system performance metrics. This chart shows only one node performing IO.&lt;/p&gt; 
&lt;p&gt;Now let’s move on to look at aggregate file system throughput as shown in figure 3.&lt;/p&gt; 
&lt;div id="attachment_3542" style="width: 814px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3542" loading="lazy" class="size-full wp-image-3542" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/16/CleanShot-2024-04-16-at-16.57.56.png" alt="Figure 3 – Screenshot of the aggregate 1TB/s of file system Read/Write throughput via fio" width="804" height="468"&gt;
 &lt;p id="caption-attachment-3542" class="wp-caption-text"&gt;Figure 3 – Screenshot of the aggregate 1TB/s of file system Read/Write throughput via fio&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 3 shows that our 120-node cluster achieved over 1 TByte/s of FSx for Lustre throughput, which is the maximum throughput we configured our file system for when we launched it. These are impressive results for a file system you can stand up or stand down &lt;em&gt;on-demand&lt;/em&gt;, and pay only as you go.&lt;/p&gt; 
&lt;p&gt;You don’t need to amortize your storage costs for years to justify running a project anymore – you can have 1 TB/s for just the hours or days you need high-speed file level access. During off-peak times, you can stand down your FSx for Lustre file system, and it remains possible to access the data at lower speed in the underlying repositories via the S3 object protocol. This allows you to stage, de-stage or browse your content, and run FSx for Lustre only when needed for application performance.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Developing and deploying HPC and AI workloads is complex and difficult enough without fighting the headwinds of equipment availability, power and cooling issues, datacenter build out, and vendor interoperability. By leveraging FSx for Lustre, you can enable your organization to cloud burst HPC or AI workloads to AWS, or easily migrate workloads to AWS.&lt;/p&gt; 
&lt;p&gt;But if your organization is just setting out on an HPC or AI development journey – you can instead start today and shave months or years off your project. If you want to stand on the shoulders of others you can use cluster and filesystem stacks created by experts who’ve all done this before. You can launch customizable templates from our &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;HPC Recipes Library&lt;/a&gt;, which have all been built to be interoperable, and simple to use. Reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt; if you have ideas for how we can make it easier for you.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Run simulations using multiple containers in a single AWS Batch job</title>
		<link>https://aws.amazon.com/blogs/hpc/run-simulations-using-multiple-containers-in-a-single-aws-batch-job/</link>
		
		<dc:creator><![CDATA[Matt Hansen]]></dc:creator>
		<pubDate>Mon, 22 Apr 2024 15:39:25 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">fe5f2de80116bdc4f1c348b4fd64c544ff490f56</guid>

					<description>Run simulations using multiple containers in a single AWS Batch job Matthew Hansen, Principal Solutions Architect, AWS Advanced Computing &amp;amp; Simulation Recently, AWS Batch launched a new feature that makes it possible to run multiple containers within a single job. This enables new scenarios customers have asked about like simulations for autonomous vehicles, multi-robot collaboration, […]</description>
										<content:encoded>&lt;p&gt;Run simulations using multiple containers in a single AWS Batch job&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Matthew Hansen, Principal Solutions Architect, AWS Advanced Computing &amp;amp; Simulation&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Recently, AWS Batch launched a new feature that makes it possible to run multiple containers within a single job. This enables new scenarios customers have asked about like simulations for autonomous vehicles, multi-robot collaboration, and other advanced simulations.&lt;/p&gt; 
&lt;p&gt;For autonomous system (AS) developers, this means you can keep your simulation and test scenario code in separate containers from the autonomy and sensor pipelines you want to test. This helps you test your modular system design (like in a software-defined vehicle) which has many components communicating over a local network.&lt;/p&gt; 
&lt;p&gt;Prior to this launch, you could run these types of simulations in Batch, but: you were limited to a single container per job. For some users that meant creating a very large container (tens of gigabytes) containing all the code for the system, plus a high-fidelity simulator, &lt;em&gt;and&lt;/em&gt; all the test code to run the scenarios. This blows out container build-times and generates long download times. It’ also an unnecessary coupling of components of the autonomous system. We heard you, so today we can talk about multi-container support in AWS Batch to address these issues.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll look at some multi-container simulation use-cases, learn how AWS Partners have used this to run their simulators using Batch, and then show you how to quickly get started running all kinds of multi-container jobs in Batch.&lt;/p&gt; 
&lt;h2&gt;Complex simulations&lt;/h2&gt; 
&lt;h4&gt;Autonomous Vehicles&lt;/h4&gt; 
&lt;p&gt;Today’s autonomous vehicles are simulated for millions of miles before they’re tested in the real world. These vehicles often have multiple sensors, including lidar and cameras, feeding into perception pipelines which are detecting other vehicles, pedestrians, traffic signals, and everything else needed for safe driving. They also have one or more control algorithms that are navigating via GPS, operating the steering and brakes, monitoring heading, speed, fuel, distance travelled, and other systems to operate the vehicle.&lt;/p&gt; 
&lt;p&gt;Because of this, the number of required software components can be quite large. These components are usually developed by different engineering teams, then integrated for system testing. By enabling each team to build their entire stack or pipeline into a separate container, engineers can test against the other components more easily, without having to rebuild everything into one, massive, monolithic container. Figure 1 is an example of an autonomous vehicle simulation. There are five total containers, &amp;nbsp;one for the test scenario runner, one for the simulator, and three containers for the autonomous vehicle, representing the lidar processing, camera pipeline, and the autonomous control.&lt;/p&gt; 
&lt;div id="attachment_3577" style="width: 769px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3577" loading="lazy" class="size-full wp-image-3577" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.29.13.png" alt="Figure 1 - An example of an autonomous vehicle simulation. There’s a container for the test scenario runner and the simulator, and three containers for the autonomous vehicle, representing the lidar processing, camera pipeline and the autonomous control." width="759" height="441"&gt;
 &lt;p id="caption-attachment-3577" class="wp-caption-text"&gt;Figure 1 – An example of an autonomous vehicle simulation. There’s a container for the test scenario runner and the simulator, and three containers for the autonomous vehicle, representing the lidar processing, camera pipeline and the autonomous control.&lt;/p&gt;
&lt;/div&gt; 
&lt;h4&gt;Robotics&lt;/h4&gt; 
&lt;p&gt;Another use case needing multiple containers is robotics simulation. We’re using robots in growing numbers for logistics, healthcare, and other industries. Robots – like autonomous cars – can also have multiple sensors (lidars, cameras, odometry, inertial measurement units), and a control stack for moving, all of which can be containerized.&lt;/p&gt; 
&lt;p&gt;Often we want to simulate multiple robots in a single environment, like a warehouse. That means you can go from running a few containers, to running hundreds at once in a single simulation. In the multi-robot example diagram (Figure 2), you can see there are separate containers for the test scenario runner, the simulation app, and four robots, each with containers for lidar, vision processing and control.&lt;/p&gt; 
&lt;div id="attachment_3578" style="width: 817px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3578" loading="lazy" class="size-full wp-image-3578" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.29.40.png" alt="Figure 2 - A four robot simulation in an AWS Batch multi-node parallel job. On the main node (Node 1) there’s a container each for the test scenario runner and the simulator, and four nodes of three containers for each robot, representing the lidar processing, camera pipeline and the autonomous control." width="807" height="425"&gt;
 &lt;p id="caption-attachment-3578" class="wp-caption-text"&gt;Figure 2 – A four robot simulation in an AWS Batch multi-node parallel job. On the main node (Node 1) there’s a container each for the test scenario runner and the simulator, and four nodes of three containers for each robot, representing the lidar processing, camera pipeline and the autonomous control.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;These are just two examples of simulations that can be run in Batch with multi-container support.&lt;/p&gt; 
&lt;h2&gt;Partners love this&lt;/h2&gt; 
&lt;p&gt;A number of AWS partners have used Batch for some time to provide simulation software and services for the AV/ADAS and robotics communities. Over the next few months, we’ll have more posts in this channel which dive deeper into the work they’re all doing.&lt;/p&gt; 
&lt;h3&gt;IPG Automotive&lt;/h3&gt; 
&lt;div id="attachment_3579" style="width: 369px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-3579" loading="lazy" class="size-full wp-image-3579" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.30.32.png" alt="Figure 3 - IPG CarMaker is an autonomous car simulator" width="359" height="396"&gt;
 &lt;p id="caption-attachment-3579" class="wp-caption-text"&gt;Figure 3 – IPG CarMaker is an autonomous car simulator&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://ipg-automotive.com/en/"&gt;IPG Automotive&lt;/a&gt; is an AWS partner that’s been developing software for decades. They’re best known for their AV/ADAS simulators: CarMaker, TruckMaker and MotorcycleMaker. IPG has been partnering with the AWS Batch team to develop the multi-container feature for Batch to enable their customers to scale out these simulations on AWS.&lt;/p&gt; 
&lt;p&gt;“&lt;em&gt;Developing cutting-edge autonomous vehicles and ADAS technologies requires hundreds of thousands of hours of testing within simulated environments mirroring real-world driving scenarios&lt;/em&gt;,” according to David Howarth, who is the Director of Business Development at IPG Automotive North America. “&lt;em&gt;By using AWS Batch multi-container jobs for our simulations, IPG’s customers can now seamlessly separate their CarMaker simulator, the 3D virtual environment, and their sensor pipelines into different containers on AWS. This capability accelerates both DevOps and debugging processes, significantly enhancing overall efficiency.&lt;/em&gt;”&lt;/p&gt; 
&lt;h3&gt;Robotec.ai&lt;/h3&gt; 
&lt;div id="attachment_3580" style="width: 394px" class="wp-caption alignleft"&gt;
 &lt;img aria-describedby="caption-attachment-3580" loading="lazy" class="size-full wp-image-3580" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.31.08.png" alt="Figure 4 - Simulated autonomous mining operation in RoSi" width="384" height="355"&gt;
 &lt;p id="caption-attachment-3580" class="wp-caption-text"&gt;Figure 4 – Simulated autonomous mining operation in RoSi&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://robotec.ai/"&gt;Robotec.ai&lt;/a&gt; is a growing simulation software company that has developed their own simulator called RoSi. They are working with customers on AV/ADAS, robotics, and autonomous mining operations.&lt;/p&gt; 
&lt;p&gt;“&lt;em&gt;Our customers demand thorough and safe testing of autonomous vehicles within a simulated environment, for example one of our mining customers, Boliden&lt;/em&gt;,” said Michal Niezgoda, CEO of Robotec.ai. “&lt;em&gt;The AWS Batch multi-container jobs feature streamlines our simulations at scale and powers our RoSi simulator to meet customer needs.”&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;“By using AWS Batch multi-container jobs, we can easily execute a large number of operational scenarios for a mining site with just a few clicks,” &lt;/em&gt;said Peter Burman, Program Manager of Boliden.&lt;em&gt; “This new feature, as part of AWS Batch, integrates seamlessly with other AWS services, allowing us to use the scaling and scheduling capabilities of the cloud and optimize compute costs.”&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;MORAI.ai&lt;/h3&gt; 
&lt;p&gt;MORAI, a technology company from Korea, offers a digital twin simulation system that accelerates the development and testing of autonomous vehicles, urban air mobility (UAM), autonomous mobile robots (AMR), and maritime autonomous surface ships (MASS).&lt;/p&gt; 
&lt;div id="attachment_3581" style="width: 322px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-3581" loading="lazy" class="size-full wp-image-3581" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.31.38.png" alt="Figure 5 - MORAI Simulation" width="312" height="499"&gt;
 &lt;p id="caption-attachment-3581" class="wp-caption-text"&gt;Figure 5 – MORAI Simulation&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;“&lt;em&gt;Before AWS Batch multi-container jobs, building custom simulation infrastructures for each client took several months. With the multi-container jobs feature, we can get our customers up and running with their simulations in just a few days&lt;/em&gt;,” said Jun Hong, co-founder and head of the R&amp;amp;D center at MORAI. “&lt;em&gt;This is crucial as they conduct extensive testing and validation to ensure their autonomous systems, including AV/ADAS, robotics, and maritime applications, are prepared to safely manage any scenarios they might face in the real world. The new feature significantly streamlines job preparation and reduces our reliance on in-house tool development&lt;/em&gt;.”&lt;/p&gt; 
&lt;h2&gt;How it works&lt;/h2&gt; 
&lt;p&gt;Let’s talk about the basic workflow for running multi-container simulations in Batch. The diagram in Figure 3 shows a high-level overview of the workflow we’ll discuss.&lt;/p&gt; 
&lt;div id="attachment_3582" style="width: 776px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3582" loading="lazy" class="wp-image-3582 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.32.00.png" alt="Figure 6 - The AWS Batch simulation flow. A user creates a multi-container job definition, then submits a job to the job queue for a compute environment." width="766" height="357"&gt;
 &lt;p id="caption-attachment-3582" class="wp-caption-text"&gt;Figure 6 – The AWS Batch simulation flow. A user creates a multi-container job definition, then submits a job to the job queue for a compute environment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 6 – The AWS Batch simulation flow. A user creates a multi-container job definition, then submits a job to the job queue for a compute environment.&lt;/p&gt; 
&lt;p&gt;To run a simulation job in AWS Batch you need to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Build and push your containers into a repository, like Amazon ECR.&lt;/li&gt; 
 &lt;li&gt;Create the Batch resources including: 
  &lt;ol&gt; 
   &lt;li&gt;A compute environment (CE)&lt;/li&gt; 
   &lt;li&gt;A job queue (JQ)&lt;/li&gt; 
   &lt;li&gt;A multi-container job definition&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;Submit the job to the job queue&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Running an example simulation job using multiple containers&lt;/h2&gt; 
&lt;p&gt;Let’s use this framework to walk through a simple example that demonstrates how to start two containers in a Batch job so they can communicate with each other. One container is a &lt;em&gt;talker&lt;/em&gt; that publishes a message and the other is a &lt;em&gt;listener&lt;/em&gt; that will receive and echo the message. We’ll use &lt;a href="https://docs.ros.org/en/humble/index.html"&gt;ROS 2&lt;/a&gt;, a software library for autonomous vehicles and robotics, to communicate between the two containers. Figure 4 illustrates the simple communication. The &lt;em&gt;talker&lt;/em&gt; container will publish ‘hello’ on the ROS 2 topic named &lt;em&gt;/chatter&lt;/em&gt; and the &lt;em&gt;listener&lt;/em&gt; will echo the message.&lt;/p&gt; 
&lt;div id="attachment_3583" style="width: 580px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3583" loading="lazy" class="size-full wp-image-3583" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.32.28.png" alt="Figure 7 - An example Batch multi-container job with two containers—a talker and a listener." width="570" height="371"&gt;
 &lt;p id="caption-attachment-3583" class="wp-caption-text"&gt;Figure 7 – An example Batch multi-container job with two containers—a talker and a listener.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To run this example, we’ll follow the steps we outlined already to create the Batch job, using the console and the AWS Command-Line Interface (CLI). We’ll use a container image that already exists in Amazon ECR, so there’s no need to build or push the image.&lt;/p&gt; 
&lt;h3&gt;Step 1 – create a compute environment&lt;/h3&gt; 
&lt;p&gt;First, create an EC2 based compute environment from the AWS Batch console:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Log into your AWS Account, and navigate to the AWS Batch console&lt;/li&gt; 
 &lt;li&gt;In the navigation pane, choose &lt;strong&gt;Environments&lt;/strong&gt;, then &lt;strong&gt;Create &lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_3584" style="width: 863px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3584" loading="lazy" class="size-full wp-image-3584" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.33.01.png" alt="Figure 8 - Compute Environments console view" width="853" height="387"&gt;
 &lt;p id="caption-attachment-3584" class="wp-caption-text"&gt;Figure 8 – Compute Environments console view&lt;/p&gt;
&lt;/div&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Select &lt;strong&gt;Amazon&lt;/strong&gt; &lt;strong&gt;EC2&lt;/strong&gt; Orchestration Type, then&lt;strong&gt; Confirm&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Input a &lt;strong&gt;Name, &lt;/strong&gt;&lt;code&gt;CE_EC2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select an &lt;strong&gt;Instance Role &lt;/strong&gt;(if you don’t have an instance role follow &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/instance_IAM_role.html"&gt;the user guide&lt;/a&gt;, or choose &lt;strong&gt;Create new role&lt;/strong&gt;)&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;Next,&lt;/strong&gt; then &lt;strong&gt;Next&lt;/strong&gt; again, your default VPC and subnet information should be automatically added&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;Next&lt;/strong&gt;, then &lt;strong&gt;Create Compute Environment&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You’ve now created a compute environment. You can view it any time from the navigation pane under &lt;strong&gt;Compute Environments&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;div id="attachment_3585" style="width: 864px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3585" loading="lazy" class="size-full wp-image-3585" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.33.27.png" alt="Figure 9 - Compute Environment console with newly created environment" width="854" height="394"&gt;
 &lt;p id="caption-attachment-3585" class="wp-caption-text"&gt;Figure 9 – Compute Environment console with newly created environment&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Step 2 – create a job queue&lt;/h3&gt; 
&lt;p&gt;Next create a Job Queue for your Compute Environment from the AWS Batch console:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;On the navigation pane, choose &lt;strong&gt;Job Queues&lt;/strong&gt;, then &lt;strong&gt;Create &lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_3586" style="width: 862px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3586" loading="lazy" class="size-full wp-image-3586" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.33.45.png" alt="Figure 10 - Job queue console view" width="852" height="391"&gt;
 &lt;p id="caption-attachment-3586" class="wp-caption-text"&gt;Figure 10 – Job queue console view&lt;/p&gt;
&lt;/div&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Select &lt;strong&gt;Amazon EC2&lt;/strong&gt; Orchestration Type&lt;/li&gt; 
 &lt;li&gt;Input a &lt;strong&gt;Name&lt;/strong&gt;, &lt;code&gt;JQ_EC2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;From the &lt;strong&gt;Connected compute environments&lt;/strong&gt; drop down list, select the environment you created in step 1 (&lt;code&gt;CE_EC2&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;Create Job Queue&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You’ve now created a Job Queue to submit your job into. You can view it any time from the navigation pane under &lt;strong&gt;Job Queues.&lt;/strong&gt;&lt;/p&gt; 
&lt;div id="attachment_3587" style="width: 869px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3587" loading="lazy" class="size-full wp-image-3587" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.34.09.png" alt="Figure 11 - Job queue console with newly created queue" width="859" height="394"&gt;
 &lt;p id="caption-attachment-3587" class="wp-caption-text"&gt;Figure 11 – Job queue console with newly created queue&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Step 3 – create a job definition&lt;/h3&gt; 
&lt;p&gt;For this step, we’ll use the AWS CLI to create the Job Definition for our job using &lt;em&gt;JSON input&lt;/em&gt;. For instructions on setting up the AWS CLI, see the &lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html"&gt;Getting Started&lt;/a&gt; guide. You’ll need to update to the latest CLI version to get the new Batch multi-container functionality.&lt;/p&gt; 
&lt;p&gt;Create a new file called &lt;code&gt;ros2-talker-listener.json&lt;/code&gt; and copy the following JSON into that file.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;{
  "jobDefinitionName": "ros2-talker-listener",
  "type": "container",
  "platformCapabilities": [
    "EC2"
  ],
  "timeout": { 
      "attemptDurationSeconds": 60
  },
  "ecsProperties": {
    "taskProperties": [
      {
        "ipcMode": "task",
        "containers": [
          {
            "essential": true,
            "command": ["/bin/bash", "-c", "ros2 topic pub -t 10 /chatter std_msgs/msg/String '{data: hello}'"],
            "image": "public.ecr.aws/docker/library/ros:humble",
            "name": "talker",
            "resourceRequirements": [
              {
                "type": "VCPU",
                "value": "1"
              },
              {
                "type": "MEMORY",
                "value": "2048"
              }
            ]
          },
          {
            "essential": true,
            "command": ["/bin/bash", "-c", "ros2 topic echo /chatter"],
            "image": "public.ecr.aws/docker/library/ros:humble",
            "name": "listener",
            "dependsOn": [
            {
              "condition": "START",
              "containerName": "talker"
            }
            ],
            "resourceRequirements": [
              {
                "type": "VCPU",
                "value": "1"
              },
              {
                "type": "MEMORY",
                "value": "2048"
              }
            ]
          }
        ]
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The JSON job definition above includes a new ecsProperties object that defines the two containers we’re running, the &lt;em&gt;talker&lt;/em&gt; and &lt;em&gt;listener&lt;/em&gt; containers. For each container, we’ll pull the image ros:humble from public ECR, and run the command inside each container. This example also requires us to set ipcMode to task so that the containers can communicate, and creates a dependency that the &lt;em&gt;listener&lt;/em&gt; container dependsOn the &lt;em&gt;talker&lt;/em&gt; getting to the START condition. For more details see the &lt;a href="https://docs.aws.amazon.com/batch/latest/APIReference/API_SubmitJob.html"&gt;AWS Batch SubmitJob API documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To create the job definition, we’ll use the AWS CLI again:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws batch register-job-definition --cli-input-json file://ros2-talker-listener.json&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It’s possible you may see an error that says:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;Unknown parameter in input: "ecsProperties", must be one of: jobDefinitionName, type, parameters, schedulingPriority, containerProperties, nodeProperties, retryStrategy, propagateTags, timeout, tags, platformCapabilities, eksProperties&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This means your AWS CLI version &lt;em&gt;doesn’t recognize the new parameters to the API&lt;/em&gt;. You’re probably using a version you installed before Batch had this feature, so you need to update it. For instructions on updating the AWS CLI, see the &lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html"&gt;Getting Started&lt;/a&gt; guide.&lt;/p&gt; 
&lt;p&gt;You can now view your job-definition from the Batch console under &lt;strong&gt;Job definitions:&lt;/strong&gt;&lt;/p&gt; 
&lt;div id="attachment_3588" style="width: 864px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3588" loading="lazy" class="size-full wp-image-3588" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.34.57.png" alt="Figure 12 - Job definitions console view" width="854" height="389"&gt;
 &lt;p id="caption-attachment-3588" class="wp-caption-text"&gt;Figure 12 – Job definitions console view&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Step 4 – submit your job&lt;/h3&gt; 
&lt;p&gt;Now that you’ve created the job definition, you can submit the job using the AWS CLI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws batch submit-job --job-queue JQ_EC2 --job-name ros2-talker-listener --job-definition ros2-talker-listener&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 5 – view results&lt;/h3&gt; 
&lt;p&gt;Now, return to the AWS Batch console and select &lt;strong&gt;Jobs&lt;/strong&gt; from the left menu. Then choose your job queue (JQ_EC2) from the drop-down menu. You should see your job listed. Click on the job name to view the job details.&lt;/p&gt; 
&lt;div id="attachment_3589" style="width: 867px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3589" loading="lazy" class="size-full wp-image-3589" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.35.21.png" alt="Figure 13 - Job details console view" width="857" height="389"&gt;
 &lt;p id="caption-attachment-3589" class="wp-caption-text"&gt;Figure 13 – Job details console view&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;After a few minutes, you should see that the job succeeded. You can now view the logs to see what happened.&lt;/p&gt; 
&lt;p&gt;On the &lt;strong&gt;Job attempts&lt;/strong&gt; tab, select the Log stream name to view the log events in AWS CloudWatch.&lt;/p&gt; 
&lt;div id="attachment_3590" style="width: 865px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3590" loading="lazy" class="size-full wp-image-3590" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.35.36.png" alt="Figure 14 - CloudWatch logs console view of talker output" width="855" height="389"&gt;
 &lt;p id="caption-attachment-3590" class="wp-caption-text"&gt;Figure 14 – CloudWatch logs console view of talker output&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;You should see the logs for the &lt;em&gt;talker&lt;/em&gt; looping 10 times publishing ‘hello’.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;Waiting for at least 1 matching subscription(s)...
publisher: beginning loop                         
publishing #1: std_msgs.msg.String(data='hello')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Similarly, from the Batch Job details view, select the log name for the listener node, and you should see the log of the &lt;em&gt;listener&lt;/em&gt; echoing the message 10 times.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;data: hello
---
data: hello
---
data: hello
---
data: hello
---
data: hello
---
data: hello
---
data: hello
---
data: hello
---
data: hello
---
data: hello
---
&lt;/code&gt;&lt;/pre&gt; 
&lt;div id="attachment_3591" style="width: 877px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3591" loading="lazy" class="size-full wp-image-3591" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.36.00.png" alt="Figure 15 - CloudWatch logs console view of listener output" width="867" height="391"&gt;
 &lt;p id="caption-attachment-3591" class="wp-caption-text"&gt;Figure 15 – CloudWatch logs console view of listener output&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;And that’s it for running and viewing the &lt;em&gt;talker – listener&lt;/em&gt; example. This example demonstrated how you can start two containers in an AWS Batch job and they can communicate with each other.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we discussed some multi-container simulation use cases, learned how some AWS Partners &amp;nbsp;are using it to run their simulators on AWS Batch, and learned how to quickly get started running multi-container jobs in Batch using a simple example.&lt;/p&gt; 
&lt;p&gt;For more help using AWS Batch for simulations, follow the User Guide &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/Batch_GetStarted.html"&gt;Getting Started with AWS Batch&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Linter rules for Nextflow to improve the detection of errors before runtime</title>
		<link>https://aws.amazon.com/blogs/hpc/linter-rules-for-nextflow-to-improve-the-detection-of-errors-before-runtime/</link>
		
		<dc:creator><![CDATA[Mark Schreiber]]></dc:creator>
		<pubDate>Tue, 16 Apr 2024 14:58:36 +0000</pubDate>
				<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Life Sciences]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Genomics]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<guid isPermaLink="false">918a6c3069ff95919afafe3727710e60191c475d</guid>

					<description>Check out this post to learn how linter rules for Nextflow's DSL can help you catch errors in your workflows before runtime, which means greater developer productivity, which leads directly to a faster time to science.</description>
										<content:encoded>&lt;p&gt;&lt;a href="https://www.nextflow.io/"&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3518" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/Linter-rules-for-Nextflow-to-improve-the-detection-of-errors-before-runtime-1.png" alt="Linter rules for Nextflow to improve the detection of errors before runtime" width="380" height="212"&gt;Nextflow&lt;/a&gt; is a popular domain-specific language (DSL) and runtime used to define workflows that string together multiple processing steps into a pipeline. This allows it to perform quite complex genomics or scientific analyses including for machine-learning.&lt;/p&gt; 
&lt;p&gt;Workflows defined in Nextflow code can leverage container orchestration technologies to deploy containerized workloads across clusters, clouds, or HPC environments. As an interpreted language, errors in a Nextflow script are only revealed at runtime. This increases the time and cost of developing and debugging a workflow which could be reduced if errors could be spotted earlier.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll discuss how we created &lt;strong&gt;linter rules&lt;/strong&gt; for Nextflow DSL 2, how these rules can be extended and how you can use them to check your scripts before runtime.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;Nextflow is commonly used by customers on AWS. Some use the &lt;a href="https://aws.amazon.com/healthomics/"&gt;AWS HealthOmics&lt;/a&gt; managed workflow service and others choose to deploy their own Nextflow engine, integrated with AWS Batch. The HealthOmics service provides a fully-managed experience while &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch &lt;/a&gt;provides a more generalized mechanism for running batch computing tasks.&lt;/p&gt; 
&lt;p&gt;A key difference between the Nextflow DSL and a traditional programming language is that Nextflow code is interpreted at runtime rather than compiled. This provides flexibility, because workflows can be developed quickly without a build step. But it also means that you might not detect errors in the Nextflow script until the interpreter lands on that specific part of the code during an actual workflow run.&lt;/p&gt; 
&lt;p&gt;Because genomics workflows often involve processing large volumes of data, a workflow run could run for hours (and hours) before failing on a simple coding mistake. Having to restart these long-running analyses is frustrating and could be costly. This runtime-evaluation model argues for a static analysis tool that can scan Nextflow code &lt;em&gt;before&lt;/em&gt; workflows are run to detect issues early – known as a linter.&lt;/p&gt; 
&lt;h2&gt;Introducing a linter for Nextflow&lt;/h2&gt; 
&lt;p&gt;To address the need for static analysis, we’ve developed a static linter that can analyze Nextflow workflow code by taking advantage of the fact that the Nextflow DSL uses &lt;a href="https://groovy-lang.org/"&gt;Groovy&lt;/a&gt;, a dynamic language for the Java Virtual Machine, as its underlying implementation language. By building on Groovy, we can parse Nextflow code and analyze it, syntactically and semantically, without needing to execute the workflow itself.&lt;/p&gt; 
&lt;p&gt;We built our linter using &lt;a href="https://codenarc.org/"&gt;CodeNarc&lt;/a&gt;, an open-source static analysis tool used to enforce Groovy coding standards and best practices. Under the covers, CodeNarc uses Groovy’s own parser to generate an abstract syntax tree (AST) representing the structure of the code. It then provides a framework for coding semantic rules that can traverse this AST using the &lt;a href="https://en.wikipedia.org/wiki/Visitor_pattern"&gt;visitor pattern&lt;/a&gt; and check for rule violations. We can take advantage of this capability to analyze Nextflow scripts for potential issues.&lt;/p&gt; 
&lt;h2&gt;Understanding the Nextflow AST&lt;/h2&gt; 
&lt;p&gt;An abstract syntax tree is a hierarchical data structure that models code at increasing levels of granularity. The root node represents the entire script or program. Child nodes represent top-level statements and expressions with their children representing sub-expressions. By traversing this tree, and examining nodes with semantic meaning, rules can gather the information we need to detect problems.&lt;/p&gt; 
&lt;p&gt;We built a simple application called AST Echo to determine which AST node types need to be visited to evaluate different Nextflow language elements. Our app parses Nextflow code and prints out the AST hierarchy along with the Groovy node-type associated with each Nextflow construct.&lt;/p&gt; 
&lt;p&gt;Using this approach, we were able to discover that a Nextflow process declaration is a Groovy &lt;code&gt;MethodCallExpression&lt;/code&gt; and that a Nextflow process block maps to a Groovy &lt;code&gt;ClosureExpression&lt;/code&gt; containing a &lt;code&gt;BlockExpression&lt;/code&gt;. Here’s a partial example of a Nextflow DSL script. We’ve made the &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/blob/main/examples/example.nf"&gt;full source available on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-clike"&gt;nextflow.enable.dsl=2

process foo {
    container 'ubuntu:latest'
    cpus 1

    output:
    path 'foo.txt'

    script:
    """
    your_command &amp;gt; foo.txt
    """
}

// other processess …

workflow {
    data = channel.fromPath('/some/path/*.txt')
    foo()
    // further process calls …
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Providing this as input to our AST Echo app will produce something like this.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;(∅:∅)-(∅:∅) &amp;lt;BlockStatement&amp;gt;:  { (nextflow.enable.dsl = 2); this.process(this.foo({ -&amp;gt; ... })); this.process(this.bar({ -&amp;gt; ... })); this.workflow({ -&amp;gt; ... }) }
  -&amp;gt; (1:1)-(1:22) &amp;lt;ExpressionStatement&amp;gt;:  (nextflow.enable.dsl = 2)
    -&amp;gt; (1:1)-(1:22) &amp;lt;BinaryExpression&amp;gt;: (nextflow.enable.dsl = 2)
      -&amp;gt; (1:1)-(1:20) &amp;lt;PropertyExpression&amp;gt;: nextflow.enable.dsl
        -&amp;gt; (1:9)-(1:16) &amp;lt;PropertyExpression&amp;gt;: nextflow.enable
          -&amp;gt; (1:1)-(1:9) &amp;lt;VariableExpression&amp;gt;: nextflow
          -&amp;gt; (1:10)-(1:16) &amp;lt;ConstantExpression&amp;gt;: enable
        -&amp;gt; (1:17)-(1:20) &amp;lt;ConstantExpression&amp;gt;: dsl
      -&amp;gt; (1:21)-(1:22) &amp;lt;ConstantExpression&amp;gt;: 2
  -&amp;gt; (3:1)-(14:2) &amp;lt;ExpressionStatement&amp;gt;:  this.process(this.foo({ -&amp;gt; ... }))
    -&amp;gt; (3:1)-(14:2) &amp;lt;MethodCallExpression&amp;gt;: this.process(this.foo({ -&amp;gt; ... }))
      -&amp;gt; (3:1)-(3:1) &amp;lt;VariableExpression&amp;gt;: this
      -&amp;gt; (3:1)-(3:8) &amp;lt;ConstantExpression&amp;gt;: process
      -&amp;gt; (3:9)-(14:2) &amp;lt;ArgumentListExpression&amp;gt;: (this.foo({ -&amp;gt; ... }))
        -&amp;gt; (3:9)-(14:2) &amp;lt;MethodCallExpression&amp;gt;: this.foo({ -&amp;gt; ... })
          -&amp;gt; (3:9)-(3:9) &amp;lt;VariableExpression&amp;gt;: this
          -&amp;gt; (3:9)-(3:12) &amp;lt;ConstantExpression&amp;gt;: foo
          -&amp;gt; (3:13)-(14:2) &amp;lt;ArgumentListExpression&amp;gt;: ({ -&amp;gt; ... })
            -&amp;gt; (3:13)-(14:2) &amp;lt;ClosureExpression&amp;gt;: { -&amp;gt; ... }
              -&amp;gt; (4:5)-(14:1) &amp;lt;BlockStatement&amp;gt;:  { this.container(ubuntu:latest); this.cpus(1); this.path(foo.txt); 
    your_command &amp;gt; foo.txt
     }
                -&amp;gt; (4:5)-(4:30) &amp;lt;ExpressionStatement&amp;gt;:  this.container(ubuntu:latest)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The root of the AST is on the first line. Each subsequent line is a node of the tree indented by the depth of the node.&lt;/p&gt; 
&lt;p&gt;The start and stop locations of the code contained in the node are displayed first with their line number and character offset separated by ‘:’. The start and stop are separated by ‘-’. The type of Groovy expression or statement is displayed surrounded by angle brackets. Following this is the code fragment, possibly truncated, contained by the node as interpreted by the Groovy parser.&lt;/p&gt; 
&lt;h2&gt;Writing linter rules&lt;/h2&gt; 
&lt;p&gt;Armed with these mappings we can write CodeNarc rules targeting the relevant AST node types.&lt;/p&gt; 
&lt;p&gt;CodeNarc rules define a set of methods matching AST node types which will be called by CodeNarc when the corresponding nodes are visited during AST traversal. In the body of these visit methods, we can write logic to gather information and detect rule violations. Any issues can be reported by adding them to CodeNarc’s violation list.&lt;/p&gt; 
&lt;p&gt;For example, we can write a rule that checks if CPU or memory resource requests for processes fall within valid ranges. The rule would override CodeNarcs’s &lt;code&gt;AbstractAstVisitor&lt;/code&gt; &lt;code&gt;visitMethodCallExpression&lt;/code&gt; method which represents a method call in Groovy. It would check if the method call is requesting CPU resources and then evaluate if the arguments are valid.&lt;/p&gt; 
&lt;p&gt;Let’s look at part of a Groovy implementation of a rule to check the &lt;code&gt;cpu&lt;/code&gt; directive by overriding the &lt;code&gt;visitMethodCallExpression&lt;/code&gt; :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-clike"&gt;class CpuAstVisitor extends AbstractAstVisitor {
    def MIN_CPU = 2
    def MAX_CPU = 96

    @Override
    void visitMethodCallExpression(MethodCallExpression expression) {
        if(expression.getMethodAsString() == 'cpus'){
            checkOneArgument(expression)
        }

        super.visitMethodCallExpression(expression)
    }


    private checkOneArgument(final MethodCallExpression expression){
        def methodArguments = AstUtil.getMethodArguments(expression)
        if (methodArguments.size() == 0) {
            addViolation(expression, 'the cpus directive must have one argument')
            return new EmptyExpression()
        } else if (methodArguments.size() &amp;gt; 1) {
            addViolation(expression, 'the cpus directive must have only one argument')
        }

        if( methodArguments.first() instanceof ConstantExpression){
           checkNumeric((ConstantExpression)methodArguments.first())
        }
    }

    private checkNumeric(ConstantExpression expression){
        try {
            def val = Integer.parseInt(expression.value.toString())
            checkMinMax(expression, val)
        } catch (NumberFormatException ignored){
            addViolation(expression,
                    "'${expression.value}' is not a valid number.")
        }

    }
    private void checkMinMax(Expression exp, final int val) {
        if (val &amp;lt; MIN_CPU) {
            addViolation(exp,
                    "The minimum CPU count is '$MIN_CPU'.")
        } else if (val &amp;gt; MAX_CPU) {
            addViolation(exp,
                    "The maximum CPU count is '$MAX_CPU'.")
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;During the check, we validate several conditions. Violations are recorded by calling the &lt;code&gt;addViolation&lt;/code&gt; method. At the end of the checks the &lt;code&gt;visitMethodCallExpression&lt;/code&gt; of the super-class is invoked to continue the traversal of the AST. We’ve put the &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/blob/main/linter-rules/src/main/groovy/software/amazon/nextflow/rules/healthomics/CpuRule.groovy"&gt;full source on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;It’s critical to rigorously test rules to ensure they work as expected. CodeNarc provides a convenient rule-testing harness. It allows us to define test Nextflow code snippets along with expected rule violations. CodeNarc then runs analysis on these code fragments and compares the actual violations it finds to the expected results, ensuring that all &lt;em&gt;expected&lt;/em&gt; violations and no &lt;em&gt;unexpected&lt;/em&gt; violations are detected.&lt;/p&gt; 
&lt;p&gt;For example, we can test that the rule in from our previous code list is violated when the &lt;code&gt;cpus&lt;/code&gt; value is 97 but not violated when the value is 2.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-clike"&gt;@Test
void cpuRule_MaxViolation(){
    final SOURCE =
'''
process MY_PROCESS {
 cpus 97
}
'''
    assertSingleViolation(SOURCE, 3, 'cpus 97', “The maximum CPU count is ‘96’”)
    }
@Test
void cpuRule_NoViolationsMin(){
    final SOURCE =
'''
process MY_PROCESS {
 cpus 2
}
'''
    assertNoViolations(SOURCE)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This checks that the code defined by &lt;code&gt;SOURCE&lt;/code&gt; and asserts that a single violation occurs when &lt;code&gt;cpus&lt;/code&gt; is set to 97 and that no violations occur when cpus are set to 2. We’ve put the &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/blob/main/linter-rules/src/test/groovy/software/amazon/nextflow/rules/healthomics/CpuRuleTest.groovy"&gt;full source of this test on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Running the Linter Rules&lt;/h2&gt; 
&lt;p&gt;To run the linter rules you build the package and then run from the command line with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;java  -Dorg.slf4j.simpleLogger.defaultLogLevel=error \
  -classpath ./linter-rules/build/libs/linter-rules-0.1.jar:CodeNarc-3.3.0-all.jar:slf4j-api-1.7.36.jar:slf4j-simple-1.7.36.jar \
  org.codenarc.CodeNarc \
  -report=text:stdout \
  -rulesetfiles=rulesets/healthomics.xml \
  -includes=**/**.nf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;-includes&lt;/code&gt; argument will inspect all filenames matching the &lt;code&gt;*.nf&lt;/code&gt; pattern in the current working directory, &lt;em&gt;and&lt;/em&gt; any sub-directory. To adjust the rules to be run, you adjust &lt;code&gt;-rulesetfiles&lt;/code&gt; to specify either a custom ruleset in CodeNarc format, or a prebuilt set like &lt;code&gt;rulesets/general.xml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To simplify this, we’ve provided a &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/blob/main/Dockerfile"&gt;Dockerfile&lt;/a&gt; that includes all required dependencies with a convenient entry point along with &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/tree/main?tab=readme-ov-file#docker"&gt;build and usage instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Using this mechanism, the command to run with Docker becomes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;docker run -v $PWD:/data -e ruleset=healthomics linter-rules-for-nextflow&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find a prebuilt image in the &lt;a href="https://gallery.ecr.aws/aws-genomics/linter-rules-for-nextflow"&gt;Amazon ECR Public Gallery&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Detecting runtime compatibility issues&lt;/h2&gt; 
&lt;p&gt;Nextflow provides portability across environments, but workflows can still be written in ways that tie them to certain runtime capabilities.&lt;/p&gt; 
&lt;p&gt;We’ve developed rules that can detect the use of Nextflow features that may not be supported in AWS HealthOmics. For example, AWS HealthOmics ignores certain process directives that are only relevant to other runtime environments. Our rules warn when these directives are used so workflows designed for HealthOmics avoid incompatibility issues.&lt;/p&gt; 
&lt;p&gt;The AWS HealthOmics workflow service provides a managed environment optimized for security, scalability, and cost efficiency. Workflows must adhere to certain constraints around storage, containers, and infrastructure. We’ve built rules that check Nextflow code for patterns that violate HealthOmics environment policies.&lt;/p&gt; 
&lt;p&gt;For example, one rule checks that pipeline inputs are loaded from Amazon Simple Storage Service (Amazon S3) buckets, or HealthOmics Sequence Stores, and that pipeline container images are being pulled from Amazon ECR Private repositories.&lt;/p&gt; 
&lt;p&gt;Other rules check that resource requests don’t exceed HealthOmics instance type capabilities. Configuring these as linter rules allows workflows to be pre-validated rather than discovering issues mid-execution.&lt;/p&gt; 
&lt;p&gt;In addition to AWS HealthOmics specific checks, we’ve also &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/tree/main/linter-rules/src/main/groovy/software/amazon/nextflow/rules"&gt;created some general rules&lt;/a&gt; that detect invalid syntax, potential portability issues, and other common anti-patterns. For example, &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/blob/main/linter-rules/src/main/groovy/software/amazon/nextflow/rules/AllowedDirectivesRule.groovy"&gt;a rule that detects&lt;/a&gt; the use of undefined process directives – this often indicates a typo or misunderstanding of the DSL.&lt;/p&gt; 
&lt;h2&gt;Flexible, extensible, and open-source&lt;/h2&gt; 
&lt;p&gt;The linter lets you &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow?tab=readme-ov-file#general-nf-rules-only"&gt;include only rule sets you’re interested in&lt;/a&gt;. You can focus strictly on general Nextflow rules or both general rules and HealthOmics best practices. Over time, we expect the rules library will grow to cover more scenarios.&lt;/p&gt; 
&lt;p&gt;We architected our linter so that rules can easily be contributed by the Nextflow community.&lt;/p&gt; 
&lt;p&gt;We’ve released the linter rules and the AST Echo utility as &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow"&gt;open-source projects on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The code we’ve provided there uses the Apache 2.0 license. We welcome issues, pull requests, and additional rules from the Nextflow community.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;By providing early feedback on Nextflow code, this linter can improve developer productivity, reduce errors, and make workflows more portable. We encourage all Nextflow pipeline authors to integrate it into their continuous integration pipelines and contribute to its evolution.&lt;/p&gt; 
&lt;p&gt;Static analysis of domain-specific languages like Nextflow opens up new possibilities for accelerating scientific advancements through code quality and collaboration.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Intel Open Omics Acceleration Framework on AWS: fast, cost-efficient, and seamless</title>
		<link>https://aws.amazon.com/blogs/hpc/intel-open-omics-acceleration-framework-on-aws-fast-cost-efficient-and-seamless/</link>
		
		<dc:creator><![CDATA[Olivia Choudhury]]></dc:creator>
		<pubDate>Tue, 09 Apr 2024 13:50:53 +0000</pubDate>
				<category><![CDATA[Healthcare]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Genomics]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<guid isPermaLink="false">1b05a2f936ae2290795be4f7bdad69e08725240e</guid>

					<description>With genomics and multi-omics research generating more data than ever, the Open Omics Acceleration Framework from Intel Labs aims to provide a highly productive platform for researchers. Check out recent results in this new blog post.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Olivia Choudhury, PhD and Aniket Deshpande from AWS; Sanchit Misra, PhD, Vasimuddin Md., PhD,&amp;nbsp;Narendra Chaudhary, PhD, Saurabh Kalikar, PhD, and Manasi Tiwari, PhD, Research Scientists at Intel Labs India; Ashish Kumar Patel, contingent worker with Intel Technology India Pvt. Ltd.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;We are living in the exciting times of the rapidly growing field of omics, including genomics, proteomics, transcriptomics, and metabolomics data. Our ability to measure omics data is increasing at a dramatic pace and new data science (AI and data management) pipelines are being developed and quickly standardized. Cloud plays a key role in this initiative by providing massive compute and storage to enable public data repositories, large collaborative projects, and consortia.&lt;/p&gt; 
&lt;p&gt;To drive and realize the promise of omics, Intel Labs is building the Open Omics Acceleration Framework as a highly productive platform for biologists and data scientists, enabling them to harness computing and data at unprecedented scale and speed at lower costs.&lt;/p&gt; 
&lt;p&gt;In this post, we benchmark three standard omics pipelines – AlphaFold2-based protein folding, DeepVariant-based variant calling, and Scanpy-based single-cell RNA-Seq analysis – available in the latest version of Open Omics Acceleration Framework (v. 2.1) against prior CPU baseline on Xeon-based Amazon Elastic Compute Cloud (Amazon EC2) instances to showcase its performance and cost benefits.&lt;/p&gt; 
&lt;h2&gt;Open Omics Acceleration Framework&lt;/h2&gt; 
&lt;p&gt;The Open Omics Acceleration Framework is a one-click, containerized, customizable, open-sourced framework for accelerating analysis of omics datasets. The framework is being built with a modular design that keeps in mind the different ways the users would want to interact with it. As shown in Figure 1, it consists of three layers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pipeline layer&lt;/strong&gt;: for users who are looking for a one-click solution to run standard pipelines. The latest version (v. 2.1) supports the following pipelines: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/tree/main/pipelines/fq2sortedbam"&gt;&lt;strong&gt;fq2sortedbam&lt;/strong&gt;&lt;/a&gt;: Given gzipped fastq files from a sample, this workflow performs bwa-mem2-based sequence mapping and sorting to output the sorted BAM file.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/tree/main/pipelines/deepvariant-based-germline-variant-calling-fq2vcf"&gt;&lt;strong&gt;DeepVariant-based germline pipeline for variant calling (fq2vcf)&lt;/strong&gt;&lt;/a&gt;: Given paired end gzipped fastq files from a sample, this workflow performs sequence mapping, sorting and variant calling (using DeepVariant) to output a vcf file.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/tree/main/pipelines/alphafold2-based-protein-folding"&gt;&lt;strong&gt;AlphaFold2-based protein folding&lt;/strong&gt;&lt;/a&gt;: Given one or more protein sequences, this workflow performs preprocessing (database search and multiple sequence alignment) and structure prediction (using AlphaFold2) to output the structure(s) of the protein sequences.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/tree/main/pipelines/single-cell-RNA-seq-analysis"&gt;&lt;strong&gt;Single-cell RNA-Seq analysis&lt;/strong&gt;&lt;/a&gt;: Given a cell by gene matrix, this&amp;nbsp;Scanpy workflow performs data load and preprocessing (filter, linear regression and normalization), dimensionality reduction (PCA), clustering (Louvain / Leiden / kmeans) to cluster the cells into different cell types and visualize those clusters (UMAP / t-SNE).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toolkit (applications) layer&lt;/strong&gt;: for users who want to use individual tools or to create their own custom pipelines by combining various tools.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Building blocks layer&lt;/strong&gt;: for tool developers, this layer consists of key building blocks – biology-specific and generic-AI algorithms and data structures – that can replace ones used in existing tools to accelerate them or can be used as ingredients to build new efficient tools.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div id="attachment_3467" style="width: 1966px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3467" loading="lazy" class="size-full wp-image-3467" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.14.22@2x.png" alt="Figure 1: Open Omics Acceleration Framework v. 2.1. This modular architecture consists of 3 layers: pipeline, toolkit (application), and building blocks layers. " width="1956" height="874"&gt;
 &lt;p id="caption-attachment-3467" class="wp-caption-text"&gt;Figure 1: Open Omics Acceleration Framework v. 2.1. This modular architecture consists of 3 layers: pipeline, toolkit (application), and building blocks layers.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Benchmarking the Open Omics Acceleration Framework on Amazon EC2 instances&lt;/h2&gt; 
&lt;h3&gt;Amazon EC2 instances used for benchmarking&lt;/h3&gt; 
&lt;p&gt;The five types of Amazon EC2 instances used in this benchmarking study are detailed in Table 1. All of them are powered by 4th Generation Intel® Xeon® Scalable processors. We executed all the experiments using Ubuntu version 22.04. Across the three pipelines, the execution time includes all the steps performed including reading/writing input/output files from/to local disk and therefore, includes both compute and file IO. The cloud costs are computed by multiplying the total execution time in hours by the per hour instance cost.&lt;/p&gt; 
&lt;div id="attachment_3468" style="width: 1542px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3468" loading="lazy" class="size-full wp-image-3468" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.16.20@2x.png" alt="Table 1: Details of the Amazon EC2 instance types used for benchmarking. On-Demand pricing is from the published date for the us-east-2 (Ohio) region and is subject to change over time. Please consult the Amazon EC2 pricing page for current pricing in your region." width="1532" height="696"&gt;
 &lt;p id="caption-attachment-3468" class="wp-caption-text"&gt;Table 1: Details of the Amazon EC2 instance types used for benchmarking. On-Demand pricing is from the published date for the us-east-2 (Ohio) region and is subject to change over time. Please consult the Amazon EC2 pricing page for current pricing in your region.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;To run these pipelines, you need an AWS account with permissions to provision Amazon Simple Storage Service (S3) buckets for input and output data storage, and sufficient permissions/limits to provision Amazon EC2 c7i, m7i and r7i instances.&lt;/p&gt; 
&lt;h2&gt;Steps for benchmarking&lt;/h2&gt; 
&lt;p&gt;The configuration details and steps used for benchmarking baseline and Open Omics versions of all three pipelines on Amazon EC2 instances are detailed on this &lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/blob/main/benchmarking/AWS-Intel-blog-v2.1-2024/README.md"&gt;GitHub page&lt;/a&gt; of the Open Omics Acceleration Framework. The typical process involves launching the corresponding EC2 Instances, connecting to the instances, installing the software, downloading the datasets, and executing the baseline and Open Omics versions. In the following subsections, we will share an overview of the pipelines and report the benchmarking results.&lt;/p&gt; 
&lt;h2&gt;AlphaFold2-based protein folding pipeline&lt;/h2&gt; 
&lt;p&gt;The protein folding problem has significant implications for drug discovery, biotechnology, and understanding the mechanisms of diseases. The task entails predicting the 3D structure of a protein from its amino acid sequence. A protein’s structure governs its function. Therefore, accurate protein structure prediction is vital in biology and drug discovery, and has long been considered a holy grail problem.&lt;/p&gt; 
&lt;div id="attachment_3469" style="width: 1924px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3469" loading="lazy" class="size-full wp-image-3469" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.18.31@2x.png" alt="Figure 2: The AlphaFold2-based protein structure prediction pipeline that requires an input amino acid sequence. The pipeline has two stages, namely database search for multiple sequence alignment and Evoformer model for structure prediction." width="1914" height="256"&gt;
 &lt;p id="caption-attachment-3469" class="wp-caption-text"&gt;Figure 2: The AlphaFold2-based protein structure prediction pipeline that requires an input amino acid sequence. The pipeline has two stages, namely database search for multiple sequence alignment and Evoformer model for structure prediction.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Our AlphaFold2-based protein folding pipeline takes as input a set of amino acid sequences and outputs the set of corresponding predicted structures in PDB file format. The pipeline has two stages: 1) preprocessing that includes database search and multiple sequence alignment (MSA) over protein sequences, 2) model inference&amp;nbsp;that predict the structure of the protein using Evoformer, a Transformer architecture based Deep Learning (DL) model.&lt;/p&gt; 
&lt;p&gt;For baseline, we use OpenFold v. 1.0.1 – a faithful reproduction of &lt;a href="https://github.com/google-deepmind/alphafold"&gt;DeepMind AlphaFold2’s model&lt;/a&gt; for model inference and &lt;a href="https://github.com/soedinglab/hh-suite"&gt;hh-suite&lt;/a&gt; v. 3.3.0 and &lt;a href="https://github.com/EddyRivasLab/hmmer"&gt;hmmer&lt;/a&gt; v. 3.3.2 for preprocessing. Open Omics Acceleration Framework contains faster versions of all the steps of this pipeline accelerated using a 4th gen Intel Xeon scalable processor using Intel AMX with bfloat16 precision for DL compute, Intel AVX2 and AVX-512 for non-DL compute and cache optimizations. It also provides a parallel execution framework that folds multiple proteins in parallel in a load balanced fashion.&lt;/p&gt; 
&lt;p&gt;&lt;img loading="lazy" class="aligncenter size-full wp-image-3470" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.20.06@2x.png" alt="" width="1760" height="1210"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="aligncenter size-full wp-image-3471" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.20.14@2x.png" alt="" width="1760" height="1216"&gt;&lt;img loading="lazy" class="aligncenter size-full wp-image-3472" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.20.19@2x.png" alt="" width="1764" height="1208"&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;div id="attachment_3473" style="width: 1764px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3473" loading="lazy" class="size-full wp-image-3473" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.20.30@2x.png" alt="Figure 3: Performance and cost benefits of Open Omics Acceleration Framework on m7i.24xlarge and m7i.48xlarge over the baseline on m7i.24xlarge for AlphaFold2-based protein folding pipeline for varying protein lengths. " width="1754" height="1208"&gt;
 &lt;p id="caption-attachment-3473" class="wp-caption-text"&gt;Figure 3: Performance and cost benefits of Open Omics Acceleration Framework on m7i.24xlarge and m7i.48xlarge over the baseline on m7i.24xlarge for AlphaFold2-based protein folding pipeline for varying protein lengths.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;OpenFold is a faithful PyTorch based reproduction of AlphaFold2 which can run on CPU. We compare the OpenFold CPU baseline with Open Omics on two sets of proteins sampled from the &lt;a href="https://www.uniprot.org/proteomes/UP000001940"&gt;C. elegans proteome&lt;/a&gt;. Since a large majority of the proteins found in nature have lengths less than 1000, we create the first set with proteins of length under 1000 amino acid residues. The second set consists of proteins up to lengths that OpenFold can handle. On the same m7i.24xlarge instance as baseline, Open Omics achieves significant cost and execution time improvements. Moreover, due to its ability to fold multiple proteins in parallel, it also scales well to the larger m7i.48xlarge instance. For the first set, Open Omics achieves 10.1x speedup on preprocessing and 37.1x speedup on model inference resulting in speedup for end-to-end execution of 17x and cost reduction of 8.5x. For the second set, the speedup values for individual stages are 5.2x and 33.2x, respectively, and overall speedup is 20.8x and cost reduction is 10.4x. For longer sequences, OpenFold did not finish even after 3 days. On the other hand, Open Omics can comfortably handle sequences up to length 7500 on m7i.48xlarge instances.&lt;/p&gt; 
&lt;h2&gt;DeepVariant-based variant calling pipeline (fq2vcf)&lt;/h2&gt; 
&lt;p&gt;Variant calling is a fundamental task in DNA sequence analysis. Given the sequencing reads from an individual’s genome, variant calling identifies the variations in the reads against a reference genome. &lt;a href="https://www.nature.com/articles/nbt.4235.epdf?author_access_token=q4ZmzqvvcGBqTuKyKgYrQ9RgN0jAjWel9jnR3ZoTv0NuM3saQzpZk8yexjfPUhdFj4zyaA4Yvq0LWBoCYQ4B9vqPuv8e2HHy4vShDgEs8YxI_hLs9ov6Y1f_4fyS7kGZ"&gt;DeepVariant&lt;/a&gt;, a deep learning-based germline variant caller, is a highly accurate and widely used tool across many genomic studies.&lt;/p&gt; 
&lt;div id="attachment_3474" style="width: 1918px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3474" loading="lazy" class="size-full wp-image-3474" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.21.48@2x.png" alt="Figure 4: The DeepVariant based fq2vcf pipeline." width="1908" height="256"&gt;
 &lt;p id="caption-attachment-3474" class="wp-caption-text"&gt;Figure 4: The DeepVariant based fq2vcf pipeline.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The variant calling pipeline using DeepVariant, as shown in Figure 4, consists of the following steps: 1) mapping of the input reads to the reference genome, 2) sorting the mapped reads based on the reference coordinate, and 3) calling variants by first creating pileups of the regions that are expected to have variants followed by classifying among variants types using Inception V3 deep learning model. For baseline, we use – &lt;a href="https://github.com/lh3/bwa"&gt;BWA-MEM&lt;/a&gt; v. 0.7.17 for mapping, samtools v. 1.16.1 sort for sorting and &lt;a href="https://github.com/google/deepvariant"&gt;DeepVariant&lt;/a&gt; v. 1.5 for variant calling. &lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/tree/main/pipelines/deepvariant-based-germline-variant-calling-fq2vcf"&gt;Open Omics version of the pipeline&lt;/a&gt; is accelerated through use of:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Intel Advanced Matrix Extensions (AMX) with bfloat16 precision for DL compute, Intel AVX-512 for non-DL compute,&lt;/li&gt; 
 &lt;li&gt;Cache optimizations, and&lt;/li&gt; 
 &lt;li&gt;Scaling it to the multiple CPU instances.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div id="attachment_3475" style="width: 994px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3475" loading="lazy" class="size-full wp-image-3475" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.23.08.png" alt="Figure 5: Performance and cost benefits of Open Omics Acceleration Framework over the baseline for DeepVariant-based fq2vcf pipeline on standard 30x WGS HG001 short read dataset with GRCh38 as the reference genome. Instances used: c7i.24xlarge and c7i.48xlarge." width="984" height="945"&gt;
 &lt;p id="caption-attachment-3475" class="wp-caption-text"&gt;Figure 5: Performance and cost benefits of Open Omics Acceleration Framework over the baseline for DeepVariant-based fq2vcf pipeline on standard 30x WGS HG001 short read dataset with GRCh38 as the reference genome. Instances used: c7i.24xlarge and c7i.48xlarge.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We compare baseline with Open Omics on the standard 30x WGS paired-end short read dataset: HG001 (&lt;a href="//genomics-benchmark-datasets/google-brain/fastq/novaseq/wgs_pcr_free/30x/HG001.novaseq.pcr-free.30x.R1.fastq.gz"&gt;R1&lt;/a&gt; and &lt;a href="//genomics-benchmark-datasets/google-brain/fastq/novaseq/wgs_pcr_free/30x/HG001.novaseq.pcr-free.30x.R2.fastq.gz"&gt;R2&lt;/a&gt;). On the same instance type (c7i.24xlarge), Open Omics achieves 2.21x speedup compared to baseline resulting in a cost of just $8.8 per sample. For use-cases that require a quick turn-around time, Open Omics scales well to 4x c7i.48xlarge instances reducing execution time to nearly 21 mins at just $12.1 per sample – 1.62 times lower cost than baseline. It also scales further to 8x c7i.48xlarge instances, reducing execution time to just 16.8 mins while still keeping the cost lower than the baseline.&lt;/p&gt; 
&lt;h2&gt;Single-cell RNA-Seq analysis pipeline&lt;/h2&gt; 
&lt;p&gt;Single-cell analysis involves studying various omics (genomics, transcriptomics, proteomics, metabolomics) data and cell-cell interactions at the individual cell level. Single-cell RNA-seq (scRNA-seq) is an advanced technique that measures gene expression of individual cells, which is analyzed to study the differences in gene expression profiles across cells.&lt;/p&gt; 
&lt;p&gt;A typical workflow to analyze scRNA-seq data begins with a matrix that consists of the expression levels of the genes in each cell. The data preprocessing steps filter out the noise and uses linear regression and data normalization to correct artifacts from data collection. Subsequently, dimensionality reduction is performed followed by clustering of cells to group them by similarity in genetic activity and visualization of the clusters. With over 2 million downloads,&amp;nbsp;&lt;a href="https://github.com/scverse/scanpy"&gt;Scanpy&lt;/a&gt;&amp;nbsp;is one of the most widely used toolkits for this analysis.&lt;/p&gt; 
&lt;div id="attachment_3476" style="width: 962px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3476" loading="lazy" class="size-full wp-image-3476" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.24.18.png" alt="Figure 6: Pipeline showing the steps in analysis of single-cell RNA sequencing data starting from gene activity matrix to visualization of different cell clusters." width="952" height="249"&gt;
 &lt;p id="caption-attachment-3476" class="wp-caption-text"&gt;Figure 6: Pipeline showing the steps in analysis of single-cell RNA sequencing data starting from gene activity matrix to visualization of different cell clusters.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 6 illustrates the pipeline we used for this benchmarking. For clustering and visualization, we run all of the options displayed above to demonstrate the benefit of the Open Omics Acceleration Framework on all of them. For the baseline, we use &lt;a href="https://github.com/scverse/scanpy"&gt;scanpy&lt;/a&gt; v. 1.9.1. The &lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/tree/main/pipelines/single-cell-RNA-seq-analysis"&gt;Open Omics version&lt;/a&gt; is accelerated through: 1) parallelization and fusing of operations in data pre-processing, 2) use of efficient implementations from &lt;a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/scikit-learn.html"&gt;Intel&lt;sup&gt;®&lt;/sup&gt; Extension for Scikit-learn&lt;/a&gt;, &lt;a href="https://katanagraph.ai/"&gt;Katana Graph&lt;/a&gt; and Intel Lab’s &lt;a href="https://github.com/IntelLabs/Trans-Omics-Acceleration-Library"&gt;Trans-Omics Acceleration Library&lt;/a&gt; – some of which were accelerated as a part of this effort.&lt;/p&gt; 
&lt;div id="attachment_3477" style="width: 964px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3477" loading="lazy" class="size-full wp-image-3477" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.25.02.png" alt="Figure 7: Performance and cost benefits of Open Omics Acceleration Framework over the baseline for Single-cell RNA-Seq pipeline for standard dataset of 1.3 million mouse brain cells." width="954" height="961"&gt;
 &lt;p id="caption-attachment-3477" class="wp-caption-text"&gt;Figure 7: Performance and cost benefits of Open Omics Acceleration Framework over the baseline for Single-cell RNA-Seq pipeline for standard dataset of 1.3 million mouse brain cells.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We compare baseline with Open Omics on the standard dataset of &lt;a href="https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad"&gt;1.3 million mouse brain cells&lt;/a&gt;. The baseline requires the larger memory r7i.24xlarge instance. On the same instance type, Open Omics achieves a 29.3x speedup compared to baseline resulting in a cost of nearly $0.6 per sample. This cost is further improved with the optimizations applied to reduce memory requirements that enables us to use the less expensive compute optimized c7i.24xlarge instance, reducing the cost to just $0.4 per sample.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;To accelerate the ongoing revolution of omics, Intel has built the Open Omics acceleration framework. The current version of Open Omics running on AWS instances based on 4&lt;sup&gt;th&lt;/sup&gt; generation Intel Xeon processors provides significant performance and cost benefits for key pipelines for protein folding, variant calling, and single-cell RNA-Seq analysis.&lt;/p&gt; 
&lt;p&gt;To learn more about performance of Open Omics and comparison with other solutions, please refer to the following research blogs –&amp;nbsp;&lt;a href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Xeon-is-all-you-need-for-AI-inference-Performance/post/1506083"&gt;July’23&lt;/a&gt;, &lt;a href="https://aws.amazon.com/blogs/hpc/accelerating-genomics-pipelines-using-intel-open-omics-on-aws/"&gt;Aug’22&lt;/a&gt;, &lt;a href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715"&gt;June’22&lt;/a&gt;, and the GitHub repository –&amp;nbsp;&lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework"&gt;Open Omics Acceleration Framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.s&lt;/em&gt;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.28.09.png" alt="Sanchit Misra" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Sanchit Misra&lt;/h3&gt; 
  &lt;p&gt;Sanchit Misra, PhD is a senior research scientist and leads the research efforts in building SW/HW computing solutions for AI driven life science at Intel Labs. Sanchit has over 15 years of experience in computational biology, AI, scaling applications on large clusters/supercomputers and driving architecture improvements. Over the years, he has led Intel’s various collaborations with experts in computational biology, AI, HPC and architecture. Before joining Intel Labs, he earned his PhD in high performance computational biology from Northwestern University.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.28.15.png" alt="Vasimuddin Md" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Vasimuddin Md&lt;/h3&gt; 
  &lt;p&gt;Vasimuddin Md., PhD is a research scientist at Parallel Computing Lab, Intel Labs. He obtained his Ph.D. from IIT Bombay and has a decade of research experience in the field of Computational Biology, HPC, and AI. At Intel Labs, he works on building computational solutions (both hardware and software) for various challenging problems in Biology.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.28.22.png" alt="Narendra Chaudhary" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Narendra Chaudhary&lt;/h3&gt; 
  &lt;p&gt;Narendra Chaudhary, PhD is a research scientist and conducts computational biology/HPC research at Intel Labs. Dr. Chaudhary has experience in improving efficiency and scaling of deep learning applications. Before joining Intel Labs, he earned his PhD in Electrical and Electronics Engineering from Texas A&amp;amp;M University.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.28.27.png" alt="Saurabh Kalikar" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Saurabh Kalikar&lt;/h3&gt; 
  &lt;p&gt;Saurabh Kalikar, PhD is a research scientist at Intel – Parallel Computing Lab, where he is involved in designing and implementing performant solutions for the applications running on modern CPUs. His research interests lie in Parallel/Distributed Computing, Compilers, and Architecture-Aware Optimization. Prior to joining Intel Labs, he earned his PhD from Indian Institute of Technology Madras.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.28.32.png" alt="Manasi Tiwari" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Manasi Tiwari&lt;/h3&gt; 
  &lt;p&gt;Manasi Tiwari, PhD is a research scientist at Intel – Parallel Computing Lab. She works on ideating and developing parallel solutions for Computational Biology algorithms which are at the intersection of HPC and AI. These solutions range from architecture dependent SIMD solutions to large scale multi-node solutions. Prior to joining Intel, she earned her PhD from Indian Institute of Science, Bangalore.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.28.37.png" alt="Ashishkumar Patel" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Ashishkumar Patel&lt;/h3&gt; 
  &lt;p&gt;Ashishkumar Patel is a contingent worker at Intel Technology India Pvt. Ltd. He earned his master’s degree in computer engineering from the Sardar Vallabhbhai National Institute of Technology (NIT), Surat. Ashishkumar has a background in academia and research. Throughout his career, he has made significant contributions to various projects, particularly in the fields of deep learning and computational biology.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Using a digital twin for sensitivity analysis to determine sensor placement in a roll-to-roll manufacturing web-line</title>
		<link>https://aws.amazon.com/blogs/hpc/using-a-digital-twin-for-sensitivity-analysis-to-determine-sensor-placement-in-a-roll-to-roll-manufacturing-web-line/</link>
		
		<dc:creator><![CDATA[Ross Pivovar]]></dc:creator>
		<pubDate>Tue, 02 Apr 2024 15:02:29 +0000</pubDate>
				<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">3f0b08d93c50365a758b4efba227be21f49403be</guid>

					<description>What's the best way to select sensors to capture key data for your digital twin without overspending? Check out our latest blog post on using ML and sensitivity studies to optimize sensor selection.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Ross Pivovar, Solution Architect, and Adam Rasheed, Head of Emerging Workloads &amp;amp; Technologies, AWS and Orang Vahid, Director of Engineering Services and Kayla Rossi, Application Engineer, Maplesoft&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;In our previous posts we discussed &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/"&gt;how to setup a Level 4 Digital Twin (DT)&lt;/a&gt; that adapts to changing environments &lt;em&gt;and&lt;/em&gt; &lt;a href="https://aws.amazon.com/blogs/hpc/using-a-level-4-digital-twin-for-scenario-analysis-and-risk-assessment-of-manufacturing-production-on-aws/"&gt;how to use the L4 DT to perform forecasting, scenario analysis, and risk assessment&lt;/a&gt; based on incoming measurement data. In this post today, we’ll discuss methods to find the optimal number, type, and placement of sensors to maximize the accuracy of your Level 3 or Level 4 DT. If you need a refresher, you can check out the &lt;a href="https://aws.amazon.com/blogs/iot/digital-twins-on-aws-unlocking-business-value-and-outcomes/"&gt;AWS digital twin leveling framework&lt;/a&gt;. In practice, we don’t have the luxury of adding thousands of sensors to our system. Sensors incur a capital cost, require maintenance and calibration by technicians, inevitably fail (requiring replacement), and they need actual physical space to be installed. In contrast, too few sensors and we miss valuable data that could indicate performance issues, possible system failures, and potential safety issues. For complicated systems or &lt;em&gt;system-of-systems&lt;/em&gt;, the number of variables of interest can range from tens to thousands. Hence, it becomes quite difficult and tedious to manually understand the relationship between different factors that impact the performance of the overall system.&lt;/p&gt; 
&lt;p&gt;In this post, we leverage a combination of our &lt;a href="https://www.maplesoft.com/products/maplesim/"&gt;MapleSim&lt;/a&gt; DT, machine learning (ML) models, and Shapley sensitivity studies to analyze the relationships between different variables and identify the best sensor placement to trade-off cost versus prediction capability. A sensitivity analysis determines how responsive your engineering models’ outputs are to changes in key input parameters, identifying which variables have the greatest impact on the results. We use &lt;a href="https://github.com/aws-samples/twinflow"&gt;TwinFlow&lt;/a&gt; to deploy and execute our studies on AWS. We also briefly explore how generative AI can assist in sensor selection.&lt;/p&gt; 
&lt;h2&gt;Where are sensors needed, what type, and how many&lt;/h2&gt; 
&lt;p&gt;As in the prior posts, we consider the web-handling process in roll-to-roll manufacturing for continuous materials like paper, film, and textiles. This example web-handling process involves unwinding the material from a spool, guiding it through various treatments like printing or coating, and then winding it onto individual rolls.&lt;/p&gt; 
&lt;p&gt;Figure 1 is a schematic diagram of the same roll-to-roll manufacturing line discussed in those earlier posts. The manufacturing line consists of 9 rollers (labeled with “R”) and 12 material spans (labeled with “S”). The operator’s goal is to maximize throughput/yield of the manufacturing line without damaging the product or creating safety hazards. It is critical to monitor various stages in the process for quality control.&lt;/p&gt; 
&lt;p&gt;It’s tempting to instrument each span or roller with pressure gauges, load cells (to measure tension), feed line sensors (for linear velocity), and roller tachometers (RPM) sensors to track the progress of the roll-to-roll material in the line. This is rarely practical however, due to the high cost of installing and maintaining the sensors. Furthermore, in some cases, there may not be physical space in the facility for all of them.&lt;/p&gt; 
&lt;div id="attachment_3437" style="width: 590px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3437" loading="lazy" class="size-full wp-image-3437" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.18.37.png" alt="Figure 1 Schematic diagram of the web-handling equipment in which each span is labeled as S1 through S12 and rollers are label R1 through R10." width="580" height="165"&gt;
 &lt;p id="caption-attachment-3437" class="wp-caption-text"&gt;Figure 1 Schematic diagram of the web-handling equipment in which each span is labeled as S1 through S12 and rollers are label R1 through R10.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For our use case, it is known that the minimum information needed to prevent product quality issues include the tension in the material of each span and whether the material is dragging over the rollers (slip) instead of rotating with the rollers.&lt;/p&gt; 
&lt;p&gt;When considering how to efficiently avoid quality issues, the questions to be answered are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;When is tension above 175 Newtons (a tension failure threshold)?&lt;/li&gt; 
 &lt;li&gt;When is slip velocity greater than 1×10&lt;sup&gt;-3&lt;/sup&gt; m/s (a differential velocity failure threshold)?&lt;/li&gt; 
 &lt;li&gt;Which sensors are needed to monitor these failure mechanisms?&lt;/li&gt; 
 &lt;li&gt;Are all sensor locations needed?&lt;/li&gt; 
 &lt;li&gt;Can downstream sensors provide information about upstream effects?&lt;/li&gt; 
 &lt;li&gt;Is there sensor redundancy?&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In this example we have at least 42 potential sensors to be included in our manufacturing line. One option is to install sensors to directly measure the tension and differential velocity at all locations. This is not a viable solution for operators due to the cost of installing and maintaining the tension load cells and velocity sensors.&lt;/p&gt; 
&lt;p&gt;Instead, we can use a digital twin which provides a physics-based simulation of the process, enabling the combination of sensor data with predictions. A digital twin provides both insights into the process and the potential to answer our questions listed above. Our goal changes from directly measuring with physical sensors to identifying which combination of sensors can be used to calibrate a digital twin such that we can create high-accuracy virtual sensors.&lt;/p&gt; 
&lt;p&gt;Specifically, for our example model of the manufacturing line, the digital twin requires calibration of the roller viscous damping coefficients (see Figure 2). The roller viscous damping coefficient refers to a parameter that can’t be directly measured but reflects the bearing loss due to internal friction as the rollers rotate. &amp;nbsp;Once the viscous damping coefficients in the model are calibrated, the remaining properties of interest that we want to measure with sensors can instead be predicted purely based on the physics in the model. The question then remains, which sensors do we need in the roll-to-roll manufacturing line to calibrate the viscous damping coefficients in the model?&lt;/p&gt; 
&lt;div id="attachment_3438" style="width: 710px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3438" loading="lazy" class="size-full wp-image-3438" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.19.04.png" alt="Figure 2 The inputs and outputs of the roll-to-roll manufacturing web line digital twin" width="700" height="187"&gt;
 &lt;p id="caption-attachment-3438" class="wp-caption-text"&gt;Figure 2 The inputs and outputs of the roll-to-roll manufacturing web line digital twin.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Finding correlations in complicated systems&lt;/h2&gt; 
&lt;p&gt;To determine which sensors are needed to calibrate a model, a traditional approach is to use one-at-a-time perturbations in which a user alters one variable at a time within the model. While simple, this method misses cross-correlation/interactions that are often present within the physical phenomena of the system.&lt;/p&gt; 
&lt;p&gt;More advanced methods like &lt;a href="https://en.wikipedia.org/wiki/Variance-based_sensitivity_analysis"&gt;Sobel sensitivities&lt;/a&gt; rely on Monte Carlo sampling to numerically determine both main effects and interactions. Monte Carlo sampling consists of running a simulation model repeatedly (thousands of times), with each simulation using slightly different inputs. The correlation between the inputs and the outputs can then be quantified in a probabilistic manner for further analysis.&lt;/p&gt; 
&lt;p&gt;However, Sobel sensitivities can potentially provide incorrect results for highly non-linear systems, miss complicated interactions, or are unable to decouple correlated inputs. Additionally, the Sobel sensitivities are designed for global sensitivities.&lt;/p&gt; 
&lt;p&gt;The method of choice in this blog post, albeit not exclusively better, is to use Shapley sensitivities. Common implementations use an ML model to learn from the Monte Carlo simulations, and then apply game theory to assess the main effects and interactions, regardless of non-linearities or complicated interactions.&lt;/p&gt; 
&lt;p&gt;Even better, the method can provide sensitivities that either summarize effects across the entire modeling space (global sensitivities) or look at effects locally around a specific point in the space (local sensitivities).&lt;/p&gt; 
&lt;p&gt;Deploying our MapleSim digital twin on AWS using TwinFlow, we can generate a large sample set (10,000 simulations) to assess the Shapley sensitivities. We can either graphically review the Shapley sensitivities (shown in Figure 3) or look at the raw output (saved to SQL or csv file). For our specific example we have 12 spans and 9 rollers, with 2 properties each resulting in a total of 270 sensitivities relating the viscous damping coefficients (b-labels) to our desired outputs.&lt;/p&gt; 
&lt;p&gt;Graphically reviewing the sensitivities enables an ad-hoc assessment of the magnitudes. For example, in Figure 3a, we see the sensitivity of the R1 (roller 1) slip velocity to the damping coefficients in each of the rollers. It’s clear that the R1 slip velocity is most sensitive to the damping coefficient of the first roller (labeled b1) and positively correlated (meaning R1 slip velocity increases as the R1 damping coefficient increases).&lt;/p&gt; 
&lt;p&gt;The damping coefficient for the other rollers (b2 – b10) has minimal impact on R1 slip velocity – which agrees with our intuition. Figure 3b shows that the tension in the first span (S1) is negatively correlated with the damping coefficient of roller 1. Again, this is logical because we know the tension is likely to decrease as the material is slipping over roller 1. We also see that the viscous damping for roller 2 (labeled b2) has a possible impact on span 1 tension, although we would have to perform additional analysis to confirm it’s statistically significant.&lt;/p&gt; 
&lt;p&gt;Figure 3c demonstrates the power of Shapley sensitivity analysis as it identifies potential interaction effects that are not easy to determine by simple logical reasoning. In this case, we see that the slip velocity for Roller 9 is sensitive to viscous damping in both roller 9 (labeled b9) and roller 10 (labeled b10). This interaction is complex, however, in that R9 slip velocity is &lt;em&gt;negatively correlated&lt;/em&gt; to its own viscous damping (meaning R9 slip velocity decreases as b9 viscous damping increases), and is &lt;em&gt;positively correlated&lt;/em&gt; with higher magnitude with b10 viscous damping. This complex interaction is the result of the design and layout of the roll-to-roll manufacturing web line. Figure 3d shows the corresponding graph of tension in span 9 (S9) being nearly equally sensitive to both viscous damping b9 and b10.&lt;/p&gt; 
&lt;div id="attachment_3439" style="width: 751px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3439" loading="lazy" class="size-full wp-image-3439" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.19.51.png" alt="Figure 3 Example output of Shapley sensitivity analysis." width="741" height="967"&gt;
 &lt;p id="caption-attachment-3439" class="wp-caption-text"&gt;Figure 3 Example output of Shapley sensitivity analysis.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://github.com/aws-samples/twinflow"&gt;TwinFlow&lt;/a&gt; includes the TwinStat module which provides a library of algorithms and methods that are commonly used for probabilistic modeling for digital twins. In particular, TwinStat uses a &lt;em&gt;Gaussian Kernel Density Estimator&lt;/em&gt; to produce the probability density distributions and cumulative density distributions for all outputs of the digital twin.&lt;/p&gt; 
&lt;p&gt;We gain further insights by reviewing the probability distributions for roller slip velocity and span tension. For example, Figure 4a show the probability density distribution of the R3 slip velocity obtained from the Monte Carlo sampling analysis where we varied the input damping coefficients. Figure 4b shows the cumulative distribution for the same data. The vertical dashed line indicates the slip velocity failure threshold and the probability curve crosses this threshold at approximately 90%, representing the probability of measurements being at or below the threshold.&lt;/p&gt; 
&lt;p&gt;We can tell that 90% of the cases have a slip velocity below the failure threshold of 1×10&lt;sup&gt;-3&lt;/sup&gt; m/s, indicating that it’s unlikely that we’ll see roller slip at the R3 location. This suggests it might be feasible to forgo installation of a costly sensor in the R3 location.&lt;/p&gt; 
&lt;div id="attachment_3440" style="width: 864px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3440" loading="lazy" class="size-full wp-image-3440" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.20.41.png" alt="Figure 4 Probability of various slip velocities depending on changes in the roller viscous damping coefficients. " width="854" height="372"&gt;
 &lt;p id="caption-attachment-3440" class="wp-caption-text"&gt;Figure 4 Probability of various slip velocities depending on changes in the roller viscous damping coefficients.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In contrast, Figure 5 &amp;nbsp;shows that tension failures occur frequently for span 4. Figure 5b shows that only 38% of the cases remain below the tension failure threshold of 175 N. This means the probability of exceeding the failure threshold is 62% (calculated as 100% – 38%). The high failure rate suggests that we need to invest in the sensors to monitor span 4 tension.&lt;/p&gt; 
&lt;div id="attachment_3441" style="width: 858px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3441" loading="lazy" class="size-full wp-image-3441" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.21.01.png" alt="Figure 5 Probability of various slip velocities depending on changes in the roller viscous damping coefficients." width="848" height="370"&gt;
 &lt;p id="caption-attachment-3441" class="wp-caption-text"&gt;Figure 5 Probability of various slip velocities depending on changes in the roller viscous damping coefficients.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Performing similar analysis for all the roller slip velocities and span tensions reveals that spans 4,5,6 often exceed the tension failure threshold, but all other spans rarely or never fail. Similarly, rollers 1,5,6,7,8,9 often exceed the slip velocity failure threshold, but all other rollers rarely fail. This information alone already tells us which spans and rollers need monitoring with sensors, and which sensors we can remove or leave out.&lt;/p&gt; 
&lt;p&gt;Our goal is to minimize the cost of installing and maintaining sensors, while still being able to monitor the manufacturing process with sufficient resolution to minimize product defects. With this in mind, we note that angular velocity (RPM) sensors are much cheaper and less intrusive than tension load cells and linear velocity sensors.&lt;/p&gt; 
&lt;p&gt;The Shapley analysis provides cross-correlated sensitivities between potential sensors and each viscous damping coefficient. As our next step, we queried the dataset to generate the heatmap shown in Figure 6. A sensitivity heatmap like this visualizes the relationship between inputs and outputs of the digital twin. Green shades show a positive relationship – increasing the input increases the output. Red shades indicate a negative correlation – rising input values cause output values to decrease. The intensity of the color corresponds to the strength of the sensitivity. Higher saturation means the input has a larger effect on that output. Cells colored close to yellow have little to no sensitivity. Examining the heatmap reveals which input variables drive which outputs. Inputs that lack significant sensitivity to any outputs may represent opportunities to eliminate sensors to reduce costs.&lt;/p&gt; 
&lt;div id="attachment_3442" style="width: 868px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3442" loading="lazy" class="size-full wp-image-3442" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.21.29.png" alt="Figure 6 Heatmap of Shapley results" width="858" height="282"&gt;
 &lt;p id="caption-attachment-3442" class="wp-caption-text"&gt;Figure 6 Heatmap of Shapley results&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We can’t directly measure the damping coefficients in the system. Instead, we’ll use a combination of sensors with our digital twin to estimate the damping coefficients. The sensors provide data that allows for probabilistic inference with a digital twin for the damping coefficients. Once we have estimates for the damping coefficients, we can use them in the digital twin models to predict the values of other unmeasured variables in the system.&lt;/p&gt; 
&lt;p&gt;You’ll notice in Figure 6 that each sensor provides a different sensitivity and thus a different level of useful information. Some sensors can be strategically chosen due to either providing more information than other sensors, or information about multiple locations in the assembly line.&lt;/p&gt; 
&lt;p&gt;We know tension and linear velocity sensors are expensive so we are looking to see if RPM (“_w” in the table) can be used instead. For example, b1 exhibits sensitivity to “Roller1_w”, which also is sensitive to “S1_Tension” and “Roller1_Slip”, which means that “Roller1_w” should provide adequate information.&lt;/p&gt; 
&lt;p&gt;This analysis can reveal some non-intuitive results like that S5_Tension is more sensitive to b2 and b4 due to the blue nip roller pressure points in Figure 1. Doing these comparisons for all properties tells us that RPM should provide adequate information for all locations in the manufacturing line.&lt;/p&gt; 
&lt;p&gt;To verify the hypothesis that using lower-cost RPM sensors is as effective as the costlier slip velocity sensors/load cells, we used historical data with an &lt;em&gt;Unscented Kalman Filter&lt;/em&gt; (available in TwinStat) to calibrate the Digital Twin. An Unscented Kalman Filter is a probabilistic Bayesian state estimation method that can be used to calibrate the model coefficients in our Level 4 Digital Twin. We showed this calibration approach &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/"&gt;in our earlier post&lt;/a&gt; discussing Level 4 DT setup and implementation.&lt;/p&gt; 
&lt;p&gt;We started by assuming installation of all 42 sensors, then used engineering analysis and customer feedback to determine that roller slip velocity and span tension are the critical quality parameters. This allowed us to reduce the number of sensors to 21. Shapley analysis then identified which specific rollers and spans were most failure-prone, enabling a further reduction to just 9 key sensors.&lt;/p&gt; 
&lt;p&gt;Finally, we substituted lower-cost RPM sensors for some slip velocity sensors and load cells, reducing cost while preserving monitoring capability. Through systematic evaluation we reduced 42 potential sensors down to 9 optimized, affordable sensors that will provide all the information we need to calibrate the Digital Twin, removing sensor redundancy.&lt;/p&gt; 
&lt;h2&gt;Using Amazon Bedrock to augment traditional sensitivity analysis&lt;/h2&gt; 
&lt;p&gt;The traditional manual approach to sensitivity analysis may be too time consuming or may include too many values to evaluate. Further data analysis techniques may be required such as using regressions on the data. However, as an experiment we decided to provide &lt;a href="https://aws.amazon.com/bedrock/"&gt;Amazon Bedrock&lt;/a&gt; with the Figure 6 data. In Figure 7 we presented this problem to the Claude 2.1 LLM model. Interestingly it correctly selected the RPM variables, but also concluded it would be possible to select a subset. The LLM model thinks it is possible to use fewer sensors than the human has selected to calibrate the digital twin. The LLM is attempting to take advantage of the fact that some viscous damping coefficients can potentially be used to predict multiple spans or roller properties. In a follow-up blog post, we will perform a more detailed exploration to assess the ability of using LLMs to assist with sensitivity analysis.&lt;/p&gt; 
&lt;p&gt;&lt;img loading="lazy" class="aligncenter size-full wp-image-3443" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.23.17.png" alt="" width="1379" height="582"&gt;&lt;/p&gt; 
&lt;div id="attachment_3444" style="width: 623px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3444" loading="lazy" class="size-full wp-image-3444" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.24.27.png" alt="Figure 7 Comparison with Amazon Bedrock (Claude 2.1 Model). Note the raw data given to the LLM is not shown in the screenshots provided here." width="613" height="389"&gt;
 &lt;p id="caption-attachment-3444" class="wp-caption-text"&gt;Figure 7 Comparison with Amazon Bedrock (Claude 2.1 Model). Note the raw data given to the LLM is not shown in the screenshots provided here.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;AWS architecture&lt;/h2&gt; 
&lt;p&gt;The AWS architecture used for the sensitivity study is depicted in Figure 8. Steps 1 – 3 are the same as described in the previous &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/"&gt;blog post&lt;/a&gt; for the Level 4 digital twin in which we install TwinFlow on an Amazon Elastic Compute Cloud (Amazon EC2) instance, setup a container which includes our MapleSim model, and push it up to Amazon ECR where the container is accessible to any AWS service.&lt;/p&gt; 
&lt;p&gt;We can also store the model in an &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service&lt;/a&gt; (Amazon S3) bucket which will download the model when needed. This may be advantageous if we’re modifying the model for various studies.&lt;/p&gt; 
&lt;p&gt;When running a sensitivity study with TwinFlow (leveraging the TwinStat automation for sensitivity studies), it automatically sets up tables (creates the table and schema) within &lt;a href="https://aws.amazon.com/rds/"&gt;Amazon RDS&lt;/a&gt; (AWS SQL service). Based on user defined inputs and outputs in a JSON file, TwinFlow (leveraging the TwinStat automation) creates the input perturbation run list and executes in parallel all simulations using &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;. &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; is our cloud-native HPC scheduler that includes backend options for &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon ECS&lt;/a&gt;, which is an AWS-specific container orchestrations service, &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt;, which is a serverless execution option, and &lt;a href="https://aws.amazon.com/eks/"&gt;Amazon EKS&lt;/a&gt; which is our Kubernetes option.&lt;/p&gt; 
&lt;p&gt;We chose Amazon ECS because we wanted Amazon EC2 instances with larger numbers of CPUs than those available for Fargate. Fargate can create and deploy instances faster than ECS, however for tightly coupled numerical simulations often it’s better to scale up to minimize network latency thus minimizing runtime. Also, ECS enables fully-automated deployment unlike EKS. The specific Amazon EC2 instance type is automatically selected by AWS Batch auto-scaling based on the user-defined CPU/GPU and memory requirements.&lt;/p&gt; 
&lt;p&gt;Once all simulations have completed on AWS Batch, TwinStat downloads all data from the &lt;a href="https://aws.amazon.com/rds/"&gt;Amazon RDS&lt;/a&gt; tables and automatically performs the Shapley sensitivity study producing images of the probability distributions and sensitivity bar charts storing them in the local file system of the EC2 instance. A user then can review the results by downloading the images locally or store them in an Amazon S3 bucket for archiving.&lt;/p&gt; 
&lt;div id="attachment_3445" style="width: 760px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3445" loading="lazy" class="size-full wp-image-3445" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.25.06.png" alt="Figure 8 AWS cloud architecture used to perform sensitivity studies with our L4 Digital Twin" width="750" height="537"&gt;
 &lt;p id="caption-attachment-3445" class="wp-caption-text"&gt;Figure 8 AWS cloud architecture used to perform sensitivity studies with our L4 Digital Twin&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;In this post, we showed how to perform a sensitivity analysis using a Digital Twin to identify the minimum set of sensors and sensor locations while maintaining effective monitoring of our roll-to-roll manufacturing process. MapleSim provides the physics-based model in the form of an FMU and TwinFlow allows us to run scalable number of scenarios on AWS, providing efficient and elastic computing resources.&lt;/p&gt; 
&lt;p&gt;In this Digital Twin example, the utility of a physics-based digital twin shows how we can quickly obtain actionable insights that directly impact business metrics. Digital twins are extremely versatile and can be leveraged at all phases of product development, from inception, implementation, to monitoring.&lt;/p&gt; 
&lt;p&gt;If you want to request a proof of concept or if you have feedback on the AWS tools, please reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Renewable energy transition: examining the impacts of wind energy through simulation</title>
		<link>https://aws.amazon.com/blogs/hpc/renewable-energy-transition-examining-the-impacts-of-wind-energy-through-simulation/</link>
		
		<dc:creator><![CDATA[Remco Verzijlbergh]]></dc:creator>
		<pubDate>Tue, 26 Mar 2024 13:21:20 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">812815631cfcc972ff806cbc6735cc3480bc8fc4</guid>

					<description>As we move towards a greener future, understanding wind energy's climate impacts is key. Check out this blog post by our friends at Whiffle, to learn how large-scale simulations reveal wind power's effect on our atmosphere.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3403" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/06/Renewable-energy-transition-examining-the-impacts-of-wind-energy-through-simulation-mini.png" alt="Renewable energy transition: examining the impacts of wind energy through simulation" width="380" height="212"&gt;This post was contributed by&lt;/em&gt;&lt;em&gt; Remco&amp;nbsp;Verzijlbergh, co-founder and CEO, Peter Baas, &lt;/em&gt;&lt;em&gt;R&amp;amp;D Specialist &lt;/em&gt;&lt;em&gt;at&amp;nbsp;Whiffle, and Ilan Gleiser &lt;/em&gt;&lt;em&gt;Principal ML Specialist, &lt;/em&gt;&lt;em&gt;Milo Oostergo, Principal Startup SA, and Ross Pivovar Senior SA, Simulations at AWS&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The world is currently facing a crucial turning point in the fight against climate change, and renewable energy sources like wind energy are playing a major role. As we continue to move towards a more sustainable future, it is important to understand the impacts of the anticipated large-scale roll-out of wind energy. This is where atmospheric model simulation comes into play.&lt;/p&gt; 
&lt;p&gt;By performing large scale simulations on HPC clusters in the cloud, built using &amp;nbsp;Amazon EC2 P4d instances and powered by Nvidia A100 GPUs, we can get a better understanding of how wind energy affects our atmosphere and the environment as a whole.&lt;/p&gt; 
&lt;p&gt;In this blog post, we’ll delve into the realm of simulations and explore the impact of wind energy on our planet.&lt;/p&gt; 
&lt;h2&gt;The Rising Power of Wind Energy&lt;/h2&gt; 
&lt;p&gt;In the quest for a sustainable future, wind energy has emerged as a powerful player in the fight against climate change. The rising power of wind energy has captured the attention of scientists, policymakers, and the general public alike. It has quickly become a driving force in the transition towards renewable sources of energy, and for good reason.&lt;/p&gt; 
&lt;p&gt;Harnessing the power of wind is not a new concept. Humans have been utilizing wind energy for centuries, from simple windmills for grinding grains to more advanced wind turbines that generate electricity. However, it is only in recent decades that wind energy has gained significant momentum as a viable alternative to fossil fuels.&lt;/p&gt; 
&lt;p&gt;One of the main reasons for the increasing popularity of wind energy is its environmental benefits. Unlike traditional energy sources such as coal or natural gas, wind energy does not produce harmful emissions or contribute to air pollution. This is a crucial advantage in the battle against climate change, as reducing greenhouse gas emissions is essential to mitigating its impact.&lt;/p&gt; 
&lt;p&gt;Furthermore, wind energy is a renewable resource, meaning that it will never run out. As long as the wind keeps blowing, we can continue to harness its power. This is in stark contrast to fossil fuels, which are finite resources that will eventually be depleted.&lt;/p&gt; 
&lt;p&gt;Another key factor driving the rise of wind energy is its economic potential. As technology has advanced, the cost of producing wind energy has steadily decreased. In fact, wind energy is now one of the cheapest sources of electricity in many parts of the world. This affordability, coupled with the growing demand for clean energy, has led to a boom in wind energy installations globally.&lt;/p&gt; 
&lt;p&gt;In addition to its environmental and economic benefits, wind energy also has the potential to enhance energy security. Unlike fossil fuels, which are often imported from other countries, wind energy can be produced locally. This reduces dependence on foreign sources of energy and strengthens national energy independence.&lt;/p&gt; 
&lt;p&gt;The increasing power of wind energy is evident in the rapid growth of wind farms around the world. These vast fields of turbines are capable of generating large amounts of electricity, powering homes, businesses, and even entire cities. As technology continues to advance, wind turbines are becoming more efficient and capable of harnessing even more energy from the wind.&lt;/p&gt; 
&lt;p&gt;Harnessing wind energy has significant positive effects on the atmosphere. By replacing traditional energy sources with wind energy, we can reduce greenhouse gas emissions and combat climate change. Wind energy does not release carbon dioxide or other pollutants that contribute to air pollution and global warming.&lt;/p&gt; 
&lt;p&gt;Moreover, wind energy can contribute to improved air quality. By reducing reliance on fossil fuels, which emit harmful pollutants such as sulfur dioxide and nitrogen oxide, wind energy helps to alleviate respiratory issues and promote healthier living conditions.&lt;/p&gt; 
&lt;h2&gt;Unraveling the Data: Using Simulations to Predict Future Scenarios&lt;/h2&gt; 
&lt;p&gt;As we strive for a greener future, simulations play a vital role in understanding the impacts of wind energy on our planet. Using superior computing power to solve the complexities of the weather with unparalleled accuracy, we can unravel the data and predict future scenarios. By simulating large-scale wind farms and analyzing the interaction with the atmosphere, we can gain valuable insights into the sustainability and environmental effects of renewable energy. These simulations offer a powerful tool for decision-making and policy development, ensuring a more informed and efficient transition to a sustainable energy future.&lt;/p&gt; 
&lt;h2&gt;Who is Whiffle?&lt;/h2&gt; 
&lt;p&gt;Whiffle services are designed to empower renewable energy industries and optimize operations by providing accurate meteorological forecasts, precise wind and solar power production forecasts, comprehensive wind resource modelling and detailed energy yield assessments. They are the driving force behind the revolutionary &lt;a href="https://whiffle.nl/solutions/whiffle-wind/"&gt;Whiffle Wind web app&lt;/a&gt;, which is significant in renewable energy development. With their cutting-edge Large-Eddy Simulation (LES)-powered weather Computational Fluid Dynamics (CFD) modelling technology, Whiffle is helping to pave the way for a greener and more sustainable future.&lt;/p&gt; 
&lt;p&gt;What sets Whiffle Wind apart is its &lt;a href="https://whiffle.nl/solutions/whiffle-wind/"&gt;user-friendly interface&lt;/a&gt;, in contrast to traditional weather-modeling software which is often complicated and cumbersome. Whiffle Wind makes it easier to access and interpret the data with relative ease, making it a valuable tool for professionals and enthusiasts alike.&lt;/p&gt; 
&lt;p&gt;Whiffle’s ultra-high resolution atmospheric model is also important to renewable energy development and operations. While traditional weather models work with grid blocks of 10 by 10 kilometers, Whiffle works on a resolution of 100 x 100 meters – a hundred times more information density on all weather-defining factors. With its hyper-local forecasts and simulations, Whiffle can help businesses plan for adverse weather events, ensuring the safety and efficiency of renewable energy operations.&lt;/p&gt; 
&lt;p&gt;Whiffle’s stack can also deliver highly accurate predictions, allowing businesses to optimize their operational performance. Whiffle’s model includes elements like buildings and trees. High-resolution digital terrain models are used to allow for detailed simulation in complex terrain. Users can upload wind turbines at any desired location to assess their energy production. As a full-blown weather model, Whiffle’s LES also calculates cloud coverage, which allows for detailed solar radiation predictions. &amp;nbsp;This means that whether you need to consider wind farms or solar installations, Whiffle can help you to get the insights you need for making informed decisions.&lt;/p&gt; 
&lt;p&gt;Core to Whiffle’s technology is &lt;strong&gt;Large-Eddy Simulation&lt;/strong&gt; (LES). This advanced approach to weather modelling follows the laws of atmospheric physics very closely, allowing for detailed simulations of turbulent atmospheric flows. By capturing the small-scale processes that traditional numerical weather prediction models miss, Whiffle can provide a more comprehensive understanding of weather conditions.&lt;/p&gt; 
&lt;div id="attachment_3399" style="width: 703px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3399" loading="lazy" class="size-full wp-image-3399" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/06/CleanShot-2024-03-06-at-13.57.44.png" alt="Fig 1. Achieve accuracy tailored to your specific site requirements, irrespective of terrain complexity, location or the intricacies of wind turbine clusters." width="693" height="749"&gt;
 &lt;p id="caption-attachment-3399" class="wp-caption-text"&gt;Fig 1. Achieve accuracy tailored to your specific site requirements, irrespective of terrain complexity, location or the intricacies of wind turbine clusters.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Delving into large-scale wind farms: impact and sustainability&lt;/h2&gt; 
&lt;p&gt;As the renewable energy transition gains momentum, the installation of large-scale wind farms is becoming increasingly common. For example, the installed capacity of offshore wind energy in the Dutch part of the North Sea is expected to grow dramatically in the next few decades. Several research institutes and the Dutch government are examining scenarios with up to 100 GW (around 5,000 to 10,000 large wind turbines) of offshore wind in 2050. For comparison: the currently installed capacity is around 5 GW, so this means a 20-fold increase.&lt;/p&gt; 
&lt;p&gt;However, the impact of such a large-scale roll-out of wind energy on the atmosphere and vice versa is highly uncertain. Understanding these interactions is crucial for the wind energy sector and policy makers as they strive to develop sustainable and environmentally friendly solutions.&lt;/p&gt; 
&lt;p&gt;This is where the &lt;a href="https://wins50.nl/"&gt;WINS50&lt;/a&gt; project comes into play. The project, carried out by Whiffle,&amp;nbsp;&lt;a href="https://wins50.nl/projectpartners"&gt;TU Delft&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href="https://wins50.nl/projectpartners"&gt;KNMI, &lt;/a&gt;aims to reduce uncertainties surrounding the interaction between a large-scale roll-out of offshore wind energy and the atmosphere. To achieve this, the project uses Whiffle’s high-resolution turbine-resolving weather model, GRASP. With this advanced model, researchers from Whiffle are simulating the entire Dutch North Sea with the currently-installed wind power capacity &lt;em&gt;and&lt;/em&gt; with the projected 2050 capacity. The data they produce will be made available to the wind energy sector and will be used for in-depth studies.&lt;/p&gt; 
&lt;p&gt;Within the WINS50 project, a multi-GPU LES model has been developed to enable computational domains of hundreds by hundreds of kilometers. This model will provide even more detailed and accurate information about the atmospheric interactions of large-scale wind farms, helping to inform future decision-making and policy development.&lt;/p&gt; 
&lt;p&gt;With the combination of advanced simulation models and the collaboration between research institutes and the wind energy sector, we can gain a deeper understanding of the impact and sustainability of large-scale wind farms. By reducing uncertainties and improving our knowledge, we can continue to drive the renewable energy transition forward and create a greener and more sustainable future for all.&lt;/p&gt; 
&lt;h2&gt;Reference Architecture&lt;/h2&gt; 
&lt;div id="attachment_3400" style="width: 914px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3400" loading="lazy" class="size-full wp-image-3400" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/06/CleanShot-2024-03-06-at-13.58.30.png" alt=" Fig 2. AWS High Performance Compute reference architecture comprised of multi GPU AWS Parallel Cluster open-source tool that builds a complete HPC environment with all the benefits of the cloud built-in." width="904" height="412"&gt;
 &lt;p id="caption-attachment-3400" class="wp-caption-text"&gt;&lt;br&gt;Fig 2. AWS High Performance Compute reference architecture comprised of multi GPU AWS Parallel Cluster open-source tool that builds a complete HPC environment with all the benefits of the cloud built-in.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To run the simulations, Whiffle used&amp;nbsp;&lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt;&amp;nbsp;– a fully-supported open-source tool that builds a complete HPC environment with all the benefits of the cloud built-in. That includes the elasticity to scale horizontally (and contract) based on need, instant access to the latest technologies (CPUs, GPUs, and accelerators), and the flexibility to iterate resource selection in minutes to optimize costs or squeeze out better performance.&lt;/p&gt; 
&lt;p&gt;For the cluster configuration, Whiffle used a single queue with four&lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt; P4d&lt;/a&gt; instances that are each powered by eight GPUs. The NVSwitch GPU interconnect enables each GPU to communicate with every other GPU in the same instance with 600 GB/s bidirectional throughput and single-hop latency.&lt;/p&gt; 
&lt;p&gt;Whiffle’s testing showed that the LES model scaled extremely well when run with 8 GPUs in parallel. It supports their philosophy of keeping the model architecture as simple as possible so they can quickly adopt advances in computing and cloud infrastructures. As the performance of parallel GPU compute resources grow, we can all start planning for continental or global scale weather forecasting on a 100m resolution in the future.&lt;/p&gt; 
&lt;p&gt;The data set and simulation results are stored on &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt; which is mounted on the head node and compute nodes, and provides sub-millisecond latencies, high throughput, and millions of IOPS.&lt;/p&gt; 
&lt;p&gt;The final results from the simulation are stored in &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service&lt;/a&gt; (Amazon S3) after post-processing.&lt;/p&gt; 
&lt;h2&gt;The dataset&lt;/h2&gt; 
&lt;p&gt;The dataset is the result from the WINS50 LES described above and it’s &lt;a href="https://registry.opendata.aws/whiffle-wins50/"&gt;available in the Registry of Open Data on AWS&lt;/a&gt;. It’s approximately 40TB in size, and consists of three types of data:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Three-dimensional meteorological data over the entire domain at an hourly resolution for selected heights above the surface. Variables included here are wind speed, wind direction, temperature, etc. In addition, surface variables such as the surface pressure of downward solar radiation are available in two-dimensional format at hourly resolution.&lt;/li&gt; 
 &lt;li&gt;For 600 locations distributed over the domain many meteorological variables are provided over the entire atmospheric column with a 10-minute resolution. Turbine data gives energy production, wind speed and thrust for all the simulated turbines, i.e. more than 10,000 for the 2050 scenario. Turbine data is available at 5-minute resolution.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There are many possible use cases for the data, including scientific fields from the atmospheric sciences and energy system sciences, too. Within the energy sector, the data can inform grid operators, wind farm owners, developers of new wind farms and energy traders. Finally, the data can be used to train AI based models, opening more use cases where computationally-fast models are required.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;As we forge ahead in the renewable energy transition, there are bound to be both opportunities and challenges on the horizon. The growing popularity of wind energy presents a unique opportunity to transition towards a more sustainable future. By harnessing the power of the wind, we can significantly reduce our carbon footprint and combat climate change.&lt;/p&gt; 
&lt;p&gt;Advancements in technology, like Whiffle’s simulation tools coupled with the use of HPC clusters on AWS, offer new avenues for improving the efficiency and accuracy of wind energy simulations.&lt;/p&gt; 
&lt;p&gt;But there are challenges. Wildlife impact and visual concerns must be carefully addressed to ensure the widespread adoption of wind energy is safe and beneficial. Acknowledging these challenges and leveraging the power of simulation and HPC, we can work to achieve that.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Choosing the right compute orchestration tool for your research workload</title>
		<link>https://aws.amazon.com/blogs/hpc/choosing-the-right-compute-orchestration-tool-for-your-research-workload/</link>
		
		<dc:creator><![CDATA[Patrick Guha]]></dc:creator>
		<pubDate>Tue, 19 Mar 2024 13:52:27 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">dacdd14c4bfac6c9245765550bc892676dd27683</guid>

					<description>Running big research jobs on AWS but not sure where to start? We break down options like Batch, ECS, EKS, and others to pick the right tool for your needs. Lots of examples for genomics, ML, engineering, and more!</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright wp-image-3375 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/boofla88_a_scientist_choosing_between_several_options_manga_sty_6b82dff1-510f-404c-97aa-360c46741720.png" alt="Choosing the right compute orchestration tool for your research workload" width="380" height="212"&gt;&lt;/p&gt; 
&lt;p&gt;Research organizations around the world run large-scale simulations, analyses, models, and other distributed, compute-intensive workloads on AWS every day. These jobs depend on an orchestration layer to coordinate tasks across the compute fleet.&lt;/p&gt; 
&lt;p&gt;As a researcher or systems administrator providing services for researchers, it can be difficult to choose which AWS service or solution to use because there are various options for different kinds of workloads.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll describe some typical research use cases and explain which AWS tool we think best fits that workload.&lt;/p&gt; 
&lt;h2&gt;Understanding your workload&lt;/h2&gt; 
&lt;p&gt;Before diving into the specifics of each tool, it’s important to understand the nature of your workload.&lt;/p&gt; 
&lt;p&gt;Factors like the requirement for tightly coupled processes, the use of containers, the need for machine learning capabilities, or the necessity for a cloud desktop are pivotal in your decision-making process.&lt;/p&gt; 
&lt;p&gt;Research is not a monolith, so AWS supports a diverse range of HPC-based research, from engineering simulations and drug discovery to genomics, machine learning (ML), financial risk analysis, and social sciences.&lt;/p&gt; 
&lt;p&gt;Also: the tool you choose is not exclusive. Customers can have a mix of solutions to meet their needs all in the same account.&lt;/p&gt; 
&lt;h2&gt;A deep dive into AWS compute orchestration tools&lt;/h2&gt; 
&lt;h3&gt;AWS ParallelCluster for classic HPC clusters&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; is a flexible tool for building and managing HPC clusters on AWS. It’s ideal for tightly coupled workloads, like running simulations or analytics that require a traditional HPC cluster. It supports &lt;a href="https://aws.amazon.com/hpc/efa/"&gt;Elastic Fabric Adapter&lt;/a&gt; (EFA) networking out-of-the-box for low latency and high throughput inter-instance communication, and a high-performance file system (Lustre – available through the &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt; managed service).&lt;/p&gt; 
&lt;p&gt;ParallelCluster provides a familiar interface with a job scheduler &amp;nbsp;– Slurm – making it easy to migrate or burst workloads from an on-premises cluster environment you’re possibly already using.&lt;/p&gt; 
&lt;div id="attachment_3432" style="width: 1234px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3432" loading="lazy" class="wp-image-3432 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/20/CleanShot-2024-03-20-at-12.55.01.png" alt="" width="1224" height="533"&gt;
 &lt;p id="caption-attachment-3432" class="wp-caption-text"&gt;Figure 1 – Overview of AWS ParallelCluster and its components for HPC workloads. Integration with Slurm and Amazon EC2 right-sizes compute node numbers based on the job queue. Amazon FSx for Lustre allows for access to a high-performance file system while also taking advantage of Amazon S3 object storage. All of this is connected using Elastic Fabric Adapter (EFA) which provide extremely high-performance connectivity and scaling for tightly-coupled workloads.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;AWS Batch for container-based jobs&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; is suited for highly parallel, container-based jobs, including tightly-coupled workloads. It provides a fully-managed scheduler with seamless integration into container orchestrators, like &lt;a href="https://aws.amazon.com/eks/"&gt;Amazon Elastic Kubernetes Service&lt;/a&gt; (EKS) and &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon Elastic Container Service&lt;/a&gt; (ECS), allowing researchers to leverage existing containerized applications. A typical workload might involve independently running jobs on generic/non-specific compute, leveraging native AWS integrations, or requiring horizontal scalability through MPI or NCCL.&lt;/p&gt; 
&lt;div id="attachment_3367" style="width: 733px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3367" loading="lazy" class="size-full wp-image-3367" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.13.58.png" alt="Figure 2 – AWS Batch workflow illustrating container-based job processing and integration with AWS services. Compatibility with Amazon EKS and ECS allows for flexibility at the Compute Environment layer. " width="723" height="361"&gt;
 &lt;p id="caption-attachment-3367" class="wp-caption-text"&gt;Figure 2 – AWS Batch workflow illustrating container-based job processing and integration with AWS services. Compatibility with Amazon EKS and ECS allows for flexibility at the Compute Environment layer.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Amazon SageMaker for machine learning projects&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/sagemaker/"&gt;Amazon SageMaker&lt;/a&gt; is ideal for machine learning workloads, especially those developed in Jupyter Notebooks. While not focused on foundational building blocks for research computing, it instead provides a managed ecosystem of ML and data science tools, covering the entire spectrum from data discovery and exploration to model training and deployment.&lt;/p&gt; 
&lt;p&gt;SageMaker notebooks provide an interactive development environment, allowing researchers to develop and test models easily. SageMaker also contains pre-trained models, allowing researchers to jump-start their ML projects. It also provides managed inference endpoints, making it easier to deploy models and serve predictions.&lt;/p&gt; 
&lt;div id="attachment_3368" style="width: 587px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3368" loading="lazy" class="wp-image-3368 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.14.53.png" alt="Figure 3 – Amazon SageMaker ecosystem showcasing high-level end-to-end process from data preparation to model deployment. Integrates with services like Amazon EFS for a local file system in notebooks and also with highly-optimized AWS Deep Learning Containers for training models." width="577" height="309"&gt;
 &lt;p id="caption-attachment-3368" class="wp-caption-text"&gt;Figure 3 – &lt;a href="https://aws.amazon.com/pm/sagemaker"&gt;Amazon SageMaker&lt;/a&gt; ecosystem showcasing high-level end-to-end process from data preparation to model deployment. Integrates with services like &lt;a href="https://aws.amazon.com/efs/"&gt;Amazon EFS&lt;/a&gt; for a local file system in notebooks and also with highly-optimized AWS Deep Learning Containers for training models.&lt;/p&gt;
&lt;/div&gt; 
&lt;h4&gt;Let’s talk about the underlying compute resources&lt;/h4&gt; 
&lt;p&gt;The three services we just mentioned can take advantage of Amazon Elastic Compute Cloud (Amazon EC2) &lt;a href="https://aws.amazon.com/ec2/spot/"&gt;Spot Instances&lt;/a&gt; and also &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt;. Spot Instances are spare EC2 capacity, offered at a discounted rate but they can be reclaimed with a 2-minute warning.&lt;/p&gt; 
&lt;p&gt;SageMaker, Batch, and ParallelCluster can all use Spot Instances to take advantage of their favorable economics. In the case of Spot Instances, you’ll need to verify that your workload can tolerate interruptions from reclaimed capacity, or that the service can shift load on your behalf to avoid interrupting your processes. There are AWS technology partners, like &lt;a href="https://aws.amazon.com/marketplace/seller-profile?id=3b7c724c-fae7-4187-ae45-de1625e51395"&gt;MemVerge&lt;/a&gt;, that can handle this for you using &lt;a href="https://aws.amazon.com/blogs/hpc/save-up-to-90-using-ec2-spot-even-for-long-running-hpc-jobs/"&gt;OS-level memory checkpointing&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Fargate is a serverless compute engine that can be used for workloads running on Batch with ECS, and in native ECS and EKS clusters. It abstracts away the need for additional servers or infrastructure-related parameters (like instance type) to run containers. Fargate also has a few caveats regarding hardware specifications which you can find &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/fargate.html#when-to-use-fargate"&gt;in our documentation&lt;/a&gt;. But – generally speaking – it’s worthwhile to see if you can use Spot or Fargate with AWS orchestration tools for your research.&lt;/p&gt; 
&lt;h3&gt;Amazon Lightsail for Research for individual cloud desktops&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/lightsail/research/"&gt;Amazon Lightsail for Research&lt;/a&gt; is a simple (but powerful) solution for researchers looking for a predictably-priced, all-in-one cloud desktop. It’s tailored specifically for researchers, providing hardware specifications that are optimized for efficient research and a seamless user experience. Lightsail offers a range of pre-configured virtual private servers that can be customized to meet researchers’ needs and comes with research applications, like Scilab and RStudio. With its easy-to-use interface and affordable pricing, Lightsail for Research provides a reliable and efficient way for researchers to get started with AWS.&lt;/p&gt; 
&lt;div id="attachment_3369" style="width: 662px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3369" loading="lazy" class="size-full wp-image-3369" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.15.11.png" alt="Figure 4 – Researchers can use Amazon Lightsail for Research’s simplified management interface and options to deploy their favorite applications like Jupyter, RStudio, and Scilab. " width="652" height="319"&gt;
 &lt;p id="caption-attachment-3369" class="wp-caption-text"&gt;Figure 4 – Researchers can use Amazon Lightsail for Research’s simplified management interface and options to deploy their favorite applications like Jupyter, RStudio, and Scilab.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Research and Engineering Studio on AWS for managing cloud desktops at scale&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/hpc/res/"&gt;Research and Engineering Studio on AWS&lt;/a&gt; (RES) is an open-source web-based portal for administrators to create and manage secure, cloud-based research and engineering environments. It is ideal for research organizations that want a central IT team to easily manage the underlying infrastructure for multiple research environments. It provides one-click deployment for getting started quickly but can be customized to meet an organization’s specific needs.&lt;/p&gt; 
&lt;p&gt;Administrators can create virtual collaboration spaces for specific sets of users to access shared resources and collaborate. Users get a single pane of glass for launching and accessing virtual desktops to conduct scientific research, product design, engineering simulations, or data analysis workloads.&lt;/p&gt; 
&lt;div id="attachment_3371" style="width: 707px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3371" loading="lazy" class="size-full wp-image-3371" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.15.57.png" alt="Figure 5 – Researchers and admins alike can leverage RES to create Engineering Virtual Desktops (eVDI) backed by Amazon EC2. The RES Virtual Desktop screen shown here lists all the eVDI sessions a user created with controls to spin up, shut down, or schedule uptime. " width="697" height="404"&gt;
 &lt;p id="caption-attachment-3371" class="wp-caption-text"&gt;Figure 5 – Researchers and admins alike can leverage RES to create Engineering Virtual Desktops (eVDI) backed by Amazon EC2. The RES Virtual Desktop screen shown here lists all the eVDI sessions a user created with controls to spin up, shut down, or schedule uptime.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;AWS HealthOmics for bioinformatics&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/healthomics/"&gt;AWS HealthOmics&lt;/a&gt; is a comprehensive solution for bioinformatics work. It facilitates raw genomic storage and processing, allowing researchers to store and analyze genomic data. It supports popular bioinformatics workflow definition languages like WDL and Nextflow, enabling researchers to process and analyze genomic data efficiently.&lt;/p&gt; 
&lt;p&gt;HealthOmics even offers researchers the choice to bring their own workflow or use pre-built &lt;a href="https://docs.aws.amazon.com/omics/latest/dev/service-workflows.html"&gt;Ready2Run workflows&lt;/a&gt;. Ready2Run workflows are designed by industry leading third-party software companies like Sentieon, Inc. and NVIDIA, and includes common open-source pipelines like AlphaFold for protein structure prediction. Ready2Run workflows don’t require you to manage software tools or workflow scripts – this can save researchers significant amounts of time.&lt;/p&gt; 
&lt;div id="attachment_3372" style="width: 1017px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3372" loading="lazy" class="size-full wp-image-3372" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.17.22.png" alt="Figure 6 – AWS HealthOmics platform structure highlighting genomic data processing and analysis capabilities. Raw sequence and reference data can be processed through Nextflow or WDL workflows and then analyzed via AWS analytics services such as Amazon Athena." width="1007" height="286"&gt;
 &lt;p id="caption-attachment-3372" class="wp-caption-text"&gt;Figure 6 – AWS HealthOmics platform structure highlighting genomic data processing and analysis capabilities. Raw sequence and reference data can be processed through Nextflow or WDL workflows and then analyzed via AWS analytics services such as &lt;a href="https://aws.amazon.com/athena/"&gt;Amazon Athena&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Leveraging next-generation serverless technologies&lt;/h3&gt; 
&lt;p&gt;In the past decade, AWS has pioneered the field of serverless computing. Serverless computing is a model where you can build and deploy applications without managing server infrastructure. Instead of spinning up a full virtual machine that comes with overhead like patching and monitoring, you can abstract it away and focus just on the code or process you intend to run.&lt;/p&gt; 
&lt;p&gt;This is great for use cases like event handling or asynchronous tasks, but researchers have been using serverless computing to speed up embarrassingly parallel workloads, too – including ML hyperparameter optimization, genome search, and even MapReduce.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; is a serverless interface for running code without managing servers. It’s designed for loosely coupled workloads, allowing researchers to run code in response to events like changes in data (or the arrival of data). Lambda scales automatically, enabling you to run thousands of concurrent executions.&lt;/p&gt; 
&lt;p&gt;Even better: Lambda integrates with more than 200 other AWS services, making it easier for you to build quite complex workflows. It provides integration with &lt;a href="https://aws.amazon.com/step-functions/"&gt;AWS Step Functions&lt;/a&gt;, too, which means you can create visual workflows and construct multi-step, distributed applications. Lambda, combined with Step Functions, is useful for workloads that involve different compute steps, data needs, and may even require decision gates or human input.&lt;/p&gt; 
&lt;div id="attachment_3373" style="width: 860px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3373" loading="lazy" class="size-full wp-image-3373" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.18.06.png" alt="Figure 7 – Illustration of a sample serverless computing architecture: a data stream of jobs to be processed is picked up by AWS Lambda and put into a downstream Amazon SQS queue. AWS Step Functions then reads from this queue and handles the heavy lifting of distributed compute orchestration via Lambda worker functions. Step Functions also leverages Amazon DynamoDB for workload state management, event handling with Amazon EventBridge and storing processed results in Amazon S3. " width="850" height="263"&gt;
 &lt;p id="caption-attachment-3373" class="wp-caption-text"&gt;Figure 7 – Illustration of a sample serverless computing architecture: a data stream of jobs to be processed is picked up by AWS Lambda and put into a downstream &lt;a href="https://aws.amazon.com/sqs/"&gt;Amazon SQS&lt;/a&gt; queue. AWS Step Functions then reads from this queue and handles the heavy lifting of distributed compute orchestration via Lambda worker functions. Step Functions also leverages &lt;a href="https://aws.amazon.com/dynamodb/"&gt;Amazon DynamoDB&lt;/a&gt; for workload state management, event handling with &lt;a href="https://aws.amazon.com/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt; and storing processed results in Amazon S3.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion: making the right choice&lt;/h2&gt; 
&lt;p&gt;Choosing the right AWS compute orchestration tool is not about finding a one-size-fits-all solution but about aligning the tool’s capabilities with the specific requirements of your workload. The nuances of your project, the nature of your data, and your computational needs &lt;em&gt;should&lt;/em&gt; guide your decision.&lt;/p&gt; 
&lt;p&gt;Start with a small workload to gauge the tool’s compatibility and scalability with your project. AWS has a comprehensive suite of services and is ready to support you at every step of your journey, ensuring that you have the right resources and environment to push the boundaries of your research.&lt;/p&gt; 
&lt;p&gt;AWS Partners are also here to help you implement these tools. Partners like &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-7shbi5vvdvezu"&gt;Ronin&lt;/a&gt; and &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-ppiyshk2oin3i?sr=0-1&amp;amp;ref_=beagle&amp;amp;applicationId=AWSMPContessa"&gt;Ansys&lt;/a&gt; bring valuable expertise that can accelerate your time to research on AWS.&lt;/p&gt; 
&lt;p&gt;We recommend consulting with your research team and AWS account team to help you make the best decision for your project. As your research evolves, AWS’s scalable and diverse computing environment will continue to provide the necessary tools and support to meet your computational needs. To dive deeper into the nuances of a few of the tools we touched on in this post, we encourage you to check out some of our previous posts about &lt;a href="https://aws.amazon.com/blogs/hpc/choosing-between-batch-or-parallelcluster-for-hpc/"&gt;Choosing between AWS Batch or AWS ParallelCluster for HPC&lt;/a&gt;, &lt;a href="https://aws.amazon.com/blogs/hpc/why-use-fargate-with-aws-batch-for-serverless-batch-compute/"&gt;why you should use Fargate with AWS Batch&lt;/a&gt;, and how you can &lt;a href="https://aws.amazon.com/blogs/hpc/save-up-to-90-using-ec2-spot-even-for-long-running-hpc-jobs/"&gt;save up to 90% using EC2 Spot&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
	</channel>
</rss>