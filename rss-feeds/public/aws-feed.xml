<?xml version="1.0" encoding="UTF-8" standalone="no"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" version="2.0">

<channel>
	<title>AWS HPC Blog</title>
	<atom:link href="https://aws.amazon.com/blogs/hpc/feed/" rel="self" type="application/rss+xml"/>
	<link>https://aws.amazon.com/blogs/hpc/</link>
	<description>Just another Amazon Web Services site</description>
	<lastBuildDate>Wed, 15 May 2024 19:24:34 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>Using machine learning to drive faster automotive design cycles</title>
		<link>https://aws.amazon.com/blogs/hpc/using-machine-learning-to-drive-faster-automotive-design-cycles/</link>
		
		<dc:creator><![CDATA[Steven Miller]]></dc:creator>
		<pubDate>Mon, 13 May 2024 14:40:10 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AI]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">2c1df43a0e4299fe6554a7b7f672836e27c25a46</guid>

					<description>Aerospace and automotive companies are speeding up their product design using AI. In this post we'll discuss how they're using machine learning to shift design cycles from hours to seconds using surrogate models.</description>
										<content:encoded>&lt;p&gt;The automotive product engineering process involves months or years of iterative design reviews and refinement, with back-and-forth feedback between stakeholders regularly to adjust designs and evaluate the impact of design changes on engineering metrics like the coefficient of drag. Between each design iteration, engineers wait hours or days for simulations to complete, which means they can only execute a handful of design decisions each week.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll show how automakers can reduce cycle times from hours to seconds by leveraging surrogate machine-learning (ML) models in place of HPC, physics-based simulations and create subtle design variations for non-parametric geometries.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;In short&lt;/strong&gt;: automotive engineers can use emerging ML methodologies to speed up the product engineering process.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;Typically, the automotive product design process involves conceptualization, computer-aided design (CAD) geometry creation, and engineering simulations for validating the designs to ensure aerodynamic efficiency and desired structural stability.&lt;/p&gt; 
&lt;p&gt;Recent advances in AI/ML and Generative AI technology including algorithms such as MeshGraphNets, U-Nets and Variational Autoencoders, have provided a path towards accelerating the design process through fast ML inferences which allow us to run fewer full-fidelity physics-based HPC simulations, which can take hours, while gathering insight into large design spaces.&lt;/p&gt; 
&lt;p&gt;In this work, we’ll focus on computational fluid dynamics (CFD) simulations for intelligent aerodynamic surface design, though there are often other design considerations like impact response, noise, vibration and harshness.&lt;/p&gt; 
&lt;p&gt;The goal of this work is &lt;em&gt;not&lt;/em&gt; to develop a comprehensive toolkit for aerodynamic design – we want to present a simple user experience to show how engineers can use scientific ML methods, together with AWS services to deliver business value. We’ll mainly use &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;Nice DCV&lt;/a&gt;, &lt;a href="https://aws.amazon.com/pm/serv-s3/"&gt;Amazon S3&lt;/a&gt; and &lt;a href="https://aws.amazon.com/pm/sagemaker/"&gt;Amazon SageMaker&lt;/a&gt;. We’ll also explain how to build a web application that uses ML and generative design to enhance these development processes.&lt;/p&gt; 
&lt;div id="attachment_3604" style="width: 857px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3604" class="size-full wp-image-3604" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.33.24.png" alt="Figure 1 – Overview of Workflow Structure incorporating Web Application UI overview, AI/ML Methodology and AWS Implementation" width="847" height="433"&gt;
 &lt;p id="caption-attachment-3604" class="wp-caption-text"&gt;Figure 1 – Overview of Workflow Structure incorporating Web Application UI overview, AI/ML Methodology and AWS Implementation&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Roadmap for this post&lt;/h2&gt; 
&lt;p&gt;Much like the workflow chart in Figure 1, we’ll divide this post into three parts.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, we’ll start by explaining how to build a minimally viable web application including the end user experience and core components starting from an AWS architecture blueprint, highlighting some key components.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Next&lt;/strong&gt;, we’ll dive deep into the underlying ML models including the training methodology and inference deployment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Finally&lt;/strong&gt;, we’ll wrap up with a brief discussion about how we imagine automakers using workflows like this in practice to augment and accelerate the product design lifecycles.&lt;/p&gt; 
&lt;h2&gt;Cloud implementation and AWS architecture&lt;/h2&gt; 
&lt;p&gt;Let’s look at how to quickly and securely implement a minimally-viable web application. Using that, we’ll generate ground truth meshes, run OpenFOAM simulations, and train the machine learning models using AWS HPC services.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AWS architecture.&lt;/strong&gt; We don’t want to completely replace HPC simulations – these are necessary for verification and validation. We want to enable engineers to quickly build a web application inside their environment, access it securely, and store all their related artifacts. For this reason, we’ve chosen a minimalist architecture and we’ll leave it up to the automaker to decide how to integrate this into their existing environment.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 2: AWS Architecture diagram for deployment on a g5 instance using DCV, with endpoints to retrieve data or optionally submit compute jobs on self-managed ML endpoints (AWS Batch) or Amazon SageMaker&lt;/em&gt;&lt;/p&gt; 
&lt;div id="attachment_3605" style="width: 665px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3605" loading="lazy" class="size-full wp-image-3605" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.34.05.png" alt="Figure 2: AWS Architecture diagram for deployment on a g5 instance using DCV, with endpoints to retrieve data or optionally submit compute jobs on self-managed ML endpoints (AWS Batch) or Amazon SageMaker" width="655" height="345"&gt;
 &lt;p id="caption-attachment-3605" class="wp-caption-text"&gt;Figure 2: AWS Architecture diagram for deployment on a g5 instance using DCV, with endpoints to retrieve data or optionally submit compute jobs on self-managed ML endpoints (AWS Batch) or Amazon SageMaker&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We’re hosting the web application on a g5.12xlarge Amazon Elastic Compute Cloud (Amazon EC2) instance. The &lt;a href="https://aws.amazon.com/ec2/instance-types/g5/"&gt;G5&lt;/a&gt; has 4 x A10G NVIDIA GPUs to serve the machine learning models and we’re using the &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;NICE DCV&lt;/a&gt; streaming protocol to securely stream the desktop to the end.&lt;/p&gt; 
&lt;p&gt;We’re also keeping the EC2 instance secure inside of a VPC and we only allow TCP traffic to flow from the instance to a single end user.&lt;/p&gt; 
&lt;p&gt;We train drag prediction models using Amazon SageMaker, and we securely store the inputs and outputs (including model artifacts) in Amazon Simple Storage Service (Amazon S3).&lt;/p&gt; 
&lt;p&gt;We use AWS Batch to launch the CFD simulations to create the ground truth data.&lt;/p&gt; 
&lt;p&gt;To keep all communications secure, we use VPC endpoints for all key services.&lt;/p&gt; 
&lt;h2&gt;Application workflow and user experience&lt;/h2&gt; 
&lt;p&gt;Now we can build the minimally viable application to show this new workflow. The web application layer consists of five main modules: application, inference configuration, pretrained models, processed meshes, and utilities.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Application.&lt;/strong&gt; The core application, built on the open-source &lt;a href="https://streamlit.io/"&gt;Streamlit&lt;/a&gt; library, serves the user interface and all the orchestration to deliver the user experience. We used a 3D plotting library (&lt;a href="https://docs.pyvista.org/version/stable/"&gt;PyVista&lt;/a&gt;) to display 3D visualizations and enable user interaction with STL meshes (original and design targets) and ML and physics-based computational fluid dynamics results, including pressure cross-sections and 3D streamlines.&lt;/p&gt; 
&lt;p&gt;In this example, we’ll begin with a choice of a base car model (coupe or estate/station-wagon), and then provide the user with design modification options. In this case, some features of the car (like front bumper shape, windscreen angle, trunk depth, and cabin height) are provided, but we could easily extend this to other design features we’re interested in.&lt;/p&gt; 
&lt;p&gt;For each of these combinations, the design variations are produced through non-parametric “morphing” of the base meshes using &lt;em&gt;radial basis function (RBF) interpolation&lt;/em&gt;. If we wanted to explore parametric design, and the base CAD geometries were available, we could potentially incorporate them into the workflow. The user interface is shown in Figure 3.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure3 - Using machine learning to drive faster automotive design cycles." width="500" height="375" src="https://www.youtube-nocookie.com/embed/2Z06wjAPVkA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Radial basis function (RBF) – &lt;/strong&gt;RBF interpolation lets us distort meshes at specific parts of the car, while keeping other features constant – including the chassis platform and tire radius. RBF relies on the concept of a deformation map between the original mesh and the deformed mesh and we get to control the extent of deformation using the slider in the user interface.&lt;/p&gt; 
&lt;p&gt;To enable engineers to show specifically how the mesh has been deformed, we’ve provided a side-by-side comparison of the original mesh and the new mesh in the UI (which you can see in Figure 4) before we pass the deformed mesh along to the ML model for inference.&lt;/p&gt; 
&lt;div id="attachment_3607" style="width: 831px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3607" loading="lazy" class="size-full wp-image-3607" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.34.57.png" alt="Figure 4 – Deformed geometry (left) with a wireframe overlay of original mesh highlighting change and original mesh (right)" width="821" height="348"&gt;
 &lt;p id="caption-attachment-3607" class="wp-caption-text"&gt;Figure 4 – Deformed geometry (left) with a wireframe overlay of original mesh highlighting change and original mesh (right)&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Inference configuration.&lt;/strong&gt; We trained a hierarchical machine learning model for a CFD flow field and drag predictions – we’ll explain that more soon. At runtime, we hosted these on different GPUs which allowed us to parallelize the inference of individual sub-models. To get inferences from the ML model, we specify the key inputs and parameters using inference configurations. These inputs include the decimated surface meshes of the car and a geometry for volume calculations. Additional input parameters include the batch size for inference, number of workers and other neural network associated hyperparameters.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Processed mesh.&lt;/strong&gt; When a user requests an inference, the application generates a processed &lt;a href="https://www.hdfgroup.org/solutions/hdf5/"&gt;HDF5&lt;/a&gt; file which contains the new mesh and the corresponding flow fields. These HDF5 files are then exposed to the end user via an interactive 3D visualization. There are some extra components that support the application: a function to convert meshes to HDF5 files, another function to apply the RBF to morph meshes based on user inputs, and finally a function to run inferences and gather outputs.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deployment.&lt;/strong&gt; Since this application is meant for demonstration purposes only, we designed it to run on a single compute instance, and chose a g5.12xlarge that includes 4 x GPUs that’s able to run the entire application. To speed the application, we distribute individual ML sub-models across the GPUs. To keep it all secure, we isolate the instances from the public internet in a private VPC. And we used NICE DCV to access the instance for an easier remote desktop experience.&lt;/p&gt; 
&lt;p&gt;Geometric and ML models&lt;/p&gt; 
&lt;p&gt;To enable engineers to rapidly iterate, potentially even &lt;em&gt;during&lt;/em&gt; design conversations, our web application predicts the &lt;em&gt;coefficient of drag&lt;/em&gt; (Cd) and the flow fields on unseen geometries. We used a hierarchy of ML models for the flow fields and drag predictions.&lt;/p&gt; 
&lt;p&gt;In both cases, we trained our models using synthetic data that we generated by morphing the open source &lt;a href="https://www.epc.ed.tum.de/en/aer/research-groups/automotive/drivaer/"&gt;DrivAer&lt;/a&gt; data set, and then ran full-fidelity CFD simulations on all these geometries using AWS Batch with the open-source &lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Synthetic data generation&lt;/strong&gt;. To create training data for this project, we used the RBF to strategically morph the underlying mesh of both the coupe and the SUV. This method has broad applicability and can be generically applied to many parametric and non-parametric STL meshes.&lt;/p&gt; 
&lt;p&gt;For each area of the car, we specified points in an overlayed 1000-point (10x10x10) cubic lattice, bounded by the dimensions of the car. This allowed us to create 400 variations (100 for each of the area of the car) that we used as the basis for ground-truth simulations. Figure 5 shows how we used the RBF to deform the mesh &lt;em&gt;and&lt;/em&gt; create the training data for the ML model. Each time frame in the video corresponds to a different deformed mesh.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Using machine learning to drive faster automotive design cycles - Figure5" width="500" height="281" src="https://www.youtube-nocookie.com/embed/aBcO03OBTm0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p style="text-align: left"&gt;&lt;em&gt;Figure 5 – Synthetic mesh generation using morphing of individual features of a car shown in sequence&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;After we created 400 variations of the base meshes, we used OpenFOAM to create CFD simulations with steady-state RANS simulations using the k-ω steady-state turbulence model with a fixed inlet velocity of 20m/s.&lt;/p&gt; 
&lt;p&gt;We ran these simulations in parallel using AWS Batch on c5.24xlarge instances using MPI for parallelism – this took around 7 hours to run. We could have chosen larger mesh sizes, but we settled on a mesh size that allowed resolving fine geometry – in a reasonable compute time.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Coefficient of drag with MeshGraphNets (MGN) –&lt;/strong&gt; MGN is a deep-learning framework for learning mesh-based simulations which represents the mesh as a graph where information is propagated across the graph’s nodes and edges. MGN excels at efficiently capturing unstructured mesh topologies and it’s a great fit for predicting Cd values. To train the MGN model, we used the synthetic data that we generated using the RBF, and OpenFOAM. During training, we provided the underlying mesh and the Coefficient of Drag (Cd) metrics as inputs.&lt;/p&gt; 
&lt;p&gt;We then trained a model on 320 samples, with each mesh decimated while preserving topological features for computational efficiency, containing approximately 6,000 nodes. We used a p3.2xlarge instance for 500 epochs, with a total training time of 46 minutes.&lt;/p&gt; 
&lt;p&gt;The model achieved a MAPE (&lt;em&gt;mean absolute percentage error&lt;/em&gt;) of 2.5% in drag coefficient (Cd) when predicting on unseen data, with an average inference time of 0.028 seconds per sample. Figure 6 shows the predicted vs. actual drag coefficient (Cd) plot for train, validation, and unseen test data (car meshes).&lt;/p&gt; 
&lt;div id="attachment_3609" style="width: 718px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3609" loading="lazy" class="size-full wp-image-3609" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.36.06.png" alt="Figure 6: Predicted vs. actual drag coefficient (Cd) plot for train, validation, and unseen test data (i.e. car meshes)." width="708" height="426"&gt;
 &lt;p id="caption-attachment-3609" class="wp-caption-text"&gt;Figure 6: Predicted vs. actual drag coefficient (Cd) plot for train, validation, and unseen test data (i.e. car meshes).&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Although the model didn’t match the ground truth CFD simulations exactly, this level of agreement from the MGN surrogate model may prove acceptable during the initial design iterations leading up to a final, high-fidelity simulation. After that comes real-world wind tunnel testing, which has an inherent uncertainty in measurements.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Flow Fields using deep convolutional neural networks (CNNs).&lt;/strong&gt; To predict pressure distributions and velocity fields for full 3D flow, we used a CNN architecture called &lt;a href="https://github.com/wolny/pytorch-3dunet"&gt;U-Net&lt;/a&gt; which has shown promise in high-resolution volume segmentation and regression tasks. This architecture included encoding and decoding convolutional layers with skip connections. We trained individual U-Nets for each of the pressure and velocity primary variables.&lt;/p&gt; 
&lt;p&gt;We converted the 3D volume outputs from 400 x OpenFOAM simulations into the training (375) and validation (25) ground-truth datasets. We used a p5.48xlarge instance with 8 x H100 GPUs for approximately 28 hours. We saw a volume RSME (&lt;em&gt;root-mean-square-error&lt;/em&gt;) of around 3% for pressure, and 5% for combined velocity magnitudes, for unseen data not in the training or validation sets – although this is dependent on the degree of mesh warping away from the initial baseline mesh. Figure 7 shows the differences in velocity slices an unseen mesh.&lt;/p&gt; 
&lt;div id="attachment_3610" style="width: 680px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3610" loading="lazy" class="size-full wp-image-3610" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.36.52.png" alt="Figure 7: ML Predicted (Top) vs Ground Truth OpenFOAM (Bottom) X-Component Velocity" width="670" height="516"&gt;
 &lt;p id="caption-attachment-3610" class="wp-caption-text"&gt;Figure 7: ML Predicted (Top) vs Ground Truth OpenFOAM (Bottom) X-Component Velocity&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Business value creation&lt;/h2&gt; 
&lt;p&gt;We’ve now shown how to rapidly build a minimally viable web interface that engineers can use to take advantage of these ML models to speed up the product engineering process. But now we need to couple this with a new business process so automakers can realize actual business value.&lt;/p&gt; 
&lt;div id="attachment_3611" style="width: 855px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3611" loading="lazy" class="size-full wp-image-3611" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.37.12.png" alt="Figure 8: Overall product design iterations using machine learning surrogate models to accelerate the cycles" width="845" height="168"&gt;
 &lt;p id="caption-attachment-3611" class="wp-caption-text"&gt;Figure 8: Overall product design iterations using machine learning surrogate models to accelerate the cycles&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Using the speed of the ML surrogate models, it’s possible to explore hundreds of designs per week, and we can now make meetings more collaborative.&lt;/p&gt; 
&lt;p&gt;Instead of reviewing results from prior runs, engineers can solicit feedback from their peers real-time0, and get the results in 20-60 seconds, avoiding costly wait times. This will require engineers to think differently about product design reviews and meetings. Engineers will need to set expectations accordingly and be prepared to guide conversations. But this enables engineers to explore a greater design space than they otherwise could – especially given budget or time constraints. We’re confident this approach will lead to new innovations.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;With the right combination of machine-learning and process change, automakers can use the architecture explained here to build tailored solutions to speed up the product engineering process, reduce the cost of compute, and explore a greater number of design options in a shorter period of time. If you or your team want to discuss the business case, a recommended implementation path with our Professional Services team, or technical solution blueprint in more detail, you can reach out to us at ask-hpc@amazon.com.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Announcing the High Performance Software Foundation (HPSF)</title>
		<link>https://aws.amazon.com/blogs/hpc/announcing-the-high-performance-software-foundation/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Mon, 13 May 2024 05:55:13 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">e91b47faf0b755001ca4998fd17206fcbdaf12f6</guid>

					<description>We're excited to share how we’re involved in launching the High Performance Software Foundation to increase access to and adoption of HPC. By bringing together key players to collaborate, we can lower barriers and accelerate development of portable HPC software stacks.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="size-full wp-image-3644 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/09/AdobeStock_144582125-resized.png" alt="Announcing the High Performance Software Foundation" width="380" height="253"&gt;&lt;/em&gt;In high performance computing (HPC), where speed and efficiency are paramount, open-source software provides the load-bearing, structural support. Our community has long recognized the immense potential of collaborative development, sharing of resources, and the power of community-driven innovation.&lt;/p&gt; 
&lt;p&gt;So we’re excited to announce that we’re a Premier founding member of the new &lt;strong&gt;High Performance Software Foundation&lt;/strong&gt; (HPSF). The HPSF was launched today in Hamburg at &lt;a href="https://www.isc-hpc.com/"&gt;ISC’24&lt;/a&gt;, by the Linux Foundation, the nonprofit organization that enables mass innovation through open-source. The HPSF has strong support across the HPC landscape.&lt;/p&gt; 
&lt;p&gt;Through a series of technical projects, the HPSF aims to build, promote, and advance a portable core software stack for HPC to increase adoption by lowering barriers to contribution, supporting open-source development efforts, and making HPC more accessible to all of us. We’ve seen from our work with RIKEN to &lt;a href="https://aws.amazon.com/blogs/publicsector/why-fugaku-japans-fastest-supercomputer-went-virtual-on-aws/"&gt;extend Fugaku to the cloud&lt;/a&gt;&amp;nbsp;how impactful this can be.&lt;/p&gt; 
&lt;p&gt;Recent years have seen extraordinary demand for HPC across domains which are critical to all of us, from climate modeling and genomics to drug-discovery and engineering design. With the emergence of machine learning and AI, there’s never been a more important task than to reinforce the structures which have made all of this possible.&lt;/p&gt; 
&lt;h2&gt;How can HPSF help?&lt;/h2&gt; 
&lt;p&gt;The HPSF aims to make life easier for HPC developers through a number of focused initiatives, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Continuous Integration (CI) resources tailored for HPC projects&lt;/li&gt; 
 &lt;li&gt;Continuously built, turnkey software stacks&lt;/li&gt; 
 &lt;li&gt;Architecture support&lt;/li&gt; 
 &lt;li&gt;Performance regression testing and benchmarking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AWS has been supporting many of these efforts for the &lt;a href="https://spack.io/"&gt;Spack&lt;/a&gt; community since 2021, leading to the launch of the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-the-spack-rolling-binary-cache/"&gt;Spack Rolling Binary Cache&lt;/a&gt; two years ago. We’ve all learned a lot from this relationship, which we hope the wider HPC open-source community will be able to benefit from, through the foundation.&lt;/p&gt; 
&lt;h2&gt;First steps&lt;/h2&gt; 
&lt;p&gt;One of HPSF’s first jobs is to set up the technical advisory committee (TAC) which will manage working groups tackling a variety of HPC topics. Drawing from member organizations and community participants, the TAC will follow a governance model based on the &lt;a href="https://www.cncf.io/"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF).&lt;/p&gt; 
&lt;p&gt;The HPSF launches today with the following technical projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Spack&lt;/strong&gt;: the HPC package manager&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Kokkos&lt;/strong&gt;: a performance-portable programming model for writing modern C++ applications in a hardware-agnostic way&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Viskores (formerly VTK-m)&lt;/strong&gt;: a toolkit of scientific visualization algorithms for accelerator architectures&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HPCToolkit&lt;/strong&gt;: performance measurement and analysis tools for computers ranging from laptops to GPU-accelerated supercomputers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Apptainer&lt;/strong&gt;: Formerly known as Singularity, Apptainer is a Linux Foundation project providing a high performance, full featured HPC and computing optimized container subsystem&lt;/li&gt; 
 &lt;li&gt;&lt;b&gt;E4S: &lt;/b&gt;a curated, hardened distribution of scientific software packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting involved&lt;/h2&gt; 
&lt;p&gt;The HPSF welcomes organizations from across the HPC community to become involved and help drive innovation in open-source HPC solutions.&lt;/p&gt; 
&lt;p&gt;To learn more about the HPSF, to contribute, or to become a member, you can visit the &lt;a href="https://hpsf.io/"&gt;HPSF website&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The HPSF’s commitment to supporting and nurturing these vital software packages will enable the HPC community to have access to the tools and resources necessary to push the boundaries of scientific discovery. We’re thrilled to be part of it.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Best practices for running molecular dynamics simulations on AWS Graviton3E</title>
		<link>https://aws.amazon.com/blogs/hpc/best-practices-for-running-molecular-dynamics-simulations-on-aws-graviton3e/</link>
		
		<dc:creator><![CDATA[Nathaniel Ng]]></dc:creator>
		<pubDate>Tue, 07 May 2024 13:55:18 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[GROMACS]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">1e63bf1a121b398b36fb135821ca6835db681e7f</guid>

					<description>If you run molecular dynamics simulations, you need to read this. We walk through running benchmarks of popular apps like GROMACS and LAMMPS on new Hpc7g instances and Graviton3E processors. The results - up to 35% better vector performance versus Graviton3! Learn how to optimize your own workflows.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Nathaniel Ng, Shun Utsui, and James Chen from AWS Solution Architecture&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Last year, &lt;a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc7g-instances-powered-by-aws-graviton3e-processors-optimized-for-high-performance-computing-workloads/"&gt;we announced&lt;/a&gt; the general availability of Hpc7g instances, the instance type focused on HPC workloads powered by AWS Graviton3E. Graviton3E processors deliver up to 35% higher vector-instruction performance &lt;a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc7g-instances-powered-by-aws-graviton3e-processors-optimized-for-high-performance-computing-workloads/"&gt;compared to Graviton3&lt;/a&gt;, providing higher performance benefits for HPC applications.&lt;/p&gt; 
&lt;p&gt;Molecular dynamics (MD) is a domain that frequently leverages HPC resources. Previously, customers ran their MD workloads using predominantly x86 architectures, but we’ve heard that many are interested in understanding the performance they can get on Graviton3E.&lt;/p&gt; 
&lt;p&gt;So, in this post, we’ll show how you can run MD workloads on Hpc7g instances using AWS ParallelCluster, a supported open-source cluster management tool that allows you to deploy a scalable HPC environment on AWS in a matter of minutes. We’ll use &lt;a href="https://www.gromacs.org/"&gt;GROMACS&lt;/a&gt; and &lt;a href="https://www.lammps.org/"&gt;LAMMPS&lt;/a&gt; as examples – two very popular MD applications. And we’ll highlight the best practices for the tools – and the compiler flags – we used to achieve optimal performance.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Key to the architecture is AWS ParallelCluster. Customers can install the ParallelCluster CLI on their laptops using Python’s pip package manager, and use it to deploy a cluster in the cloud from their laptops by supplying a short configuration file.&lt;/p&gt; 
&lt;p&gt;Through the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcui-using-v3.html"&gt;ParallelCluster UI&lt;/a&gt;, you can use a wizard to configure a cluster without editing a configuration file or installing anything on your laptop. Details on how to set up a ParallelCluster environment can be found in &lt;a href="https://www.hpcworkshops.com/01-hpc-overview.html"&gt;this workshop&lt;/a&gt;. You can also find a &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/try_hpc7g"&gt;one-click launchable stack&lt;/a&gt; in the HPC Recipe Library (on GitHub), which will build an Hpc7g cluster for you after asking a minimum of questions.&lt;/p&gt; 
&lt;div id="attachment_3496" style="width: 831px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3496" loading="lazy" class="size-full wp-image-3496" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.40.13.png" alt=" Figure 1: Architecture using AWS ParallelCluster to run MD workloads with AWS Graviton 3E instances. " width="821" height="378"&gt;
 &lt;p id="caption-attachment-3496" class="wp-caption-text"&gt;&lt;br&gt;Figure 1: Architecture using AWS ParallelCluster to run MD workloads with AWS Graviton 3E instances.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;ParallelCluster uses Slurm for its job scheduler, to dynamically scale the number of Hpc7g.16xlarge compute instances, responding to the queue.&lt;/p&gt; 
&lt;p&gt;These instances are powered by custom-built AWS Graviton3E processors. They feature the latest DDR5 memory offering 50% more bandwidth compared to DDR4, and they carry 200Gbps network interfaces with the Elastic Fabric Adapter (EFA). Graviton3E processors implement Scalable Vector Extension (SVE) of the Neoverse V1 architecture and hence can deliver up to 2x better performance for floating point codes than Graviton2.&lt;/p&gt; 
&lt;p&gt;To achieve sufficient performance on storage I/O, we deployed 4.8 TB of Amazon FSx for Lustre – a fully-managed, high-performance Lustre file system. We selected the &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/using-fsx-lustre.html"&gt;PERSISTENT_2&lt;/a&gt; deployment type with a disk throughput of &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/performance.html"&gt;1000 MBps/TiB of storage provisioned&lt;/a&gt;, and backed it with an Amazon Simple Storage Service (Amazon S3) bucket.&lt;/p&gt; 
&lt;p&gt;We’ve documented all the details of our ParallelCluster configuration in our &lt;a href="https://github.com/aws-samples/aws-graviton-md-example"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Development tooling&lt;/h2&gt; 
&lt;p&gt;We tested several configurations and concluded that we recommend using the following compilers and libraries for most use cases. You can check our &lt;a href="https://github.com/aws-samples/aws-graviton-md-example"&gt;GitHub repository&lt;/a&gt; to find out best practices for compiler flags when building MD applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Compiler&lt;/strong&gt;: Arm compiler for Linux (ACfL) version 23.04 or later&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Library&lt;/strong&gt;: Arm performance libraries (ArmPL) version 23.04 or later, included in ACfL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MPI&lt;/strong&gt;: Open MPI version 4.1.5 or later&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It’s worth noting that Arm Compilers and Performance Libraries for HPC developers are &lt;a href="https://community.arm.com/arm-community-blogs/b/high-performance-computing-blog/posts/arm-compilers-and-libraries-for-hpc-now-free"&gt;now available for no cost&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In the rest of this post, we’ll explain why we preferred these tools, by diving deeper into the performance of GROMACS and LAMMPS with different compilers, compiler options, and input files.&lt;/p&gt; 
&lt;h2&gt;GROMACS&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.gromacs.org/about.html"&gt;GROMACS&lt;/a&gt; is an open-source software suite for high-performance molecular dynamics and output analysis. It’s widely adopted in the computer simulation fields not only for biochemical molecules but also for non-biological systems like polymers and fluid dynamics. The GROMACS community have optimized it to make great use of the SIMD capabilities of many modern HPC architectures.&lt;/p&gt; 
&lt;p&gt;For Arm architectures, the &lt;a href="https://manual.gromacs.org/current/install-guide/index.html#simd-support"&gt;code supports both NEON (ASIMD) and SVE instructions&lt;/a&gt;. In this post, we used GNU 12.2 and the Arm compiler for Linux (ACfL) 23.04 as the testing compiler suite, with Arm Performance Library (ArmPL) 23.04 for the math library, and Open MPI 4.1.5 linked with Libfabric to build different binary executables of GROMACS 2022.5.&lt;/p&gt; 
&lt;h3&gt;Build scripts&lt;/h3&gt; 
&lt;p&gt;Our main objective was to find out the best compiler suite – and SIMD support fit – for Graviton3E-based Hpc7g instances. We’ll discuss the build scripts, job submission scripts, and performance data here – and you can find all the scripts in our &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/tree/main/codes/GROMACS"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For GROMACS’s build script, there’s no difference in the CMake options for GNU and Arm compiler for Linux.&lt;/p&gt; 
&lt;p&gt;For GNU compilers, the Open MPI 4.1.5 environment is already installed in ParallelCluster’s machine images. We recommend anyone new to the cloud to use the system default Open MPI library. However, the system default Open MPI does &lt;em&gt;not&lt;/em&gt; support ACfL, so we’ve &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/setup/2-install-openmpi-with-acfl.sh"&gt;supplied a script&lt;/a&gt; which demonstrates how to compile and install Open MPI 4.1.5 with ACfL. Once this is installed, we can use the bash module environments to switch between the GNU and Arm compiler.&lt;/p&gt; 
&lt;p&gt;With this done, it’s now possible to build executables for GROMACS 2022.5 for both SVE or NEON/ASIMD instruction sets.&lt;/p&gt; 
&lt;p&gt;We’ve stored the procedures for this in yet another &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/tree/main/codes/GROMACS"&gt;reference script&lt;/a&gt;. We only need to change the parameter &lt;code&gt;GMX_SIMD&lt;/code&gt; in the configuration setup. For the SVE-enable binary, the parameter is –&lt;code&gt;DGMX_SIMD=ARM_SVE&lt;/code&gt;. For building the NEON/ASIMD-enabled binary, you’ll need to switch the parameter to &lt;code&gt;-DGMX_SIMD= ARM_NEON_ASIMD&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Test cases&lt;/h3&gt; 
&lt;p&gt;We applied three standard test cases for GROMACS from the &lt;a href="https://repository.prace-ri.eu/git/UEABS/ueabs"&gt;Unified European Application Benchmark Suite (UEABS)&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://repository.prace-ri.eu/ueabs/GROMACS/2.2/GROMACS_TestCaseA.tar.xz"&gt;Test Case A &lt;/a&gt;is the ion channel system of the membrane protein GluCl embedded in a DOPC membrane and solvated in TIP3P water. This system contains 142k atoms.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://repository.prace-ri.eu/ueabs/GROMACS/2.2/GROMACS_TestCaseB.tar.xz"&gt;Test Case B&lt;/a&gt; is a model of cellulose and lignocellulosic biomass in an aqueous solution. The system has 3.3 million atoms.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://repository.prace-ri.eu/ueabs/GROMACS/2.2/GROMACS_TestCaseC.tar.xz"&gt;Test Case C&lt;/a&gt; is the standard test case for NAMD benchmarks. The system is a 3 x 3 x 3 replica of the STMV (&lt;em&gt;Satellite Tobacco Mosaic Virus&lt;/em&gt;). The size of model is about 28 million atoms.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For all these cases, we set the total simulation steps to 10k.&lt;/p&gt; 
&lt;p&gt;To find out the best combination of compiler and SIMD setup, we started to test the application performance on a single Hpc7g.16xlarge instance. Figure 2 charts the performance we saw for test case A (142K atoms). ACfL with SVE-enabled generated the best performance. The binary this generated was about 9-10% faster than the one using NEON/ASIMD. The binary produced by the ACfL with SVE ran 6% faster than when we used the GNU compiler with SVE.&lt;/p&gt; 
&lt;div id="attachment_3497" style="width: 714px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3497" loading="lazy" class="size-full wp-image-3497" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.41.42.png" alt="Figure 2: Performance of GROMACS 2022.5 for GluCl Ion Channel system (142K atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enable binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs. " width="704" height="466"&gt;
 &lt;p id="caption-attachment-3497" class="wp-caption-text"&gt;Figure 2: Performance of GROMACS 2022.5 for GluCl Ion Channel system (142K atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enable binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 3 charts the performance we saw for test case B (3.3M atoms). The best setup we found – again – was to use the ACfL with SVE enabled. We measured a 28% improvement for the SVE-enabled binary, compared with the NEON/ASIMD-enable binary when we used this compiler.&lt;/p&gt; 
&lt;div id="attachment_3498" style="width: 717px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3498" loading="lazy" class="size-full wp-image-3498" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.42.04.png" alt="Figure 3: Performance of GROMACS 2022.5 for cellulose and lignocellulosic biomass (3.3M atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enabled binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs. " width="707" height="465"&gt;
 &lt;p id="caption-attachment-3498" class="wp-caption-text"&gt;Figure 3: Performance of GROMACS 2022.5 for cellulose and lignocellulosic biomass (3.3M atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enabled binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The performance results for test case C (28M atoms) are shown in Figure 4. Again, we saw a similar pattern to before: the ACfL with SVE enabled was the best option for GROMACS running on Hpc7g instance. In this case, the performance delta was 19% compared to NEON-ASIMD. And again, the binary generated by ACfL was 6% faster than the one built by the GNU compiler.&lt;/p&gt; 
&lt;div id="attachment_3499" style="width: 716px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3499" loading="lazy" class="size-full wp-image-3499" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.42.44.png" alt="Figure 4: Performance of GROMACS 2022.5 for Satellite Tobacco Mosaic Virus, STMV (28M atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enable binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs. " width="706" height="462"&gt;
 &lt;p id="caption-attachment-3499" class="wp-caption-text"&gt;Figure 4: Performance of GROMACS 2022.5 for Satellite Tobacco Mosaic Virus, STMV (28M atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enable binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Based on the results running on single Hpc7g instance, we concluded that ACfL with SVE-enabled SIMD, generated the best performance results.&lt;/p&gt; 
&lt;p&gt;By default, the configuration tool in GROMACS detected the CPU automatically, and selected the SIMD support correctly, too. GROMACS also detected the configuration of SIMD correctly for AWS Graviton3.&lt;/p&gt; 
&lt;p&gt;We chose test case C to highlight the scalability for performance on Hpc7g when we ran across multiple nodes. The result charted in Figure 5 confirms this – scalability of performance is near-linear with EFA enabled.&lt;/p&gt; 
&lt;div id="attachment_3500" style="width: 895px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3500" loading="lazy" class="size-full wp-image-3500" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.43.17.png" alt="Figure 5: Scalability of Test Case C running on the Hpc7g-based cluster with, and without, EFA. 200Gpbs EFA contributes the scalability in the cases running beyond 2 compute instances. The binary was compiled with ACfL with SVE-enabled. All the data points are based on the average of three individual runs. " width="885" height="579"&gt;
 &lt;p id="caption-attachment-3500" class="wp-caption-text"&gt;Figure 5: Scalability of Test Case C running on the Hpc7g-based cluster with, and without, EFA. 200Gpbs EFA contributes the scalability in the cases running beyond 2 compute instances. The binary was compiled with ACfL with SVE-enabled. All the data points are based on the average of three individual runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Conclusion for GROMACS&lt;/h3&gt; 
&lt;p&gt;Based on these performance results for three quite different, but well-known test cases, we found that SVE-enabled binary was faster than using ASIMD/Neon instructions. ACfL produced faster code than the GNU compiler. Specifically for Hpc7g, ACfL with SVE-enabled SIMD setting was the best configuration when paired with the latest ArmPL and Open MPI with EFA enabled.&lt;/p&gt; 
&lt;h2&gt;LAMMPS&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.lammps.org/"&gt;LAMMPS&lt;/a&gt; (Large-scale Atomic/Molecular Massively-Parallel Simulator) is a classical molecular dynamics simulator, used for particle-based modelling of materials. It was developed at Sandia National Laboratories, and is available as an open-source tool, distributed under GPLv2. You can download the source code from the &lt;a href="https://github.com/lammps/lammps/releases"&gt;LAMMPS GitHub repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In terms of algorithms, LAMMPS uses parallel spatial decomposition, as well as parallel FFTs for long-range Coloumbic interactions. The computational cost of scaling with the number of atoms is O(N) for short-range interactions, and O(N log N) when computations involve Coloumbic interactions with FFT-based methods.&lt;/p&gt; 
&lt;p&gt;To prepare for LAMMPS running on AWS, we can follow the same steps as before for GROMACS for the AWS ParallelCluster, GCC, ACfL, and Open MPI setup. The LAMMPS &lt;a href="https://docs.lammps.org/Install.html"&gt;install guide&lt;/a&gt; has several options for installation, but to use the latest version of LAMMPS, and more recent versions of GCC, ACfL, and Open MPI, LAMMPS must be compiled from source.&lt;/p&gt; 
&lt;p&gt;LAMMPS has specific Makefiles that are optimized for Arm architecture available: &lt;a href="https://github.com/lammps/lammps/blob/develop/src/MAKE/MACHINES/Makefile.aarch64_arm_openmpi_armpl"&gt;Makefile.aarch64_arm_openmpi_armpl&lt;/a&gt; and &lt;a href="https://github.com/lammps/lammps/blob/develop/src/MAKE/MACHINES/Makefile.aarch64_g++_openmpi_armpl"&gt;Makefile.aarch64_g++_openmpi_armpl&lt;/a&gt;, for use with ACfL and GCC compilers respectively. In the case of ACfL, we used &lt;code&gt;-march=armv8-a+sve&lt;/code&gt; &lt;a href="https://developer.arm.com/documentation/102476/0100/Programming-with-SVE/Auto-vectorization"&gt;to add the SVE instructions&lt;/a&gt;, and checked that switching between &lt;code&gt;-march=armv8-a+sve&lt;/code&gt; and &lt;code&gt;-march=armv8-a+simd&lt;/code&gt; did not impact performance. For &lt;em&gt;both&lt;/em&gt; ACfL and GCC Makefiles, we added &lt;code&gt;-fopenmp&lt;/code&gt; to &lt;code&gt;CCFLAGS&lt;/code&gt; and &lt;code&gt;LINKFLAGS&lt;/code&gt; to enable OpenMP. We’ve summarized the final settings in Table 1.&lt;/p&gt; 
&lt;div id="attachment_3501" style="width: 927px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3501" loading="lazy" class="wp-image-3501 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.44.36.png" alt="Table 1: Compiler settings for GCC and ACfL" width="917" height="247"&gt;
 &lt;p id="caption-attachment-3501" class="wp-caption-text"&gt;Table 1: Compiler settings for GCC and ACfL&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To compile LAMMPS, the final Makefiles available in our GitHub repo, were &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/LAMMPS/2a-compile-lammps-acfl-sve.sh"&gt;2a-compile-lammps-acfl-sve.sh&lt;/a&gt; for ACfL and &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/LAMMPS/2b-compile-lammps-gcc.sh"&gt;2b-compile-lammps-gcc.sh&lt;/a&gt; for GCC.&lt;/p&gt; 
&lt;p&gt;The key points to note in our LAMMPS compile script involve loading the correct environment modules – &lt;code&gt;libfabric-aws/1.17.1&lt;/code&gt; and &lt;code&gt;armpl/23.04.1&lt;/code&gt; for both compilers, &lt;code&gt;acfl/23.04.1&lt;/code&gt; for ACfL, and &lt;code&gt;gnu/12.2.0&lt;/code&gt; for GCC – and replacing the &lt;code&gt;CCFLAGS&lt;/code&gt; and &lt;code&gt;LINKFLAGS&lt;/code&gt; variables with those from Table 1. For ACfL, we set both &lt;code&gt;PATH&lt;/code&gt; and &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to the Open MPI installation folders.&lt;/p&gt; 
&lt;p&gt;We pulled the LAMMPS source code from the git repository into the install folder (&lt;code&gt;~/software&lt;/code&gt; in our case), and checked out the 23 June 2022 branch for compilation (&lt;code&gt;git checkout stable_23Jun2022_update4&lt;/code&gt;). You can check out other releases, too. If you want the default stable branch, use &lt;code&gt;git checkout stable&lt;/code&gt;. Use &lt;code&gt;make clean-all&lt;/code&gt; to remove all the intermediate object files and executable files, and &lt;code&gt;make no-all&lt;/code&gt; to turn off all options. Next, we ran &lt;code&gt;make yes-most&lt;/code&gt; to install most of the packages, which was enough for us to test all five LAMMPS benchmarks.&lt;/p&gt; 
&lt;p&gt;Once we compiled LAMMPS, we were able to submit jobs using Slurm. We’ve included sample job scripts &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/LAMMPS/3a-lammps-acfl-sve.sh"&gt;3a-lammps-acfl-sve.sh&lt;/a&gt; (for ACfL-compiled LAMMPS) and &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/LAMMPS/3b-lammps-gcc.sh"&gt;3b-lammps-gcc.sh&lt;/a&gt; (for GCC-compiled LAMMPS) in our repo. In the final command, you should execute LAMMPS using &lt;code&gt;mpirun&lt;/code&gt;. The &lt;code&gt;-var x&lt;/code&gt;, &lt;code&gt;-var y&lt;/code&gt;, and &lt;code&gt;-var z&lt;/code&gt; flags specify the &lt;code&gt;NX&lt;/code&gt;, &lt;code&gt;NY&lt;/code&gt;, and &lt;code&gt;NZ&lt;/code&gt; parameters passed to the LAMMPS input file, and the &lt;code&gt;-in&lt;/code&gt; parameter indicates the LAMMPS input file (&lt;code&gt;in.lj&lt;/code&gt; in our example).&lt;/p&gt; 
&lt;p&gt;For the LAMMPS test runs, we chose the five benchmark cases listed at the &lt;a href="https://www.lammps.org/bench.html"&gt;LAMMPS Benchmarks site&lt;/a&gt; as described in Table 2:&lt;/p&gt; 
&lt;div id="attachment_3502" style="width: 638px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3502" loading="lazy" class="size-full wp-image-3502" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.48.36.png" alt="Table 2: Description of the 5 LAMMPS benchmarks" width="628" height="353"&gt;
 &lt;p id="caption-attachment-3502" class="wp-caption-text"&gt;Table 2: Description of the 5 LAMMPS benchmarks&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We ran a comparison between LAMMPS compiled with the GCC, and LAMMPS compiled with ACfL for all of the 5 input files (&lt;code&gt;in.lj&lt;/code&gt;, &lt;code&gt;in.chain&lt;/code&gt;, &lt;code&gt;in.eam&lt;/code&gt;,&lt;code&gt; in.chute&lt;/code&gt;, and &lt;code&gt;in.rhodo&lt;/code&gt;), running on a single Hpc7g.16xlarge instance.&lt;/p&gt; 
&lt;p&gt;For each of the input files, we submitted three jobs with GCC and three jobs with ACfL, and we used the average of the runs to report the results. We took the speed from the &lt;code&gt;log.lammps&lt;/code&gt; output file, reported in tau/day for the &lt;code&gt;chain&lt;/code&gt;, &lt;code&gt;chute&lt;/code&gt;, and &lt;code&gt;lj&lt;/code&gt; test cases, and ns/day for the &lt;code&gt;eam&lt;/code&gt; and &lt;code&gt;rhodo&lt;/code&gt; test cases.&lt;/p&gt; 
&lt;p&gt;The LAMMPS benchmarks each contain 32,000 atoms (for &lt;code&gt;NX=NY=NZ=1&lt;/code&gt;). We chose &lt;code&gt;NX=NY=NZ=8&lt;/code&gt; as a balance between &lt;code&gt;NX=NY=NZ=1&lt;/code&gt; (with high variability in performance due to the short run time) and &lt;code&gt;NX=NY=NZ=32&lt;/code&gt; (resulting in out of memory errors).&lt;/p&gt; 
&lt;div id="attachment_3504" style="width: 896px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3504" loading="lazy" class="size-full wp-image-3504" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.51.47.png" alt="Figure 6. Performance of LAMMPS a single Hpc7g.16xlarge instance with NX=NY=NZ=8, when compiled using the GCC and ACfL compilers. Speed is in tau/day for the chain, chute, and lj test cases, and ns/day for the eam and rhodo test cases." width="886" height="562"&gt;
 &lt;p id="caption-attachment-3504" class="wp-caption-text"&gt;Figure 6. Performance of LAMMPS a single Hpc7g.16xlarge instance with NX=NY=NZ=8, when compiled using the GCC and ACfL compilers. Speed is in tau/day for the chain, chute, and lj test cases, and ns/day for the eam and rhodo test cases.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;ACfL consistently outperformed GCC for all the single-node runs, with improvements ranging from 2.3% to 46%.&lt;/p&gt; 
&lt;div id="attachment_3503" style="width: 885px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3503" loading="lazy" class="size-full wp-image-3503" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.50.35.png" alt="Table 3: Speedups with ACfL over GCC for the 5 benchmark cases on a single Hpc7g.16xlarge instance with NX=NY=NZ=8." width="875" height="124"&gt;
 &lt;p id="caption-attachment-3503" class="wp-caption-text"&gt;Table 3: Speedups with ACfL over GCC for the 5 benchmark cases on a single Hpc7g.16xlarge instance with NX=NY=NZ=8.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For the multi-node runs, we further increased &lt;code&gt;NX=NY=NZ&lt;/code&gt; to &lt;code&gt;32&lt;/code&gt;, to increase the runtime – and therefore reduce the variability – in the results. This brought the total number of atoms to 32,000 x 32 x 32 x 32 = ~ 1 billion atoms.&lt;/p&gt; 
&lt;p&gt;We chose to focus only on the Lennard Jones benchmark and tested both GCC- and ACfL- compiled LAMMPS with &lt;code&gt;OMP_NUM_THREADS=1,2,4&lt;/code&gt;. For 8 nodes (512 cores) and above, we observed better results with &lt;code&gt;OMP_NUM_THREADS=2&lt;/code&gt; for both compilers. &lt;em&gt;Unlike the single node runs&lt;/em&gt;, we observed that GCC-compiled LAMMPS outperformed its ACfL-compiled counterpart. We tested the GCC-compiled version up to 128 nodes.&lt;/p&gt; 
&lt;p&gt;We’ve plotted the results in Figure 7.&lt;/p&gt; 
&lt;div id="attachment_3505" style="width: 906px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3505" loading="lazy" class="size-full wp-image-3505" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.53.46.png" alt="Figure 7: Scalability of GCC-compiled LAMMPS for Lennard Jones benchmark case (1 billion atoms) using multiple Hpc7g.16xlarge instances. The line for linear scaling is plotted in grey." width="896" height="617"&gt;
 &lt;p id="caption-attachment-3505" class="wp-caption-text"&gt;Figure 7: Scalability of GCC-compiled LAMMPS for Lennard Jones benchmark case (1 billion atoms) using multiple Hpc7g.16xlarge instances. The line for linear scaling is plotted in grey.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we showed how you can run GROMACS and LAMMPS on Hpc7g-based instance. We discussed the ideal toolchain and SIMD setup based on the results we saw.&lt;/p&gt; 
&lt;p&gt;We didn’t modify the source code of GROMACS and LAMMPS, choosing instead to leverage “auto-vectorization” implemented in the compilers.&lt;/p&gt; 
&lt;p&gt;Arm’s compiler for Linux (ACfL) with SVE enabled SIMD boosted GROMACS performance up to 22% compared with the GNU compiler and NEON/ASIMD on Hpc7g instances. Arm compiler for Linux sped up LAMMPS simulation between 2.3% to 46%, depending on the case, compared to the GNU compiler on a single node basis. For LAMMPS, we also saw performance improvements at larger core counts with hybrid MPI + OpenMP parallelization.&lt;/p&gt; 
&lt;p&gt;We’ve made all our build scripts – and job submission scripts – available in our GitHub samples repo, and we’d encourage you to use this if you want to build on this work for your own research workloads. If you need to discuss any of this, feel free to reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Accelerate drug discovery with NVIDIA BioNeMo Framework on Amazon EKS</title>
		<link>https://aws.amazon.com/blogs/hpc/accelerate-drug-discovery-with-nvidia-bionemo-framework-on-amazon-eks/</link>
		
		<dc:creator><![CDATA[Doruk Ozturk]]></dc:creator>
		<pubDate>Wed, 01 May 2024 15:01:56 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">6ce72440eac8d6fda6dcdd01cb3b62a20165c5d0</guid>

					<description>This post was contributed by Doruk Ozturk and Ankur Srivastava at AWS, and Neel Patel at NVIDIA. Introduction Drug discovery is a long and expensive process. Pharmaceutical companies must sift through thousands of compound possibilities to find potential new drugs to treat diseases. This process takes multiple years and costs billions of dollars, with the […]</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Doruk Ozturk and Ankur Srivastava at AWS, and Neel Patel at NVIDIA.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Drug discovery is a long and expensive process. Pharmaceutical companies must sift through thousands of compound possibilities to find potential new drugs to treat diseases. This process takes multiple years and costs billions of dollars, with the majority of the candidates failing during clinical trials.&lt;/p&gt; 
&lt;p&gt;As generative artificial intelligence (generative AI) continues to transform industries, the life sciences sector is leveraging these advanced technologies to accelerate drug discovery. Generative AI tools powered by deep learning models make it possible to analyze massive datasets, identify patterns, and generate insights to aid the search for new drug compounds. However, running these generative AI workloads requires a full-stack approach that combines robust computing infrastructure with optimized domain-specific software that can accelerate time to solution.&lt;/p&gt; 
&lt;p&gt;In this blog post, we’ll show you how to leverage the NVIDIA BioNeMo platform on Amazon Elastic Kubernetes Service (Amazon EKS) to accelerate drug discovery by using generative AI and other machine learning technologies.&lt;/p&gt; 
&lt;h2&gt;NVIDIA BioNeMo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/clara/bionemo/"&gt;NVIDIA BioNeMo&lt;/a&gt; is a generative AI platform for drug discovery that simplifies and accelerates the training of models using your own data. BioNeMo provides researchers and developers a fast and easy way to build and integrate state-of-the-art generative AI applications across the entire drug discovery pipeline—from target identification to lead optimization—with AI workflows for 3D protein structure prediction, de novo design, virtual screening, docking, and property prediction.&lt;/p&gt; 
&lt;p&gt;The BioNeMo framework facilitates centralized model training, optimization, fine-tuning, and inferencing for protein and molecular design. Researchers can build and train foundation models from scratch at scale, or use pre-trained model checkpoints provided with the BioNeMo Framework for fine-tuning for downstream tasks. Currently, BioNeMo supports models such as ESM1nv, ESM2nv, ProtT5nv, DNABERT, OpenFold, EquiDock, DiffDock, and MegaMolBART. To read more about BioNeMo, visit &lt;a href="https://docs.nvidia.com/bionemo-framework/latest/"&gt;the documentation page&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_3527" style="width: 1689px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3527" loading="lazy" class="wp-image-3527 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/15/HPCBlog-281-fig1-1.png" alt="Figure 1: This image shows the workflow for developing models on NVIDIA BioNeMo. The process is divided into phases for model development and customization and then fine-tuning and deployment." width="1679" height="691"&gt;
 &lt;p id="caption-attachment-3527" class="wp-caption-text"&gt;Figure 1: This image shows the workflow for developing models on NVIDIA BioNeMo. The process is divided into phases for model development and customization and then fine-tuning and deployment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In this post, we’ll walk through how to deploy the NVIDIA BioNeMo Framework on Amazon Elastic Kubernetes Service (Amazon EKS). Amazon EKS provides a fully managed Kubernetes service, making it simpler to run distributed, containerized generative AI workloads at scale. The process will cover:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Setting up an EKS cluster with NVIDIA GPU nodes&lt;/li&gt; 
 &lt;li&gt;Leveraging Amazon FSx for Lustre for high-performance data storage and sharing&lt;/li&gt; 
 &lt;li&gt;Downloading and ingesting Uniref-50 data in a machine learning friendly format&lt;/li&gt; 
 &lt;li&gt;Running a distributed pre-training job to train the ESM-1nv model&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;div id="attachment_3528" style="width: 1145px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3528" loading="lazy" class="size-full wp-image-3528" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/15/HPCBlog-281-fig2-1.png" alt="Figure 2:&amp;nbsp; This architecture diagram shows Amazon EKS cluster with GPU nodes, Amazon FSx for Lustre filesystem, and BioNeMo containers. GPU nodes are optimized for machine learning workloads. The FSx filesystem enables fast access to data needed for distributed training. Amazon CloudWatch is used for logging and monitoring." width="1135" height="1003"&gt;
 &lt;p id="caption-attachment-3528" class="wp-caption-text"&gt;Figure 2: This architecture diagram shows Amazon EKS cluster with GPU nodes, Amazon FSx for Lustre filesystem, and BioNeMo containers. GPU nodes are optimized for machine learning workloads. The FSx filesystem enables fast access to data needed for distributed training. Amazon CloudWatch is used for logging and monitoring.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We leveraged the &lt;a href="https://github.com/aws-ia/terraform-aws-eks-blueprints"&gt;Amazon EKS Blueprints for Terraform &lt;/a&gt;and &lt;a href="https://github.com/awslabs/data-on-eks"&gt;Data on EKS&lt;/a&gt; open source projects to build a robust Kubernetes infrastructure on EKS using Infrastructure as Code best practices. These projects enabled us to quickly stand up an EKS cluster while meeting security and operational excellence requirements.&lt;/p&gt; 
&lt;p&gt;In particular, the Terraform blueprints let us create a production-grade EKS cluster with networking, security groups, node groups, and other critical components out-of-the-box. The Data on EKS repository provided examples for running data-intensive workloads such as Apache Spark and TensorFlow on EKS. Together, these tools allowed us to launch an EKS cluster purpose-built for large-scale data processing in a reproducible and automated fashion. We can easily scale the cluster up to hundreds of nodes to handle compute-intensive jobs. Adopting these community-built modules accelerated our delivery timelines while ensuring the infrastructure remained secure, observable, and operationally robust as it scaled. The flexibility to customize the modules as needed also enabled us to tailor the infrastructure to the specific needs of our data workloads.&lt;/p&gt; 
&lt;h1&gt;The NVIDIA BioNeMo on EKS Blueprint&lt;/h1&gt; 
&lt;p&gt;We published all of the templates we used to deploy BioNeMo on EKS as a new Data on EKS &lt;a href="https://github.com/awslabs/data-on-eks/tree/main/ai-ml/bionemo"&gt;blueprint&lt;/a&gt; on GitHub. We’ll continue to iterate and improve on this blueprint over time, so you should bookmark and refer to the &lt;a href="https://awslabs.github.io/data-on-eks/docs/gen-ai/training/bionemo"&gt;official documentation&lt;/a&gt; on the Data on EKS website moving forward. However, we will discuss key configuration details and technical details here for reference.&lt;/p&gt; 
&lt;p&gt;Step one, as always, is to prepare the input data as a machine learning friendly structure. Uniref50 contains over 50 million unique protein sequences clustered from UniProt at 50% identity. This comprehensive set provides a foundation for vital tasks such as gene annotation and protein family prediction.&lt;/p&gt; 
&lt;p&gt;To leverage Uniref50’s scale, we downloaded and organized the data into training, validation, and test partitions. This layout enhances downstream machine learning by enabling effective model building, evaluation, and monitoring of overfitting. An Amazon FSx for Lustre shared filesystem provided the high-throughput storage needed for data sharing across the compute cluster nodes.&lt;/p&gt; 
&lt;p&gt;After data preparation, we are ready to run the pretraining job. It is important to leverage all available NVIDIA GPU resources for maximum performance. P5 instances powered by NVIDIA H100 Tensor Core GPUs offer the best performance, however, in this example, we used two p3.16xlarge instances with eight GPUs each for demonstration purposes. To utilize all 16 GPUs, we configured the &lt;code&gt;PytorchJob&lt;/code&gt; &lt;a href="https://github.com/awslabs/data-on-eks/blob/main/ai-ml/bionemo/examples/training/esm1nv_pretrain-job.yaml"&gt;custom resource&lt;/a&gt; with one GPU per replica:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-yaml"&gt;resources:
  requests:
    nvidia.com/gpu: 1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;We set replicas to match the total number of available GPUs across the worker nodes:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-yaml"&gt;pytorchReplicaSpecs:
&amp;nbsp; Worker:
&amp;nbsp;&amp;nbsp;&amp;nbsp; replicas: 16&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;To allocate 8 processes per 16-GPU node, we set:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-yaml"&gt;nprocPerNode: "8"&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;By default, workloads that don’t require GPUs won’t be scheduled on our GPU nodes. To make sure our BioNeMo pods were scheduled on the GPU nodes, we added tolerations:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-yaml"&gt;tolerations:
-  Key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;To utilize a larger cluster for BioNeMo analysis, change the above values to match the cluster size. For example, to fully leverage a cluster of 8 p3.8xlarge instances, each with 4 GPUs, you would need to change the following parameters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Set “&lt;code&gt;nprocPerNode&lt;/code&gt;” to 4 to indicate the number of GPUs available per node.&lt;/li&gt; 
 &lt;li&gt;Set “&lt;code&gt;replicas&lt;/code&gt;” to 32 to total the number of GPUs across the 8 nodes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As each p3.8xlarge instance contains 4 GPUs, 8 instances x 4 GPUs per instance = 32 GPUs in the cluster.&lt;/p&gt; 
&lt;p&gt;By properly configuring these parameters according to the resources provisioned, you can efficiently parallelize the training across the all-available GPUs in the cluster. For more information on configuring Pytorch and Kubeflow for your specific needs, read the &lt;a href="https://pytorch.org/docs/stable/distributed.html"&gt;Pytorch distributed module documentation&lt;/a&gt; and the &lt;a href="https://www.kubeflow.org/docs/components/training/"&gt;Kubeflow Training Operator’s documentation&lt;/a&gt;&lt;u&gt;,&lt;/u&gt; respectively.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In summary, leveraging technologies such as NVIDIA BioNeMo on Amazon EKS can accelerate AI-powered drug discovery by orders of magnitude. By combining the power of generative models with robust infrastructure for distributed training, researchers can rapidly analyze massive protein datasets to uncover new drug compound candidates. Automating infrastructure deployment using Terraform modules and best practices for maximum GPU utilization, storage, and monitoring enables workloads to benefit from security, scalability, and observability. Subject matter experts are a key resource for drug discover, and purpose-built generative AI platforms on cloud-native infrastructure can augment their creativity and intuition. As this technology continues maturing, we may see further breakthroughs in delivering life-saving treatments to patients faster.&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/15/nilkanthp-nvidia-profile.png" alt="Neel Patel" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Neel Patel&lt;/h3&gt; 
  &lt;p&gt;Neel Patel is a drug discovery scientist at NVIDIA, who focuses on cheminformatics and computational structural biology. Before joining NVIDIA, Patel was a computational chemist at Takeda Pharmaceuticals. He holds a Ph.D. from the University of Southern California. He lives in San Diego with his family and enjoys hiking and travelling.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Data, emerging technologies, and the circular economy: how Accenture and AWS are unlocking environmental and business impact</title>
		<link>https://aws.amazon.com/blogs/hpc/data-emerging-technologies-and-the-circular-economy-how-accenture-and-aws-are-unlocking-environmental-and-business-impact/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Tue, 30 Apr 2024 12:27:06 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">cfe27b15e171b02db52875e9424a371ce2457466</guid>

					<description>Realizing the $4.5 trillion circular economy opportunity requires accurate data, scalable HPC and agile tools. Read this post to discover how AWS and Accenture partner for real progress.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright wp-image-3572 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/Data-technology-and-the-circular-economy-how-Accenture-and-AWS-are-unlocking-environmental-and-business-impact.png" alt="Data, technology, and the circular economy: how Accenture and AWS are unlocking environmental and business impact" width="380" height="212"&gt;This post was contributed by Ilan Gleiser, Principal Specialist, Emerging Technologies at AWS, Joshua Curtis, Circular Intelligence Global Lead and Patrick Ford, Circular Intelligence North America Lead, Accenture Sustainability Services&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;It is well documented that the circular economy is an opportunity for positive impact on business and society. Accenture’s analysis presents an economic opportunity of $4.5tn value is at stake for the global economy to 2030 by departing from our current ‘&lt;em&gt;take-make-waste&lt;/em&gt;’ economic system [1]. Ellen MacArthur Foundation outlines the importance of circularity as a solution to climate change, with 45% of the required carbon emission reductions to achieve a 1.5-degree world coming from how we make and consume products [2].&lt;/p&gt; 
&lt;p&gt;It’s clear that resource use and circularity are critical to the creation of a sustainable, healthy economy. But how do we realize this value? How does a business identify where and how circular strategies can create financial and environmental impact?&lt;/p&gt; 
&lt;p&gt;In this post, we will explore the challenges to achieving accurate and actionable data on circular economy performance and impact, and the solutions that lie in emerging technologies. Join us as we explore opportunities for kicking off and accelerating the data transformation needed to drive authentic, impact-driven progress on circularity.&lt;/p&gt; 
&lt;h2&gt;Achieving data-driven circularity&lt;/h2&gt; 
&lt;p&gt;The importance of transitioning to circular business models is why the European Commission is, for the first time, making measurement and disclosure of resource use and circular economy impacts mandatory for companies. The newly-launched European Sustainability Reporting Standards (ESRS) include a requirement [3] for companies – where material – to report on circular economy metrics like:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;the percentage of material used for products and packaging that are renewable, recycled or re-used&lt;/li&gt; 
 &lt;li&gt;the volume of waste by stream that is recovered by destination&lt;/li&gt; 
 &lt;li&gt;the financial effects of material risks and opportunities arising from resource use&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;But how do we calculate these metrics? How do we collect, aggregate, and analyze the data in a way that doesn’t require significant time and resources year on year? How do we not only do this to understand where we are, but also to determine where we need to go?&lt;/p&gt; 
&lt;p&gt;While there is an increasing number of new sustainability measurement products being presented to the market, there is no one ‘tool’ to answer these questions. In fact, these questions themselves are not new — they are the essence of data-driven decision-making which is the foundation of any profitable business. The circular measurement challenge is a data challenge and must be approached as such.&lt;/p&gt; 
&lt;p&gt;What is new – and evolving – is the potential application of digital technologies to support the transformation of circular economy data management for companies. For example, the advancement of the Internet of Things (IoT) enables tracking of product movement and health through use phases; machine learning algorithms can help companies identify patterns in circular economy data, like trends in demand for certain recycled materials; and blockchain technology can be used to create a transparent and secure ledger of circular-related transactions, enabling stakeholders to track and verify the movement of materials and products through the value chain.&lt;/p&gt; 
&lt;p&gt;As we continue to tackle the circular measurement challenge, it is essential to approach it with a data-driven mindset. Digital technologies have the potential to revolutionize how we manage and analyze circular economy data, allowing us to create a more sustainable and efficient economy for the future. Accenture and AWS are collaborating to make these applications a reality.&lt;/p&gt; 
&lt;h2&gt;The challenges to circular data transformation&lt;/h2&gt; 
&lt;p&gt;To help ensure digital solutions are effective in managing circular economy performance, it’s crucial to design them to address specific challenges faced by businesses. Let’s begin by exploring these challenges.&lt;/p&gt; 
&lt;p&gt;First, selecting the right metrics themselves is not straightforward. We mentioned the European Commission’s regulations ESRS E5 on resource use and the Circular Economy. They provide headline metrics for business disclosure. The &lt;a href="https://pacecircular.org/sites/default/files/2023-01/CEIC_Circular%20Target%20Activation%20Guides_FINAL_01182023.pdf"&gt;Circular Target-Setting Guidance&lt;/a&gt; from the Circular Economy Indicators Coalition (CEIC), a partnership between The Platform for Accelerating the Circular Economy (PACE) and Circle Economy (supported by Accenture) provides an overview of leading measurement methodologies and approaches for business implementation.&lt;/p&gt; 
&lt;p&gt;These are important starting points for business across industries, but they don’t account for the specific value chains or functional priorities of businesses in different sectors. For example, the metrics to measure circular economy performance (and therefore the data required) vary significantly for a fashion retailer compared to an oil and gas major. Ultimately, selecting the right metrics must be led by each business, drawing on the wealth of supporting materials, best practices and market standards.&lt;/p&gt; 
&lt;p&gt;Next comes the hard part: identifying, collecting and transforming the data. Comprehensive circular measurement relies on data from across the value chain, often not tracked in existing enterprise systems. The foundational data itself is simple enough – what materials are being used and where do they come from; what waste is being produced and where is it going – are tangible examples. The challenge is collecting that data across product lines, business units, and geographies and then transforming the data to be usable. For example, when calculating your percentage of materials that are recycled, renewable or re-used (as ESRS E5 requires), materials data must be segmented in ways not currently built into enterprise data capture. Without technology, this requires line-by-line segmentation based on data that is available e.g. through supplier declarations. The bottom line is that collecting data to measure circular performance is an arduous process, requiring time and costs, and is hindered by data gaps. To do this at the business level, in a way that enables action, is not only helped by digital technologies, it depends upon them.&lt;/p&gt; 
&lt;p&gt;Finally, companies must transform this data into actionable insights to guide decision-making. Circularity is not an end, but a means to optimize planetary and business impact. To accelerate this impact, resource use data like the above example regarding materials that are recycled, renewable or re-used, must be connected with other internal and external data sets like sales data and emissions intensity factors. Companies must understand how different material choices impact carbon emissions, as well as procurement costs and business profitability. The true story of corporate circularity is of trade-offs and investment requirements to capture long-term value. Without a comprehensive approach to circular economy measurement and data transformation, understanding those trade-offs properly and making impact-driven decisions is impossible. This again adds complexity to data collection and analysis, with the only solution for ongoing insight generation being an automated, centralized approach, like a circular and/or sustainability data lake, which combines data sets and applies analytics solutions for calculation and visualization.&lt;/p&gt; 
&lt;div id="attachment_3564" style="width: 867px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3564" loading="lazy" class="size-full wp-image-3564" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-13.24.08.png" alt="Figure 1. The Circular Business Hub: Illustration of data visualization for circular economy performance management. Companies must overcome challenges to achieve measurement and visualization of circularity at each pillar of the business value chain." width="857" height="537"&gt;
 &lt;p id="caption-attachment-3564" class="wp-caption-text"&gt;Figure 1. The Circular Business Hub: Illustration of data visualization for circular economy performance management. Companies must overcome challenges to achieve measurement and visualization of circularity at each pillar of the business value chain.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;The role of emerging technologies&lt;/h2&gt; 
&lt;p&gt;Accenture and AWS are collaborating to bring the best of their combined data, technology and sustainability expertise to transform circular economy data management. AWS offers the broadest set of capabilities in artificial intelligence (AI), machine learning (ML), Internet of Things (IoT), big data analytics, and high-performance computing (HPC) in the market. Accenture is the world’s leading integrator of AWS solutions and technologies – they’ve completed over 1,100 projects with us over 15 years of partnership.&lt;/p&gt; 
&lt;p&gt;Teaming up on the circular economy, Accenture brings its 12+ years of client experience in circular economy strategy and implementation, and 100+ global circular economy experts, to guide the application of these digital solutions to unlock value for joint customers from data-driven circularity.&lt;/p&gt; 
&lt;p&gt;Accenture has developed core assets as part of a suite of circular intelligence solutions, powered by AWS. These include industry-specific KPI frameworks, a foundational data model and a proof-of-concept dashboard to act as a platform for client co-development and customization. Building upon these assets, a core priority of this collaboration is to enable the automation of circular data ingestion, transformation and analysis to enable ongoing performance measurement and generation of actionable insights for companies across the value chain. To do this, we’re leveraging &lt;a href="https://www.accenture.com/us-en/services/cloud/aws-business-group#block-velocity"&gt;Velocity&lt;/a&gt;, a co-funded and co-developed platform that adds new cloud innovations up to 50% faster, to develop the data architecture which pulls in resource use and circular economy-related datasets into a central circular data lake.&lt;/p&gt; 
&lt;p&gt;A circular data lake (Figure 2) works by taking raw data from siloed sources and bringing it into a unified, organized system. This process is made possible using the AWS Transfer family, which collects data and brings it into a raw layer.&lt;/p&gt; 
&lt;div id="attachment_3565" style="width: 835px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3565" loading="lazy" class="size-full wp-image-3565" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-13.24.48.png" alt="Figure 2. Reference Architecture of an automated circular data lake for managing circular economy measurement and insight generation" width="825" height="375"&gt;
 &lt;p id="caption-attachment-3565" class="wp-caption-text"&gt;Figure 2. Reference Architecture of an automated circular data lake for managing circular economy measurement and insight generation&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Once the data is in the raw layer, one or more &lt;em&gt;ETL&lt;/em&gt; (extract, transform, and load) workstreams are triggered. These ETL workstreams work together to clean, refine, and organize the data. To facilitate the ETL process, the reference architecture employs several AWS Glue jobs and AWS Lambda functions. These functions and jobs help to ensure that the ETL workflows are fully orchestrated and operate efficiently and effectively, to produce the refined data necessary for circular metrics and KPIs.&lt;/p&gt; 
&lt;p&gt;Once the data is fully curated and prepared, it’s ready for business intelligence and machine learning (AWS Athena, Amazon Redshift, and Amazon SageMaker). These services enable companies to derive value from the data by analyzing it, identifying patterns and trends. By doing so, it’s possible to transform siloed data from a wide range of sources into actionable insights that can drive circular business progress.&lt;/p&gt; 
&lt;p&gt;The circular data lake is designed to centralize all data related to a business’ resource use and subsequent impact. With this, it must be designed and implemented in connection with other environmental objectives like climate and nature-related data sources. Indeed, the circular data lake is designed to be a component of an organization’s overall sustainability data lake fabric, creating a single system of record for ESG data management.&lt;/p&gt; 
&lt;p&gt;Building upon the circular data architecture, created and tested with joint customers, Accenture and AWS are building solutions for companies focused on creating value from data in functional areas of circular transformation. This means going beyond just measuring circular economy performance (which we refer to as ‘foundational’ use cases) to applying digital solutions for value creation from circular insights (these are ‘advanced’ use cases).&lt;/p&gt; 
&lt;div id="attachment_3566" style="width: 836px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3566" loading="lazy" class="size-full wp-image-3566" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-13.25.35.png" alt="Figure 3. Overview of foundational and advanced circular intelligence use cases at each stage of an industry-agnostic value chain " width="826" height="385"&gt;
 &lt;p id="caption-attachment-3566" class="wp-caption-text"&gt;Figure 3. Overview of foundational and advanced circular intelligence use cases at each stage of an industry-agnostic value chain&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 3 shows examples of circular economy use cases split between foundational and advanced. Circular intelligence helps companies measure the impact of their investments and compare results over time. For more examples of how digital technologies enable a circular economy, please see &lt;a href="https://aws.amazon.com/blogs/hpc/how-to-make-digital-technologies-for-the-circular-economy-work-for-your-business/"&gt;this&lt;/a&gt; blog post.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 3. Overview of foundational and advanced circular intelligence use cases at each stage of an industry-agnostic value chain &lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;The circular intelligence maturity journey&lt;/h2&gt; 
&lt;p&gt;Let’s look at an example of how AWS services and solutions are being used to enable foundational and advanced use cases in the procurement stage of the product development lifecycle.&lt;/p&gt; 
&lt;h3&gt;Example: how to transition away from virgin, non-renewable materials to optimize for decarbonization, cost reduction and risk management?&lt;/h3&gt; 
&lt;p&gt;The materials that companies buy for products and services are a critical component of their environmental impact, while the reliance on virgin, finite materials also presents a growing supply chain risk for many industries.&lt;/p&gt; 
&lt;p&gt;To achieve foundational circular procurement decision-making, machine learning algorithms can analyze past procurement data and identify patterns and insights to make better purchasing decisions. For example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;With AWS’s &lt;a href="https://aws.amazon.com/solutions/ai-ml/intelligent-document-processing/"&gt;Intelligent Document Processing&lt;/a&gt; solutions, companies can streamline the ingestion of bills of materials across product lines. By automating the procurement data ingestion process, companies can measure their circular input footprint (share of circular materials per product and/or packaging type), as well as the cost and carbon impact of circular inputs. This allows for faster and more accurate decision-making when selecting sustainable alternatives.&lt;/li&gt; 
 &lt;li&gt;Another powerful tool is &lt;a href="https://aws.amazon.com/textract/"&gt;Amazon Textract&lt;/a&gt;, which can automatically ingest supplier declarations, saving time and increasing accuracy. By digitizing these declarations, companies can easily track progress toward their circular procurement goals and identify areas that require improvement.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These applications are a significant value add for companies in reducing the time required to assess circular performance in procurement, while enabling decisions that account for trade-offs, and can accelerate the highest value initiatives.&lt;/p&gt; 
&lt;p&gt;The value opportunity doesn’t end there, however.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;With generative AI services, like &lt;a href="https://aws.amazon.com/bedrock/"&gt;Amazon Bedrock&lt;/a&gt;, companies can build on this foundation for predictive analytics, real-time scenario modeling and advanced forecasting – AWS Partner &lt;a href="https://www.simudyne.com/"&gt;Simudyne&lt;/a&gt;, uses high-performance computing enabled simulations, coupled with LLM powered chatbots, to provide recommendations of how companies can decarbonize their supply chains, and therefore reduce their scope 3 emissions.&lt;/li&gt; 
 &lt;li&gt;By leveraging market data and research to identify opportunities for alternative materials and supply chain risk assessment, companies can gain an edge over the competition. For example, AWS partner &lt;a href="https://goodchemistry.com/"&gt;Good Chemistry&lt;/a&gt;, uses &lt;a href="https://aws.amazon.com/hpc/"&gt;HPC clusters&lt;/a&gt;, orchestrated by AWS Batch, to &lt;a href="https://aws.amazon.com/blogs/hpc/massively-scaling-quantum-chemistry-to-support-a-circular-economy/"&gt;design new materials&lt;/a&gt; and accelerate the identification of molecules to substitute or destroy toxic chemicals from the production process and environment.&lt;/li&gt; 
 &lt;li&gt;By using SageMaker for its &lt;a href="https://aws.amazon.com/blogs/machine-learning/remote-monitoring-of-raw-material-supply-chains-for-sustainability-with-amazon-sagemaker-geospatial-capabilities/"&gt;geospatial capabilities&lt;/a&gt;, companies are proactively addressing potential environmental risks. By monitoring deforestation in real-time with satellite data, IOT sensors and drones, coupled with machine learning algorithms, regulators, insurance companies and businesses can be aware of the risk of their current material footprint.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Overall, the use of AWS emerging technologies helps companies, governments and other stakeholders, transition towards circular materials in the short and long term, making sustainability a priority in their procurement process.&lt;/p&gt; 
&lt;h2&gt;Call to action&lt;/h2&gt; 
&lt;p&gt;Each company’s journey on circular economy measurement will look different, depending on industry, strategic priorities, and technical maturity. At Accenture and AWS, we believe solutions must be fit-for-purpose and co-designed for the specific business context. While there are growing options for ‘plug-and-play’ sustainability solutions, specifically in carbon management, developing the data foundation for automated baselining across the value chain and moving towards value-creating technology applications requires a level of customization and co-development using existing products and repeatable solutions where available.&lt;/p&gt; 
&lt;p&gt;The first step to get started is defining your circular economy blueprint with key metrics across each area of your business. This is the foundation for a data-driven strategy and many companies are working hard to collect data for reporting on circular performance on an annual basis. To then understand and plan for foundational and advanced circular measurement use cases, it is key to assess your current functional and technical maturity to measure and manage these metrics. Identifying where the raw data lives, how it is collected and stored, and who is responsible for it is critical to map the journey towards automation. From there, it is a case of prioritization of metrics and use cases through engaging with stakeholders across different business groups. The end-users of the data and insights must be involved in the full journey to ensure the applied solution serves the required needs.&lt;/p&gt; 
&lt;p&gt;The next step is to design, build and test. Cross-functional teams will need to work together to integrate solutions as part of a circular data lake, and develop the user interface for practical, day-to-day business decision making. Proof-of-Concepts (POCs) are valuable in building the foundational technical architecture, illustrating the value with a tangible output, and establishing buy-in from leadership and key stakeholders. This can then prime you for rapid deployment and scale.&lt;/p&gt; 
&lt;p&gt;From there, deployment must be thought of as a multi-generational journey. Companies should focus on their key requirements for a Minimum Viable Product (MVP) solution that supports their core business priorities. Be it cross-value chain performance management, or specific use case advancement, the MVP must also include solutions for data pipeline automation and a data governance strategy. Accenture and AWS are supporting companies across these phases of implementation, including the development of targeted POCs and MVPs to tackle the key challenges your business is facing on circular measurement and prove the value of emerging technologies applied to these challenges. This landscape is rapidly evolving and our joint assets are developing with every implementation; co-development with leading businesses is at the heart of this.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Circular economy measurement requires more than an off-the-shelf sustainability solution; it is a data and technology problem that requires a comprehensive approach. By leveraging the power of AWS and Accenture’s expertise, businesses can unlock the full potential of the circular economy. By capturing data into a purpose driven data lake and running advanced analytics, our joint approach provides businesses with actionable insights and recommendations for optimizing resource usage and reducing waste. By prioritizing sustainability and embracing greater intelligence, businesses can drive positive environmental and business impacts, paving the way for a more sustainable future.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;h4&gt;References&lt;/h4&gt; 
&lt;p&gt;[1] Lacy, Long and Spindler, The Circular Economy Handbook: Realizing the Circular Advantage (2019)&lt;br&gt; [2] Ellen MacArthur Foundation, Completing the Picture: How the Circular Economy Tackles Climate Change (2021)&lt;br&gt; [3]European Sustainability Reporting Standards in scope for companies with a turnover above €150 million in EU&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Optimizing MPI application performance on hpc7a by effectively using both EFA devices</title>
		<link>https://aws.amazon.com/blogs/hpc/optimizing-mpi-application-performance-on-hpc7a-by-effectively-using-both-efa-devices/</link>
		
		<dc:creator><![CDATA[Sai Sunku]]></dc:creator>
		<pubDate>Wed, 24 Apr 2024 15:04:39 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[FEA]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[MPI]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">08bac8f4eb9c45d176364cc57f8dc42002333566</guid>

					<description>Get the inside scoop on optimizing your MPI apps and configuration for AWS's powerful new Hpc7a instances. Dual rail gives these instances huge networking potential @ 300 Gb/s - if properly used. This post provides benchmarks, sample configs, and real speedup numbers to help you maximize network performance. Whether you run weather simulations, CFD, or other HPC workloads, you'll find practical tips for your codes.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Sai Sunku, Software Engineer, Annapurna Labs, Matt Koop, Principal Engineer and Karthik Raman, Principal Performance Engineer, HPC Engineering&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The hpc7a instance type is the latest generation AMD-based HPC instance type offered by AWS. It’s available in multiple sizes and offers superior performance compared to the previous generation hpc6a instance type. See our &lt;a href="https://aws.amazon.com/blogs/hpc/deep-dive-into-hpc7a-the-newest-amd-powered-member-of-the-hpc-instance-family/"&gt;previous post&lt;/a&gt; for a deep dive on the hpc7a instance itself and this &lt;a href="https://aws.amazon.com/blogs/hpc/instance-sizes-in-the-amazon-ec2-hpc7-family-a-different-experience/"&gt;other post&lt;/a&gt; that describes the different instance sizes.&lt;/p&gt; 
&lt;p&gt;If you’re an MPI user, you’ll want to know that Hpc7a’s network bandwidth of 300 Gbps is 3x higher than hpc6a. We’ve done this by using &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#network-cards"&gt;two network cards&lt;/a&gt;, where each card is exposed to the user software as a different PCIe device. Effectively using the two network cards is essential for achieving the best performance on hpc7a.&lt;/p&gt; 
&lt;p&gt;In this post we’ll show you how to configure your application and MPI to use both network cards so you can achieve the greatest performance for your codes. We’ll also discuss some benchmarks you can look at to verify that you’re &lt;em&gt;actually&lt;/em&gt; using both network cards. Finally, we’ll dig into some application results to give you a taste of the speedup you can get in your applications.&lt;/p&gt; 
&lt;h2&gt;Under the hood&lt;/h2&gt; 
&lt;p&gt;Let’s start with the PCIe topology. As &lt;a href="https://aws.amazon.com/blogs/hpc/deep-dive-into-hpc7a-the-newest-amd-powered-member-of-the-hpc-instance-family/"&gt;the post on instance sizes&lt;/a&gt; described, all hpc7a instance sizes use the same underlying hardware, but with different cores enabled or disabled to make it easy to get the best memory bandwidth per core for most codes. The hardware is based on the 4th generation AMD EPYC (Genoa) processor with 192 physical cores in a 2-socket configuration with 96 physical cores per socket.&lt;/p&gt; 
&lt;p&gt;In multi-domain NUMA systems, not all PCIe devices are created equal. Processes can be more efficient if they use the PCIe device closest to them and avoid communication across sockets. It’s no different for EFA devices. Using the network card closest to your MPI process allows for &lt;em&gt;much&lt;/em&gt; better performance. The topology for EFA devices is shown in Figure 1.&lt;/p&gt; 
&lt;div id="attachment_3554" style="width: 899px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3554" loading="lazy" class="size-full wp-image-3554" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-11.50.35.png" alt="Figure 1 – Topology of the EFA devices in hpc7a. Each green square represents a processor core and the orange rectangle the L3 cache shared by 8 cores. There are two NUMA domains with 96 cores per domain connected by a high speed interconnect. The two EFA devices are associated with one NUMA node each." width="889" height="395"&gt;
 &lt;p id="caption-attachment-3554" class="wp-caption-text"&gt;Figure 1 – Topology of the EFA devices in hpc7a. Each green square represents a processor core and the orange rectangle the L3 cache shared by 8 cores. There are two NUMA domains with 96 cores per domain connected by a high speed interconnect. The two EFA devices are associated with one NUMA node each.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Running MPI applications&lt;/h2&gt; 
&lt;p&gt;The good news is that the hpc7a instance is designed to work right out of the box with your MPI for a variety of expected uses.&lt;/p&gt; 
&lt;p&gt;Many MPIs refer to using multiple network cards as “multi-rail,” where each “rail” refers to a single network card, or in our case, an EFA device. In both Open MPI and Intel MPI, when using a multi-rail configuration with EFA, each rank will be assigned a single “rail.”&lt;/p&gt; 
&lt;p&gt;This means that in a two rail setup, half of the ranks on the instance will be assigned to one rail, and the other half to the other rail. This is generally desirable, because each rank is communicating separately – and for many applications that can lead to more efficient communication. Our tests have shown that real applications can be significantly faster when they use both EFA devices and you’ll see that in the data at the end of this post, too.&lt;/p&gt; 
&lt;p&gt;If you run your application with 192 ranks on a hpc7a.96xlarge instance with one rank per core, then each rank should automatically use the correct network card. But if you choose to run your application with fewer ranks per instance (maybe your application is memory intensive or you’re optimizing for licensing costs) then you need to consider which process is running on which core. You can configure the process placement through your scheduler or MPI. But this is also exactly why we offer smaller hpc7a instance sizes with some of the cores disabled. For many applications, using a smaller instance size will assign the MPI ranks to the correct cores and thus provide the best performance.&lt;/p&gt; 
&lt;p&gt;However, if you use a bandwidth benchmark that uses only &lt;em&gt;one process per instance&lt;/em&gt; (like the &lt;code&gt;osu_bw&lt;/code&gt; benchmark), you can only see the bandwidth associated with a single rail. Figure 2 shows the results of &lt;code&gt;osu_bw&lt;/code&gt; benchmark with 2 x hpc7a instances. The measured bandwidth is significantly smaller than the advertised rate of 300 Gbps. This lower bandwidth is because the benchmark is running a single rank per &lt;em&gt;node&lt;/em&gt;. Virtually all real applications run multiple ranks per node.&lt;/p&gt; 
&lt;h3&gt;How can you make sure that different ranks are assigned to different EFA devices?&lt;/h3&gt; 
&lt;p&gt;If you’re using Open MPI v 4.1.0 or later or the newly released Intel MPI 2021.12, &lt;strong&gt;you don’t have to do much&lt;/strong&gt;. Open MPI and the latest Intel MPI can detect the two EFA devices and use them both, exactly as you’d expect.&lt;/p&gt; 
&lt;p&gt;However, to get the same experience with Intel MPI versions 2021.11 &lt;em&gt;or earlier&lt;/em&gt;, you need to set an environment variable &lt;code&gt;I_MPI_MULTIRAL=1&lt;/code&gt; and pass that environment variable to the &lt;code&gt;mpirun&lt;/code&gt; or &lt;code&gt;mpiexec&lt;/code&gt; command. Typically, that means running your application like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;mpirun -genv I_MPI_MULTIRAIL=1 -n &amp;lt;number of processes&amp;gt; &amp;lt;your application&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Support for this environment variable was added to Intel MPI version 2021.6.&lt;/p&gt; 
&lt;div id="attachment_3555" style="width: 546px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3555" loading="lazy" class="size-full wp-image-3555" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-11.52.00.png" alt="Figure 2 – OSU single pair bandwidth benchmark for Intel MPI and Open MPI. The measured bandwidth is significantly smaller than the maximum value of 300 Gbps for hpc7a." width="536" height="377"&gt;
 &lt;p id="caption-attachment-3555" class="wp-caption-text"&gt;Figure 2 – OSU single pair bandwidth benchmark for Intel MPI and Open MPI. The measured bandwidth is significantly smaller than the maximum value of 300 Gbps for hpc7a.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Performance measurements&lt;/h2&gt; 
&lt;p&gt;To measure the bandwidth with both EFA devices in use, we need a multi-pair bandwidth benchmark similar to the osu_mbw_mr benchmark. In this benchmark, the MPI ranks are grouped into pairs and the ranks in each pair send data to each other.&lt;/p&gt; 
&lt;p&gt;Figure 3a shows the results of an internally developed multi-pair bandwidth benchmark similar to &lt;code&gt;osu_mbw_mr&lt;/code&gt; for two hpc7a instances with 192 ranks on each instance. With &lt;code&gt;I_MPI_MULTIRAIL=0&lt;/code&gt;, Intel MPI was unable to take advantage of both network cards and we see a bandwidth value close to 150 Gbps. With &lt;code&gt;I_MPI_MULTIRAIL=1&lt;/code&gt;, we see the expected value close to 300 Gbps. Open MPI shows a bandwidth close to 300 Gbps out of the box.&lt;/p&gt; 
&lt;p&gt;While bandwidth is important if your application is sending large messages, the number of messages sent, or the message rate, becomes more important for smaller messages.&lt;/p&gt; 
&lt;p&gt;Figure 3b shows the message rate for 2kiB messages. Using both EFA devices led to a significant improvement in the message rate at small message sizes as well. So whether your application sends a few large messages or many small messages, you can improve its performance by using both EFA devices.&lt;/p&gt; 
&lt;div id="attachment_3556" style="width: 891px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3556" loading="lazy" class="size-full wp-image-3556" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-11.52.42.png" alt="Figure 3 – Multi-pair bandwidth and message rate benchmarks for Intel MPI with I_MPI_MULTIRAIL=0, Intel MPI with I_MPI_MULTIRAIL=1 and Open MPI. The message rate is for messages 2kiB in size. Intel MPI with I_MPI_MULTIRAIL=0 is unable to take advantage of both EFA devices and shows a lower bandwidth and message rate.=" width="881" height="325"&gt;
 &lt;p id="caption-attachment-3556" class="wp-caption-text"&gt;Figure 3 – Multi-pair bandwidth and message rate benchmarks for Intel MPI with I_MPI_MULTIRAIL=0, Intel MPI with I_MPI_MULTIRAIL=1 and Open MPI. The message rate is for messages 2kiB in size. Intel MPI with I_MPI_MULTIRAIL=0 is unable to take advantage of both EFA devices and shows a lower bandwidth and message rate.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Finally, let’s look at some application benchmarks to see how setting &lt;code&gt;I_MPI_MULTIRAIL=1&lt;/code&gt; can speed up actual applications.&lt;/p&gt; 
&lt;p&gt;Figure 4 shows the results for the &lt;a href="https://www2.mmm.ucar.edu/wrf/src/conus2.5km.tar.gz"&gt;CONUS 2.5km benchmark&lt;/a&gt; performance using WRF v4.2.2. We used the Intel compiler version 2022.1.2 and Intel MPI 2021.9.0 to compile and run WRF and calculated the speedup based on the total compute time. For 32 instances, setting &lt;code&gt;I_MPI_MULTIRAIL=1&lt;/code&gt; led to a 10% increase in the speedup. For 192 instances, the increase was over 30%.&lt;/p&gt; 
&lt;div id="attachment_3557" style="width: 515px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3557" loading="lazy" class="size-full wp-image-3557" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-11.53.44.png" alt="Figure 4 –Speedup in runtime for WRF CONUS 2.5km benchmark. At large instance counts, we see that I_MPI_MULTIRAIL=1 shows a higher speedup." width="505" height="374"&gt;
 &lt;p id="caption-attachment-3557" class="wp-caption-text"&gt;Figure 4 –Speedup in runtime for WRF CONUS 2.5km benchmark. At large instance counts, we see that I_MPI_MULTIRAIL=1 shows a higher speedup.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Finally, figure 5 shows the results for the &lt;a href="https://www.ansys.com/en-in/it-solutions/benchmarks-overview/ansys-fluent-benchmarks/ansys-fluent-benchmarks-release-19/external-flow-over-a-formula-1-race-car"&gt;External flow over a Formula-1 race car&lt;/a&gt; test-case, running on ANSYS Fluent and Intel MPI 2021.9.0. We used the solver rating to compare performance. For 48 instances, setting &lt;code&gt;I_MPI_MULTIRAIL=1&lt;/code&gt; boosted the solver rating by &amp;gt; 15%.&lt;/p&gt; 
&lt;div id="attachment_3558" style="width: 600px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3558" loading="lazy" class="size-full wp-image-3558" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-11.54.12.png" alt="Figure 5 –Solver rating for f1_racecar_140M test case in ANSYS Fluent. At large instance counts, we see that I_MPI_MULTIRAIL=1 provides a significant boost in solver rating." width="590" height="409"&gt;
 &lt;p id="caption-attachment-3558" class="wp-caption-text"&gt;Figure 5 –Solver rating for &lt;code&gt;f1_racecar_140M&lt;/code&gt; test case in ANSYS Fluent. At large instance counts, we see that&lt;code&gt; I_MPI_MULTIRAIL=1&lt;/code&gt; provides a significant boost in solver rating.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we talked about the PCIe topology of hpc7a instances, how MPI applications use EFA devices and how you can make tuning changes to achieve best performance on hpc7a. If you’re using Open MPI, you can get the best performance right out of the box. If you’re using Intel MPI, we suggest upgrading to the latest 2021.12 release to achieve the same outcome, or set the environment variable &lt;code&gt;I_MPI_MULTIRAIL=1&lt;/code&gt; in your job scripts.&lt;/p&gt; 
&lt;p&gt;We hope you get great performance from the hpc7a instances for your HPC workloads. Reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt; with your thoughts and questions. Happy computing!&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Build and deploy a 1 TB/s file system in under an hour</title>
		<link>https://aws.amazon.com/blogs/hpc/build-and-deploy-a-1-tb-s-file-system-in-under-an-hour/</link>
		
		<dc:creator><![CDATA[Randy Seamans]]></dc:creator>
		<pubDate>Tue, 23 Apr 2024 14:26:36 +0000</pubDate>
				<category><![CDATA[Amazon FSx]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<guid isPermaLink="false">2f2bf7a37ba03c570aa2e58624876ea742c109fd</guid>

					<description>Want to set up a high-speed shared file system for your #HPC or #AI workloads in under an hour? Learn how with this new blog post.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3546" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/16/Red-speedometer.png" alt="" width="380" height="212"&gt;High throughput shared files systems are an essential part of any HPC or AI environment. Whether you want to train large language models (LLMs), find effective new drugs, or lead the search for new sources of energy, shared file systems are responsible for feeding compute and storing the hard-won results.&lt;/p&gt; 
&lt;p&gt;If you manage or use an on-premise environment, you know the complexity, value, and cost of providing a high throughput persistent data repository. You probably also know it typically takes weeks or months to plan and provision a large Lustre environment from scratch.&lt;/p&gt; 
&lt;p&gt;In this post, I’ll show how to build persistent shared file systems capable of &lt;em&gt;terabytes per second&lt;/em&gt; in AWS in under an hour.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;The dynamic nature of &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt; enables your organization to leverage the massive scale of AWS compute, network, and storage for your HPC and AI applications while only paying for actual usage. If your organization is trying to reduce both time and cost to reach a solution, FSx for Lustre combined with &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html"&gt;AWS Accelerated Instances&lt;/a&gt; gets you off to a great start without waiting for on-premise resources.&lt;/p&gt; 
&lt;p&gt;While live at the &lt;a href="https://sc23.supercomputing.org/"&gt;SuperComputing 2023&lt;/a&gt; AWS booth, I demonstrated a 1.1 PiB FSx for Lustre file system capable of over &lt;strong&gt;a terabyte per second&lt;/strong&gt; from a standard AWS account in the Northern Virginia region.&lt;/p&gt; 
&lt;p&gt;Prior to building the file system, your account quotas for FSx for Lustre must be increased. There is no charge for maintaining an increased quota. But most importantly, I did not have to architect or pre-provision storage, storage servers, interconnect, nor select the proper Lustre configuration to meet my performance goals.&lt;/p&gt; 
&lt;p&gt;As shown in figure 1, after completing a few questions, the FSx service started creating a high-throughput, persistent, SSD backed POSIX-compliant file system.&lt;/p&gt; 
&lt;div id="attachment_3540" style="width: 1006px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3540" loading="lazy" class="size-full wp-image-3540" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/16/CleanShot-2024-04-16-at-16.56.22.png" alt="Figure 1 – Screenshot of the ease of creating a Persistent FSx for Lustre 1 TB/s file system in progress" width="996" height="454"&gt;
 &lt;p id="caption-attachment-3540" class="wp-caption-text"&gt;Figure 1 – Screenshot of the ease of creating a Persistent FSx for Lustre 1 TB/s file system in progress&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;By relieving the heavy lift of deployment, not only is there a huge savings in time, but the human effort previously used to plan, evaluate, procure, rack, build, configure, and manage can be redirected to tasks that more directly impact the time to solution.&lt;/p&gt; 
&lt;h2&gt;How long does it take for the file system to become ready?&lt;/h2&gt; 
&lt;p&gt;When a FSx for Lustre file system is created, you can create an empty file system, or you can import existing metadata from data repositories. &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/overview-dra-data-repo.html"&gt;Data Repositories&lt;/a&gt; are simply &lt;a href="https://aws.amazon.com/s3/"&gt;S3 buckets&lt;/a&gt; or &lt;a href="https://repost.aws/knowledge-center/s3-prefix-nested-folders-difference"&gt;S3 Prefixes&lt;/a&gt; that exist in AWS.&lt;/p&gt; 
&lt;p&gt;When creating my 1 PiB file system at SC23, the build completed in far less than one hour – &lt;em&gt;including importing the metadata&lt;/em&gt;. You can specify one data repository association (DRA) on build, and you can add up to seven more (as needed) after initial build. Of course, the length of time to load a DRA is directly dependent upon the throughput level, and the volume of metadata.&lt;/p&gt; 
&lt;p&gt;What sort of environments does FSx for Lustre enable? At SC23, I used a cluster of over 120 i3en.24xlarge Amazon EC2 compute-optimized instances, each with a 100 Gbit/s &lt;a href="https://aws.amazon.com/hpc/efa/"&gt;Elastic Fabric Adapter&lt;/a&gt; for network connectivity.&lt;/p&gt; 
&lt;p&gt;Recently we introduced new &lt;a href="https://aws.amazon.com/ec2/instance-types/#HPC_Optimized"&gt;HPC optimized instances&lt;/a&gt; that support EFA bandwidth up to 300 Gb/s (Hpc7a instance). You can &lt;a href="https://www.youtube.com/watch?v=i2T7Hi2Yjoc"&gt;find out more about them&lt;/a&gt; from our HPC Tech Shorts channel on YouTube. If you need to scale your compute for heavy duty GPU workloads, FSx for Lustre also can connect to the &lt;a href="https://aws.amazon.com/ec2/ultraclusters/"&gt;EC2 UltraCluster&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The particular instances you choose for any given workload should be tailored to your application’s requirements to keep your costs low, and speed of execution high.&lt;/p&gt; 
&lt;p&gt;Now, once our build and/or load is complete, let’s move on to unit testing of our new file system.&lt;/p&gt; 
&lt;h2&gt;How fast can this go?&lt;/h2&gt; 
&lt;div id="attachment_3549" style="width: 813px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3549" loading="lazy" class="size-full wp-image-3549" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/16/CleanShot-2024-04-16-at-16.57.20-1.png" alt="Figure 2 – Screenshot of a single cluster node achieving 9.25 GByte/s of file system Read/Write throughput via fio" width="803" height="418"&gt;
 &lt;p id="caption-attachment-3549" class="wp-caption-text"&gt;Figure 2 – Screenshot of a single cluster node achieving 9.25 GByte/s of file system Read/Write throughput via fio&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In figure 2 above, you can see the standard file benchmark &lt;a href="https://fio.readthedocs.io/en/latest/fio_doc.html"&gt;fio&lt;/a&gt; was used with a 1 megabyte IO size to drive both read and write throughput from &lt;strong&gt;a single node&lt;/strong&gt;, using most of the 100 Gbps &lt;a href="https://aws.amazon.com/hpc/efa/"&gt;Elastic Fabric Adapter (EFA)&lt;/a&gt; throughput. Note that 9.25 Gigabyte/s was achieved with absolutely no tuning for optimal block size or network settings, to get close to line speed. The SC23 demonstration was not designed to show best-case benchmarked performance, but to demonstrate non-cached, sustained performance available without any tuning, using default settings. Stay tuned to this channel for more posts explaining how to optimize FSx for Lustre performance using popular Lustre benchmarking tools.&lt;/p&gt; 
&lt;p&gt;In the background of figure 2, you can see the &lt;a href="https://aws.amazon.com/cloudwatch/"&gt;AWS CloudWatch&lt;/a&gt; dashboard I created using a few clicks to monitor file system performance metrics. This chart shows only one node performing IO.&lt;/p&gt; 
&lt;p&gt;Now let’s move on to look at aggregate file system throughput as shown in figure 3.&lt;/p&gt; 
&lt;div id="attachment_3542" style="width: 814px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3542" loading="lazy" class="size-full wp-image-3542" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/16/CleanShot-2024-04-16-at-16.57.56.png" alt="Figure 3 – Screenshot of the aggregate 1TB/s of file system Read/Write throughput via fio" width="804" height="468"&gt;
 &lt;p id="caption-attachment-3542" class="wp-caption-text"&gt;Figure 3 – Screenshot of the aggregate 1TB/s of file system Read/Write throughput via fio&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 3 shows that our 120-node cluster achieved over 1 TByte/s of FSx for Lustre throughput, which is the maximum throughput we configured our file system for when we launched it. These are impressive results for a file system you can stand up or stand down &lt;em&gt;on-demand&lt;/em&gt;, and pay only as you go.&lt;/p&gt; 
&lt;p&gt;You don’t need to amortize your storage costs for years to justify running a project anymore – you can have 1 TB/s for just the hours or days you need high-speed file level access. During off-peak times, you can stand down your FSx for Lustre file system, and it remains possible to access the data at lower speed in the underlying repositories via the S3 object protocol. This allows you to stage, de-stage or browse your content, and run FSx for Lustre only when needed for application performance.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Developing and deploying HPC and AI workloads is complex and difficult enough without fighting the headwinds of equipment availability, power and cooling issues, datacenter build out, and vendor interoperability. By leveraging FSx for Lustre, you can enable your organization to cloud burst HPC or AI workloads to AWS, or easily migrate workloads to AWS.&lt;/p&gt; 
&lt;p&gt;But if your organization is just setting out on an HPC or AI development journey – you can instead start today and shave months or years off your project. If you want to stand on the shoulders of others you can use cluster and filesystem stacks created by experts who’ve all done this before. You can launch customizable templates from our &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;HPC Recipes Library&lt;/a&gt;, which have all been built to be interoperable, and simple to use. Reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt; if you have ideas for how we can make it easier for you.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Run simulations using multiple containers in a single AWS Batch job</title>
		<link>https://aws.amazon.com/blogs/hpc/run-simulations-using-multiple-containers-in-a-single-aws-batch-job/</link>
		
		<dc:creator><![CDATA[Matt Hansen]]></dc:creator>
		<pubDate>Mon, 22 Apr 2024 15:39:25 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">fe5f2de80116bdc4f1c348b4fd64c544ff490f56</guid>

					<description>Run simulations using multiple containers in a single AWS Batch job Matthew Hansen, Principal Solutions Architect, AWS Advanced Computing &amp;amp; Simulation Recently, AWS Batch launched a new feature that makes it possible to run multiple containers within a single job. This enables new scenarios customers have asked about like simulations for autonomous vehicles, multi-robot collaboration, […]</description>
										<content:encoded>&lt;p&gt;Run simulations using multiple containers in a single AWS Batch job&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Matthew Hansen, Principal Solutions Architect, AWS Advanced Computing &amp;amp; Simulation&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Recently, AWS Batch launched a new feature that makes it possible to run multiple containers within a single job. This enables new scenarios customers have asked about like simulations for autonomous vehicles, multi-robot collaboration, and other advanced simulations.&lt;/p&gt; 
&lt;p&gt;For autonomous system (AS) developers, this means you can keep your simulation and test scenario code in separate containers from the autonomy and sensor pipelines you want to test. This helps you test your modular system design (like in a software-defined vehicle) which has many components communicating over a local network.&lt;/p&gt; 
&lt;p&gt;Prior to this launch, you could run these types of simulations in Batch, but: you were limited to a single container per job. For some users that meant creating a very large container (tens of gigabytes) containing all the code for the system, plus a high-fidelity simulator, &lt;em&gt;and&lt;/em&gt; all the test code to run the scenarios. This blows out container build-times and generates long download times. It’ also an unnecessary coupling of components of the autonomous system. We heard you, so today we can talk about multi-container support in AWS Batch to address these issues.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll look at some multi-container simulation use-cases, learn how AWS Partners have used this to run their simulators using Batch, and then show you how to quickly get started running all kinds of multi-container jobs in Batch.&lt;/p&gt; 
&lt;h2&gt;Complex simulations&lt;/h2&gt; 
&lt;h4&gt;Autonomous Vehicles&lt;/h4&gt; 
&lt;p&gt;Today’s autonomous vehicles are simulated for millions of miles before they’re tested in the real world. These vehicles often have multiple sensors, including lidar and cameras, feeding into perception pipelines which are detecting other vehicles, pedestrians, traffic signals, and everything else needed for safe driving. They also have one or more control algorithms that are navigating via GPS, operating the steering and brakes, monitoring heading, speed, fuel, distance travelled, and other systems to operate the vehicle.&lt;/p&gt; 
&lt;p&gt;Because of this, the number of required software components can be quite large. These components are usually developed by different engineering teams, then integrated for system testing. By enabling each team to build their entire stack or pipeline into a separate container, engineers can test against the other components more easily, without having to rebuild everything into one, massive, monolithic container. Figure 1 is an example of an autonomous vehicle simulation. There are five total containers, &amp;nbsp;one for the test scenario runner, one for the simulator, and three containers for the autonomous vehicle, representing the lidar processing, camera pipeline, and the autonomous control.&lt;/p&gt; 
&lt;div id="attachment_3577" style="width: 769px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3577" loading="lazy" class="size-full wp-image-3577" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.29.13.png" alt="Figure 1 - An example of an autonomous vehicle simulation. There’s a container for the test scenario runner and the simulator, and three containers for the autonomous vehicle, representing the lidar processing, camera pipeline and the autonomous control." width="759" height="441"&gt;
 &lt;p id="caption-attachment-3577" class="wp-caption-text"&gt;Figure 1 – An example of an autonomous vehicle simulation. There’s a container for the test scenario runner and the simulator, and three containers for the autonomous vehicle, representing the lidar processing, camera pipeline and the autonomous control.&lt;/p&gt;
&lt;/div&gt; 
&lt;h4&gt;Robotics&lt;/h4&gt; 
&lt;p&gt;Another use case needing multiple containers is robotics simulation. We’re using robots in growing numbers for logistics, healthcare, and other industries. Robots – like autonomous cars – can also have multiple sensors (lidars, cameras, odometry, inertial measurement units), and a control stack for moving, all of which can be containerized.&lt;/p&gt; 
&lt;p&gt;Often we want to simulate multiple robots in a single environment, like a warehouse. That means you can go from running a few containers, to running hundreds at once in a single simulation. In the multi-robot example diagram (Figure 2), you can see there are separate containers for the test scenario runner, the simulation app, and four robots, each with containers for lidar, vision processing and control.&lt;/p&gt; 
&lt;div id="attachment_3578" style="width: 817px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3578" loading="lazy" class="size-full wp-image-3578" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.29.40.png" alt="Figure 2 - A four robot simulation in an AWS Batch multi-node parallel job. On the main node (Node 1) there’s a container each for the test scenario runner and the simulator, and four nodes of three containers for each robot, representing the lidar processing, camera pipeline and the autonomous control." width="807" height="425"&gt;
 &lt;p id="caption-attachment-3578" class="wp-caption-text"&gt;Figure 2 – A four robot simulation in an AWS Batch multi-node parallel job. On the main node (Node 1) there’s a container each for the test scenario runner and the simulator, and four nodes of three containers for each robot, representing the lidar processing, camera pipeline and the autonomous control.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;These are just two examples of simulations that can be run in Batch with multi-container support.&lt;/p&gt; 
&lt;h2&gt;Partners love this&lt;/h2&gt; 
&lt;p&gt;A number of AWS partners have used Batch for some time to provide simulation software and services for the AV/ADAS and robotics communities. Over the next few months, we’ll have more posts in this channel which dive deeper into the work they’re all doing.&lt;/p&gt; 
&lt;h3&gt;IPG Automotive&lt;/h3&gt; 
&lt;div id="attachment_3579" style="width: 369px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-3579" loading="lazy" class="size-full wp-image-3579" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.30.32.png" alt="Figure 3 - IPG CarMaker is an autonomous car simulator" width="359" height="396"&gt;
 &lt;p id="caption-attachment-3579" class="wp-caption-text"&gt;Figure 3 – IPG CarMaker is an autonomous car simulator&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://ipg-automotive.com/en/"&gt;IPG Automotive&lt;/a&gt; is an AWS partner that’s been developing software for decades. They’re best known for their AV/ADAS simulators: CarMaker, TruckMaker and MotorcycleMaker. IPG has been partnering with the AWS Batch team to develop the multi-container feature for Batch to enable their customers to scale out these simulations on AWS.&lt;/p&gt; 
&lt;p&gt;“&lt;em&gt;Developing cutting-edge autonomous vehicles and ADAS technologies requires hundreds of thousands of hours of testing within simulated environments mirroring real-world driving scenarios&lt;/em&gt;,” according to David Howarth, who is the Director of Business Development at IPG Automotive North America. “&lt;em&gt;By using AWS Batch multi-container jobs for our simulations, IPG’s customers can now seamlessly separate their CarMaker simulator, the 3D virtual environment, and their sensor pipelines into different containers on AWS. This capability accelerates both DevOps and debugging processes, significantly enhancing overall efficiency.&lt;/em&gt;”&lt;/p&gt; 
&lt;h3&gt;Robotec.ai&lt;/h3&gt; 
&lt;div id="attachment_3580" style="width: 394px" class="wp-caption alignleft"&gt;
 &lt;img aria-describedby="caption-attachment-3580" loading="lazy" class="size-full wp-image-3580" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.31.08.png" alt="Figure 4 - Simulated autonomous mining operation in RoSi" width="384" height="355"&gt;
 &lt;p id="caption-attachment-3580" class="wp-caption-text"&gt;Figure 4 – Simulated autonomous mining operation in RoSi&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://robotec.ai/"&gt;Robotec.ai&lt;/a&gt; is a growing simulation software company that has developed their own simulator called RoSi. They are working with customers on AV/ADAS, robotics, and autonomous mining operations.&lt;/p&gt; 
&lt;p&gt;“&lt;em&gt;Our customers demand thorough and safe testing of autonomous vehicles within a simulated environment, for example one of our mining customers, Boliden&lt;/em&gt;,” said Michal Niezgoda, CEO of Robotec.ai. “&lt;em&gt;The AWS Batch multi-container jobs feature streamlines our simulations at scale and powers our RoSi simulator to meet customer needs.”&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;“By using AWS Batch multi-container jobs, we can easily execute a large number of operational scenarios for a mining site with just a few clicks,” &lt;/em&gt;said Peter Burman, Program Manager of Boliden.&lt;em&gt; “This new feature, as part of AWS Batch, integrates seamlessly with other AWS services, allowing us to use the scaling and scheduling capabilities of the cloud and optimize compute costs.”&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;MORAI.ai&lt;/h3&gt; 
&lt;p&gt;MORAI, a technology company from Korea, offers a digital twin simulation system that accelerates the development and testing of autonomous vehicles, urban air mobility (UAM), autonomous mobile robots (AMR), and maritime autonomous surface ships (MASS).&lt;/p&gt; 
&lt;div id="attachment_3581" style="width: 322px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-3581" loading="lazy" class="size-full wp-image-3581" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.31.38.png" alt="Figure 5 - MORAI Simulation" width="312" height="499"&gt;
 &lt;p id="caption-attachment-3581" class="wp-caption-text"&gt;Figure 5 – MORAI Simulation&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;“&lt;em&gt;Before AWS Batch multi-container jobs, building custom simulation infrastructures for each client took several months. With the multi-container jobs feature, we can get our customers up and running with their simulations in just a few days&lt;/em&gt;,” said Jun Hong, co-founder and head of the R&amp;amp;D center at MORAI. “&lt;em&gt;This is crucial as they conduct extensive testing and validation to ensure their autonomous systems, including AV/ADAS, robotics, and maritime applications, are prepared to safely manage any scenarios they might face in the real world. The new feature significantly streamlines job preparation and reduces our reliance on in-house tool development&lt;/em&gt;.”&lt;/p&gt; 
&lt;h2&gt;How it works&lt;/h2&gt; 
&lt;p&gt;Let’s talk about the basic workflow for running multi-container simulations in Batch. The diagram in Figure 3 shows a high-level overview of the workflow we’ll discuss.&lt;/p&gt; 
&lt;div id="attachment_3582" style="width: 776px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3582" loading="lazy" class="wp-image-3582 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.32.00.png" alt="Figure 6 - The AWS Batch simulation flow. A user creates a multi-container job definition, then submits a job to the job queue for a compute environment." width="766" height="357"&gt;
 &lt;p id="caption-attachment-3582" class="wp-caption-text"&gt;Figure 6 – The AWS Batch simulation flow. A user creates a multi-container job definition, then submits a job to the job queue for a compute environment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 6 – The AWS Batch simulation flow. A user creates a multi-container job definition, then submits a job to the job queue for a compute environment.&lt;/p&gt; 
&lt;p&gt;To run a simulation job in AWS Batch you need to:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Build and push your containers into a repository, like Amazon ECR.&lt;/li&gt; 
 &lt;li&gt;Create the Batch resources including: 
  &lt;ol&gt; 
   &lt;li&gt;A compute environment (CE)&lt;/li&gt; 
   &lt;li&gt;A job queue (JQ)&lt;/li&gt; 
   &lt;li&gt;A multi-container job definition&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;Submit the job to the job queue&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Running an example simulation job using multiple containers&lt;/h2&gt; 
&lt;p&gt;Let’s use this framework to walk through a simple example that demonstrates how to start two containers in a Batch job so they can communicate with each other. One container is a &lt;em&gt;talker&lt;/em&gt; that publishes a message and the other is a &lt;em&gt;listener&lt;/em&gt; that will receive and echo the message. We’ll use &lt;a href="https://docs.ros.org/en/humble/index.html"&gt;ROS 2&lt;/a&gt;, a software library for autonomous vehicles and robotics, to communicate between the two containers. Figure 4 illustrates the simple communication. The &lt;em&gt;talker&lt;/em&gt; container will publish ‘hello’ on the ROS 2 topic named &lt;em&gt;/chatter&lt;/em&gt; and the &lt;em&gt;listener&lt;/em&gt; will echo the message.&lt;/p&gt; 
&lt;div id="attachment_3583" style="width: 580px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3583" loading="lazy" class="size-full wp-image-3583" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.32.28.png" alt="Figure 7 - An example Batch multi-container job with two containers—a talker and a listener." width="570" height="371"&gt;
 &lt;p id="caption-attachment-3583" class="wp-caption-text"&gt;Figure 7 – An example Batch multi-container job with two containers—a talker and a listener.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To run this example, we’ll follow the steps we outlined already to create the Batch job, using the console and the AWS Command-Line Interface (CLI). We’ll use a container image that already exists in Amazon ECR, so there’s no need to build or push the image.&lt;/p&gt; 
&lt;h3&gt;Step 1 – create a compute environment&lt;/h3&gt; 
&lt;p&gt;First, create an EC2 based compute environment from the AWS Batch console:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Log into your AWS Account, and navigate to the AWS Batch console&lt;/li&gt; 
 &lt;li&gt;In the navigation pane, choose &lt;strong&gt;Environments&lt;/strong&gt;, then &lt;strong&gt;Create &lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_3584" style="width: 863px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3584" loading="lazy" class="size-full wp-image-3584" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.33.01.png" alt="Figure 8 - Compute Environments console view" width="853" height="387"&gt;
 &lt;p id="caption-attachment-3584" class="wp-caption-text"&gt;Figure 8 – Compute Environments console view&lt;/p&gt;
&lt;/div&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Select &lt;strong&gt;Amazon&lt;/strong&gt; &lt;strong&gt;EC2&lt;/strong&gt; Orchestration Type, then&lt;strong&gt; Confirm&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Input a &lt;strong&gt;Name, &lt;/strong&gt;&lt;code&gt;CE_EC2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select an &lt;strong&gt;Instance Role &lt;/strong&gt;(if you don’t have an instance role follow &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/instance_IAM_role.html"&gt;the user guide&lt;/a&gt;, or choose &lt;strong&gt;Create new role&lt;/strong&gt;)&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;Next,&lt;/strong&gt; then &lt;strong&gt;Next&lt;/strong&gt; again, your default VPC and subnet information should be automatically added&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;Next&lt;/strong&gt;, then &lt;strong&gt;Create Compute Environment&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You’ve now created a compute environment. You can view it any time from the navigation pane under &lt;strong&gt;Compute Environments&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;div id="attachment_3585" style="width: 864px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3585" loading="lazy" class="size-full wp-image-3585" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.33.27.png" alt="Figure 9 - Compute Environment console with newly created environment" width="854" height="394"&gt;
 &lt;p id="caption-attachment-3585" class="wp-caption-text"&gt;Figure 9 – Compute Environment console with newly created environment&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Step 2 – create a job queue&lt;/h3&gt; 
&lt;p&gt;Next create a Job Queue for your Compute Environment from the AWS Batch console:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;On the navigation pane, choose &lt;strong&gt;Job Queues&lt;/strong&gt;, then &lt;strong&gt;Create &lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_3586" style="width: 862px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3586" loading="lazy" class="size-full wp-image-3586" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.33.45.png" alt="Figure 10 - Job queue console view" width="852" height="391"&gt;
 &lt;p id="caption-attachment-3586" class="wp-caption-text"&gt;Figure 10 – Job queue console view&lt;/p&gt;
&lt;/div&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Select &lt;strong&gt;Amazon EC2&lt;/strong&gt; Orchestration Type&lt;/li&gt; 
 &lt;li&gt;Input a &lt;strong&gt;Name&lt;/strong&gt;, &lt;code&gt;JQ_EC2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;From the &lt;strong&gt;Connected compute environments&lt;/strong&gt; drop down list, select the environment you created in step 1 (&lt;code&gt;CE_EC2&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;Create Job Queue&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You’ve now created a Job Queue to submit your job into. You can view it any time from the navigation pane under &lt;strong&gt;Job Queues.&lt;/strong&gt;&lt;/p&gt; 
&lt;div id="attachment_3587" style="width: 869px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3587" loading="lazy" class="size-full wp-image-3587" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.34.09.png" alt="Figure 11 - Job queue console with newly created queue" width="859" height="394"&gt;
 &lt;p id="caption-attachment-3587" class="wp-caption-text"&gt;Figure 11 – Job queue console with newly created queue&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Step 3 – create a job definition&lt;/h3&gt; 
&lt;p&gt;For this step, we’ll use the AWS CLI to create the Job Definition for our job using &lt;em&gt;JSON input&lt;/em&gt;. For instructions on setting up the AWS CLI, see the &lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html"&gt;Getting Started&lt;/a&gt; guide. You’ll need to update to the latest CLI version to get the new Batch multi-container functionality.&lt;/p&gt; 
&lt;p&gt;Create a new file called &lt;code&gt;ros2-talker-listener.json&lt;/code&gt; and copy the following JSON into that file.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;{
  "jobDefinitionName": "ros2-talker-listener",
  "type": "container",
  "platformCapabilities": [
    "EC2"
  ],
  "timeout": { 
      "attemptDurationSeconds": 60
  },
  "ecsProperties": {
    "taskProperties": [
      {
        "ipcMode": "task",
        "containers": [
          {
            "essential": true,
            "command": ["/bin/bash", "-c", "ros2 topic pub -t 10 /chatter std_msgs/msg/String '{data: hello}'"],
            "image": "public.ecr.aws/docker/library/ros:humble",
            "name": "talker",
            "resourceRequirements": [
              {
                "type": "VCPU",
                "value": "1"
              },
              {
                "type": "MEMORY",
                "value": "2048"
              }
            ]
          },
          {
            "essential": true,
            "command": ["/bin/bash", "-c", "ros2 topic echo /chatter"],
            "image": "public.ecr.aws/docker/library/ros:humble",
            "name": "listener",
            "dependsOn": [
            {
              "condition": "START",
              "containerName": "talker"
            }
            ],
            "resourceRequirements": [
              {
                "type": "VCPU",
                "value": "1"
              },
              {
                "type": "MEMORY",
                "value": "2048"
              }
            ]
          }
        ]
      }
    ]
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The JSON job definition above includes a new ecsProperties object that defines the two containers we’re running, the &lt;em&gt;talker&lt;/em&gt; and &lt;em&gt;listener&lt;/em&gt; containers. For each container, we’ll pull the image ros:humble from public ECR, and run the command inside each container. This example also requires us to set ipcMode to task so that the containers can communicate, and creates a dependency that the &lt;em&gt;listener&lt;/em&gt; container dependsOn the &lt;em&gt;talker&lt;/em&gt; getting to the START condition. For more details see the &lt;a href="https://docs.aws.amazon.com/batch/latest/APIReference/API_SubmitJob.html"&gt;AWS Batch SubmitJob API documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To create the job definition, we’ll use the AWS CLI again:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws batch register-job-definition --cli-input-json file://ros2-talker-listener.json&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It’s possible you may see an error that says:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;Unknown parameter in input: "ecsProperties", must be one of: jobDefinitionName, type, parameters, schedulingPriority, containerProperties, nodeProperties, retryStrategy, propagateTags, timeout, tags, platformCapabilities, eksProperties&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This means your AWS CLI version &lt;em&gt;doesn’t recognize the new parameters to the API&lt;/em&gt;. You’re probably using a version you installed before Batch had this feature, so you need to update it. For instructions on updating the AWS CLI, see the &lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html"&gt;Getting Started&lt;/a&gt; guide.&lt;/p&gt; 
&lt;p&gt;You can now view your job-definition from the Batch console under &lt;strong&gt;Job definitions:&lt;/strong&gt;&lt;/p&gt; 
&lt;div id="attachment_3588" style="width: 864px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3588" loading="lazy" class="size-full wp-image-3588" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.34.57.png" alt="Figure 12 - Job definitions console view" width="854" height="389"&gt;
 &lt;p id="caption-attachment-3588" class="wp-caption-text"&gt;Figure 12 – Job definitions console view&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Step 4 – submit your job&lt;/h3&gt; 
&lt;p&gt;Now that you’ve created the job definition, you can submit the job using the AWS CLI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws batch submit-job --job-queue JQ_EC2 --job-name ros2-talker-listener --job-definition ros2-talker-listener&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 5 – view results&lt;/h3&gt; 
&lt;p&gt;Now, return to the AWS Batch console and select &lt;strong&gt;Jobs&lt;/strong&gt; from the left menu. Then choose your job queue (JQ_EC2) from the drop-down menu. You should see your job listed. Click on the job name to view the job details.&lt;/p&gt; 
&lt;div id="attachment_3589" style="width: 867px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3589" loading="lazy" class="size-full wp-image-3589" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.35.21.png" alt="Figure 13 - Job details console view" width="857" height="389"&gt;
 &lt;p id="caption-attachment-3589" class="wp-caption-text"&gt;Figure 13 – Job details console view&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;After a few minutes, you should see that the job succeeded. You can now view the logs to see what happened.&lt;/p&gt; 
&lt;p&gt;On the &lt;strong&gt;Job attempts&lt;/strong&gt; tab, select the Log stream name to view the log events in AWS CloudWatch.&lt;/p&gt; 
&lt;div id="attachment_3590" style="width: 865px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3590" loading="lazy" class="size-full wp-image-3590" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.35.36.png" alt="Figure 14 - CloudWatch logs console view of talker output" width="855" height="389"&gt;
 &lt;p id="caption-attachment-3590" class="wp-caption-text"&gt;Figure 14 – CloudWatch logs console view of talker output&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;You should see the logs for the &lt;em&gt;talker&lt;/em&gt; looping 10 times publishing ‘hello’.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;Waiting for at least 1 matching subscription(s)...
publisher: beginning loop                         
publishing #1: std_msgs.msg.String(data='hello')
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Similarly, from the Batch Job details view, select the log name for the listener node, and you should see the log of the &lt;em&gt;listener&lt;/em&gt; echoing the message 10 times.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;data: hello
---
data: hello
---
data: hello
---
data: hello
---
data: hello
---
data: hello
---
data: hello
---
data: hello
---
data: hello
---
data: hello
---
&lt;/code&gt;&lt;/pre&gt; 
&lt;div id="attachment_3591" style="width: 877px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3591" loading="lazy" class="size-full wp-image-3591" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-14.36.00.png" alt="Figure 15 - CloudWatch logs console view of listener output" width="867" height="391"&gt;
 &lt;p id="caption-attachment-3591" class="wp-caption-text"&gt;Figure 15 – CloudWatch logs console view of listener output&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;And that’s it for running and viewing the &lt;em&gt;talker – listener&lt;/em&gt; example. This example demonstrated how you can start two containers in an AWS Batch job and they can communicate with each other.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we discussed some multi-container simulation use cases, learned how some AWS Partners &amp;nbsp;are using it to run their simulators on AWS Batch, and learned how to quickly get started running multi-container jobs in Batch using a simple example.&lt;/p&gt; 
&lt;p&gt;For more help using AWS Batch for simulations, follow the User Guide &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/Batch_GetStarted.html"&gt;Getting Started with AWS Batch&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Linter rules for Nextflow to improve the detection of errors before runtime</title>
		<link>https://aws.amazon.com/blogs/hpc/linter-rules-for-nextflow-to-improve-the-detection-of-errors-before-runtime/</link>
		
		<dc:creator><![CDATA[Mark Schreiber]]></dc:creator>
		<pubDate>Tue, 16 Apr 2024 14:58:36 +0000</pubDate>
				<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Life Sciences]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Genomics]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<guid isPermaLink="false">918a6c3069ff95919afafe3727710e60191c475d</guid>

					<description>Check out this post to learn how linter rules for Nextflow's DSL can help you catch errors in your workflows before runtime, which means greater developer productivity, which leads directly to a faster time to science.</description>
										<content:encoded>&lt;p&gt;&lt;a href="https://www.nextflow.io/"&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3518" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/Linter-rules-for-Nextflow-to-improve-the-detection-of-errors-before-runtime-1.png" alt="Linter rules for Nextflow to improve the detection of errors before runtime" width="380" height="212"&gt;Nextflow&lt;/a&gt; is a popular domain-specific language (DSL) and runtime used to define workflows that string together multiple processing steps into a pipeline. This allows it to perform quite complex genomics or scientific analyses including for machine-learning.&lt;/p&gt; 
&lt;p&gt;Workflows defined in Nextflow code can leverage container orchestration technologies to deploy containerized workloads across clusters, clouds, or HPC environments. As an interpreted language, errors in a Nextflow script are only revealed at runtime. This increases the time and cost of developing and debugging a workflow which could be reduced if errors could be spotted earlier.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll discuss how we created &lt;strong&gt;linter rules&lt;/strong&gt; for Nextflow DSL 2, how these rules can be extended and how you can use them to check your scripts before runtime.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;Nextflow is commonly used by customers on AWS. Some use the &lt;a href="https://aws.amazon.com/healthomics/"&gt;AWS HealthOmics&lt;/a&gt; managed workflow service and others choose to deploy their own Nextflow engine, integrated with AWS Batch. The HealthOmics service provides a fully-managed experience while &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch &lt;/a&gt;provides a more generalized mechanism for running batch computing tasks.&lt;/p&gt; 
&lt;p&gt;A key difference between the Nextflow DSL and a traditional programming language is that Nextflow code is interpreted at runtime rather than compiled. This provides flexibility, because workflows can be developed quickly without a build step. But it also means that you might not detect errors in the Nextflow script until the interpreter lands on that specific part of the code during an actual workflow run.&lt;/p&gt; 
&lt;p&gt;Because genomics workflows often involve processing large volumes of data, a workflow run could run for hours (and hours) before failing on a simple coding mistake. Having to restart these long-running analyses is frustrating and could be costly. This runtime-evaluation model argues for a static analysis tool that can scan Nextflow code &lt;em&gt;before&lt;/em&gt; workflows are run to detect issues early – known as a linter.&lt;/p&gt; 
&lt;h2&gt;Introducing a linter for Nextflow&lt;/h2&gt; 
&lt;p&gt;To address the need for static analysis, we’ve developed a static linter that can analyze Nextflow workflow code by taking advantage of the fact that the Nextflow DSL uses &lt;a href="https://groovy-lang.org/"&gt;Groovy&lt;/a&gt;, a dynamic language for the Java Virtual Machine, as its underlying implementation language. By building on Groovy, we can parse Nextflow code and analyze it, syntactically and semantically, without needing to execute the workflow itself.&lt;/p&gt; 
&lt;p&gt;We built our linter using &lt;a href="https://codenarc.org/"&gt;CodeNarc&lt;/a&gt;, an open-source static analysis tool used to enforce Groovy coding standards and best practices. Under the covers, CodeNarc uses Groovy’s own parser to generate an abstract syntax tree (AST) representing the structure of the code. It then provides a framework for coding semantic rules that can traverse this AST using the &lt;a href="https://en.wikipedia.org/wiki/Visitor_pattern"&gt;visitor pattern&lt;/a&gt; and check for rule violations. We can take advantage of this capability to analyze Nextflow scripts for potential issues.&lt;/p&gt; 
&lt;h2&gt;Understanding the Nextflow AST&lt;/h2&gt; 
&lt;p&gt;An abstract syntax tree is a hierarchical data structure that models code at increasing levels of granularity. The root node represents the entire script or program. Child nodes represent top-level statements and expressions with their children representing sub-expressions. By traversing this tree, and examining nodes with semantic meaning, rules can gather the information we need to detect problems.&lt;/p&gt; 
&lt;p&gt;We built a simple application called AST Echo to determine which AST node types need to be visited to evaluate different Nextflow language elements. Our app parses Nextflow code and prints out the AST hierarchy along with the Groovy node-type associated with each Nextflow construct.&lt;/p&gt; 
&lt;p&gt;Using this approach, we were able to discover that a Nextflow process declaration is a Groovy &lt;code&gt;MethodCallExpression&lt;/code&gt; and that a Nextflow process block maps to a Groovy &lt;code&gt;ClosureExpression&lt;/code&gt; containing a &lt;code&gt;BlockExpression&lt;/code&gt;. Here’s a partial example of a Nextflow DSL script. We’ve made the &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/blob/main/examples/example.nf"&gt;full source available on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-clike"&gt;nextflow.enable.dsl=2

process foo {
    container 'ubuntu:latest'
    cpus 1

    output:
    path 'foo.txt'

    script:
    """
    your_command &amp;gt; foo.txt
    """
}

// other processess …

workflow {
    data = channel.fromPath('/some/path/*.txt')
    foo()
    // further process calls …
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Providing this as input to our AST Echo app will produce something like this.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;(∅:∅)-(∅:∅) &amp;lt;BlockStatement&amp;gt;:  { (nextflow.enable.dsl = 2); this.process(this.foo({ -&amp;gt; ... })); this.process(this.bar({ -&amp;gt; ... })); this.workflow({ -&amp;gt; ... }) }
  -&amp;gt; (1:1)-(1:22) &amp;lt;ExpressionStatement&amp;gt;:  (nextflow.enable.dsl = 2)
    -&amp;gt; (1:1)-(1:22) &amp;lt;BinaryExpression&amp;gt;: (nextflow.enable.dsl = 2)
      -&amp;gt; (1:1)-(1:20) &amp;lt;PropertyExpression&amp;gt;: nextflow.enable.dsl
        -&amp;gt; (1:9)-(1:16) &amp;lt;PropertyExpression&amp;gt;: nextflow.enable
          -&amp;gt; (1:1)-(1:9) &amp;lt;VariableExpression&amp;gt;: nextflow
          -&amp;gt; (1:10)-(1:16) &amp;lt;ConstantExpression&amp;gt;: enable
        -&amp;gt; (1:17)-(1:20) &amp;lt;ConstantExpression&amp;gt;: dsl
      -&amp;gt; (1:21)-(1:22) &amp;lt;ConstantExpression&amp;gt;: 2
  -&amp;gt; (3:1)-(14:2) &amp;lt;ExpressionStatement&amp;gt;:  this.process(this.foo({ -&amp;gt; ... }))
    -&amp;gt; (3:1)-(14:2) &amp;lt;MethodCallExpression&amp;gt;: this.process(this.foo({ -&amp;gt; ... }))
      -&amp;gt; (3:1)-(3:1) &amp;lt;VariableExpression&amp;gt;: this
      -&amp;gt; (3:1)-(3:8) &amp;lt;ConstantExpression&amp;gt;: process
      -&amp;gt; (3:9)-(14:2) &amp;lt;ArgumentListExpression&amp;gt;: (this.foo({ -&amp;gt; ... }))
        -&amp;gt; (3:9)-(14:2) &amp;lt;MethodCallExpression&amp;gt;: this.foo({ -&amp;gt; ... })
          -&amp;gt; (3:9)-(3:9) &amp;lt;VariableExpression&amp;gt;: this
          -&amp;gt; (3:9)-(3:12) &amp;lt;ConstantExpression&amp;gt;: foo
          -&amp;gt; (3:13)-(14:2) &amp;lt;ArgumentListExpression&amp;gt;: ({ -&amp;gt; ... })
            -&amp;gt; (3:13)-(14:2) &amp;lt;ClosureExpression&amp;gt;: { -&amp;gt; ... }
              -&amp;gt; (4:5)-(14:1) &amp;lt;BlockStatement&amp;gt;:  { this.container(ubuntu:latest); this.cpus(1); this.path(foo.txt); 
    your_command &amp;gt; foo.txt
     }
                -&amp;gt; (4:5)-(4:30) &amp;lt;ExpressionStatement&amp;gt;:  this.container(ubuntu:latest)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The root of the AST is on the first line. Each subsequent line is a node of the tree indented by the depth of the node.&lt;/p&gt; 
&lt;p&gt;The start and stop locations of the code contained in the node are displayed first with their line number and character offset separated by ‘:’. The start and stop are separated by ‘-’. The type of Groovy expression or statement is displayed surrounded by angle brackets. Following this is the code fragment, possibly truncated, contained by the node as interpreted by the Groovy parser.&lt;/p&gt; 
&lt;h2&gt;Writing linter rules&lt;/h2&gt; 
&lt;p&gt;Armed with these mappings we can write CodeNarc rules targeting the relevant AST node types.&lt;/p&gt; 
&lt;p&gt;CodeNarc rules define a set of methods matching AST node types which will be called by CodeNarc when the corresponding nodes are visited during AST traversal. In the body of these visit methods, we can write logic to gather information and detect rule violations. Any issues can be reported by adding them to CodeNarc’s violation list.&lt;/p&gt; 
&lt;p&gt;For example, we can write a rule that checks if CPU or memory resource requests for processes fall within valid ranges. The rule would override CodeNarcs’s &lt;code&gt;AbstractAstVisitor&lt;/code&gt; &lt;code&gt;visitMethodCallExpression&lt;/code&gt; method which represents a method call in Groovy. It would check if the method call is requesting CPU resources and then evaluate if the arguments are valid.&lt;/p&gt; 
&lt;p&gt;Let’s look at part of a Groovy implementation of a rule to check the &lt;code&gt;cpu&lt;/code&gt; directive by overriding the &lt;code&gt;visitMethodCallExpression&lt;/code&gt; :&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-clike"&gt;class CpuAstVisitor extends AbstractAstVisitor {
    def MIN_CPU = 2
    def MAX_CPU = 96

    @Override
    void visitMethodCallExpression(MethodCallExpression expression) {
        if(expression.getMethodAsString() == 'cpus'){
            checkOneArgument(expression)
        }

        super.visitMethodCallExpression(expression)
    }


    private checkOneArgument(final MethodCallExpression expression){
        def methodArguments = AstUtil.getMethodArguments(expression)
        if (methodArguments.size() == 0) {
            addViolation(expression, 'the cpus directive must have one argument')
            return new EmptyExpression()
        } else if (methodArguments.size() &amp;gt; 1) {
            addViolation(expression, 'the cpus directive must have only one argument')
        }

        if( methodArguments.first() instanceof ConstantExpression){
           checkNumeric((ConstantExpression)methodArguments.first())
        }
    }

    private checkNumeric(ConstantExpression expression){
        try {
            def val = Integer.parseInt(expression.value.toString())
            checkMinMax(expression, val)
        } catch (NumberFormatException ignored){
            addViolation(expression,
                    "'${expression.value}' is not a valid number.")
        }

    }
    private void checkMinMax(Expression exp, final int val) {
        if (val &amp;lt; MIN_CPU) {
            addViolation(exp,
                    "The minimum CPU count is '$MIN_CPU'.")
        } else if (val &amp;gt; MAX_CPU) {
            addViolation(exp,
                    "The maximum CPU count is '$MAX_CPU'.")
        }
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;During the check, we validate several conditions. Violations are recorded by calling the &lt;code&gt;addViolation&lt;/code&gt; method. At the end of the checks the &lt;code&gt;visitMethodCallExpression&lt;/code&gt; of the super-class is invoked to continue the traversal of the AST. We’ve put the &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/blob/main/linter-rules/src/main/groovy/software/amazon/nextflow/rules/healthomics/CpuRule.groovy"&gt;full source on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;It’s critical to rigorously test rules to ensure they work as expected. CodeNarc provides a convenient rule-testing harness. It allows us to define test Nextflow code snippets along with expected rule violations. CodeNarc then runs analysis on these code fragments and compares the actual violations it finds to the expected results, ensuring that all &lt;em&gt;expected&lt;/em&gt; violations and no &lt;em&gt;unexpected&lt;/em&gt; violations are detected.&lt;/p&gt; 
&lt;p&gt;For example, we can test that the rule in from our previous code list is violated when the &lt;code&gt;cpus&lt;/code&gt; value is 97 but not violated when the value is 2.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-clike"&gt;@Test
void cpuRule_MaxViolation(){
    final SOURCE =
'''
process MY_PROCESS {
 cpus 97
}
'''
    assertSingleViolation(SOURCE, 3, 'cpus 97', “The maximum CPU count is ‘96’”)
    }
@Test
void cpuRule_NoViolationsMin(){
    final SOURCE =
'''
process MY_PROCESS {
 cpus 2
}
'''
    assertNoViolations(SOURCE)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This checks that the code defined by &lt;code&gt;SOURCE&lt;/code&gt; and asserts that a single violation occurs when &lt;code&gt;cpus&lt;/code&gt; is set to 97 and that no violations occur when cpus are set to 2. We’ve put the &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/blob/main/linter-rules/src/test/groovy/software/amazon/nextflow/rules/healthomics/CpuRuleTest.groovy"&gt;full source of this test on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Running the Linter Rules&lt;/h2&gt; 
&lt;p&gt;To run the linter rules you build the package and then run from the command line with&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;java  -Dorg.slf4j.simpleLogger.defaultLogLevel=error \
  -classpath ./linter-rules/build/libs/linter-rules-0.1.jar:CodeNarc-3.3.0-all.jar:slf4j-api-1.7.36.jar:slf4j-simple-1.7.36.jar \
  org.codenarc.CodeNarc \
  -report=text:stdout \
  -rulesetfiles=rulesets/healthomics.xml \
  -includes=**/**.nf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;-includes&lt;/code&gt; argument will inspect all filenames matching the &lt;code&gt;*.nf&lt;/code&gt; pattern in the current working directory, &lt;em&gt;and&lt;/em&gt; any sub-directory. To adjust the rules to be run, you adjust &lt;code&gt;-rulesetfiles&lt;/code&gt; to specify either a custom ruleset in CodeNarc format, or a prebuilt set like &lt;code&gt;rulesets/general.xml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To simplify this, we’ve provided a &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/blob/main/Dockerfile"&gt;Dockerfile&lt;/a&gt; that includes all required dependencies with a convenient entry point along with &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/tree/main?tab=readme-ov-file#docker"&gt;build and usage instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Using this mechanism, the command to run with Docker becomes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;docker run -v $PWD:/data -e ruleset=healthomics linter-rules-for-nextflow&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can find a prebuilt image in the &lt;a href="https://gallery.ecr.aws/aws-genomics/linter-rules-for-nextflow"&gt;Amazon ECR Public Gallery&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Detecting runtime compatibility issues&lt;/h2&gt; 
&lt;p&gt;Nextflow provides portability across environments, but workflows can still be written in ways that tie them to certain runtime capabilities.&lt;/p&gt; 
&lt;p&gt;We’ve developed rules that can detect the use of Nextflow features that may not be supported in AWS HealthOmics. For example, AWS HealthOmics ignores certain process directives that are only relevant to other runtime environments. Our rules warn when these directives are used so workflows designed for HealthOmics avoid incompatibility issues.&lt;/p&gt; 
&lt;p&gt;The AWS HealthOmics workflow service provides a managed environment optimized for security, scalability, and cost efficiency. Workflows must adhere to certain constraints around storage, containers, and infrastructure. We’ve built rules that check Nextflow code for patterns that violate HealthOmics environment policies.&lt;/p&gt; 
&lt;p&gt;For example, one rule checks that pipeline inputs are loaded from Amazon Simple Storage Service (Amazon S3) buckets, or HealthOmics Sequence Stores, and that pipeline container images are being pulled from Amazon ECR Private repositories.&lt;/p&gt; 
&lt;p&gt;Other rules check that resource requests don’t exceed HealthOmics instance type capabilities. Configuring these as linter rules allows workflows to be pre-validated rather than discovering issues mid-execution.&lt;/p&gt; 
&lt;p&gt;In addition to AWS HealthOmics specific checks, we’ve also &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/tree/main/linter-rules/src/main/groovy/software/amazon/nextflow/rules"&gt;created some general rules&lt;/a&gt; that detect invalid syntax, potential portability issues, and other common anti-patterns. For example, &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow/blob/main/linter-rules/src/main/groovy/software/amazon/nextflow/rules/AllowedDirectivesRule.groovy"&gt;a rule that detects&lt;/a&gt; the use of undefined process directives – this often indicates a typo or misunderstanding of the DSL.&lt;/p&gt; 
&lt;h2&gt;Flexible, extensible, and open-source&lt;/h2&gt; 
&lt;p&gt;The linter lets you &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow?tab=readme-ov-file#general-nf-rules-only"&gt;include only rule sets you’re interested in&lt;/a&gt;. You can focus strictly on general Nextflow rules or both general rules and HealthOmics best practices. Over time, we expect the rules library will grow to cover more scenarios.&lt;/p&gt; 
&lt;p&gt;We architected our linter so that rules can easily be contributed by the Nextflow community.&lt;/p&gt; 
&lt;p&gt;We’ve released the linter rules and the AST Echo utility as &lt;a href="https://github.com/awslabs/linter-rules-for-nextflow"&gt;open-source projects on GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The code we’ve provided there uses the Apache 2.0 license. We welcome issues, pull requests, and additional rules from the Nextflow community.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;By providing early feedback on Nextflow code, this linter can improve developer productivity, reduce errors, and make workflows more portable. We encourage all Nextflow pipeline authors to integrate it into their continuous integration pipelines and contribute to its evolution.&lt;/p&gt; 
&lt;p&gt;Static analysis of domain-specific languages like Nextflow opens up new possibilities for accelerating scientific advancements through code quality and collaboration.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Intel Open Omics Acceleration Framework on AWS: fast, cost-efficient, and seamless</title>
		<link>https://aws.amazon.com/blogs/hpc/intel-open-omics-acceleration-framework-on-aws-fast-cost-efficient-and-seamless/</link>
		
		<dc:creator><![CDATA[Olivia Choudhury]]></dc:creator>
		<pubDate>Tue, 09 Apr 2024 13:50:53 +0000</pubDate>
				<category><![CDATA[Healthcare]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Genomics]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<guid isPermaLink="false">1b05a2f936ae2290795be4f7bdad69e08725240e</guid>

					<description>With genomics and multi-omics research generating more data than ever, the Open Omics Acceleration Framework from Intel Labs aims to provide a highly productive platform for researchers. Check out recent results in this new blog post.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Olivia Choudhury, PhD and Aniket Deshpande from AWS; Sanchit Misra, PhD, Vasimuddin Md., PhD,&amp;nbsp;Narendra Chaudhary, PhD, Saurabh Kalikar, PhD, and Manasi Tiwari, PhD, Research Scientists at Intel Labs India; Ashish Kumar Patel, contingent worker with Intel Technology India Pvt. Ltd.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;We are living in the exciting times of the rapidly growing field of omics, including genomics, proteomics, transcriptomics, and metabolomics data. Our ability to measure omics data is increasing at a dramatic pace and new data science (AI and data management) pipelines are being developed and quickly standardized. Cloud plays a key role in this initiative by providing massive compute and storage to enable public data repositories, large collaborative projects, and consortia.&lt;/p&gt; 
&lt;p&gt;To drive and realize the promise of omics, Intel Labs is building the Open Omics Acceleration Framework as a highly productive platform for biologists and data scientists, enabling them to harness computing and data at unprecedented scale and speed at lower costs.&lt;/p&gt; 
&lt;p&gt;In this post, we benchmark three standard omics pipelines – AlphaFold2-based protein folding, DeepVariant-based variant calling, and Scanpy-based single-cell RNA-Seq analysis – available in the latest version of Open Omics Acceleration Framework (v. 2.1) against prior CPU baseline on Xeon-based Amazon Elastic Compute Cloud (Amazon EC2) instances to showcase its performance and cost benefits.&lt;/p&gt; 
&lt;h2&gt;Open Omics Acceleration Framework&lt;/h2&gt; 
&lt;p&gt;The Open Omics Acceleration Framework is a one-click, containerized, customizable, open-sourced framework for accelerating analysis of omics datasets. The framework is being built with a modular design that keeps in mind the different ways the users would want to interact with it. As shown in Figure 1, it consists of three layers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pipeline layer&lt;/strong&gt;: for users who are looking for a one-click solution to run standard pipelines. The latest version (v. 2.1) supports the following pipelines: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/tree/main/pipelines/fq2sortedbam"&gt;&lt;strong&gt;fq2sortedbam&lt;/strong&gt;&lt;/a&gt;: Given gzipped fastq files from a sample, this workflow performs bwa-mem2-based sequence mapping and sorting to output the sorted BAM file.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/tree/main/pipelines/deepvariant-based-germline-variant-calling-fq2vcf"&gt;&lt;strong&gt;DeepVariant-based germline pipeline for variant calling (fq2vcf)&lt;/strong&gt;&lt;/a&gt;: Given paired end gzipped fastq files from a sample, this workflow performs sequence mapping, sorting and variant calling (using DeepVariant) to output a vcf file.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/tree/main/pipelines/alphafold2-based-protein-folding"&gt;&lt;strong&gt;AlphaFold2-based protein folding&lt;/strong&gt;&lt;/a&gt;: Given one or more protein sequences, this workflow performs preprocessing (database search and multiple sequence alignment) and structure prediction (using AlphaFold2) to output the structure(s) of the protein sequences.&lt;/li&gt; 
   &lt;li&gt;&lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/tree/main/pipelines/single-cell-RNA-seq-analysis"&gt;&lt;strong&gt;Single-cell RNA-Seq analysis&lt;/strong&gt;&lt;/a&gt;: Given a cell by gene matrix, this&amp;nbsp;Scanpy workflow performs data load and preprocessing (filter, linear regression and normalization), dimensionality reduction (PCA), clustering (Louvain / Leiden / kmeans) to cluster the cells into different cell types and visualize those clusters (UMAP / t-SNE).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toolkit (applications) layer&lt;/strong&gt;: for users who want to use individual tools or to create their own custom pipelines by combining various tools.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Building blocks layer&lt;/strong&gt;: for tool developers, this layer consists of key building blocks – biology-specific and generic-AI algorithms and data structures – that can replace ones used in existing tools to accelerate them or can be used as ingredients to build new efficient tools.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div id="attachment_3467" style="width: 1966px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3467" loading="lazy" class="size-full wp-image-3467" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.14.22@2x.png" alt="Figure 1: Open Omics Acceleration Framework v. 2.1. This modular architecture consists of 3 layers: pipeline, toolkit (application), and building blocks layers. " width="1956" height="874"&gt;
 &lt;p id="caption-attachment-3467" class="wp-caption-text"&gt;Figure 1: Open Omics Acceleration Framework v. 2.1. This modular architecture consists of 3 layers: pipeline, toolkit (application), and building blocks layers.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Benchmarking the Open Omics Acceleration Framework on Amazon EC2 instances&lt;/h2&gt; 
&lt;h3&gt;Amazon EC2 instances used for benchmarking&lt;/h3&gt; 
&lt;p&gt;The five types of Amazon EC2 instances used in this benchmarking study are detailed in Table 1. All of them are powered by 4th Generation Intel® Xeon® Scalable processors. We executed all the experiments using Ubuntu version 22.04. Across the three pipelines, the execution time includes all the steps performed including reading/writing input/output files from/to local disk and therefore, includes both compute and file IO. The cloud costs are computed by multiplying the total execution time in hours by the per hour instance cost.&lt;/p&gt; 
&lt;div id="attachment_3468" style="width: 1542px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3468" loading="lazy" class="size-full wp-image-3468" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.16.20@2x.png" alt="Table 1: Details of the Amazon EC2 instance types used for benchmarking. On-Demand pricing is from the published date for the us-east-2 (Ohio) region and is subject to change over time. Please consult the Amazon EC2 pricing page for current pricing in your region." width="1532" height="696"&gt;
 &lt;p id="caption-attachment-3468" class="wp-caption-text"&gt;Table 1: Details of the Amazon EC2 instance types used for benchmarking. On-Demand pricing is from the published date for the us-east-2 (Ohio) region and is subject to change over time. Please consult the Amazon EC2 pricing page for current pricing in your region.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;To run these pipelines, you need an AWS account with permissions to provision Amazon Simple Storage Service (S3) buckets for input and output data storage, and sufficient permissions/limits to provision Amazon EC2 c7i, m7i and r7i instances.&lt;/p&gt; 
&lt;h2&gt;Steps for benchmarking&lt;/h2&gt; 
&lt;p&gt;The configuration details and steps used for benchmarking baseline and Open Omics versions of all three pipelines on Amazon EC2 instances are detailed on this &lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/blob/main/benchmarking/AWS-Intel-blog-v2.1-2024/README.md"&gt;GitHub page&lt;/a&gt; of the Open Omics Acceleration Framework. The typical process involves launching the corresponding EC2 Instances, connecting to the instances, installing the software, downloading the datasets, and executing the baseline and Open Omics versions. In the following subsections, we will share an overview of the pipelines and report the benchmarking results.&lt;/p&gt; 
&lt;h2&gt;AlphaFold2-based protein folding pipeline&lt;/h2&gt; 
&lt;p&gt;The protein folding problem has significant implications for drug discovery, biotechnology, and understanding the mechanisms of diseases. The task entails predicting the 3D structure of a protein from its amino acid sequence. A protein’s structure governs its function. Therefore, accurate protein structure prediction is vital in biology and drug discovery, and has long been considered a holy grail problem.&lt;/p&gt; 
&lt;div id="attachment_3469" style="width: 1924px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3469" loading="lazy" class="size-full wp-image-3469" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.18.31@2x.png" alt="Figure 2: The AlphaFold2-based protein structure prediction pipeline that requires an input amino acid sequence. The pipeline has two stages, namely database search for multiple sequence alignment and Evoformer model for structure prediction." width="1914" height="256"&gt;
 &lt;p id="caption-attachment-3469" class="wp-caption-text"&gt;Figure 2: The AlphaFold2-based protein structure prediction pipeline that requires an input amino acid sequence. The pipeline has two stages, namely database search for multiple sequence alignment and Evoformer model for structure prediction.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Our AlphaFold2-based protein folding pipeline takes as input a set of amino acid sequences and outputs the set of corresponding predicted structures in PDB file format. The pipeline has two stages: 1) preprocessing that includes database search and multiple sequence alignment (MSA) over protein sequences, 2) model inference&amp;nbsp;that predict the structure of the protein using Evoformer, a Transformer architecture based Deep Learning (DL) model.&lt;/p&gt; 
&lt;p&gt;For baseline, we use OpenFold v. 1.0.1 – a faithful reproduction of &lt;a href="https://github.com/google-deepmind/alphafold"&gt;DeepMind AlphaFold2’s model&lt;/a&gt; for model inference and &lt;a href="https://github.com/soedinglab/hh-suite"&gt;hh-suite&lt;/a&gt; v. 3.3.0 and &lt;a href="https://github.com/EddyRivasLab/hmmer"&gt;hmmer&lt;/a&gt; v. 3.3.2 for preprocessing. Open Omics Acceleration Framework contains faster versions of all the steps of this pipeline accelerated using a 4th gen Intel Xeon scalable processor using Intel AMX with bfloat16 precision for DL compute, Intel AVX2 and AVX-512 for non-DL compute and cache optimizations. It also provides a parallel execution framework that folds multiple proteins in parallel in a load balanced fashion.&lt;/p&gt; 
&lt;p&gt;&lt;img loading="lazy" class="aligncenter size-full wp-image-3470" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.20.06@2x.png" alt="" width="1760" height="1210"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="aligncenter size-full wp-image-3471" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.20.14@2x.png" alt="" width="1760" height="1216"&gt;&lt;img loading="lazy" class="aligncenter size-full wp-image-3472" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.20.19@2x.png" alt="" width="1764" height="1208"&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;div id="attachment_3473" style="width: 1764px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3473" loading="lazy" class="size-full wp-image-3473" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.20.30@2x.png" alt="Figure 3: Performance and cost benefits of Open Omics Acceleration Framework on m7i.24xlarge and m7i.48xlarge over the baseline on m7i.24xlarge for AlphaFold2-based protein folding pipeline for varying protein lengths. " width="1754" height="1208"&gt;
 &lt;p id="caption-attachment-3473" class="wp-caption-text"&gt;Figure 3: Performance and cost benefits of Open Omics Acceleration Framework on m7i.24xlarge and m7i.48xlarge over the baseline on m7i.24xlarge for AlphaFold2-based protein folding pipeline for varying protein lengths.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;OpenFold is a faithful PyTorch based reproduction of AlphaFold2 which can run on CPU. We compare the OpenFold CPU baseline with Open Omics on two sets of proteins sampled from the &lt;a href="https://www.uniprot.org/proteomes/UP000001940"&gt;C. elegans proteome&lt;/a&gt;. Since a large majority of the proteins found in nature have lengths less than 1000, we create the first set with proteins of length under 1000 amino acid residues. The second set consists of proteins up to lengths that OpenFold can handle. On the same m7i.24xlarge instance as baseline, Open Omics achieves significant cost and execution time improvements. Moreover, due to its ability to fold multiple proteins in parallel, it also scales well to the larger m7i.48xlarge instance. For the first set, Open Omics achieves 10.1x speedup on preprocessing and 37.1x speedup on model inference resulting in speedup for end-to-end execution of 17x and cost reduction of 8.5x. For the second set, the speedup values for individual stages are 5.2x and 33.2x, respectively, and overall speedup is 20.8x and cost reduction is 10.4x. For longer sequences, OpenFold did not finish even after 3 days. On the other hand, Open Omics can comfortably handle sequences up to length 7500 on m7i.48xlarge instances.&lt;/p&gt; 
&lt;h2&gt;DeepVariant-based variant calling pipeline (fq2vcf)&lt;/h2&gt; 
&lt;p&gt;Variant calling is a fundamental task in DNA sequence analysis. Given the sequencing reads from an individual’s genome, variant calling identifies the variations in the reads against a reference genome. &lt;a href="https://www.nature.com/articles/nbt.4235.epdf?author_access_token=q4ZmzqvvcGBqTuKyKgYrQ9RgN0jAjWel9jnR3ZoTv0NuM3saQzpZk8yexjfPUhdFj4zyaA4Yvq0LWBoCYQ4B9vqPuv8e2HHy4vShDgEs8YxI_hLs9ov6Y1f_4fyS7kGZ"&gt;DeepVariant&lt;/a&gt;, a deep learning-based germline variant caller, is a highly accurate and widely used tool across many genomic studies.&lt;/p&gt; 
&lt;div id="attachment_3474" style="width: 1918px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3474" loading="lazy" class="size-full wp-image-3474" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.21.48@2x.png" alt="Figure 4: The DeepVariant based fq2vcf pipeline." width="1908" height="256"&gt;
 &lt;p id="caption-attachment-3474" class="wp-caption-text"&gt;Figure 4: The DeepVariant based fq2vcf pipeline.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The variant calling pipeline using DeepVariant, as shown in Figure 4, consists of the following steps: 1) mapping of the input reads to the reference genome, 2) sorting the mapped reads based on the reference coordinate, and 3) calling variants by first creating pileups of the regions that are expected to have variants followed by classifying among variants types using Inception V3 deep learning model. For baseline, we use – &lt;a href="https://github.com/lh3/bwa"&gt;BWA-MEM&lt;/a&gt; v. 0.7.17 for mapping, samtools v. 1.16.1 sort for sorting and &lt;a href="https://github.com/google/deepvariant"&gt;DeepVariant&lt;/a&gt; v. 1.5 for variant calling. &lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/tree/main/pipelines/deepvariant-based-germline-variant-calling-fq2vcf"&gt;Open Omics version of the pipeline&lt;/a&gt; is accelerated through use of:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Intel Advanced Matrix Extensions (AMX) with bfloat16 precision for DL compute, Intel AVX-512 for non-DL compute,&lt;/li&gt; 
 &lt;li&gt;Cache optimizations, and&lt;/li&gt; 
 &lt;li&gt;Scaling it to the multiple CPU instances.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div id="attachment_3475" style="width: 994px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3475" loading="lazy" class="size-full wp-image-3475" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.23.08.png" alt="Figure 5: Performance and cost benefits of Open Omics Acceleration Framework over the baseline for DeepVariant-based fq2vcf pipeline on standard 30x WGS HG001 short read dataset with GRCh38 as the reference genome. Instances used: c7i.24xlarge and c7i.48xlarge." width="984" height="945"&gt;
 &lt;p id="caption-attachment-3475" class="wp-caption-text"&gt;Figure 5: Performance and cost benefits of Open Omics Acceleration Framework over the baseline for DeepVariant-based fq2vcf pipeline on standard 30x WGS HG001 short read dataset with GRCh38 as the reference genome. Instances used: c7i.24xlarge and c7i.48xlarge.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We compare baseline with Open Omics on the standard 30x WGS paired-end short read dataset: HG001 (&lt;a href="//genomics-benchmark-datasets/google-brain/fastq/novaseq/wgs_pcr_free/30x/HG001.novaseq.pcr-free.30x.R1.fastq.gz"&gt;R1&lt;/a&gt; and &lt;a href="//genomics-benchmark-datasets/google-brain/fastq/novaseq/wgs_pcr_free/30x/HG001.novaseq.pcr-free.30x.R2.fastq.gz"&gt;R2&lt;/a&gt;). On the same instance type (c7i.24xlarge), Open Omics achieves 2.21x speedup compared to baseline resulting in a cost of just $8.8 per sample. For use-cases that require a quick turn-around time, Open Omics scales well to 4x c7i.48xlarge instances reducing execution time to nearly 21 mins at just $12.1 per sample – 1.62 times lower cost than baseline. It also scales further to 8x c7i.48xlarge instances, reducing execution time to just 16.8 mins while still keeping the cost lower than the baseline.&lt;/p&gt; 
&lt;h2&gt;Single-cell RNA-Seq analysis pipeline&lt;/h2&gt; 
&lt;p&gt;Single-cell analysis involves studying various omics (genomics, transcriptomics, proteomics, metabolomics) data and cell-cell interactions at the individual cell level. Single-cell RNA-seq (scRNA-seq) is an advanced technique that measures gene expression of individual cells, which is analyzed to study the differences in gene expression profiles across cells.&lt;/p&gt; 
&lt;p&gt;A typical workflow to analyze scRNA-seq data begins with a matrix that consists of the expression levels of the genes in each cell. The data preprocessing steps filter out the noise and uses linear regression and data normalization to correct artifacts from data collection. Subsequently, dimensionality reduction is performed followed by clustering of cells to group them by similarity in genetic activity and visualization of the clusters. With over 2 million downloads,&amp;nbsp;&lt;a href="https://github.com/scverse/scanpy"&gt;Scanpy&lt;/a&gt;&amp;nbsp;is one of the most widely used toolkits for this analysis.&lt;/p&gt; 
&lt;div id="attachment_3476" style="width: 962px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3476" loading="lazy" class="size-full wp-image-3476" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.24.18.png" alt="Figure 6: Pipeline showing the steps in analysis of single-cell RNA sequencing data starting from gene activity matrix to visualization of different cell clusters." width="952" height="249"&gt;
 &lt;p id="caption-attachment-3476" class="wp-caption-text"&gt;Figure 6: Pipeline showing the steps in analysis of single-cell RNA sequencing data starting from gene activity matrix to visualization of different cell clusters.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 6 illustrates the pipeline we used for this benchmarking. For clustering and visualization, we run all of the options displayed above to demonstrate the benefit of the Open Omics Acceleration Framework on all of them. For the baseline, we use &lt;a href="https://github.com/scverse/scanpy"&gt;scanpy&lt;/a&gt; v. 1.9.1. The &lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework/tree/main/pipelines/single-cell-RNA-seq-analysis"&gt;Open Omics version&lt;/a&gt; is accelerated through: 1) parallelization and fusing of operations in data pre-processing, 2) use of efficient implementations from &lt;a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/scikit-learn.html"&gt;Intel&lt;sup&gt;®&lt;/sup&gt; Extension for Scikit-learn&lt;/a&gt;, &lt;a href="https://katanagraph.ai/"&gt;Katana Graph&lt;/a&gt; and Intel Lab’s &lt;a href="https://github.com/IntelLabs/Trans-Omics-Acceleration-Library"&gt;Trans-Omics Acceleration Library&lt;/a&gt; – some of which were accelerated as a part of this effort.&lt;/p&gt; 
&lt;div id="attachment_3477" style="width: 964px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3477" loading="lazy" class="size-full wp-image-3477" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.25.02.png" alt="Figure 7: Performance and cost benefits of Open Omics Acceleration Framework over the baseline for Single-cell RNA-Seq pipeline for standard dataset of 1.3 million mouse brain cells." width="954" height="961"&gt;
 &lt;p id="caption-attachment-3477" class="wp-caption-text"&gt;Figure 7: Performance and cost benefits of Open Omics Acceleration Framework over the baseline for Single-cell RNA-Seq pipeline for standard dataset of 1.3 million mouse brain cells.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We compare baseline with Open Omics on the standard dataset of &lt;a href="https://rapids-single-cell-examples.s3.us-east-2.amazonaws.com/1M_brain_cells_10X.sparse.h5ad"&gt;1.3 million mouse brain cells&lt;/a&gt;. The baseline requires the larger memory r7i.24xlarge instance. On the same instance type, Open Omics achieves a 29.3x speedup compared to baseline resulting in a cost of nearly $0.6 per sample. This cost is further improved with the optimizations applied to reduce memory requirements that enables us to use the less expensive compute optimized c7i.24xlarge instance, reducing the cost to just $0.4 per sample.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;To accelerate the ongoing revolution of omics, Intel has built the Open Omics acceleration framework. The current version of Open Omics running on AWS instances based on 4&lt;sup&gt;th&lt;/sup&gt; generation Intel Xeon processors provides significant performance and cost benefits for key pipelines for protein folding, variant calling, and single-cell RNA-Seq analysis.&lt;/p&gt; 
&lt;p&gt;To learn more about performance of Open Omics and comparison with other solutions, please refer to the following research blogs –&amp;nbsp;&lt;a href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Xeon-is-all-you-need-for-AI-inference-Performance/post/1506083"&gt;July’23&lt;/a&gt;, &lt;a href="https://aws.amazon.com/blogs/hpc/accelerating-genomics-pipelines-using-intel-open-omics-on-aws/"&gt;Aug’22&lt;/a&gt;, &lt;a href="https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Intel-Labs-Accelerates-Single-cell-RNA-Seq-Analysis/post/1390715"&gt;June’22&lt;/a&gt;, and the GitHub repository –&amp;nbsp;&lt;a href="https://github.com/IntelLabs/Open-Omics-Acceleration-Framework"&gt;Open Omics Acceleration Framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.s&lt;/em&gt;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.28.09.png" alt="Sanchit Misra" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Sanchit Misra&lt;/h3&gt; 
  &lt;p&gt;Sanchit Misra, PhD is a senior research scientist and leads the research efforts in building SW/HW computing solutions for AI driven life science at Intel Labs. Sanchit has over 15 years of experience in computational biology, AI, scaling applications on large clusters/supercomputers and driving architecture improvements. Over the years, he has led Intel’s various collaborations with experts in computational biology, AI, HPC and architecture. Before joining Intel Labs, he earned his PhD in high performance computational biology from Northwestern University.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.28.15.png" alt="Vasimuddin Md" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Vasimuddin Md&lt;/h3&gt; 
  &lt;p&gt;Vasimuddin Md., PhD is a research scientist at Parallel Computing Lab, Intel Labs. He obtained his Ph.D. from IIT Bombay and has a decade of research experience in the field of Computational Biology, HPC, and AI. At Intel Labs, he works on building computational solutions (both hardware and software) for various challenging problems in Biology.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.28.22.png" alt="Narendra Chaudhary" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Narendra Chaudhary&lt;/h3&gt; 
  &lt;p&gt;Narendra Chaudhary, PhD is a research scientist and conducts computational biology/HPC research at Intel Labs. Dr. Chaudhary has experience in improving efficiency and scaling of deep learning applications. Before joining Intel Labs, he earned his PhD in Electrical and Electronics Engineering from Texas A&amp;amp;M University.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.28.27.png" alt="Saurabh Kalikar" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Saurabh Kalikar&lt;/h3&gt; 
  &lt;p&gt;Saurabh Kalikar, PhD is a research scientist at Intel – Parallel Computing Lab, where he is involved in designing and implementing performant solutions for the applications running on modern CPUs. His research interests lie in Parallel/Distributed Computing, Compilers, and Architecture-Aware Optimization. Prior to joining Intel Labs, he earned his PhD from Indian Institute of Technology Madras.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.28.32.png" alt="Manasi Tiwari" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Manasi Tiwari&lt;/h3&gt; 
  &lt;p&gt;Manasi Tiwari, PhD is a research scientist at Intel – Parallel Computing Lab. She works on ideating and developing parallel solutions for Computational Biology algorithms which are at the intersection of HPC and AI. These solutions range from architecture dependent SIMD solutions to large scale multi-node solutions. Prior to joining Intel, she earned her PhD from Indian Institute of Science, Bangalore.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/03/CleanShot-2024-04-03-at-10.28.37.png" alt="Ashishkumar Patel" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Ashishkumar Patel&lt;/h3&gt; 
  &lt;p&gt;Ashishkumar Patel is a contingent worker at Intel Technology India Pvt. Ltd. He earned his master’s degree in computer engineering from the Sardar Vallabhbhai National Institute of Technology (NIT), Surat. Ashishkumar has a background in academia and research. Throughout his career, he has made significant contributions to various projects, particularly in the fields of deep learning and computational biology.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Using a digital twin for sensitivity analysis to determine sensor placement in a roll-to-roll manufacturing web-line</title>
		<link>https://aws.amazon.com/blogs/hpc/using-a-digital-twin-for-sensitivity-analysis-to-determine-sensor-placement-in-a-roll-to-roll-manufacturing-web-line/</link>
		
		<dc:creator><![CDATA[Ross Pivovar]]></dc:creator>
		<pubDate>Tue, 02 Apr 2024 15:02:29 +0000</pubDate>
				<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">3f0b08d93c50365a758b4efba227be21f49403be</guid>

					<description>What's the best way to select sensors to capture key data for your digital twin without overspending? Check out our latest blog post on using ML and sensitivity studies to optimize sensor selection.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Ross Pivovar, Solution Architect, and Adam Rasheed, Head of Emerging Workloads &amp;amp; Technologies, AWS and Orang Vahid, Director of Engineering Services and Kayla Rossi, Application Engineer, Maplesoft&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;In our previous posts we discussed &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/"&gt;how to setup a Level 4 Digital Twin (DT)&lt;/a&gt; that adapts to changing environments &lt;em&gt;and&lt;/em&gt; &lt;a href="https://aws.amazon.com/blogs/hpc/using-a-level-4-digital-twin-for-scenario-analysis-and-risk-assessment-of-manufacturing-production-on-aws/"&gt;how to use the L4 DT to perform forecasting, scenario analysis, and risk assessment&lt;/a&gt; based on incoming measurement data. In this post today, we’ll discuss methods to find the optimal number, type, and placement of sensors to maximize the accuracy of your Level 3 or Level 4 DT. If you need a refresher, you can check out the &lt;a href="https://aws.amazon.com/blogs/iot/digital-twins-on-aws-unlocking-business-value-and-outcomes/"&gt;AWS digital twin leveling framework&lt;/a&gt;. In practice, we don’t have the luxury of adding thousands of sensors to our system. Sensors incur a capital cost, require maintenance and calibration by technicians, inevitably fail (requiring replacement), and they need actual physical space to be installed. In contrast, too few sensors and we miss valuable data that could indicate performance issues, possible system failures, and potential safety issues. For complicated systems or &lt;em&gt;system-of-systems&lt;/em&gt;, the number of variables of interest can range from tens to thousands. Hence, it becomes quite difficult and tedious to manually understand the relationship between different factors that impact the performance of the overall system.&lt;/p&gt; 
&lt;p&gt;In this post, we leverage a combination of our &lt;a href="https://www.maplesoft.com/products/maplesim/"&gt;MapleSim&lt;/a&gt; DT, machine learning (ML) models, and Shapley sensitivity studies to analyze the relationships between different variables and identify the best sensor placement to trade-off cost versus prediction capability. A sensitivity analysis determines how responsive your engineering models’ outputs are to changes in key input parameters, identifying which variables have the greatest impact on the results. We use &lt;a href="https://github.com/aws-samples/twinflow"&gt;TwinFlow&lt;/a&gt; to deploy and execute our studies on AWS. We also briefly explore how generative AI can assist in sensor selection.&lt;/p&gt; 
&lt;h2&gt;Where are sensors needed, what type, and how many&lt;/h2&gt; 
&lt;p&gt;As in the prior posts, we consider the web-handling process in roll-to-roll manufacturing for continuous materials like paper, film, and textiles. This example web-handling process involves unwinding the material from a spool, guiding it through various treatments like printing or coating, and then winding it onto individual rolls.&lt;/p&gt; 
&lt;p&gt;Figure 1 is a schematic diagram of the same roll-to-roll manufacturing line discussed in those earlier posts. The manufacturing line consists of 9 rollers (labeled with “R”) and 12 material spans (labeled with “S”). The operator’s goal is to maximize throughput/yield of the manufacturing line without damaging the product or creating safety hazards. It is critical to monitor various stages in the process for quality control.&lt;/p&gt; 
&lt;p&gt;It’s tempting to instrument each span or roller with pressure gauges, load cells (to measure tension), feed line sensors (for linear velocity), and roller tachometers (RPM) sensors to track the progress of the roll-to-roll material in the line. This is rarely practical however, due to the high cost of installing and maintaining the sensors. Furthermore, in some cases, there may not be physical space in the facility for all of them.&lt;/p&gt; 
&lt;div id="attachment_3437" style="width: 590px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3437" loading="lazy" class="size-full wp-image-3437" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.18.37.png" alt="Figure 1 Schematic diagram of the web-handling equipment in which each span is labeled as S1 through S12 and rollers are label R1 through R10." width="580" height="165"&gt;
 &lt;p id="caption-attachment-3437" class="wp-caption-text"&gt;Figure 1 Schematic diagram of the web-handling equipment in which each span is labeled as S1 through S12 and rollers are label R1 through R10.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For our use case, it is known that the minimum information needed to prevent product quality issues include the tension in the material of each span and whether the material is dragging over the rollers (slip) instead of rotating with the rollers.&lt;/p&gt; 
&lt;p&gt;When considering how to efficiently avoid quality issues, the questions to be answered are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;When is tension above 175 Newtons (a tension failure threshold)?&lt;/li&gt; 
 &lt;li&gt;When is slip velocity greater than 1×10&lt;sup&gt;-3&lt;/sup&gt; m/s (a differential velocity failure threshold)?&lt;/li&gt; 
 &lt;li&gt;Which sensors are needed to monitor these failure mechanisms?&lt;/li&gt; 
 &lt;li&gt;Are all sensor locations needed?&lt;/li&gt; 
 &lt;li&gt;Can downstream sensors provide information about upstream effects?&lt;/li&gt; 
 &lt;li&gt;Is there sensor redundancy?&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In this example we have at least 42 potential sensors to be included in our manufacturing line. One option is to install sensors to directly measure the tension and differential velocity at all locations. This is not a viable solution for operators due to the cost of installing and maintaining the tension load cells and velocity sensors.&lt;/p&gt; 
&lt;p&gt;Instead, we can use a digital twin which provides a physics-based simulation of the process, enabling the combination of sensor data with predictions. A digital twin provides both insights into the process and the potential to answer our questions listed above. Our goal changes from directly measuring with physical sensors to identifying which combination of sensors can be used to calibrate a digital twin such that we can create high-accuracy virtual sensors.&lt;/p&gt; 
&lt;p&gt;Specifically, for our example model of the manufacturing line, the digital twin requires calibration of the roller viscous damping coefficients (see Figure 2). The roller viscous damping coefficient refers to a parameter that can’t be directly measured but reflects the bearing loss due to internal friction as the rollers rotate. &amp;nbsp;Once the viscous damping coefficients in the model are calibrated, the remaining properties of interest that we want to measure with sensors can instead be predicted purely based on the physics in the model. The question then remains, which sensors do we need in the roll-to-roll manufacturing line to calibrate the viscous damping coefficients in the model?&lt;/p&gt; 
&lt;div id="attachment_3438" style="width: 710px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3438" loading="lazy" class="size-full wp-image-3438" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.19.04.png" alt="Figure 2 The inputs and outputs of the roll-to-roll manufacturing web line digital twin" width="700" height="187"&gt;
 &lt;p id="caption-attachment-3438" class="wp-caption-text"&gt;Figure 2 The inputs and outputs of the roll-to-roll manufacturing web line digital twin.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Finding correlations in complicated systems&lt;/h2&gt; 
&lt;p&gt;To determine which sensors are needed to calibrate a model, a traditional approach is to use one-at-a-time perturbations in which a user alters one variable at a time within the model. While simple, this method misses cross-correlation/interactions that are often present within the physical phenomena of the system.&lt;/p&gt; 
&lt;p&gt;More advanced methods like &lt;a href="https://en.wikipedia.org/wiki/Variance-based_sensitivity_analysis"&gt;Sobel sensitivities&lt;/a&gt; rely on Monte Carlo sampling to numerically determine both main effects and interactions. Monte Carlo sampling consists of running a simulation model repeatedly (thousands of times), with each simulation using slightly different inputs. The correlation between the inputs and the outputs can then be quantified in a probabilistic manner for further analysis.&lt;/p&gt; 
&lt;p&gt;However, Sobel sensitivities can potentially provide incorrect results for highly non-linear systems, miss complicated interactions, or are unable to decouple correlated inputs. Additionally, the Sobel sensitivities are designed for global sensitivities.&lt;/p&gt; 
&lt;p&gt;The method of choice in this blog post, albeit not exclusively better, is to use Shapley sensitivities. Common implementations use an ML model to learn from the Monte Carlo simulations, and then apply game theory to assess the main effects and interactions, regardless of non-linearities or complicated interactions.&lt;/p&gt; 
&lt;p&gt;Even better, the method can provide sensitivities that either summarize effects across the entire modeling space (global sensitivities) or look at effects locally around a specific point in the space (local sensitivities).&lt;/p&gt; 
&lt;p&gt;Deploying our MapleSim digital twin on AWS using TwinFlow, we can generate a large sample set (10,000 simulations) to assess the Shapley sensitivities. We can either graphically review the Shapley sensitivities (shown in Figure 3) or look at the raw output (saved to SQL or csv file). For our specific example we have 12 spans and 9 rollers, with 2 properties each resulting in a total of 270 sensitivities relating the viscous damping coefficients (b-labels) to our desired outputs.&lt;/p&gt; 
&lt;p&gt;Graphically reviewing the sensitivities enables an ad-hoc assessment of the magnitudes. For example, in Figure 3a, we see the sensitivity of the R1 (roller 1) slip velocity to the damping coefficients in each of the rollers. It’s clear that the R1 slip velocity is most sensitive to the damping coefficient of the first roller (labeled b1) and positively correlated (meaning R1 slip velocity increases as the R1 damping coefficient increases).&lt;/p&gt; 
&lt;p&gt;The damping coefficient for the other rollers (b2 – b10) has minimal impact on R1 slip velocity – which agrees with our intuition. Figure 3b shows that the tension in the first span (S1) is negatively correlated with the damping coefficient of roller 1. Again, this is logical because we know the tension is likely to decrease as the material is slipping over roller 1. We also see that the viscous damping for roller 2 (labeled b2) has a possible impact on span 1 tension, although we would have to perform additional analysis to confirm it’s statistically significant.&lt;/p&gt; 
&lt;p&gt;Figure 3c demonstrates the power of Shapley sensitivity analysis as it identifies potential interaction effects that are not easy to determine by simple logical reasoning. In this case, we see that the slip velocity for Roller 9 is sensitive to viscous damping in both roller 9 (labeled b9) and roller 10 (labeled b10). This interaction is complex, however, in that R9 slip velocity is &lt;em&gt;negatively correlated&lt;/em&gt; to its own viscous damping (meaning R9 slip velocity decreases as b9 viscous damping increases), and is &lt;em&gt;positively correlated&lt;/em&gt; with higher magnitude with b10 viscous damping. This complex interaction is the result of the design and layout of the roll-to-roll manufacturing web line. Figure 3d shows the corresponding graph of tension in span 9 (S9) being nearly equally sensitive to both viscous damping b9 and b10.&lt;/p&gt; 
&lt;div id="attachment_3439" style="width: 751px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3439" loading="lazy" class="size-full wp-image-3439" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.19.51.png" alt="Figure 3 Example output of Shapley sensitivity analysis." width="741" height="967"&gt;
 &lt;p id="caption-attachment-3439" class="wp-caption-text"&gt;Figure 3 Example output of Shapley sensitivity analysis.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;a href="https://github.com/aws-samples/twinflow"&gt;TwinFlow&lt;/a&gt; includes the TwinStat module which provides a library of algorithms and methods that are commonly used for probabilistic modeling for digital twins. In particular, TwinStat uses a &lt;em&gt;Gaussian Kernel Density Estimator&lt;/em&gt; to produce the probability density distributions and cumulative density distributions for all outputs of the digital twin.&lt;/p&gt; 
&lt;p&gt;We gain further insights by reviewing the probability distributions for roller slip velocity and span tension. For example, Figure 4a show the probability density distribution of the R3 slip velocity obtained from the Monte Carlo sampling analysis where we varied the input damping coefficients. Figure 4b shows the cumulative distribution for the same data. The vertical dashed line indicates the slip velocity failure threshold and the probability curve crosses this threshold at approximately 90%, representing the probability of measurements being at or below the threshold.&lt;/p&gt; 
&lt;p&gt;We can tell that 90% of the cases have a slip velocity below the failure threshold of 1×10&lt;sup&gt;-3&lt;/sup&gt; m/s, indicating that it’s unlikely that we’ll see roller slip at the R3 location. This suggests it might be feasible to forgo installation of a costly sensor in the R3 location.&lt;/p&gt; 
&lt;div id="attachment_3440" style="width: 864px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3440" loading="lazy" class="size-full wp-image-3440" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.20.41.png" alt="Figure 4 Probability of various slip velocities depending on changes in the roller viscous damping coefficients. " width="854" height="372"&gt;
 &lt;p id="caption-attachment-3440" class="wp-caption-text"&gt;Figure 4 Probability of various slip velocities depending on changes in the roller viscous damping coefficients.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In contrast, Figure 5 &amp;nbsp;shows that tension failures occur frequently for span 4. Figure 5b shows that only 38% of the cases remain below the tension failure threshold of 175 N. This means the probability of exceeding the failure threshold is 62% (calculated as 100% – 38%). The high failure rate suggests that we need to invest in the sensors to monitor span 4 tension.&lt;/p&gt; 
&lt;div id="attachment_3441" style="width: 858px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3441" loading="lazy" class="size-full wp-image-3441" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.21.01.png" alt="Figure 5 Probability of various slip velocities depending on changes in the roller viscous damping coefficients." width="848" height="370"&gt;
 &lt;p id="caption-attachment-3441" class="wp-caption-text"&gt;Figure 5 Probability of various slip velocities depending on changes in the roller viscous damping coefficients.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Performing similar analysis for all the roller slip velocities and span tensions reveals that spans 4,5,6 often exceed the tension failure threshold, but all other spans rarely or never fail. Similarly, rollers 1,5,6,7,8,9 often exceed the slip velocity failure threshold, but all other rollers rarely fail. This information alone already tells us which spans and rollers need monitoring with sensors, and which sensors we can remove or leave out.&lt;/p&gt; 
&lt;p&gt;Our goal is to minimize the cost of installing and maintaining sensors, while still being able to monitor the manufacturing process with sufficient resolution to minimize product defects. With this in mind, we note that angular velocity (RPM) sensors are much cheaper and less intrusive than tension load cells and linear velocity sensors.&lt;/p&gt; 
&lt;p&gt;The Shapley analysis provides cross-correlated sensitivities between potential sensors and each viscous damping coefficient. As our next step, we queried the dataset to generate the heatmap shown in Figure 6. A sensitivity heatmap like this visualizes the relationship between inputs and outputs of the digital twin. Green shades show a positive relationship – increasing the input increases the output. Red shades indicate a negative correlation – rising input values cause output values to decrease. The intensity of the color corresponds to the strength of the sensitivity. Higher saturation means the input has a larger effect on that output. Cells colored close to yellow have little to no sensitivity. Examining the heatmap reveals which input variables drive which outputs. Inputs that lack significant sensitivity to any outputs may represent opportunities to eliminate sensors to reduce costs.&lt;/p&gt; 
&lt;div id="attachment_3442" style="width: 868px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3442" loading="lazy" class="size-full wp-image-3442" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.21.29.png" alt="Figure 6 Heatmap of Shapley results" width="858" height="282"&gt;
 &lt;p id="caption-attachment-3442" class="wp-caption-text"&gt;Figure 6 Heatmap of Shapley results&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We can’t directly measure the damping coefficients in the system. Instead, we’ll use a combination of sensors with our digital twin to estimate the damping coefficients. The sensors provide data that allows for probabilistic inference with a digital twin for the damping coefficients. Once we have estimates for the damping coefficients, we can use them in the digital twin models to predict the values of other unmeasured variables in the system.&lt;/p&gt; 
&lt;p&gt;You’ll notice in Figure 6 that each sensor provides a different sensitivity and thus a different level of useful information. Some sensors can be strategically chosen due to either providing more information than other sensors, or information about multiple locations in the assembly line.&lt;/p&gt; 
&lt;p&gt;We know tension and linear velocity sensors are expensive so we are looking to see if RPM (“_w” in the table) can be used instead. For example, b1 exhibits sensitivity to “Roller1_w”, which also is sensitive to “S1_Tension” and “Roller1_Slip”, which means that “Roller1_w” should provide adequate information.&lt;/p&gt; 
&lt;p&gt;This analysis can reveal some non-intuitive results like that S5_Tension is more sensitive to b2 and b4 due to the blue nip roller pressure points in Figure 1. Doing these comparisons for all properties tells us that RPM should provide adequate information for all locations in the manufacturing line.&lt;/p&gt; 
&lt;p&gt;To verify the hypothesis that using lower-cost RPM sensors is as effective as the costlier slip velocity sensors/load cells, we used historical data with an &lt;em&gt;Unscented Kalman Filter&lt;/em&gt; (available in TwinStat) to calibrate the Digital Twin. An Unscented Kalman Filter is a probabilistic Bayesian state estimation method that can be used to calibrate the model coefficients in our Level 4 Digital Twin. We showed this calibration approach &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/"&gt;in our earlier post&lt;/a&gt; discussing Level 4 DT setup and implementation.&lt;/p&gt; 
&lt;p&gt;We started by assuming installation of all 42 sensors, then used engineering analysis and customer feedback to determine that roller slip velocity and span tension are the critical quality parameters. This allowed us to reduce the number of sensors to 21. Shapley analysis then identified which specific rollers and spans were most failure-prone, enabling a further reduction to just 9 key sensors.&lt;/p&gt; 
&lt;p&gt;Finally, we substituted lower-cost RPM sensors for some slip velocity sensors and load cells, reducing cost while preserving monitoring capability. Through systematic evaluation we reduced 42 potential sensors down to 9 optimized, affordable sensors that will provide all the information we need to calibrate the Digital Twin, removing sensor redundancy.&lt;/p&gt; 
&lt;h2&gt;Using Amazon Bedrock to augment traditional sensitivity analysis&lt;/h2&gt; 
&lt;p&gt;The traditional manual approach to sensitivity analysis may be too time consuming or may include too many values to evaluate. Further data analysis techniques may be required such as using regressions on the data. However, as an experiment we decided to provide &lt;a href="https://aws.amazon.com/bedrock/"&gt;Amazon Bedrock&lt;/a&gt; with the Figure 6 data. In Figure 7 we presented this problem to the Claude 2.1 LLM model. Interestingly it correctly selected the RPM variables, but also concluded it would be possible to select a subset. The LLM model thinks it is possible to use fewer sensors than the human has selected to calibrate the digital twin. The LLM is attempting to take advantage of the fact that some viscous damping coefficients can potentially be used to predict multiple spans or roller properties. In a follow-up blog post, we will perform a more detailed exploration to assess the ability of using LLMs to assist with sensitivity analysis.&lt;/p&gt; 
&lt;p&gt;&lt;img loading="lazy" class="aligncenter size-full wp-image-3443" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.23.17.png" alt="" width="1379" height="582"&gt;&lt;/p&gt; 
&lt;div id="attachment_3444" style="width: 623px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3444" loading="lazy" class="size-full wp-image-3444" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.24.27.png" alt="Figure 7 Comparison with Amazon Bedrock (Claude 2.1 Model). Note the raw data given to the LLM is not shown in the screenshots provided here." width="613" height="389"&gt;
 &lt;p id="caption-attachment-3444" class="wp-caption-text"&gt;Figure 7 Comparison with Amazon Bedrock (Claude 2.1 Model). Note the raw data given to the LLM is not shown in the screenshots provided here.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;AWS architecture&lt;/h2&gt; 
&lt;p&gt;The AWS architecture used for the sensitivity study is depicted in Figure 8. Steps 1 – 3 are the same as described in the previous &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/"&gt;blog post&lt;/a&gt; for the Level 4 digital twin in which we install TwinFlow on an Amazon Elastic Compute Cloud (Amazon EC2) instance, setup a container which includes our MapleSim model, and push it up to Amazon ECR where the container is accessible to any AWS service.&lt;/p&gt; 
&lt;p&gt;We can also store the model in an &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service&lt;/a&gt; (Amazon S3) bucket which will download the model when needed. This may be advantageous if we’re modifying the model for various studies.&lt;/p&gt; 
&lt;p&gt;When running a sensitivity study with TwinFlow (leveraging the TwinStat automation for sensitivity studies), it automatically sets up tables (creates the table and schema) within &lt;a href="https://aws.amazon.com/rds/"&gt;Amazon RDS&lt;/a&gt; (AWS SQL service). Based on user defined inputs and outputs in a JSON file, TwinFlow (leveraging the TwinStat automation) creates the input perturbation run list and executes in parallel all simulations using &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;. &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; is our cloud-native HPC scheduler that includes backend options for &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon ECS&lt;/a&gt;, which is an AWS-specific container orchestrations service, &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt;, which is a serverless execution option, and &lt;a href="https://aws.amazon.com/eks/"&gt;Amazon EKS&lt;/a&gt; which is our Kubernetes option.&lt;/p&gt; 
&lt;p&gt;We chose Amazon ECS because we wanted Amazon EC2 instances with larger numbers of CPUs than those available for Fargate. Fargate can create and deploy instances faster than ECS, however for tightly coupled numerical simulations often it’s better to scale up to minimize network latency thus minimizing runtime. Also, ECS enables fully-automated deployment unlike EKS. The specific Amazon EC2 instance type is automatically selected by AWS Batch auto-scaling based on the user-defined CPU/GPU and memory requirements.&lt;/p&gt; 
&lt;p&gt;Once all simulations have completed on AWS Batch, TwinStat downloads all data from the &lt;a href="https://aws.amazon.com/rds/"&gt;Amazon RDS&lt;/a&gt; tables and automatically performs the Shapley sensitivity study producing images of the probability distributions and sensitivity bar charts storing them in the local file system of the EC2 instance. A user then can review the results by downloading the images locally or store them in an Amazon S3 bucket for archiving.&lt;/p&gt; 
&lt;div id="attachment_3445" style="width: 760px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3445" loading="lazy" class="size-full wp-image-3445" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/22/CleanShot-2024-03-22-at-17.25.06.png" alt="Figure 8 AWS cloud architecture used to perform sensitivity studies with our L4 Digital Twin" width="750" height="537"&gt;
 &lt;p id="caption-attachment-3445" class="wp-caption-text"&gt;Figure 8 AWS cloud architecture used to perform sensitivity studies with our L4 Digital Twin&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;In this post, we showed how to perform a sensitivity analysis using a Digital Twin to identify the minimum set of sensors and sensor locations while maintaining effective monitoring of our roll-to-roll manufacturing process. MapleSim provides the physics-based model in the form of an FMU and TwinFlow allows us to run scalable number of scenarios on AWS, providing efficient and elastic computing resources.&lt;/p&gt; 
&lt;p&gt;In this Digital Twin example, the utility of a physics-based digital twin shows how we can quickly obtain actionable insights that directly impact business metrics. Digital twins are extremely versatile and can be leveraged at all phases of product development, from inception, implementation, to monitoring.&lt;/p&gt; 
&lt;p&gt;If you want to request a proof of concept or if you have feedback on the AWS tools, please reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Renewable energy transition: examining the impacts of wind energy through simulation</title>
		<link>https://aws.amazon.com/blogs/hpc/renewable-energy-transition-examining-the-impacts-of-wind-energy-through-simulation/</link>
		
		<dc:creator><![CDATA[Remco Verzijlbergh]]></dc:creator>
		<pubDate>Tue, 26 Mar 2024 13:21:20 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">812815631cfcc972ff806cbc6735cc3480bc8fc4</guid>

					<description>As we move towards a greener future, understanding wind energy's climate impacts is key. Check out this blog post by our friends at Whiffle, to learn how large-scale simulations reveal wind power's effect on our atmosphere.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3403" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/06/Renewable-energy-transition-examining-the-impacts-of-wind-energy-through-simulation-mini.png" alt="Renewable energy transition: examining the impacts of wind energy through simulation" width="380" height="212"&gt;This post was contributed by&lt;/em&gt;&lt;em&gt; Remco&amp;nbsp;Verzijlbergh, co-founder and CEO, Peter Baas, &lt;/em&gt;&lt;em&gt;R&amp;amp;D Specialist &lt;/em&gt;&lt;em&gt;at&amp;nbsp;Whiffle, and Ilan Gleiser &lt;/em&gt;&lt;em&gt;Principal ML Specialist, &lt;/em&gt;&lt;em&gt;Milo Oostergo, Principal Startup SA, and Ross Pivovar Senior SA, Simulations at AWS&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The world is currently facing a crucial turning point in the fight against climate change, and renewable energy sources like wind energy are playing a major role. As we continue to move towards a more sustainable future, it is important to understand the impacts of the anticipated large-scale roll-out of wind energy. This is where atmospheric model simulation comes into play.&lt;/p&gt; 
&lt;p&gt;By performing large scale simulations on HPC clusters in the cloud, built using &amp;nbsp;Amazon EC2 P4d instances and powered by Nvidia A100 GPUs, we can get a better understanding of how wind energy affects our atmosphere and the environment as a whole.&lt;/p&gt; 
&lt;p&gt;In this blog post, we’ll delve into the realm of simulations and explore the impact of wind energy on our planet.&lt;/p&gt; 
&lt;h2&gt;The Rising Power of Wind Energy&lt;/h2&gt; 
&lt;p&gt;In the quest for a sustainable future, wind energy has emerged as a powerful player in the fight against climate change. The rising power of wind energy has captured the attention of scientists, policymakers, and the general public alike. It has quickly become a driving force in the transition towards renewable sources of energy, and for good reason.&lt;/p&gt; 
&lt;p&gt;Harnessing the power of wind is not a new concept. Humans have been utilizing wind energy for centuries, from simple windmills for grinding grains to more advanced wind turbines that generate electricity. However, it is only in recent decades that wind energy has gained significant momentum as a viable alternative to fossil fuels.&lt;/p&gt; 
&lt;p&gt;One of the main reasons for the increasing popularity of wind energy is its environmental benefits. Unlike traditional energy sources such as coal or natural gas, wind energy does not produce harmful emissions or contribute to air pollution. This is a crucial advantage in the battle against climate change, as reducing greenhouse gas emissions is essential to mitigating its impact.&lt;/p&gt; 
&lt;p&gt;Furthermore, wind energy is a renewable resource, meaning that it will never run out. As long as the wind keeps blowing, we can continue to harness its power. This is in stark contrast to fossil fuels, which are finite resources that will eventually be depleted.&lt;/p&gt; 
&lt;p&gt;Another key factor driving the rise of wind energy is its economic potential. As technology has advanced, the cost of producing wind energy has steadily decreased. In fact, wind energy is now one of the cheapest sources of electricity in many parts of the world. This affordability, coupled with the growing demand for clean energy, has led to a boom in wind energy installations globally.&lt;/p&gt; 
&lt;p&gt;In addition to its environmental and economic benefits, wind energy also has the potential to enhance energy security. Unlike fossil fuels, which are often imported from other countries, wind energy can be produced locally. This reduces dependence on foreign sources of energy and strengthens national energy independence.&lt;/p&gt; 
&lt;p&gt;The increasing power of wind energy is evident in the rapid growth of wind farms around the world. These vast fields of turbines are capable of generating large amounts of electricity, powering homes, businesses, and even entire cities. As technology continues to advance, wind turbines are becoming more efficient and capable of harnessing even more energy from the wind.&lt;/p&gt; 
&lt;p&gt;Harnessing wind energy has significant positive effects on the atmosphere. By replacing traditional energy sources with wind energy, we can reduce greenhouse gas emissions and combat climate change. Wind energy does not release carbon dioxide or other pollutants that contribute to air pollution and global warming.&lt;/p&gt; 
&lt;p&gt;Moreover, wind energy can contribute to improved air quality. By reducing reliance on fossil fuels, which emit harmful pollutants such as sulfur dioxide and nitrogen oxide, wind energy helps to alleviate respiratory issues and promote healthier living conditions.&lt;/p&gt; 
&lt;h2&gt;Unraveling the Data: Using Simulations to Predict Future Scenarios&lt;/h2&gt; 
&lt;p&gt;As we strive for a greener future, simulations play a vital role in understanding the impacts of wind energy on our planet. Using superior computing power to solve the complexities of the weather with unparalleled accuracy, we can unravel the data and predict future scenarios. By simulating large-scale wind farms and analyzing the interaction with the atmosphere, we can gain valuable insights into the sustainability and environmental effects of renewable energy. These simulations offer a powerful tool for decision-making and policy development, ensuring a more informed and efficient transition to a sustainable energy future.&lt;/p&gt; 
&lt;h2&gt;Who is Whiffle?&lt;/h2&gt; 
&lt;p&gt;Whiffle services are designed to empower renewable energy industries and optimize operations by providing accurate meteorological forecasts, precise wind and solar power production forecasts, comprehensive wind resource modelling and detailed energy yield assessments. They are the driving force behind the revolutionary &lt;a href="https://whiffle.nl/solutions/whiffle-wind/"&gt;Whiffle Wind web app&lt;/a&gt;, which is significant in renewable energy development. With their cutting-edge Large-Eddy Simulation (LES)-powered weather Computational Fluid Dynamics (CFD) modelling technology, Whiffle is helping to pave the way for a greener and more sustainable future.&lt;/p&gt; 
&lt;p&gt;What sets Whiffle Wind apart is its &lt;a href="https://whiffle.nl/solutions/whiffle-wind/"&gt;user-friendly interface&lt;/a&gt;, in contrast to traditional weather-modeling software which is often complicated and cumbersome. Whiffle Wind makes it easier to access and interpret the data with relative ease, making it a valuable tool for professionals and enthusiasts alike.&lt;/p&gt; 
&lt;p&gt;Whiffle’s ultra-high resolution atmospheric model is also important to renewable energy development and operations. While traditional weather models work with grid blocks of 10 by 10 kilometers, Whiffle works on a resolution of 100 x 100 meters – a hundred times more information density on all weather-defining factors. With its hyper-local forecasts and simulations, Whiffle can help businesses plan for adverse weather events, ensuring the safety and efficiency of renewable energy operations.&lt;/p&gt; 
&lt;p&gt;Whiffle’s stack can also deliver highly accurate predictions, allowing businesses to optimize their operational performance. Whiffle’s model includes elements like buildings and trees. High-resolution digital terrain models are used to allow for detailed simulation in complex terrain. Users can upload wind turbines at any desired location to assess their energy production. As a full-blown weather model, Whiffle’s LES also calculates cloud coverage, which allows for detailed solar radiation predictions. &amp;nbsp;This means that whether you need to consider wind farms or solar installations, Whiffle can help you to get the insights you need for making informed decisions.&lt;/p&gt; 
&lt;p&gt;Core to Whiffle’s technology is &lt;strong&gt;Large-Eddy Simulation&lt;/strong&gt; (LES). This advanced approach to weather modelling follows the laws of atmospheric physics very closely, allowing for detailed simulations of turbulent atmospheric flows. By capturing the small-scale processes that traditional numerical weather prediction models miss, Whiffle can provide a more comprehensive understanding of weather conditions.&lt;/p&gt; 
&lt;div id="attachment_3399" style="width: 703px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3399" loading="lazy" class="size-full wp-image-3399" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/06/CleanShot-2024-03-06-at-13.57.44.png" alt="Fig 1. Achieve accuracy tailored to your specific site requirements, irrespective of terrain complexity, location or the intricacies of wind turbine clusters." width="693" height="749"&gt;
 &lt;p id="caption-attachment-3399" class="wp-caption-text"&gt;Fig 1. Achieve accuracy tailored to your specific site requirements, irrespective of terrain complexity, location or the intricacies of wind turbine clusters.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Delving into large-scale wind farms: impact and sustainability&lt;/h2&gt; 
&lt;p&gt;As the renewable energy transition gains momentum, the installation of large-scale wind farms is becoming increasingly common. For example, the installed capacity of offshore wind energy in the Dutch part of the North Sea is expected to grow dramatically in the next few decades. Several research institutes and the Dutch government are examining scenarios with up to 100 GW (around 5,000 to 10,000 large wind turbines) of offshore wind in 2050. For comparison: the currently installed capacity is around 5 GW, so this means a 20-fold increase.&lt;/p&gt; 
&lt;p&gt;However, the impact of such a large-scale roll-out of wind energy on the atmosphere and vice versa is highly uncertain. Understanding these interactions is crucial for the wind energy sector and policy makers as they strive to develop sustainable and environmentally friendly solutions.&lt;/p&gt; 
&lt;p&gt;This is where the &lt;a href="https://wins50.nl/"&gt;WINS50&lt;/a&gt; project comes into play. The project, carried out by Whiffle,&amp;nbsp;&lt;a href="https://wins50.nl/projectpartners"&gt;TU Delft&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href="https://wins50.nl/projectpartners"&gt;KNMI, &lt;/a&gt;aims to reduce uncertainties surrounding the interaction between a large-scale roll-out of offshore wind energy and the atmosphere. To achieve this, the project uses Whiffle’s high-resolution turbine-resolving weather model, GRASP. With this advanced model, researchers from Whiffle are simulating the entire Dutch North Sea with the currently-installed wind power capacity &lt;em&gt;and&lt;/em&gt; with the projected 2050 capacity. The data they produce will be made available to the wind energy sector and will be used for in-depth studies.&lt;/p&gt; 
&lt;p&gt;Within the WINS50 project, a multi-GPU LES model has been developed to enable computational domains of hundreds by hundreds of kilometers. This model will provide even more detailed and accurate information about the atmospheric interactions of large-scale wind farms, helping to inform future decision-making and policy development.&lt;/p&gt; 
&lt;p&gt;With the combination of advanced simulation models and the collaboration between research institutes and the wind energy sector, we can gain a deeper understanding of the impact and sustainability of large-scale wind farms. By reducing uncertainties and improving our knowledge, we can continue to drive the renewable energy transition forward and create a greener and more sustainable future for all.&lt;/p&gt; 
&lt;h2&gt;Reference Architecture&lt;/h2&gt; 
&lt;div id="attachment_3400" style="width: 914px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3400" loading="lazy" class="size-full wp-image-3400" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/06/CleanShot-2024-03-06-at-13.58.30.png" alt=" Fig 2. AWS High Performance Compute reference architecture comprised of multi GPU AWS Parallel Cluster open-source tool that builds a complete HPC environment with all the benefits of the cloud built-in." width="904" height="412"&gt;
 &lt;p id="caption-attachment-3400" class="wp-caption-text"&gt;&lt;br&gt;Fig 2. AWS High Performance Compute reference architecture comprised of multi GPU AWS Parallel Cluster open-source tool that builds a complete HPC environment with all the benefits of the cloud built-in.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To run the simulations, Whiffle used&amp;nbsp;&lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt;&amp;nbsp;– a fully-supported open-source tool that builds a complete HPC environment with all the benefits of the cloud built-in. That includes the elasticity to scale horizontally (and contract) based on need, instant access to the latest technologies (CPUs, GPUs, and accelerators), and the flexibility to iterate resource selection in minutes to optimize costs or squeeze out better performance.&lt;/p&gt; 
&lt;p&gt;For the cluster configuration, Whiffle used a single queue with four&lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt; P4d&lt;/a&gt; instances that are each powered by eight GPUs. The NVSwitch GPU interconnect enables each GPU to communicate with every other GPU in the same instance with 600 GB/s bidirectional throughput and single-hop latency.&lt;/p&gt; 
&lt;p&gt;Whiffle’s testing showed that the LES model scaled extremely well when run with 8 GPUs in parallel. It supports their philosophy of keeping the model architecture as simple as possible so they can quickly adopt advances in computing and cloud infrastructures. As the performance of parallel GPU compute resources grow, we can all start planning for continental or global scale weather forecasting on a 100m resolution in the future.&lt;/p&gt; 
&lt;p&gt;The data set and simulation results are stored on &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt; which is mounted on the head node and compute nodes, and provides sub-millisecond latencies, high throughput, and millions of IOPS.&lt;/p&gt; 
&lt;p&gt;The final results from the simulation are stored in &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service&lt;/a&gt; (Amazon S3) after post-processing.&lt;/p&gt; 
&lt;h2&gt;The dataset&lt;/h2&gt; 
&lt;p&gt;The dataset is the result from the WINS50 LES described above and it’s &lt;a href="https://registry.opendata.aws/whiffle-wins50/"&gt;available in the Registry of Open Data on AWS&lt;/a&gt;. It’s approximately 40TB in size, and consists of three types of data:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Three-dimensional meteorological data over the entire domain at an hourly resolution for selected heights above the surface. Variables included here are wind speed, wind direction, temperature, etc. In addition, surface variables such as the surface pressure of downward solar radiation are available in two-dimensional format at hourly resolution.&lt;/li&gt; 
 &lt;li&gt;For 600 locations distributed over the domain many meteorological variables are provided over the entire atmospheric column with a 10-minute resolution. Turbine data gives energy production, wind speed and thrust for all the simulated turbines, i.e. more than 10,000 for the 2050 scenario. Turbine data is available at 5-minute resolution.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There are many possible use cases for the data, including scientific fields from the atmospheric sciences and energy system sciences, too. Within the energy sector, the data can inform grid operators, wind farm owners, developers of new wind farms and energy traders. Finally, the data can be used to train AI based models, opening more use cases where computationally-fast models are required.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;As we forge ahead in the renewable energy transition, there are bound to be both opportunities and challenges on the horizon. The growing popularity of wind energy presents a unique opportunity to transition towards a more sustainable future. By harnessing the power of the wind, we can significantly reduce our carbon footprint and combat climate change.&lt;/p&gt; 
&lt;p&gt;Advancements in technology, like Whiffle’s simulation tools coupled with the use of HPC clusters on AWS, offer new avenues for improving the efficiency and accuracy of wind energy simulations.&lt;/p&gt; 
&lt;p&gt;But there are challenges. Wildlife impact and visual concerns must be carefully addressed to ensure the widespread adoption of wind energy is safe and beneficial. Acknowledging these challenges and leveraging the power of simulation and HPC, we can work to achieve that.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Choosing the right compute orchestration tool for your research workload</title>
		<link>https://aws.amazon.com/blogs/hpc/choosing-the-right-compute-orchestration-tool-for-your-research-workload/</link>
		
		<dc:creator><![CDATA[Patrick Guha]]></dc:creator>
		<pubDate>Tue, 19 Mar 2024 13:52:27 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">dacdd14c4bfac6c9245765550bc892676dd27683</guid>

					<description>Running big research jobs on AWS but not sure where to start? We break down options like Batch, ECS, EKS, and others to pick the right tool for your needs. Lots of examples for genomics, ML, engineering, and more!</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright wp-image-3375 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/boofla88_a_scientist_choosing_between_several_options_manga_sty_6b82dff1-510f-404c-97aa-360c46741720.png" alt="Choosing the right compute orchestration tool for your research workload" width="380" height="212"&gt;&lt;/p&gt; 
&lt;p&gt;Research organizations around the world run large-scale simulations, analyses, models, and other distributed, compute-intensive workloads on AWS every day. These jobs depend on an orchestration layer to coordinate tasks across the compute fleet.&lt;/p&gt; 
&lt;p&gt;As a researcher or systems administrator providing services for researchers, it can be difficult to choose which AWS service or solution to use because there are various options for different kinds of workloads.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll describe some typical research use cases and explain which AWS tool we think best fits that workload.&lt;/p&gt; 
&lt;h2&gt;Understanding your workload&lt;/h2&gt; 
&lt;p&gt;Before diving into the specifics of each tool, it’s important to understand the nature of your workload.&lt;/p&gt; 
&lt;p&gt;Factors like the requirement for tightly coupled processes, the use of containers, the need for machine learning capabilities, or the necessity for a cloud desktop are pivotal in your decision-making process.&lt;/p&gt; 
&lt;p&gt;Research is not a monolith, so AWS supports a diverse range of HPC-based research, from engineering simulations and drug discovery to genomics, machine learning (ML), financial risk analysis, and social sciences.&lt;/p&gt; 
&lt;p&gt;Also: the tool you choose is not exclusive. Customers can have a mix of solutions to meet their needs all in the same account.&lt;/p&gt; 
&lt;h2&gt;A deep dive into AWS compute orchestration tools&lt;/h2&gt; 
&lt;h3&gt;AWS ParallelCluster for classic HPC clusters&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; is a flexible tool for building and managing HPC clusters on AWS. It’s ideal for tightly coupled workloads, like running simulations or analytics that require a traditional HPC cluster. It supports &lt;a href="https://aws.amazon.com/hpc/efa/"&gt;Elastic Fabric Adapter&lt;/a&gt; (EFA) networking out-of-the-box for low latency and high throughput inter-instance communication, and a high-performance file system (Lustre – available through the &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt; managed service).&lt;/p&gt; 
&lt;p&gt;ParallelCluster provides a familiar interface with a job scheduler &amp;nbsp;– Slurm – making it easy to migrate or burst workloads from an on-premises cluster environment you’re possibly already using.&lt;/p&gt; 
&lt;div id="attachment_3432" style="width: 1234px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3432" loading="lazy" class="wp-image-3432 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/20/CleanShot-2024-03-20-at-12.55.01.png" alt="" width="1224" height="533"&gt;
 &lt;p id="caption-attachment-3432" class="wp-caption-text"&gt;Figure 1 – Overview of AWS ParallelCluster and its components for HPC workloads. Integration with Slurm and Amazon EC2 right-sizes compute node numbers based on the job queue. Amazon FSx for Lustre allows for access to a high-performance file system while also taking advantage of Amazon S3 object storage. All of this is connected using Elastic Fabric Adapter (EFA) which provide extremely high-performance connectivity and scaling for tightly-coupled workloads.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;AWS Batch for container-based jobs&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; is suited for highly parallel, container-based jobs, including tightly-coupled workloads. It provides a fully-managed scheduler with seamless integration into container orchestrators, like &lt;a href="https://aws.amazon.com/eks/"&gt;Amazon Elastic Kubernetes Service&lt;/a&gt; (EKS) and &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon Elastic Container Service&lt;/a&gt; (ECS), allowing researchers to leverage existing containerized applications. A typical workload might involve independently running jobs on generic/non-specific compute, leveraging native AWS integrations, or requiring horizontal scalability through MPI or NCCL.&lt;/p&gt; 
&lt;div id="attachment_3367" style="width: 733px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3367" loading="lazy" class="size-full wp-image-3367" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.13.58.png" alt="Figure 2 – AWS Batch workflow illustrating container-based job processing and integration with AWS services. Compatibility with Amazon EKS and ECS allows for flexibility at the Compute Environment layer. " width="723" height="361"&gt;
 &lt;p id="caption-attachment-3367" class="wp-caption-text"&gt;Figure 2 – AWS Batch workflow illustrating container-based job processing and integration with AWS services. Compatibility with Amazon EKS and ECS allows for flexibility at the Compute Environment layer.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Amazon SageMaker for machine learning projects&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/sagemaker/"&gt;Amazon SageMaker&lt;/a&gt; is ideal for machine learning workloads, especially those developed in Jupyter Notebooks. While not focused on foundational building blocks for research computing, it instead provides a managed ecosystem of ML and data science tools, covering the entire spectrum from data discovery and exploration to model training and deployment.&lt;/p&gt; 
&lt;p&gt;SageMaker notebooks provide an interactive development environment, allowing researchers to develop and test models easily. SageMaker also contains pre-trained models, allowing researchers to jump-start their ML projects. It also provides managed inference endpoints, making it easier to deploy models and serve predictions.&lt;/p&gt; 
&lt;div id="attachment_3368" style="width: 587px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3368" loading="lazy" class="wp-image-3368 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.14.53.png" alt="Figure 3 – Amazon SageMaker ecosystem showcasing high-level end-to-end process from data preparation to model deployment. Integrates with services like Amazon EFS for a local file system in notebooks and also with highly-optimized AWS Deep Learning Containers for training models." width="577" height="309"&gt;
 &lt;p id="caption-attachment-3368" class="wp-caption-text"&gt;Figure 3 – &lt;a href="https://aws.amazon.com/pm/sagemaker"&gt;Amazon SageMaker&lt;/a&gt; ecosystem showcasing high-level end-to-end process from data preparation to model deployment. Integrates with services like &lt;a href="https://aws.amazon.com/efs/"&gt;Amazon EFS&lt;/a&gt; for a local file system in notebooks and also with highly-optimized AWS Deep Learning Containers for training models.&lt;/p&gt;
&lt;/div&gt; 
&lt;h4&gt;Let’s talk about the underlying compute resources&lt;/h4&gt; 
&lt;p&gt;The three services we just mentioned can take advantage of Amazon Elastic Compute Cloud (Amazon EC2) &lt;a href="https://aws.amazon.com/ec2/spot/"&gt;Spot Instances&lt;/a&gt; and also &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt;. Spot Instances are spare EC2 capacity, offered at a discounted rate but they can be reclaimed with a 2-minute warning.&lt;/p&gt; 
&lt;p&gt;SageMaker, Batch, and ParallelCluster can all use Spot Instances to take advantage of their favorable economics. In the case of Spot Instances, you’ll need to verify that your workload can tolerate interruptions from reclaimed capacity, or that the service can shift load on your behalf to avoid interrupting your processes. There are AWS technology partners, like &lt;a href="https://aws.amazon.com/marketplace/seller-profile?id=3b7c724c-fae7-4187-ae45-de1625e51395"&gt;MemVerge&lt;/a&gt;, that can handle this for you using &lt;a href="https://aws.amazon.com/blogs/hpc/save-up-to-90-using-ec2-spot-even-for-long-running-hpc-jobs/"&gt;OS-level memory checkpointing&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Fargate is a serverless compute engine that can be used for workloads running on Batch with ECS, and in native ECS and EKS clusters. It abstracts away the need for additional servers or infrastructure-related parameters (like instance type) to run containers. Fargate also has a few caveats regarding hardware specifications which you can find &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/fargate.html#when-to-use-fargate"&gt;in our documentation&lt;/a&gt;. But – generally speaking – it’s worthwhile to see if you can use Spot or Fargate with AWS orchestration tools for your research.&lt;/p&gt; 
&lt;h3&gt;Amazon Lightsail for Research for individual cloud desktops&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/lightsail/research/"&gt;Amazon Lightsail for Research&lt;/a&gt; is a simple (but powerful) solution for researchers looking for a predictably-priced, all-in-one cloud desktop. It’s tailored specifically for researchers, providing hardware specifications that are optimized for efficient research and a seamless user experience. Lightsail offers a range of pre-configured virtual private servers that can be customized to meet researchers’ needs and comes with research applications, like Scilab and RStudio. With its easy-to-use interface and affordable pricing, Lightsail for Research provides a reliable and efficient way for researchers to get started with AWS.&lt;/p&gt; 
&lt;div id="attachment_3369" style="width: 662px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3369" loading="lazy" class="size-full wp-image-3369" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.15.11.png" alt="Figure 4 – Researchers can use Amazon Lightsail for Research’s simplified management interface and options to deploy their favorite applications like Jupyter, RStudio, and Scilab. " width="652" height="319"&gt;
 &lt;p id="caption-attachment-3369" class="wp-caption-text"&gt;Figure 4 – Researchers can use Amazon Lightsail for Research’s simplified management interface and options to deploy their favorite applications like Jupyter, RStudio, and Scilab.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Research and Engineering Studio on AWS for managing cloud desktops at scale&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/hpc/res/"&gt;Research and Engineering Studio on AWS&lt;/a&gt; (RES) is an open-source web-based portal for administrators to create and manage secure, cloud-based research and engineering environments. It is ideal for research organizations that want a central IT team to easily manage the underlying infrastructure for multiple research environments. It provides one-click deployment for getting started quickly but can be customized to meet an organization’s specific needs.&lt;/p&gt; 
&lt;p&gt;Administrators can create virtual collaboration spaces for specific sets of users to access shared resources and collaborate. Users get a single pane of glass for launching and accessing virtual desktops to conduct scientific research, product design, engineering simulations, or data analysis workloads.&lt;/p&gt; 
&lt;div id="attachment_3371" style="width: 707px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3371" loading="lazy" class="size-full wp-image-3371" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.15.57.png" alt="Figure 5 – Researchers and admins alike can leverage RES to create Engineering Virtual Desktops (eVDI) backed by Amazon EC2. The RES Virtual Desktop screen shown here lists all the eVDI sessions a user created with controls to spin up, shut down, or schedule uptime. " width="697" height="404"&gt;
 &lt;p id="caption-attachment-3371" class="wp-caption-text"&gt;Figure 5 – Researchers and admins alike can leverage RES to create Engineering Virtual Desktops (eVDI) backed by Amazon EC2. The RES Virtual Desktop screen shown here lists all the eVDI sessions a user created with controls to spin up, shut down, or schedule uptime.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;AWS HealthOmics for bioinformatics&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/healthomics/"&gt;AWS HealthOmics&lt;/a&gt; is a comprehensive solution for bioinformatics work. It facilitates raw genomic storage and processing, allowing researchers to store and analyze genomic data. It supports popular bioinformatics workflow definition languages like WDL and Nextflow, enabling researchers to process and analyze genomic data efficiently.&lt;/p&gt; 
&lt;p&gt;HealthOmics even offers researchers the choice to bring their own workflow or use pre-built &lt;a href="https://docs.aws.amazon.com/omics/latest/dev/service-workflows.html"&gt;Ready2Run workflows&lt;/a&gt;. Ready2Run workflows are designed by industry leading third-party software companies like Sentieon, Inc. and NVIDIA, and includes common open-source pipelines like AlphaFold for protein structure prediction. Ready2Run workflows don’t require you to manage software tools or workflow scripts – this can save researchers significant amounts of time.&lt;/p&gt; 
&lt;div id="attachment_3372" style="width: 1017px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3372" loading="lazy" class="size-full wp-image-3372" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.17.22.png" alt="Figure 6 – AWS HealthOmics platform structure highlighting genomic data processing and analysis capabilities. Raw sequence and reference data can be processed through Nextflow or WDL workflows and then analyzed via AWS analytics services such as Amazon Athena." width="1007" height="286"&gt;
 &lt;p id="caption-attachment-3372" class="wp-caption-text"&gt;Figure 6 – AWS HealthOmics platform structure highlighting genomic data processing and analysis capabilities. Raw sequence and reference data can be processed through Nextflow or WDL workflows and then analyzed via AWS analytics services such as &lt;a href="https://aws.amazon.com/athena/"&gt;Amazon Athena&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Leveraging next-generation serverless technologies&lt;/h3&gt; 
&lt;p&gt;In the past decade, AWS has pioneered the field of serverless computing. Serverless computing is a model where you can build and deploy applications without managing server infrastructure. Instead of spinning up a full virtual machine that comes with overhead like patching and monitoring, you can abstract it away and focus just on the code or process you intend to run.&lt;/p&gt; 
&lt;p&gt;This is great for use cases like event handling or asynchronous tasks, but researchers have been using serverless computing to speed up embarrassingly parallel workloads, too – including ML hyperparameter optimization, genome search, and even MapReduce.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; is a serverless interface for running code without managing servers. It’s designed for loosely coupled workloads, allowing researchers to run code in response to events like changes in data (or the arrival of data). Lambda scales automatically, enabling you to run thousands of concurrent executions.&lt;/p&gt; 
&lt;p&gt;Even better: Lambda integrates with more than 200 other AWS services, making it easier for you to build quite complex workflows. It provides integration with &lt;a href="https://aws.amazon.com/step-functions/"&gt;AWS Step Functions&lt;/a&gt;, too, which means you can create visual workflows and construct multi-step, distributed applications. Lambda, combined with Step Functions, is useful for workloads that involve different compute steps, data needs, and may even require decision gates or human input.&lt;/p&gt; 
&lt;div id="attachment_3373" style="width: 860px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3373" loading="lazy" class="size-full wp-image-3373" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.18.06.png" alt="Figure 7 – Illustration of a sample serverless computing architecture: a data stream of jobs to be processed is picked up by AWS Lambda and put into a downstream Amazon SQS queue. AWS Step Functions then reads from this queue and handles the heavy lifting of distributed compute orchestration via Lambda worker functions. Step Functions also leverages Amazon DynamoDB for workload state management, event handling with Amazon EventBridge and storing processed results in Amazon S3. " width="850" height="263"&gt;
 &lt;p id="caption-attachment-3373" class="wp-caption-text"&gt;Figure 7 – Illustration of a sample serverless computing architecture: a data stream of jobs to be processed is picked up by AWS Lambda and put into a downstream &lt;a href="https://aws.amazon.com/sqs/"&gt;Amazon SQS&lt;/a&gt; queue. AWS Step Functions then reads from this queue and handles the heavy lifting of distributed compute orchestration via Lambda worker functions. Step Functions also leverages &lt;a href="https://aws.amazon.com/dynamodb/"&gt;Amazon DynamoDB&lt;/a&gt; for workload state management, event handling with &lt;a href="https://aws.amazon.com/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt; and storing processed results in Amazon S3.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion: making the right choice&lt;/h2&gt; 
&lt;p&gt;Choosing the right AWS compute orchestration tool is not about finding a one-size-fits-all solution but about aligning the tool’s capabilities with the specific requirements of your workload. The nuances of your project, the nature of your data, and your computational needs &lt;em&gt;should&lt;/em&gt; guide your decision.&lt;/p&gt; 
&lt;p&gt;Start with a small workload to gauge the tool’s compatibility and scalability with your project. AWS has a comprehensive suite of services and is ready to support you at every step of your journey, ensuring that you have the right resources and environment to push the boundaries of your research.&lt;/p&gt; 
&lt;p&gt;AWS Partners are also here to help you implement these tools. Partners like &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-7shbi5vvdvezu"&gt;Ronin&lt;/a&gt; and &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-ppiyshk2oin3i?sr=0-1&amp;amp;ref_=beagle&amp;amp;applicationId=AWSMPContessa"&gt;Ansys&lt;/a&gt; bring valuable expertise that can accelerate your time to research on AWS.&lt;/p&gt; 
&lt;p&gt;We recommend consulting with your research team and AWS account team to help you make the best decision for your project. As your research evolves, AWS’s scalable and diverse computing environment will continue to provide the necessary tools and support to meet your computational needs. To dive deeper into the nuances of a few of the tools we touched on in this post, we encourage you to check out some of our previous posts about &lt;a href="https://aws.amazon.com/blogs/hpc/choosing-between-batch-or-parallelcluster-for-hpc/"&gt;Choosing between AWS Batch or AWS ParallelCluster for HPC&lt;/a&gt;, &lt;a href="https://aws.amazon.com/blogs/hpc/why-use-fargate-with-aws-batch-for-serverless-batch-compute/"&gt;why you should use Fargate with AWS Batch&lt;/a&gt;, and how you can &lt;a href="https://aws.amazon.com/blogs/hpc/save-up-to-90-using-ec2-spot-even-for-long-running-hpc-jobs/"&gt;save up to 90% using EC2 Spot&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Protein language model training with NVIDIA BioNeMo framework on AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/protein-language-model-training-with-nvidia-bionemo-framework-on-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Marissa Powers]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 15:59:38 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AI]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Protein Folding]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">61fad9f084c647737166e7ff7d7a263d356d7d5e</guid>

					<description>In this new post, we discuss pre-training ESM-1nv for protein language modeling with NVIDIA BioNeMo on AWS. Learn how you can efficiently deploy and customize generative models like ESM-1nv on GPU clusters with ParallelCluster. Whether you're studying protein sequences, predicting properties, or discovering new therapeutics, this post has tips to accelerate your protein AI workloads on the cloud.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Marissa Powers and Ankur Srivastava from AWS, and Neel Patel from NVIDIA.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Proteins are large, complex biomolecules. They make up muscles, form into enzymes and antibodies, and perform signaling throughout the body. Today, &lt;a href="https://www.proteinatlas.org/humanproteome/tissue/druggable"&gt;proteins are the therapeutic target&lt;/a&gt; for the majority of pharmaceutical drugs. Increasingly, scientists are using language models to better understand protein function, generate new protein sequences, and predict protein properties [1].&lt;/p&gt; 
&lt;p&gt;With the recent proliferation of new models and tools in this field, researchers are looking for help to simplify the training, customization, and deployment of these generative AI models. And our high performance computing (HPC) customers are asking for how to easily perform distributed training with these models on AWS.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll demonstrate how to pre-train the &lt;code&gt;ESM-1nv&lt;/code&gt; model with the &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/containers/bionemo-framework"&gt;NVIDIA BioNeMo framework&lt;/a&gt; using NVIDIA GPUs on AWS ParallelCluster, an open-source cluster management tool that makes it easy for you to deploy and manage HPC clusters on AWS. &lt;a href="https://www.nvidia.com/en-us/clara/bionemo/"&gt;NVIDIA BioNeMo&lt;/a&gt; is a generative AI platform for drug discovery. It supports running commonly used models, including ESM-1, ESM-2, ProtT5nv, DNABert, MegaMolBART, DiffDock, EquiDock, and OpenFold. For the latest information on supported models, see the &lt;a href="https://docs.nvidia.com/bionemo-framework/latest/"&gt;BioNeMo framework documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_3420" style="width: 911px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3420" loading="lazy" class="size-full wp-image-3420" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/18/CleanShot-2024-03-18-at-14.35.22.png" alt="Figure 1: This image shows the workflow for developing models on NVIDIA BioNeMo. The process is divided into phases for model development and customization and then fine-tuning and deployment." width="901" height="392"&gt;
 &lt;p id="caption-attachment-3420" class="wp-caption-text"&gt;Figure 1: This image shows the workflow for developing models on NVIDIA BioNeMo. The process is divided into phases for model development and customization and then fine-tuning and deployment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;This example deployment also leverages Amazon FSx for Lustre and the Elastic Fabric Adapter (EFA), which can both be provisioned and configured by ParallelCluster. With ParallelCluster, users can scale out distributed training jobs across hundreds or thousands of vCPUs. Code examples, including cluster configuration files, are &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/14.bionemo"&gt;available on GitHub&lt;/a&gt;&lt;strong&gt;. &lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Walkthrough&lt;/h2&gt; 
&lt;p&gt;For this example, we will (1) create an HPC cluster using AWS ParallelCluster, (2) configure the cluster with BioNeMo framework and download datasets, and (3) execute a pre-training job on the cluster.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;We assume you have access to an AWS account and are authenticated in that account. The configuration file used here uses &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;p4de.24xlarge&lt;/a&gt; instance types powered by 8 x &lt;a href="https://www.nvidia.com/en-us/data-center/a100/"&gt;NVIDIA A100&lt;/a&gt; 80GB Tensor Core GPUs and was tested in the us-east-1 region. We have also tested it with &lt;a href="https://aws.amazon.com/ec2/instance-types/p5/"&gt;p5.48xlarge&lt;/a&gt; instances powered by 8 x &lt;a href="https://www.nvidia.com/en-us/data-center/h100/"&gt;NVIDIA H100&lt;/a&gt; Tensor Core GPUs, which delivered better performance. Check your AWS service quotas to ensure you have sufficient access to these instance types.&lt;/p&gt; 
&lt;p&gt;ParallelCluster must be installed on a local node or instance. &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-virtual-environment.html"&gt;Follow the instructions&lt;/a&gt; in our documentation to install ParallelCluster in a virtual environment.&lt;/p&gt; 
&lt;p&gt;Finally, to pull down the BioNeMo container, you will need an NVIDIA NGC API key. To set this up, follow the guidance under “NGC Setup” in the &lt;a href="https://docs.nvidia.com/bionemo-framework/latest/quickstart-fw.html#ngc-setup"&gt;&lt;strong&gt;BioNeMo documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Create a cluster&lt;/h2&gt; 
&lt;p&gt;The AWS &lt;code&gt;awsome-distributed-training&lt;/code&gt; &lt;a href="https://github.com/aws-samples/awsome-distributed-training"&gt;GitHub repo&lt;/a&gt; provides configuration templates for running distributed training on multiple AWS services, including Amazon EKS, AWS Batch, and AWS ParallelCluster. In this example, we’ll create a cluster using one of the provided ParallelCluster configuration files. You can see an overview of the architecture in Figure 2.&lt;/p&gt; 
&lt;div id="attachment_3421" style="width: 841px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3421" loading="lazy" class="size-full wp-image-3421" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/18/CleanShot-2024-03-18-at-14.36.56.png" alt="Figure 2 – In this distributed training architecture, ParallelCluster deploys a cluster consisting of a head node in a public subnet, a single queue of p4de.24xlarge instances in a private subnet, an FSx for Lustre filesystem, and a shared Amazon Elastic Block Storage (EBS) volume. FSx is used for input and output datasets, and the EBS volume is used to store job-specific submission scripts and log files. All the resources are deployed within a user’s own VPC." width="831" height="570"&gt;
 &lt;p id="caption-attachment-3421" class="wp-caption-text"&gt;Figure 2 – In this distributed training architecture, ParallelCluster deploys a cluster consisting of a head node in a public subnet, a single queue of p4de.24xlarge instances in a private subnet, an FSx for Lustre filesystem, and a shared Amazon Elastic Block Storage (EBS) volume. FSx is used for input and output datasets, and the EBS volume is used to store job-specific submission scripts and log files. All the resources are deployed within a user’s own VPC.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To start, clone the awsome-distributed-training GitHub repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;git clone https://github.com/aws-samples/awsome-distributed-training.git&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Navigate to the &lt;em&gt;templates&lt;/em&gt; directory in the locally cloned repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd 1.architectures/2.aws-parallelcluster&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The configuration file we will use is distributed-training-p4de_postinstall_scripts.yaml. Open the file and update to use an ubuntu-based AMI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Image:
  Os: ubuntu2004
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ParallelCluster will deploy the head node and compute nodes in subnets you specify in the YAML file. You can choose to deploy all resources in the same subnet or use separate subnets for the head node and compute nodes. We recommend that you deploy all resources in the same Availability Zone (AZ). That’s because ParallelCluster will create a Lustre file system for the cluster – which resides in a single AZ, and deploying workloads across multiple Availability Zones will cause additional cost incursions due to the cross-AZ traffic.&lt;/p&gt; 
&lt;p&gt;Update the head node subnet in L13 in the YAML file. Update the compute nodes subnet in L50:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;HeadNode:
  InstanceType: m5.8xlarge
  Networking:
    SubnetId: PLACEHOLDER_PUBLIC_SUBNET
  Ssh:
    KeyName: PLACEHOLDER_SSH_KEY
...
  SlurmQueues:
    - Name: compute-gpu
      CapacityType: ONDEMAND
      Networking:
        SubnetIds:
          - PLACEHOLDER_PRIVATE_SUBNET
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using an On-Demand Capacity Reservation (ODCR), provide the resource id in the YAML file. If not, comment out these two lines:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;      CapacityReservationTarget:
        CapacityReservationId: PLACEHOLDER_CAPACITY_RESERVATION_ID
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the YAML file has been updated, use ParallelCluster to create the cluster:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster \
   --cluster-name bionemo-cluster \
   --cluster-configuration distributed-training-p4de_postinstall_scripts.yaml \
   --region us-east-1 \
   --dryrun false
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Cluster creation will take 20-30 minutes. You can monitor the status of cluster creation with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;watch pcluster describe-cluster \
   --cluster-name bionemo-cluster \
   --region us-east-1 \
   --query clusterStatus
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The cluster will be deployed as an AWS CloudFormation stack. You can also monitor resource creation from the CloudFormation console or by querying CloudFormation in the CLI.&lt;/p&gt; 
&lt;h3&gt;Configure cluster and input datasets&lt;/h3&gt; 
&lt;p&gt;Once the cluster status is complete, follow the guidance in the &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/14.bionemo"&gt;GitHub repo&lt;/a&gt; to finish setup and model pre-training. At a high level, these steps walk through how to (1) pull down the BioNeMo framework container; (2) build an AWS-optimized image; (3) download and pre-process the &lt;code&gt;UniRef50&lt;/code&gt; dataset; and (4) run the &lt;code&gt;ESM-1nv&lt;/code&gt; pre-training job.&lt;/p&gt; 
&lt;p&gt;Once you submit the pre-training job, you can monitor progress by running tail on the output log file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;tail -f /apps/slurm-esm1nv-&amp;lt;job-id&amp;gt;.out&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more detailed monitoring of the cluster, including memory, networking, and storage usage on both the head node and compute nodes, consider creating a Grafana dashboard. A detailed guide for creating a dashboard for ParallelCluster clusters is &lt;a href="https://github.com/aws-samples/aws-parallelcluster-monitoring"&gt;available on Github&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we demonstrated how to pre-train ESM-1nv with the NVIDIA BioNeMo framework and NVIDIA GPUs on AWS ParallelCluster. For information about other models supported by the NVIDIA BioNeMo framework, see the BioNeMo framework documentation. For guides on deploying other distributed training jobs on AWS, check out the additional test case in the awsome-distributed-training repository in GitHub.&lt;/p&gt; 
&lt;p&gt;For alternative options for deploying BioNeMo framework on AWS, check out our guide for &lt;a href="https://aws.amazon.com/blogs/industries/find-the-next-blockbuster-with-nvidia-bionemo-framework-on-amazon-sagemaker/"&gt;Amazon SageMaker&lt;/a&gt;.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Reference&lt;/h3&gt; 
&lt;p&gt;[1] Ruffolo, J.A., Madani, A. Designing proteins with language models. Nat Biotechnol 42, 200–202 (2024). &lt;a href="https://doi.org/10.1038/s41587-024-02123-4"&gt;https://doi.org/10.1038/s41587-024-02123-4&lt;/a&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Introducing new alerts to help users detect and react to blocked job queues in AWS Batch</title>
		<link>https://aws.amazon.com/blogs/hpc/introducing-new-alerts-to-help-users-detect-and-react-to-blocked-job-queues-in-aws-batch/</link>
		
		<dc:creator><![CDATA[Naina Thangaraj]]></dc:creator>
		<pubDate>Thu, 14 Mar 2024 18:29:42 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">4dfd9662b13958ccd5756233dff2259adb1eb906</guid>

					<description>Heads up AWS Batch users! Learn how to get notifications when your job queue gets blocked so you can quickly troubleshoot and keep your workflows moving. Details in our blog.</description>
										<content:encoded>&lt;p&gt;As many readers will know, AWS Batch provides functionality that enables you to run batch workloads on the managed container orchestration services in AWS: Amazon ECS and Amazon EKS. One of the core concepts of Batch is that it provides a job queue you can submit your work to. Batch is designed to transition your jobs from &lt;code&gt;SUBMITTED&lt;/code&gt; to &lt;code&gt;RUNNABLE&lt;/code&gt; states if they pass preliminary checks, and from &lt;code&gt;RUNNING&lt;/code&gt; to either &lt;code&gt;FAILED&lt;/code&gt; or &lt;code&gt;SUCCEEDED&lt;/code&gt; after the job is placed on a compute resource and completes. Batch is also sends an event to Amazon CloudWatch Events for each corresponding job state update.&lt;/p&gt; 
&lt;p&gt;Sometimes, though, a &lt;code&gt;RUNNABLE&lt;/code&gt; job at the head of the queue can block all other jobs behind it from running. This could be caused by a misconfiguration in your AWS account, or it might be because your account doesn’t have access to the specific instances required for the job (like a GPU, for example). We’ve&amp;nbsp;&lt;a href="https://repost.aws/knowledge-center/batch-job-stuck-runnable-status"&gt;documented several common causes&lt;/a&gt;&amp;nbsp;– and their resolutions – but to fix the issue you&amp;nbsp;&lt;em&gt;first need to know that the blocked job queue condition exists&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Today we’re introducing&amp;nbsp;CloudWatch Events notifications for blocked job queues. We’ve designed this new feature to provide you with an event any time Batch detects that a job queue is blocked by a &lt;code&gt;RUNNABLE&lt;/code&gt; job at the head of the queue. Even better, the feature is designed to show the reason the job is stuck in&amp;nbsp;&lt;em&gt;both&lt;/em&gt;&amp;nbsp;the CloudWatch event and the&amp;nbsp;&lt;code&gt;statusReason&lt;/code&gt;&amp;nbsp;of the job&amp;nbsp;returned from &lt;code&gt;DescribeJobs&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;ListJobs&lt;/code&gt;&amp;nbsp;API calls.&lt;/p&gt; 
&lt;h2&gt;Common causes of blocked job queues&lt;/h2&gt; 
&lt;p&gt;There are many reasons why a job at the head of the queue can block other jobs behind it from running.&lt;/p&gt; 
&lt;p&gt;IAM roles, network, and security settings are often the culprits, but there are several more reasons for a blocked job queue&amp;nbsp;&lt;em&gt;that Batch can detect&lt;/em&gt;&amp;nbsp;in your environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;your queue’s attached compute environments (CEs) have all received insufficient capacity errors&lt;/li&gt; 
 &lt;li&gt;your CEs have a&amp;nbsp;&lt;code&gt;maxVcpu&lt;/code&gt;&amp;nbsp;that’s too small for the job requirements&lt;/li&gt; 
 &lt;li&gt;your CEs lack any instances that meet the job requirements&lt;/li&gt; 
 &lt;li&gt;your service role has a permission issue&lt;/li&gt; 
 &lt;li&gt;all your CEs are in an &lt;code&gt;INVALID&lt;/code&gt; state (which usually means networking or IAM roles are preventing instances joining the compute fleet)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With the new CloudWatch Events notifications we’re announcing today, Batch can now send a notification when it detects a common reason for jobs being stuck in &lt;code&gt;RUNNABLE&lt;/code&gt;, but&amp;nbsp;&lt;em&gt;not&lt;/em&gt; for every &lt;code&gt;RUNNABLE&lt;/code&gt; job simply waiting for its turn. Finally, sometimes Batch will detect a blocked job queue but can’t determine the specific reason. In this case, Batch can still send a notification of a blocked job queue, but you’ll need to do a bit of detective work to figure out the root cause.&lt;/p&gt; 
&lt;h2&gt;Taking action&lt;/h2&gt; 
&lt;p&gt;There are two ways you can programmatically act on the blocked job queue events that Batch sends to CloudWatch Events.&lt;/p&gt; 
&lt;h3&gt;Acting on events with Amazon EventBridge&lt;/h3&gt; 
&lt;p&gt;The first way you can automate an action (based on a matching event pattern) is to define Amazon EventBridge &lt;a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rules.html"&gt;rules&lt;/a&gt; with different&amp;nbsp;EventBridge &lt;a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html"&gt;targets&lt;/a&gt;&amp;nbsp;for each event type.&lt;/p&gt; 
&lt;p&gt;For example, when you receive a message about a job that requires more memory than any instance can provide, you could use an AWS Lambda function to terminate the job request—to unblock the job queue—and then send the event information into an Amazon SQS queue so you can inspect the job details later.&lt;/p&gt; 
&lt;p&gt;When you review those SQS messages, you can decide if you want to adjust the CE to meet the needs of these types of jobs,&amp;nbsp;&lt;em&gt;or&lt;/em&gt;&amp;nbsp;create a new Batch environment to handle more resource-intensive jobs.&lt;/p&gt; 
&lt;p&gt;The Batch user guide has examples showing how to&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/batch_cwet.html"&gt;listen for&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/batch_sns_tutorial.html"&gt;react to&lt;/a&gt;&amp;nbsp;the CloudWatch events using EventBridge.&lt;/p&gt; 
&lt;h3&gt;Auto terminating stuck jobs&lt;/h3&gt; 
&lt;p&gt;The second way you can act to address blocked job queues is at the level of the job queue itself.&lt;/p&gt; 
&lt;p&gt;Batch now has a&amp;nbsp;&lt;code&gt;jobStateTimeLimitActions&lt;/code&gt;&amp;nbsp;parameter for job queues that lets you automatically cancel a stuck job after a defined period of time,&amp;nbsp;&lt;code&gt;maxTimeSeconds&lt;/code&gt;. If you opt to define this parameter, Batch is designed to start the timeout clock ticking when it detects that a job is blocking the queue. Batch will also update the&amp;nbsp;&lt;code&gt;statusReason&lt;/code&gt;&amp;nbsp;field at this time. Once the stuck job at the head of the queue is cancelled, a “&lt;em&gt;Batch Job State Change”&lt;/em&gt;&amp;nbsp;CloudWatch event is emitted with the underlying reason. If Batch detects that another job at the head of the queue is blocking the queue, a &lt;em&gt;“Batch Job Queue Blocked”&lt;/em&gt; CloudWatch event is emitted, and starts a new &lt;code&gt;maxTimeSeconds&lt;/code&gt; timer to take the action you defined once the limit is reached.&lt;/p&gt; 
&lt;p&gt;Here’s an example showing how to specify that the job queue will wait for a job that is blocked by compute environments having reached maximum capacity for 4 hours (maxTimeSeconds=14400) before cancelling the job.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;"jobStateTimeLimitActions": [
      {                              
         "reason" :  "MISCONFIGURATION:COMPUTE_ENVIRONMENT_MAX_RESOURCE", 
         "state": "RUNNABLE",            
         "maxTimeSeconds" : 14400,
         "action" : "CANCEL"            
      }
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Table 1 lists the scenarios we discussed, the output message in CloudWatch Events, and the&amp;nbsp;&lt;code&gt;jobStateTimeLimitAction.reason&lt;/code&gt;&amp;nbsp;that you can specify to cancel the stuck job in the job queue. The table also lists the “&lt;em&gt;Batch Job State Change”&lt;/em&gt;&amp;nbsp;CloudWatch event message if the job was automatically canceled.&lt;/p&gt; 
&lt;table class="alignleft" style="border-color: #000000" border="0"&gt; 
 &lt;caption&gt;
  &lt;em&gt;Table 1: AWS Batch CloudWatch Events messages for blocked job queue events. The &lt;strong&gt;Scenerio&lt;/strong&gt; column describes the context Batch determined that a queue was blocked. &lt;strong&gt;Status Parameter&lt;/strong&gt; refers to key fields in event messages that are provided for an event. &lt;strong&gt;Status Parameter Value&lt;/strong&gt; lists an example value for the corresponding parameter. Note that the CAPACITY:INSUFFICIENT_INSTANCE_CAPACITY message will provide a specific instance type name, not just the one in the example.&amp;nbsp;&lt;/em&gt;
 &lt;/caption&gt; 
 &lt;thead&gt; 
  &lt;tr style="color: white;font-weight: bold;background-color: #000000"&gt; 
   &lt;td width="95"&gt;Scenario&lt;/td&gt; 
   &lt;td width="248"&gt;Status Parameter&lt;/td&gt; 
   &lt;td width="288"&gt;Status Parameter value&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td style="font-weight: bold;background-color: #d2d2d2" rowspan="3" width="95"&gt;&lt;strong&gt;All your job queue’s connected compute environments have received insufficient capacity errors.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;CAPACITY:INSUFFICIENT_INSTANCE_CAPACITY – Service cannot fulfill the capacity requested for instance type [instanceTypeName].&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="248"&gt;&lt;code&gt;jobStateTimeLimitActions.reason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;CAPACITY:INSUFFICIENT_INSTANCE_CAPACITY&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt; after job cancellation&lt;/td&gt; 
   &lt;td width="288"&gt;Canceled by JobStateTimeLimit action due to reason: CAPACITY:INSUFFICIENT_INSTANCE_CAPACITY&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="font-weight: bold;background-color: #d2d2d2" rowspan="3" width="95"&gt;&lt;strong&gt;All compute environments have maxVcpu that is smaller than job requirements.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;MISCONFIGURATION:COMPUTE_ENVIRONMENT_MAX_RESOURCE – CE(s) associated with the job queue cannot meet the CPU requirement of the job.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td width="248"&gt;&lt;code&gt;jobStateTimeLimitActions.reason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;MISCONFIGURATION:COMPUTE_ENVIRONMENT_MAX_RESOURCE&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt; after job cancellation&lt;/td&gt; 
   &lt;td width="288"&gt;Canceled by JobStateTimeLimit action due to reason: MISCONFIGURATION:COMPUTE_ENVIRONMENT_MAX_RESOURCE&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td style="font-weight: bold;background-color: #d2d2d2" rowspan="3" width="95"&gt;&lt;strong&gt;All compute environments have no connected instances that meet job requirements.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;MISCONFIGURATION:JOB_RESOURCE_REQUIREMENT – The job resource requirement (vCPU/memory/GPU) is higher than that can be met by the CE(s) attached to the job queue.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="248"&gt;&lt;code&gt;jobStateTimeLimitActions.reason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;MISCONFIGURATION:JOB_RESOURCE_REQUIREMENT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt; after job cancellation&lt;/td&gt; 
   &lt;td width="288"&gt;Canceled by JobStateTimeLimit action due to reason: MISCONFIGURATION:JOB_RESOURCE_REQUIREMENT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="font-weight: bold;background-color: #d2d2d2" rowspan="3" width="95"&gt;&lt;strong&gt;All compute environments have service role issues.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;MISCONFIGURATION:SERVICE_ROLE_PERMISSIONS – Batch service role has a permission issue.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td width="248"&gt;&lt;code&gt;jobStateTimeLimitActions.reason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;MISCONFIGURATION:SERVICE_ROLE_PERMISSIONS&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt; after job cancellation&lt;/td&gt; 
   &lt;td width="288"&gt;Canceled by JobStateTimeLimit action due to reason: MISCONFIGURATION:SERVICE_ROLE_PERMISSIONS&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td style="font-weight: bold;background-color: #d2d2d2" rowspan="3" width="95"&gt;&lt;strong&gt;All connected compute environments are invalid.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;ACTION_REQUIRED – CE(s) associated with the job queue are invalid.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="248"&gt;&lt;code&gt;jobStateTimeLimitActions.reason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;&lt;em&gt;Not applicable&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt; after job cancellation&lt;/td&gt; 
   &lt;td width="288"&gt;&lt;em&gt;Not applicable&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="font-weight: bold;background-color: #d2d2d2" rowspan="3" width="95"&gt;&lt;strong&gt;Batch has detected a blocked queue, but is unable to determine the reason.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;UNDETERMINED – Batch job is blocked, root cause is undetermined.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td width="248"&gt;&lt;code&gt;jobStateTimeLimitActions.reason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;&lt;em&gt;Not applicable&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt; after job cancellation&lt;/td&gt; 
   &lt;td width="288"&gt;&lt;em&gt;Not applicable&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;You’ll note that some reasons are not able to be used in the &lt;code&gt;jobStateTimeLimitActions&lt;/code&gt; parameter. One example is when all your queue’s attached compute environments are INVALID, or when Batch is unable to determine the root cause of the blockage. In both of those cases, we recommend setting up EventBridge rules to notify you when they occur.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;AWS Batch has introduced new functionality designed to help you detect and act on blocked job queues, where a job that’s at the head of a queue can’t run for one reason or another and prevents all the others behind it from running, too. We shared how to use EventBridge to catch these new CloudWatch Events, and the Batch job queue API to automatically terminate the stuck job and unblock the queue of jobs behind it.&lt;/p&gt; 
&lt;p&gt;Finally, we covered the most common root causes along with the error messages to look for. In most cases Batch can automatically determine the root cause of the blockage, allowing you to define specific automated actions for each class of error to take action on and unblock the queue.&lt;/p&gt; 
&lt;p&gt;To get started using AWS Batch, log into the&amp;nbsp;&lt;a href="https://console.aws.amazon.com/batch/home"&gt;AWS Management Console&lt;/a&gt;, or read the&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html"&gt;AWS Batch User Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For specific guidance on using these new events, refer to the AWS Batch Troubleshooting guide for &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/troubleshooting.html#job_stuck_in_runnable"&gt;jobs stuck in RUNNABLE&lt;/a&gt;, the &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/batch_cwe_events.html#batch-job-queue-blocked-events"&gt;blocked job queue events&lt;/a&gt; documentation, and the &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/cloudwatch_event_stream.html"&gt;Batch EventBridge&lt;/a&gt; documentation on how react to these events.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Using large-language models for ESG sentiment analysis using Databricks on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/using-large-language-models-for-esg-sentiment-analysis-using-databricks-on-aws/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Tue, 05 Mar 2024 12:34:23 +0000</pubDate>
				<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AI]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">5ff183a8b5fd12c0deb708f5dd9760595e67a42c</guid>

					<description>ESG is now a boardroom issue. See how Databricks' AI solution helps understand emissions data and meet new regulations.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="size-full wp-image-3333 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/14/boofla88_environmental_social_and_governance_policies_as_a_conc_58e6417e-1244-4776-8eef-c48fb6d9cb32.png" alt="Using large-language models for ESG sentiment analysis using Databricks on AWS" width="380" height="212"&gt;This post was contributed by Ilan Gleiser, Principal ML Specialist, Global Impact Computing, AWS, &lt;/em&gt;&lt;em&gt;Antoine Amend, Sr. Technical Director, Databricks, &lt;/em&gt;&lt;em&gt;Venkat Viswanathan, Senior Solutions Architect, AWS&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Regulators worldwide recognize the threat of climate change to economies and financial systems, forcing public companies in the US and Europe to be aware of – and disclose – their greenhouse emissions. This has led to a surge in demand from various stakeholders, including asset managers, asset owners, investors, and regulators, for machine-learning models that can analyze the plethora of data on their environmental, social and governance (ESG) policies.&lt;/p&gt; 
&lt;p&gt;Alongside regulatory change, there is a financial incentive to this endeavor. Typically, ESG ratings have a positive correlation with both valuation and profitability, &lt;a href="https://corpgov.law.harvard.edu/2020/01/14/esg-matters/"&gt;while showing a negative correlation with volatility&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In this post, we’re going to look at the challenge posed by ESG, as a data and AI problem. In the &lt;a href="https://www.databricks.com/blog/2020/07/10/a-data-driven-approach-to-environmental-social-and-governance.html"&gt;Databricks ESG Solution Accelerator&lt;/a&gt;, we will use natural language processing (NLP) to sort through the vast amounts of structured, and unstructured, data.&lt;/p&gt; 
&lt;h2&gt;Why Databricks?&lt;/h2&gt; 
&lt;p&gt;The main benefits for using the Databricks ESG Solution Accelerator are:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Detect differences between sustainability reports and real-time news data in portfolios of securities. Differences between sustainability reports and real-time news data may lead consumers and stakeholders to overvalue a company’s ESG score.&lt;/li&gt; 
 &lt;li&gt;View how companies’ business relationships with one another within a global marketplace can impact their respective ESG scores.&lt;/li&gt; 
 &lt;li&gt;Show how companies on the top decile of the ESG rank have half the volatility (as measured by &lt;em&gt;value-at-risk&lt;/em&gt;) of companies on the bottom end of the ESG ranks and better returns, indicating superior Sharpe ratios.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Users of the Databricks ESG Solution Accelerator can turn ESG risk into a source of excess returns (or &lt;em&gt;alpha&lt;/em&gt;) if spotted early in the process. This is done by processing unstructured data, like PDFs and real time news data and using machine learning (ML) algorithms that analyses the sentiment of the news data and compares with the sentiment of the data coming from the companies’ sustainability report on the same ESG policy&lt;/p&gt; 
&lt;h2&gt;The critical problem of unstructured data&lt;/h2&gt; 
&lt;p&gt;The ESG world generates data that is inherently unstructured. Out of the 40 frequently-disclosed ESG policies by companies, only ten can be measured as tangible figures while the rest are policy initiatives expressed in textual form across various IT systems. This raises the question of how AI can be applied to quantify and compare organizations ESG policies in a more objective manner.&lt;/p&gt; 
&lt;p&gt;To help customers unlock the value of their sustainability data stored in the AWS cloud, we are highlighting Databricks ESG Solution Accelerator as described in “&lt;a href="https://www.databricks.com/blog/2020/07/10/a-data-driven-approach-to-environmental-social-and-governance.html"&gt;&lt;em&gt;A Data-driven Approach to Environmental, Social, and Governance using the Databricks Solutions Accelerator&lt;/em&gt;&lt;/a&gt;” and showing you step by step, how to run the accelerator on AWS.&lt;/p&gt; 
&lt;p&gt;The notebooks show you how machine learning can enable asset managers, regulators, and investors to assess the sustainability exposure of their investments and empower their businesses with a holistic and data-driven view of their environmental, social, and corporate governance strategies.&lt;/p&gt; 
&lt;p&gt;Specifically, the first notebook will extract key ESG initiatives communicated in yearly sustainability PDF reports and compare these with real time actual media coverage from news analytics data as in Figure 1. The idea behind this approach is to spot differences between sustainability reports and real-time news data to offer decision makers with the most accurate and promptly available information.&lt;/p&gt; 
&lt;div id="attachment_3323" style="width: 840px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3323" loading="lazy" class="size-full wp-image-3323" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/13/CleanShot-2024-02-13-at-11.02.26.png" alt="Figure 1. Extract the key ESG initiatives as communicated in yearly PDF reports and compare these with the actual media coverage from news analytics data" width="830" height="406"&gt;
 &lt;p id="caption-attachment-3323" class="wp-caption-text"&gt;Figure 1. Extract the key ESG initiatives as communicated in yearly PDF reports and compare these with the actual media coverage from news analytics data&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Databricks Solution Accelerators on AWS: speedways to innovation&lt;/h2&gt; 
&lt;p&gt;Databricks ESG Solution Accelerator for AWS provides an expedited path to production for AWS customers who store their data on Amazon S3. Businesses can simplify the migration of their data and AI workloads to Databricks on AWS and quickly start utilizing the accelerator notebooks. The Databricks ESG Solution Accelerator come in a pair of notebooks that are easy to set up and provide prompt feedback, enabling asset management, MLOPS governance, and risk teams to achieve their goals in a much shorter time frame. Customers can take advantage of AWS’s computing power and virtually limitless storage, resulting in increased flexibility, scalability, and dependability at a reduced cost compared to developing an in-house solution.&lt;/p&gt; 
&lt;p&gt;To support users from different lines of business, these accelerators are designed to meet the different goals of multiple teams – portfolio managers and asset owners can use them to maximize alpha or minimize risk; regulators can identify new potential hot spots; and corporations can gather public sentiment of their news in the media.&lt;/p&gt; 
&lt;h2&gt;ESG Accelerator Logic&lt;/h2&gt; 
&lt;p&gt;Although all these accelerators work together, businesses can apply them as stand-alone projects or layer them. The process goes as follows:&lt;/p&gt; 
&lt;p&gt;Extract ESG initiatives from ESG reports using PyPDF2 and categorize them based on different topics. Then, tokenize and lemmatize the content for algorithmic ingestion. The next step is to automatically classify sentences extracted from ESG reports using atopic modeling algorithm. This classification allows customer/users to compare different companies’ by clustering them side-by-side to identify key focus areas (or ESG policies) voluntarily disclosed by companies in their sustainability reports.&lt;/p&gt; 
&lt;p&gt;To create a data-driven ESG score, the Databricks ESG Solution Accelerator runs a sentiment analysis on financial news articles related to each company is conducted using the Global Database of Event Location and Tones (GDELT) files, updated every 15mins. The assumption is that overall tone captured from financial news articles is a good proxy for companies’ ESG scores. This approach generates scores for each company across all its ESG dimensions. A propagated weighted ESG (PW-ESG) metric is then calculated to provide a global view of risk by quantifying the links between companies and assessing their importance.&lt;/p&gt; 
&lt;p&gt;Finally, to validate the assumption that high-ranked ESG companies offer better risk-adjusted returns than low-ranked ESG companies, the portfolio is split into two books – best and worst 10% ESG scores, respectively – and their historical returns and corresponding 95% Value-at-Risk are computed. The results show that low-ranked ESG portfolio has 2 times more risk than high-ranked ESG portfolio for the same level of returns.&lt;/p&gt; 
&lt;h3&gt;Benefits of notebook 1 – spotting differences between sustainability reports and real-time news data&lt;/h3&gt; 
&lt;p&gt;The goal of notebook #1 is to understand what the statements are all about, learning a vocabulary that is ESG specific with themes like diversity and inclusion, code of conduct, supporting communities and renewable energy.&lt;/p&gt; 
&lt;p&gt;The ESG Solution Accelerator supports Databricks/AWS customers who host their data on AWS, offering a machine learning platform that educates itself on a wide array of subjects and themes that are significant in today’s corporate social responsibility environment. These themes range from ‘diversity and inclusion’, ‘code of conduct’, and ‘supporting communities’, to ‘renewable energy’, ‘impact investing’, and ‘valuing employees’. The automated learning feature enables the algorithm to provide valuable insights in these specific fields, allowing businesses the opportunity to better understand and incorporate these aspects into their investment and risk management strategies.&lt;/p&gt; 
&lt;div id="attachment_3324" style="width: 765px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3324" loading="lazy" class="size-full wp-image-3324" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/13/CleanShot-2024-02-13-at-11.03.22.png" alt="Fig 2: Cluster analysis of machine learned policies emerging from unstructured Sustainability Reports PDF files" width="755" height="433"&gt;
 &lt;p id="caption-attachment-3324" class="wp-caption-text"&gt;Fig 2: Cluster analysis of machine learned policies emerging from unstructured Sustainability Reports PDF files&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3325" style="width: 774px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3325" loading="lazy" class="size-full wp-image-3325" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/13/CleanShot-2024-02-13-at-11.03.29.png" alt="Fig 3: Comparing organizations side-by-side, based on how much they disclose in each of those categories." width="764" height="554"&gt;
 &lt;p id="caption-attachment-3325" class="wp-caption-text"&gt;Fig 3: Comparing organizations side-by-side, based on how much they disclose in each of those categories.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;In a separate blog post, we show you how you can fine-tune a large language model and accelerate hyperparameter grid search for sentiment analysis with BERT models using Weights &amp;amp; Biases, Amazon EKS, and TorchElastic.&lt;/p&gt; 
&lt;h3&gt;Benefit of Notebook 2 – Understand How ESG Scores correlate with market risk and returns&lt;/h3&gt; 
&lt;p&gt;The second notebook presents a method for constructing a synthetic portfolio and incorporating ESG insights into the market risk framework. This involves considering not only a company’s outward appearance, but also its potential for performance. Research and literature suggest that companies with strong ESG practices typically exhibit lower market volatility. Our examination of value at risk highlights that a synthetic portfolio lacking such practices can be twice as volatile. Ultimately, we aim to connect research and product by integrating our findings into a comprehensive BI to AI Dashboard.&lt;/p&gt; 
&lt;p&gt;Combining all those insights, to understand what a company says about ESG vs how much a company does across those 24 machines learned policies, informs us in real time about news events that may positively or negatively affect the ESG score of every company, and in turn, the impact it may have on its market performance.&lt;/p&gt; 
&lt;p&gt;Combining all those insights into one platform helps us understand what a company says about ESG versus how much it actually does across the 24 machine-learned policies. This platform informs us in real time about news events that may positively or negatively affect the ESG score of every company, and subsequently, the impact it may have on its market performance.&lt;/p&gt; 
&lt;p&gt;ESG factors are among major market factors, like value, momentum, and volatility. The taxonomy of ESG factors has proved adaptive, as the market empirically prices new indicators. In addition, the recent advances in quantifying the effect of ESG factors on performance, in developing a regulatory and legal framework for ESG, and in establishing new ESG ratings should continue to have a positive effect on asset flows into ESG-related strategies&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Running the Accelerator on AWS&lt;/h2&gt; 
&lt;p&gt;Databricks built the ESG Solution Accelerator using the Delta Lake &lt;a href="https://www.databricks.com/glossary/medallion-architecture"&gt;Medallion Architecture&lt;/a&gt; to ingest data from raw, enriched, and purpose-built tables categorized as Bronze, Silver, and Gold Delta Lake tables.&lt;/p&gt; 
&lt;p&gt;Delta Lake is an open-source project built for data ‘lakehouses’ with compute engines, including Apache Spark, Trino, PrestoDB, Flink, and Hive, and with APIs for Scala, Java, Rust, Ruby, and Python. Delta Lake is an &lt;em&gt;ACID table storage layer&lt;/em&gt; over cloud object stores like S3 that provides data reliability including, but not limited to, schema enforcement and evolution, time travel, scalable metadata handling, audit history, DML operations, and unifies stream and batch processing. You can store the data in Delta Lake tables in your encrypted S3 bucket.&lt;/p&gt; 
&lt;div id="attachment_3326" style="width: 876px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3326" loading="lazy" class="size-full wp-image-3326" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/13/CleanShot-2024-02-13-at-11.04.47.png" alt="Fig 4. This schematic depicts how the Solutions Accelerator leverages corporate disclosures, news feeds, and portfolio ticker symbols to collect, transform, enrich, and apply machine learning models to create ESG scores from corporate disclosures and from the news feeds. This depicts the gaps between the corporate disclosures and the news feeds." width="866" height="424"&gt;
 &lt;p id="caption-attachment-3326" class="wp-caption-text"&gt;Figure 4. This schematic depicts how the Solutions Accelerator leverages corporate disclosures, news feeds, and portfolio ticker symbols to collect, transform, enrich, and apply machine learning models to create ESG scores from corporate disclosures and from the news feeds. This depicts the gaps between the corporate disclosures and the news feeds.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;A machine learning model is created using open-source MLFlow to enrich the data with ML-based approach. Databricks workloads are run within the customer’s VPC account in either Amazon EC2 instances or in Amazon Elastic Container Registry (Amazon ECR) containers.&lt;/p&gt; 
&lt;p&gt;AWS native services and AWS partner services further consume the data from Delta Lake. Some examples include using Amazon QuickSight for BI and visualization, Amazon Athena for advanced analytics, AWS Glue for data catalog, and Amazon SageMaker for more models, inference, and serving.&lt;/p&gt; 
&lt;p&gt;Along with these are ancillary components like AWS Identity &amp;amp; Access Management (IAM), which control access and set guardrails to the applications and services, and Amazon CloudWatch for monitoring the applications and services to ensure proper functioning, can be implemented.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Steps to use the template for the ESG Sentiment use case&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If you are new to AWS, &lt;a href="https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/"&gt;create and activate a new account&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you are new to Databricks on AWS, &lt;a href="https://aws.amazon.com/quickstart/architecture/databricks/#:~:text=If%20you%20don't%20already,existing%20cross%2Daccount%20IAM%20role"&gt;create and setup the Databricks account&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Login to Databricks &lt;code&gt;&amp;lt;account_name&amp;gt;.cloud.databricks.com&lt;/code&gt; with your username and password.&lt;/li&gt; 
 &lt;li&gt;Clone your repos using &lt;strong&gt;Repos&lt;/strong&gt; → &lt;strong&gt;Add Repo&lt;/strong&gt; → &lt;strong&gt;Select Git Hub&lt;/strong&gt; from the &lt;strong&gt;Git provider&lt;/strong&gt; → Enter &lt;code&gt;https://github.com/databricks-industry-solutions/esg-scoring&lt;/code&gt; in &lt;strong&gt;Git repository URL field&lt;/strong&gt; → provide your repository name in &lt;strong&gt;Repository name&lt;/strong&gt; field. Then &lt;strong&gt;submit&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Go to &lt;strong&gt;Compute&lt;/strong&gt; → &lt;strong&gt;Create Cluster&lt;/strong&gt; and select &lt;strong&gt;ML and 10.4 LTS ML&lt;/strong&gt; (Scala 2.12, Spark 3.2.1 and above) → &lt;strong&gt;Worker Type Instance Type&lt;/strong&gt;, &lt;strong&gt;Min workers&lt;/strong&gt; as 2, &lt;strong&gt;Max workers&lt;/strong&gt; as 8), and &lt;strong&gt;Driver Type&lt;/strong&gt; as Worker Type, Check &lt;strong&gt;Enable Auto scaling&lt;/strong&gt;, Check &lt;strong&gt;Terminate&lt;/strong&gt; after 60 minutes of inactivity&lt;/li&gt; 
 &lt;li&gt;Create your cluster&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Repos&lt;/strong&gt; → Select your repo → Select the Notebook (there are 5 note books) → Open the Notebook sequentially → Attach Cluster from the &lt;strong&gt;Connect&lt;/strong&gt; drop down and select the one you created in the last step —&amp;gt; &lt;strong&gt;Run&lt;/strong&gt; → &lt;strong&gt;Run All&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You can read the &lt;a href="https://www.databricks.com/blog/2020/07/10/a-data-driven-approach-to-environmental-social-and-governance.html"&gt;Databricks ESG solution accelerator explainer&lt;/a&gt; to learn more, or watch the &lt;a href="https://www.youtube.com/watch?v=tEjacxTxS_4"&gt;video&lt;/a&gt;. And you can check out the Databricks page in AWS Marketplace to get started with &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-ubybkhtmjqhhc"&gt;Databricks on AWS&lt;/a&gt;, and visit the &lt;a href="https://aws.amazon.com/marketplace/solutions/sustainability/?ref_=mp_nav_solution_sus"&gt;AWS Sustainability Solutions&lt;/a&gt; page for ready-to-deploy ESG solutions.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Through this blog, we have illustrated a streamlined method for summarizing complex documents into key ESG initiatives that offer a deeper comprehension of the sustainability aspects of your investments. With the implementation of machine learning methods powered by large language models (LLMs), we have introduced a unique approach to ESG that more effectively identifies the impact of global markets on both organizational strategy and reputational risk. Additionally, we have highlighted the significant economic influence of ESG factors on market risk calculation.&lt;/p&gt; 
&lt;p&gt;As a starting point to a data-driven ESG journey, this approach can be further improved by bringing the internal data you hold about your various investments and the additional metrics you could bring from third-party data, propagating the risks through the propagated-weighted ESG framework to keep driving more sustainable finance and impactful investments.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Improve the speed and cost of HPC deployment with Mountpoint for Amazon S3</title>
		<link>https://aws.amazon.com/blogs/hpc/improve-the-speed-and-cost-of-hpc-deployment-with-mountpoint-for-amazon-s3/</link>
		
		<dc:creator><![CDATA[Scott Ma]]></dc:creator>
		<pubDate>Tue, 27 Feb 2024 16:13:55 +0000</pubDate>
				<category><![CDATA[Amazon Simple Storage Service (S3)]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">42afa6fa28622939a4baad7a9810532f4a323104</guid>

					<description>Don't sacrifice performance OR ease of use with your HPC storage. Learn how Mountpoint for Amazon S3 combines high throughput and low latency with the simplicity of S3.</description>
										<content:encoded>&lt;p&gt;HPC workloads like genome sequencing and protein folding involve processing huge amounts of input data. Genome sequencing aims to determine an organism’s complete DNA sequence by analyzing extensive genome databases containing gene and genome reference sequences from thousands of species. Protein folding uses molecular dynamics simulations to model the physical movements of atoms and molecules in a protein.&lt;/p&gt; 
&lt;p&gt;These workloads require analyzing massive input datasets.&amp;nbsp;To support these kinds of applications that need high bandwidth, low latency, and parallel access to lots of data, AWS offers managed storage services like &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt; and &lt;a href="https://aws.amazon.com/efs"&gt;Amazon EFS &lt;/a&gt;that provide file systems optimized for compute-intensive workloads, which eliminates the need for customers to manage underlying storage infrastructure.&lt;/p&gt; 
&lt;p&gt;When selecting storage for your machine learning training data, &lt;a href="https://aws.amazon.com/s3/features/mountpoint/"&gt;Mountpoint for Amazon S3&lt;/a&gt; can provide a good alternative to support these types of workloads. Mountpoint for Amazon S3 is an open-source file client that you can use to &lt;em&gt;mount&lt;/em&gt; &lt;em&gt;an S3 bucket&lt;/em&gt; on your compute instances, accessing it as a local file system. It translates local file system API calls to REST API calls on S3 objects, and is optimized for high-throughput performance. It builds on the &lt;a href="https://docs.aws.amazon.com/sdkref/latest/guide/common-runtime.html"&gt;AWS Common Runtime&lt;/a&gt; (CRT) library, which is purpose-built for high-performance and low-resource usage to make efficient use of your fleet.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll deploy Mountpoint for Amazon S3 in an AWS ParallelCluster using the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;Community Recipe Library for HPC Infrastructure on AWS&lt;/a&gt;. We’ll then run the &lt;a href="https://ior.readthedocs.io/en/latest/index.html"&gt;IOR parallel I/O benchmark tool&lt;/a&gt;&amp;nbsp;to compare the performance of Mountpoint for Amazon S3 across the cluster, testing access speeds for reading files of varying sizes stored in Amazon S3.&lt;/p&gt; 
&lt;h2&gt;Setting expectations&lt;/h2&gt; 
&lt;p&gt;As shown in Figure 1, We should expect to see the parallel performance scaling well as we increase the number of nodes in the cluster accessing the shared Amazon S3 storage at the same time. We hope this shows you how to achieve high throughput access to virtually limitless Amazon S3 data using a simple approach running ParallelCluster.&lt;/p&gt; 
&lt;div id="attachment_3338" style="width: 1810px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3338" loading="lazy" class="size-full wp-image-3338" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/20/CleanShot-2024-02-20-at-13.25.38@2x.png" alt="Figure 1 - Read and write performance scales well with the number of nodes." width="1800" height="1058"&gt;
 &lt;p id="caption-attachment-3338" class="wp-caption-text"&gt;Figure 1 – Read and write performance scales well with the number of nodes.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;What do you need to try this out?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;AWS ParallelCluster Command line interface (CLI) with Node installed. See &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-pip.html"&gt;Installing AWS ParallelCluster in a non-virtual environment using pip&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;An existing bucket for the performance test.&lt;/li&gt; 
 &lt;li&gt;An existing Security Group, Subnet, EC2 Key&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Steps to deployment with ParallelCluster&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;For our deployment, we’ll download this ParallelCluster config file &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/aws-samples/aws-hpc-s3mountpoint/main/pcluster-example-config.yml"&gt;here&lt;/a&gt;&lt;/strong&gt;. This config file uses the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/mountpoint_s3"&gt;HPC Github Sample Recipe for Mountpoint for Amazon S3&lt;/a&gt; to help to deploy the mountpoint to the cluster nodes.&lt;/li&gt; 
 &lt;li&gt;Once you’ve downloaded the full ParallelCluster config file, update your own values in the config file,&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p style="padding-left: 40px"&gt;&lt;strong&gt;DEMO-BUCKET-NAME&lt;/strong&gt; – an existing bucket for testing. Create one if needed.&lt;br&gt; &lt;strong&gt;HOST-FILESYSTEM-PATH&lt;/strong&gt; – the path that the bucket will mount to inside the nodes. (e.g. &lt;code&gt;/testpath&lt;/code&gt;)&lt;br&gt; &lt;strong&gt;Your Subnet ID&lt;/strong&gt; – a subnet ID for deployment (e.g. &lt;code&gt;subnet-xxxxxxxx&lt;/code&gt;). Find the subnet Id in the &lt;a href="https://us-east-1.console.aws.amazon.com/vpcconsole/home?region=us-east-1#subnets:"&gt;Subnets home&lt;/a&gt;.&lt;br&gt; &lt;strong&gt;Your Security Group&lt;/strong&gt; – the id of the security group (e.g. &lt;code&gt;sg-xxxxxxxx&lt;/code&gt;). Find the security group id in the &lt;a href="https://us-east-1.console.aws.amazon.com/vpcconsole/home?region=us-east-1#SecurityGroups:"&gt;Security Groups home.&lt;/a&gt; Make sure the security group is for the same VPC the subnet is under.&lt;br&gt; &lt;strong&gt;Your ed25519 key&lt;/strong&gt; – the name of the EC2 Key Pair. You can find the name of your key in the &lt;a href="https://us-east-1.console.aws.amazon.com/ec2/home?region=us-east-1#KeyPairs:v=3;"&gt;Ec2 Key Pair home&lt;/a&gt;. If you do not have one, create a new one.&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Now, you can use this command to deploy a cluster:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster -c &amp;lt;template file&amp;gt; -r &amp;lt;region&amp;gt; -n &amp;lt;cluster_name&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deep-dive into the configuration&lt;/h2&gt; 
&lt;p&gt;To examine the config file further, it refers to three post-install scripts designed to work with &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/custom-bootstrap-actions-v3.html"&gt;ParallelCluster custom bootstrap actions&lt;/a&gt;. The first two scripts are from the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes"&gt;HPC recipes for AWS&lt;/a&gt;, whereas the last script was created specifically for this blog. These scripts are being run on the cluster head node &lt;em&gt;and&lt;/em&gt; all the compute nodes where the S3 bucket will be mounted. The script &lt;code&gt;s3-mp-install-ior.sh&lt;/code&gt; installs the IOR I/O performance benchmark suite and its necessary dependencies on the designated nodes. IOR is a parallel input/output benchmark that can test the performance of parallel storage systems using different interfaces and access patterns. The installation log, located at &lt;code&gt;/var/log/ior-install.log,&lt;/code&gt; provides debugging information about the installation process.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;install.sh&lt;/strong&gt; – installs Mountpoint for Amazon S3 and prepares the mount point directory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;mount.sh&lt;/strong&gt; – configures a systemd service that uses Mountpoint for Amazon S3 to mount a bucket to a directory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;s3-mp-install-ior.sh&lt;/strong&gt; – installs the IOR I/O performance benchmark suite. Since the performance testing is for the compute node to perform, this is optional for the head node.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Cluster head node&lt;/h2&gt; 
&lt;p&gt;Here’s an example of the Head Node section of the ParallelCluster configuration. The bucket &lt;code&gt;DEMO-BUCKET-NAME&lt;/code&gt;&amp;nbsp;is mounted to the location &lt;code&gt;HOST-FILESYSTEM-PATH&lt;/code&gt;&amp;nbsp;on the host&lt;strong&gt;.&amp;nbsp;&lt;/strong&gt;&lt;code&gt;HOST-FILESYSTEM-PATH&lt;/code&gt; is an absolute path like &lt;code&gt;/s3mountpoint&lt;/code&gt;&lt;strong&gt;.&lt;/strong&gt;&amp;nbsp;You can mount multiple buckets on a single host.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;HeadNode:
  CustomActions:
    OnNodeConfigured:
      Sequence:
        - Script: https://aws-hpc-recipes.s3.us-east-1.amazonaws.com/main/recipes/storage/mountpoint_s3/assets/install.sh
        - Script: https://aws-hpc-recipes.s3.us-east-1.amazonaws.com/main/recipes/storage/mountpoint_s3/assets/mount.sh
          Args:
            - &amp;lt;&amp;lt;DEMO-BUCKET-NAME&amp;gt;&amp;gt;
            - &amp;lt;&amp;lt;HOST-FILESYSTEM-PATH&amp;gt;&amp;gt;
            - '--allow-delete --allow-root'
  Iam:
    AdditionalIamPolicies:
      - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
    S3Access:
      - BucketName: &amp;lt;&amp;lt;DEMO-BUCKET-NAME&amp;gt;&amp;gt;
        EnableWriteAccess: true 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We’ll be using &lt;code&gt;--allow-delete --allow-root&lt;/code&gt; to enable read/write to the bucket because read/write access is required by the performance testing tool. If you’re using this mountpoint for a &lt;strong&gt;read only&lt;/strong&gt; scenario like storing training data, it’s recommended that you use &lt;code&gt;--read-only&lt;/code&gt; to prevent accidental overwriting to the training data set. For more options to configure the mount point, see the &lt;a href="https://github.com/awslabs/mountpoint-s3/blob/main/doc/CONFIGURATION.md"&gt;Mountpoint configuration documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To access the S3 bucket from the header node, you need to enable S3 access through either the S3Access configuration under &lt;strong&gt;HeadNode/Iam/S3Access&lt;/strong&gt; or by attaching an additional IAM policy under &lt;strong&gt;HeadNode/Iam/AdditionalIamPolicies&lt;/strong&gt;. You can use a managed policy such as &lt;code&gt;AmazonS3ReadOnlyAccess&lt;/code&gt; to provide generic read-only access, or you can create a custom policy with more specific permissions tailored to your use case. The key requirement is that the &lt;code&gt;EC2 InstanceRole&lt;/code&gt; for the header node must have permissions to access the S3 bucket in order for it to read data from or write data to that location. The same will apply to the &lt;code&gt;ComputeNode&lt;/code&gt; section.&lt;/p&gt; 
&lt;h3&gt;Compute nodes&lt;/h3&gt; 
&lt;p&gt;Next, let’s look at an example of mounting the same bucket &lt;code&gt;DEMO-BUCKET-NAME&lt;/code&gt;&amp;nbsp;to a local path of&amp;nbsp;&lt;code&gt;HOST-FILESYSTEM-PATH&lt;/code&gt; in the Compute Nodes section of the ParallelCluster config file.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Scheduling:
    SlurmQueues:
        - Name: demo
          CustomActions:
            OnNodeConfigured:
                Sequence:
                  - Script: https://aws-hpc-recipes.s3.us-east-1.amazonaws.com/main/recipes/storage/mountpoint_s3/assets/install.sh
                  - Script: https://aws-hpc-recipes.s3.us-east-1.amazonaws.com/main/recipes/storage/mountpoint_s3/assets/mount.sh
                    Args:
                      - &amp;lt;&amp;lt;DEMO-BUCKET-NAME&amp;gt;&amp;gt;
                      - &amp;lt;&amp;lt;HOST-FILESYSTEM-PATH&amp;gt;&amp;gt;
                      - '--allow-delete --allow-root'
                  - Script: https://raw.githubusercontent.com/aws-samples/aws-hpc-s3mountpoint/main/s3-mp-install-ior.sh
Iam:
        AdditionalIamPolicies:
          - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
        S3Access:
          - BucketName: &amp;lt;&amp;lt;DEMO-BUCKET-NAME&amp;gt;&amp;gt;
            EnableWriteAccess: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Steps to run Performance Testing&lt;/h2&gt; 
&lt;p&gt;To execute the IOR benchmark across compute nodes, we’ll use &lt;code&gt;sbatch&lt;/code&gt; to submit a job to the HPC cluster. We’ll &lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-sessions-start.html#start-ec2-console"&gt;SSH into the head node&lt;/a&gt; using Session Manager to author an &lt;code&gt;sbatch&lt;/code&gt; script that calls &lt;code&gt;mpirun&lt;/code&gt; to launch the IOR executable with our desired parameters as below. The &lt;code&gt;sbatch&lt;/code&gt; script will specify the number of tasks, and Slurm will manage the number of nodes required.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;## create the sbatch submission script
cd ~
cat &amp;gt; ior_submission.sbatch &amp;lt;&amp;lt; EOF
#!/bin/bash
#SBATCH --job-name=ior-perf-test
#SBATCH --output=%N_%x_%j_%t.out
#SBATCH --ntasks-per-node=8
#SBATCH --ntasks=8

module load intelmpi
mpirun bash -c " 
     cd &amp;lt;&amp;lt;HOST-FILESYSTEM-PATH&amp;gt;&amp;gt;
     ior -r -w -v -F -o=S@S@S -b=2000m -i=1 -t=50m -a=POSIX --posix.odirect"
EOF

## submit the script
cd ~
sbatch ior_submission.sbatch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Slurm will spawn the MPI processes across the cluster worker nodes to perform the parallel I/O tests. The output file will be available on the first compute node.&lt;/p&gt; 
&lt;h2&gt;Performance results&lt;/h2&gt; 
&lt;p&gt;Using this setup, we can use IOR to do some load tests across all our cluster compute nodes. It’s worth noting that the actual mounting operation itself takes just a few seconds&lt;/p&gt; 
&lt;p&gt;In our test runs, we used a single Amazon S3 bucket and attained exceptional read and write speeds – 4.23 GB/s (4.038 GiB/s) write throughput and 4.85 GB/s (4.622 GiB/s) read throughput per node (using c6i.16xlarge). This high read performance makes Mountpoint extremely well-suited for supporting the intensive read operations inherent in HPC workloads. Here’s some sample output from our own tests running on c6i.16xlarge instances:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-bash"&gt;Loading intelmpi version 2021.9.0
IOR-4.1.0+dev: MPI Coordinated Test of Parallel I/O
Began               : Thu Dec 14 00:13:39 2023
Command line        : /shared/ior/bin/ior -r -w -v -F -o=S@S@S -b=2000m -i=1 -t=50m -a=POSIX --posix.odirect
Machine             : Linux queue1-st-pclustercr1-1
TestID              : 0
StartTime           : Thu Dec 14 00:13:39 2023
Path                : S.00000000
FS                  : 0.0 GiB   Used FS: -nan%   Inodes: 0.0 Mi   Used Inodes: -nan%
Participating tasks : 64

Options:
api                 : POSIX
apiVersion          :
test filename       : S@S@S
access              : file-per-process
type                : independent
segments            : 1
ordering in a file  : sequential
ordering inter file : no tasks offsets
nodes               : 1
tasks               : 64
clients per node    : 64
memoryBuffer        : CPU
dataAccess          : CPU
GPUDirect           : 0
repetitions         : 1
xfersize            : 50 MiB
blocksize           : 1.95 GiB
aggregate filesize  : 125 GiB
verbose             : 1

Results:

access    bw(MiB/s)  IOPS       Latency(s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ----       ----------  ---------- ---------  --------   --------   --------   --------   ----
Commencing write performance test: Thu Dec 14 00:13:55 2023
write     4038       83.06      0.039900    2048000    51200      28.34      30.82      30.06      31.70      0
Commencing read performance test: Thu Dec 14 00:14:18 2023

read      4622       92.53      0.682358    2048000    51200      0.086130   27.67      17.48      27.69      0
remove    -          -          -           -          -          -          -          -          7.57       0
Max Write: 4037.71 MiB/sec (4233.85 MB/sec)
Max Read:  4622.06 MiB/sec (4846.58 MB/sec)

Summary of all tests:
Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev   Max(OPs)   Min(OPs)  Mean(OPs)     StdDev    Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt   blksiz    xsize aggs(MiB)   API RefNum
write        4037.71    4037.71    4037.71       0.00      80.75      80.75      80.75       0.00   31.70114         NA            NA     0     64  64    1   1     0        1         0    0      1 2097152000 52428800  128000.0 POSIX      0
read         4622.06    4622.06    4622.06       0.00      92.44      92.44      92.44       0.00   27.69330         NA            NA     0     64  64    1   1     0        1         0    0      1 2097152000 52428800  128000.0 POSIX      0
Finished            : Thu Dec 14 00:14:53 2023&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;IOR’s test is designed to scale and maintain consistent throughput &lt;em&gt;per server&lt;/em&gt; as we add more compute nodes. You also have the option to modify the performance test settings like &lt;code&gt;blockSize&lt;/code&gt; (-b) and &lt;code&gt;transferSize&lt;/code&gt; (-t) to see how the throughput changes when you adjust those values.&lt;/p&gt; 
&lt;p&gt;For our performance runs, we used the options listed in table 1. As you prepare your dataset for testing, take time to optimize the IOR settings. Tailor them to your specific data characteristics for improved accuracy. For instance, adjust the blockSize downward if holding many smaller files. Or remove the -f option if processes commonly read the same files. Taking these steps allows IOR to better simulate your real-world environment. For additional options, refer to the&lt;a href="https://ior.readthedocs.io/en/latest/userDoc/options.html"&gt; IOR official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_3339" style="width: 1846px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3339" loading="lazy" class="size-full wp-image-3339" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/20/CleanShot-2024-02-20-at-13.37.27@2x.png" alt="Table 1 - IOR Options used for our performance test runs." width="1836" height="574"&gt;
 &lt;p id="caption-attachment-3339" class="wp-caption-text"&gt;Table 1 – IOR Options used for our performance test runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;With data stored in Amazon S3 behind the scenes, Mountpoint for Amazon S3 delivers the durability, scalability, and high throughput needed to support demanding machine learning training workloads. It also provides a cost-efficient storage solution.&lt;/p&gt; 
&lt;p&gt;Mountpoint for Amazon S3 is easy to integrate with other AWS ParallelCluster, and this extends to AWS Batch, and self-managed EC2 instances, too – all popular methods for distributed training workflows. The quick, scalable, and economical nature of Amazon S3 behind Mountpoint removes traditional data storage challenges customers often face when pursuing ML training.&lt;/p&gt; 
&lt;p&gt;You can get started with Mountpoint for S3 today by building on the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/mountpoint_s3"&gt;sample we’ve provided in the HPC Recipe Library&lt;/a&gt;. We highly encourage you to closely examine how the install/configure scripts operate and other recipes in the recipe library. These recipes are designed for cross-platform compatibility and robustness working with ParallelCluster, requiring little to no modification to use.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Accelerating agent-based simulation for autonomous driving</title>
		<link>https://aws.amazon.com/blogs/hpc/accelerating-agent-based-simulation-for-autonomous-driving-with-hpc/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Mon, 26 Feb 2024 11:02:29 +0000</pubDate>
				<category><![CDATA[Automotive]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Industries]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">b5625f70c17b5e2ef046c36732be097d809e0318</guid>

					<description>AWS is powering the future of self-driving cars. Check out this post to see how high performance computing is transforming agent-based models for the CARLA RAI Challenge.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3356" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/23/boofla88_a_robot_wearing_a_hat_and_driving_a_car_on_a_sunny_day_6f8f05d1-fe46-48fc-9828-a49a1cf22794.png" alt="" width="380" height="212"&gt;Driving innovation has always been at the core of AWS and one of the realms in which this innovation is most palpable is in the burgeoning industry of autonomous vehicles. By accelerating agent-based models (ABM) simulations with high performance computing, AWS is at the forefront of this technological revolution, and nowhere is this more evident than in their support of the CARLA RAI Challenge and their commitment to Responsible AI (RAI).&lt;/p&gt; 
&lt;p&gt;In this post, we’ll tell you about the challenge itself, the tools we’ve leveraged to set it up, and show you how you can participate.&lt;/p&gt; 
&lt;h2&gt;The CARLA RAI challenge and autonomous driving&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://carla-rai-challenge.github.io/"&gt;CARLA Responsible AI Challenge&lt;/a&gt; is an initiative led by the Oxford Robotics Institute, building upon the previous &lt;a href="https://leaderboard.carla.org/challenge/"&gt;CARLA Challenges&lt;/a&gt; by extending the Leaderboard with additional RAI related metrics while leveraging the CARLA open-source autonomous driving simulator.&lt;/p&gt; 
&lt;p&gt;We’re hoping to stimulate creative minds to build capable artificial intelligence (AI) agents with the ability to navigate diverse and complex driving scenarios. The challenge has a strong emphasis on the principles of Responsible AI to promote safety in AI systems through rigorous assessment of models under &lt;em&gt;robustness&lt;/em&gt;, &lt;em&gt;environmental sustainability&lt;/em&gt;, and &lt;em&gt;transparency&lt;/em&gt; lenses.&lt;/p&gt; 
&lt;p&gt;The role of AI in autonomous driving has never been more crucial, and the CARLA RAI Challenge serves to highlight this fact. Participants are encouraged to push the limits of technology and create AI agents that can navigate in all possible driving conditions – not just ideal ones. From bustling city traffic to desolate country roads, from bright sunny days to foggy nights – the AI agent should be prepared to handle it all.&lt;/p&gt; 
&lt;div id="attachment_3349" style="width: 909px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3349" loading="lazy" class="size-full wp-image-3349" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/23/CleanShot-2024-02-23-at-13.16.47.png" alt="Fig 1: The main goal of the challenge remains the assessment of the driving proficiency of autonomous agents in realistic traffic situations, as defined in the leaderboard mechanics. Teams will have to complete 10 routes in 2 weather conditions through 5 repetitions." width="899" height="382"&gt;
 &lt;p id="caption-attachment-3349" class="wp-caption-text"&gt;Figure 1: The main goal of the challenge remains the assessment of the driving proficiency of autonomous agents in realistic traffic situations, as defined in the &lt;a href="https://leaderboard.carla.org/#task"&gt;leaderboard mechanics&lt;/a&gt;. Teams will have to complete 10 routes in 2 weather conditions through 5 repetitions. Source: CARLA Challenge Simulator.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The competition goes beyond just AI innovation. By challenging participants to ensure their AI systems can adapt to varying traffic, weather, and hardware conditions, the competition ensures driving models’ robustness.&lt;/p&gt; 
&lt;p&gt;But &lt;em&gt;Responsible AI&lt;/em&gt; is important, too. Participants are encouraged to develop transparent or explainable models to facilitate accountability. This allows for better understanding, and therefore, improvements in AI behavior.&lt;/p&gt; 
&lt;p&gt;Finally, the challenge promotes environmental sustainability by encouraging model development activities that cut down on carbon dioxide (CO2) emissions.&lt;/p&gt; 
&lt;p&gt;The timeline for the challenge is as follows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The challenge will be opened on April 10th, 2024.&lt;/li&gt; 
 &lt;li&gt;The closing date for the challenge is July 9th, 2024.&lt;/li&gt; 
 &lt;li&gt;Teams are required to submit their video presentation by July 21st, 2024.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Role of AWS in agent-based simulations&lt;/h2&gt; 
&lt;p&gt;Using AWS for agent-based simulations helps to provide a robust and scalable environment for testing – and refining – the AI agents participating in the challenge. AWS offers expansive computational resources via its cloud-based infrastructure.&lt;/p&gt; 
&lt;p&gt;AWS operates at a huge scale, making it possible to run the resource-intensive simulations needed for testing the AI agents. Each simulation is a high-fidelity virtual representation of various driving scenarios, involving many variables and needing a lot of processing power. These simulations are not just one-off events, but need to be run repeatedly as the AI agents are continuously tweaked and improved.&lt;/p&gt; 
&lt;p&gt;But it’s not just about resources: collaboration is critical for innovation. The cloud provides an environment where researchers, developers, and AI enthusiasts can come together to share ideas, learn from each other, and drive forward the development of autonomous driving technology.&lt;/p&gt; 
&lt;p&gt;This means that AWS is a collaborator and enabler in the world of autonomous driving, making significant contributions to the advancement of agent-based simulations and, by extension, the entire field of autonomous vehicles.&lt;/p&gt; 
&lt;h2&gt;The power of agent-based models in autonomous driving&lt;/h2&gt; 
&lt;p&gt;Agent-based models (ABM) are changing the world of autonomous driving. Their power lies in their ability to replicate real-world driving conditions at a local level, with remarkable accuracy and detail – the perfect environment for testing AI agents.&lt;/p&gt; 
&lt;p&gt;An agent-based model is a complex system where each entity (agent), operates independently according to a set of rules. In the context of autonomous driving, these agents might represent other vehicles, pedestrians, or even elements of the environment like traffic lights and road markings. By simulating the actions and interactions of these agents, ABMs &amp;nbsp;can recreate a wide range of driving scenarios – from rush hour in a busy city to a quiet country road at night.&lt;/p&gt; 
&lt;p&gt;Agent-based models are also flexible. We can easily modify them to include new types of agents, or adjust the behavior of existing ones, allowing researchers to simulate virtually any driving scenario imaginable. This adaptability is key for the development of AI systems trying to replicate what they might encounter on real roads.&lt;/p&gt; 
&lt;p&gt;But the true power of ABMs is that they can learn and evolve. As the agents navigate the virtual environment, they learn. Each run provides data that can be used to fine-tune the agent and optimize its algorithm. This iterative process of learning and adaptation is what makes agent-based models such a powerful tool for autonomous driving research.&lt;/p&gt; 
&lt;h2&gt;How the CARLA RAI Challenge runs on AWS&lt;/h2&gt; 
&lt;p&gt;Running multiple simulations simultaneously drastically reduces the time spent testing and refining the agents, clearly a boon for developers, who can iterate faster. That means we need to have a scalable mechanism to grow the resources when we need to do a lot of work.&lt;/p&gt; 
&lt;p&gt;Once a developer creates and containerizes their agents, they can deploy them on AWS. In Figure 2, you can tell that the first step involves using the EvalAI website to submit Docker containers to the Amazon Elastic Container Registry (ECR), makes it easy to store, share, and deploy container images.&lt;/p&gt; 
&lt;p&gt;In step 2, the user submits the simulation configuration data, which will be forwarded by EvalAI into an Amazon Simple Queue Service (Amazon SQS) queue, which then triggers an Amazon EventBridge event (step 3) that connects applications with data from diverse sources.&lt;/p&gt; 
&lt;p&gt;The event initiated by SQS leads to the execution of an AWS Step Functions pipeline (step 4), which carries out a series of tasks based on the established deployment graph. The Step Functions then execute step 5, where a Lambda function is used to stash user information into Amazon DynamoDB, which is a NoSQL database that offers excellent scalability.&lt;/p&gt; 
&lt;p&gt;Step 6 involves the actual simulation itself, which is done using the Amazon Elastic Kubernetes Service (Amazon EKS). EKS facilitates the scaling of containerized applications across a cluster. This elasticity allows for the automatic shutdown of all unused resources, too. All the operational logs and simulation results are logged into Amazon CloudWatch for analysis and diagnostics later.&lt;/p&gt; 
&lt;p&gt;Finally – at step 7 – another Lambda function stores the simulation results in an Amazon Simple Storage Service bucket (Amazon S3). This is an excellent solution for storing a lot of files. The last step also returns the results using a REST API to the user through the EvalAI web interface.&lt;/p&gt; 
&lt;div id="attachment_3350" style="width: 781px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3350" loading="lazy" class="size-full wp-image-3350" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/23/CleanShot-2024-02-23-at-13.18.08.png" alt="Figure 1 – The elastic cloud architecture for the CARLA RAI Challenge. Each stage is designed to scale, which means multiple simulations can run simultaneously. This drastically reduces the time spent on testing and refining the agents." width="771" height="390"&gt;
 &lt;p id="caption-attachment-3350" class="wp-caption-text"&gt;Figure 2 – The elastic cloud architecture powering the CARLA RAI Challenge. This architecture was introduced in the previous CARLA Challenge. Each stage is designed to scale, which means multiple simulations can run simultaneously. This drastically reduces the time spent on testing and refining the agents.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Challenge breakdown&lt;/h2&gt; 
&lt;p&gt;Here’s a breakdown of the challenge.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Getting Started with CARLA&lt;/strong&gt;: Participants need to &lt;a href="https://carla-rai-challenge.github.io/create/"&gt;download&lt;/a&gt; and set up a specific version of CARLA (0.9.10.1) on their computers. This version is necessary because it matches the environment used in the online servers where the agents are assessed.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configuration and Setup&lt;/strong&gt;: After installing CARLA, participants must adjust some configurations to link additional components specific for the challenge. This include the &lt;em&gt;Leaderboard&lt;/em&gt; system the &lt;em&gt;Scenario Runner&lt;/em&gt;, and &lt;em&gt;Routes:&lt;/em&gt; 
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Leaderboard&lt;/strong&gt;: the control center for the challenge. It runs the autonomous agent created by participants through a series of tests across different routes and traffic conditions. It evaluates how well the agent performs in each scenario.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Scenarios&lt;/strong&gt;: These are predefined traffic situations that the autonomous agent must navigate through successfully. There are ten types of scenarios, each with different parameters. These simulate real-world traffic situations in the virtual towns available in CARLA.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Routes&lt;/strong&gt;: These are paths from one point to another that the agent must follow. Routes have start and end points and may include specific weather conditions they have to simulate. Participants can train their agents using the provided routes, but the routes used for final evaluation in the challenge are a secret.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing the Agent&lt;/strong&gt;: Before submitting, we encourage participants to test their agents using a basic setup. This test them manually control an agent through a simulation, to get clear understanding of what the agent will face during the challenge.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Final Submission and Evaluation&lt;/strong&gt;: Once they complete the program, we’ll ask participants to prepare their autonomous agent as a Docker image, which will undergo rigorous testing by the Leaderboard on undisclosed routes and scenarios. It’s crucial that the agent is able to successfully navigate these routes while following traffic laws and handling a variety of traffic situations. We’ll evaluate the performance of the agent in these tasks, focusing on how the agents cope in degraded driving and traffic conditions. Special metrics in the challenge include things like robustness against harsh environmental situations, data drift, sensor failures, and environmental impacts of running the agents.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;What’s next? AWS and autonomous driving&lt;/h2&gt; 
&lt;p&gt;As the landscape of autonomous driving continues to advance, we see AWS continuing to advance. Their ongoing dedication to delivering robust, high performance computing solutions, and their support for forward-thinking initiatives like the CARLA RAI Challenge is a promising combination.&lt;/p&gt; 
&lt;p&gt;That’s because rapid progress in autonomous driving demands powerful, scalable, and reliable computational resources – definitely the cloud’s wheelhouse.&lt;/p&gt; 
&lt;p&gt;Beyond this, the contribution that AWS makes to promoting a culture of collaboration and innovation within the AI and autonomous driving communities is an important driver. The cloud is important for researchers, developers, and AI enthusiasts to exchange ideas and learn from each other. And all of this collectively pushes the boundaries of what’s possible.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>How agent-based models powered by HPC are enabling large scale economic simulations</title>
		<link>https://aws.amazon.com/blogs/hpc/how-agent-based-models-powered-by-hpc-are-enabling-large-scale-economic-simulations/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Tue, 20 Feb 2024 12:02:54 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Thought Leadership]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">cc9fc285c87dc520b888f3312bbdbf1f55224dd6</guid>

					<description>See how agent-based models, driven to scale by HPC in the cloud, are shedding new light on macroprudential policies with this post from Oxford's Institute for New Economic Thinking.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright wp-image-3306 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/12/boofla88_conways_game_of_life_3d_photo_realistic_06f8a4e2-75eb-4a1e-adba-4be1f3ce34b3.png" alt="How agent-based models powered by HPC are enabling large scale economic simulations" width="380" height="212"&gt;This post was contributed by J. Doyne&amp;nbsp;Farmer, Institute of New Economic Thinking, Oxford University, Jagoda&amp;nbsp;Kaszowska-Mojsa, Oxford University &amp;amp; Institute of Economics, Polish Academy of Sciences, Sam&amp;nbsp;Bydlon, Senior Solutions Architect and Ilan&amp;nbsp;Gleiser, Principal Machine Learning Specialist, WWSO Emerging Technologies, AWS&lt;br&gt; &lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Economists and policy makers have maintained a sustained interest in understanding the effects of macroprudential economic policies. Recently, a novel approach using agent-based models has emerged, which provides insights into the complexity of these policies.&lt;/p&gt; 
&lt;p&gt;Agent-based models (ABM) are a type of computer simulation that has proven instrumental in shedding light on how different types of economic agents interact in a heterogeneous environment.&lt;/p&gt; 
&lt;p&gt;In this post, we will explore the use of agent-based models and high performance computing on AWS used by researchers at the University of Oxford’s Institute for New Economic Thinking (INET) to enhance our comprehension of macroprudential policies and their potential implications on economic systems.&lt;/p&gt; 
&lt;h2&gt;Defining macroprudential policy&lt;/h2&gt; 
&lt;p&gt;Public policies play a pivotal role as instruments used by governments and central banks to maintain financial stability and promote sustainable economic growth. In recent years, there has been a growing focus on a significant category of policies known as &lt;em&gt;macroprudential policies&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;At its core, macroprudential policy is centered around implementing measures that are designed to mitigate the risks and vulnerabilities that are inherent in a financial system. It focuses on the stability of a financial system as a whole, rather than addressing the concerns of individual institutions. Its main objective is to prevent the build-up of systemic risks, such as excessive credit growth, asset price bubbles, and excessive leverage. These risks have the potential to precipitate financial crises and macroprudential policy is oriented towards preventing such destabilizing events.&lt;/p&gt; 
&lt;p&gt;Traditionally, macroprudential policies have been put into practice through a range of instruments, such as capital adequacy requirements, loan-to-value ratio restrictions and countercyclical buffers. These policy tools are designed to mitigate risks by affecting the behavior of financial institutions, and to influence the decisions made by households and businesses.&lt;/p&gt; 
&lt;p&gt;The most used economic models have limitations when it comes to capturing the complexities of a financial system and the interactions among the various economic agents. This is where agent-based modeling steps in as a powerful tool. ABM is a simulation approach that enables economists to model the behavior of individual agents, such as banks, households, and firms and facilitates an examination of their interactions.&lt;/p&gt; 
&lt;p&gt;Using ABM, economists can simulate how different macroprudential policies impact the behavior of agents and the overall stability of a financial system. For example, they can investigate the effectiveness of different capital requirements on mitigating systemic risks or explore the consequences of changing loan-to-value ratios on housing markets.&lt;/p&gt; 
&lt;p&gt;An interesting case study that highlights the potential of ABM in understanding macroprudential policy is the work by Dr. Jagoda Kaszowska-Mojsa from the University of Oxford (INET) &amp;amp; the Institute of Economics, Polish Academy of Sciences. In her article titled “&lt;em&gt;Macroprudential Policy in a Heterogeneous Environment: An Application of Agent-Based Approach in Systemic Risk Modelling&lt;sup&gt;1&lt;/sup&gt;&lt;/em&gt;”, she used ABM to study the impact of different policy measures on the stability of a banking system. Through simulations, she showed how the heterogeneity of agents, and their interactions can affect the effectiveness of policy interventions.&lt;/p&gt; 
&lt;p&gt;Using ABM to analyze macroprudential policies offers several advantages. Firstly, it provides a more realistic representation of a financial system and economy by capturing the diversity of economic agents and their decision-making processes. Secondly, ABM enables us to analyze nonlinear and dynamic interactions, which are often difficult to capture in traditional models. Lastly, ABM can provide insights into the unintended consequences of policy interventions and help policy makers design more robust and effective measures.&lt;/p&gt; 
&lt;p&gt;Looking ahead, the use of ABM in understanding macroprudential policies holds significant potential.&lt;/p&gt; 
&lt;p style="text-align: left"&gt;&lt;em&gt;“Agent based models are about to be the next technology revolution. In economics, we have shown&lt;/em&gt;&lt;em&gt;&lt;sup&gt;2&lt;/sup&gt; how agent-based models can make better real-time (before the fact) predictions than standard models. This is just the tip of a large iceberg …”&lt;/em&gt;&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;em&gt;Prof. J. Doyne Farmer, INET (Institute of New Economic Thinking), Oxford University&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Limitations of economic models&lt;/h2&gt; 
&lt;p&gt;The widely embraced economic models such as the Dynamic Stochastic General Equilibrium (DSGE) framework have also been used to assess the effects of macroprudential policies. However, these models still rely on the &lt;em&gt;representative agent&lt;/em&gt; assumption, which assumes that all individuals in the economy behave in the same way. This assumption limits their ability to capture the diversity and complexity of economic agents in the real world.&amp;nbsp;Efforts to integrate the heterogeneity of agents are currently in the early stages in most of the DSGE models, which primarily rely on stylized solutions. A case in point is the DSGE-3D model, which was developed and is used by the European Systemic Risk Board and the European Central Bank and has three layers of default (3D) that are aimed at scrutinizing the impact of macroprudential policies on the stability of financial systems&lt;sup&gt;3&lt;/sup&gt;.&lt;/p&gt; 
&lt;p&gt;This is where Agent-Based Modelling (ABM) takes center stage. A large-scale, data-driven ABM presents a more realistic framework for comprehending the consequences of macroprudential policies. It empowers researchers to construct an environment that embraces the inherent diversity of economic agents by encompassing households, businesses, and financial institutions. By simulating the behaviors of these diverse agents, ABM can provide valuable insights into the intricate dynamics and interactions within a financial system and the broader economy.&lt;/p&gt; 
&lt;p&gt;Furthermore, agent-based modelling offers the ability to conduct counterfactual simulations, which is a valuable tool for assessing the impact of macroprudential policies on both a financial system and the broader economy. For instance, researchers can use ABM to simulate the consequences of various policy instruments, such as changes in capital requirements or loan-to-value ratios, on the stability of a financial system. This approach furnishes a more holistic comprehension of the potential impacts of these policies than is typically achievable within the conventional economic frameworks.&lt;/p&gt; 
&lt;p&gt;Furthermore, ABM can shed light on the stabilizing effects of macroprudential policies during economic or financial distress. For instance, the Basel III regulatory framework, which was adopted in 2010-2011, imposes stricter capital requirements for financial institutions that are designed to enhance their resilience. ABM helps to assess the effectiveness of such policies in mitigating systemic risks and preventing financial crises.&lt;/p&gt; 
&lt;p&gt;In summary, most of the economic models have limitations when it comes to capturing the diversity and complexity of economic agents. ABM offers a more realistic and comprehensive framework for understanding the impact of macroprudential policies, thereby enabling counterfactual simulations – and providing insights into the stability of a financial system.&lt;/p&gt; 
&lt;h2&gt;An overview of agent-based modelling for simulating economics&lt;/h2&gt; 
&lt;p&gt;Agent-based modelling (ABM) is an innovative approach that has gained increasing popularity in economics and policy research. Unlike most economic models that assume rational and homogeneous agents, ABM provides a more realistic representation of the complexity and diversity of economic agents in a system. In this section, we will provide an overview of ABM and its key features.&lt;/p&gt; 
&lt;p&gt;At its core, ABM is a simulation-based modelling approach that focuses on individual agents and their interactions within a system. These agents can represent a variety of economic actors, such as households, firms, and financial institutions. Each agent has its own set of characteristics, preferences, and decision-making processes, which are influenced by both internal and external factors.&lt;/p&gt; 
&lt;p&gt;The main advantage of ABM is its ability to capture emergent properties and complex interactions that are often overlooked by standard models. Rather than assuming linear relationships, ABM enables the modelling of nonlinear dynamics, feedback loops, and the amplification of shocks within a system. This enables economists to explore how small changes in individual behaviors can have significant implications for the overall stability and functioning of a system.&lt;/p&gt; 
&lt;p&gt;ABM also provides the versatility to accommodate varying degrees of agent heterogeneity. This implies that the agents within the model can possess diverse levels of knowledge, information, and decision-making rules. To illustrate, within a banking system, certain banks may exhibit a greater degree of risk aversion, while others may have a higher risk appetite. By capturing these nuanced distinctions, ABM enables researchers to investigate how different types of agents react to policy interventions and how their interactions shape the dynamics of a system.&lt;/p&gt; 
&lt;p&gt;To implement ABM, researchers typically use computer simulations. Simulating systems with the scale and complexity of the real world, however, requires a great deal of computing power. High performance computing services like those offered by Amazon Web Services (AWS) Advanced Computing Team offer the necessary power to run, calibrate, test, and validate complex ABM simulations in a cost-efficient manner.&lt;/p&gt; 
&lt;p&gt;Case study: the Polish economy: applying an ABM to macroprudential policy&lt;/p&gt; 
&lt;p&gt;Organizations like the Institute for New Economic Thinking (INET) have been at the forefront of promoting ABM and supporting research in this area. They provide resources, funding, and a platform for economists to share their findings and collaborate. This support has been instrumental in advancing the use of ABM for analyzing policies and has fostered innovation in the field.&lt;/p&gt; 
&lt;p&gt;One notable case study that showcases the application of ABM in this context is the study by Dr. Jagoda Kaszowska-Mojsa from the University of Oxford (INET) &amp;amp; the Institute of Economics, Polish Academy of Sciences. Recently, Dr. Kaszowska-Mojsa teamed up with the AWS Global Impact Compute team to scale ABMs that explore how the implementation of new macroprudential policies can impact financial stability while minimizing their contribution to societal inequality.&lt;/p&gt; 
&lt;p&gt;In this project (‘&lt;em&gt;MACROPRU&lt;/em&gt;’), Dr. Kaszowska-Mojsa used state-of-the-art, large-scale, data-driven agent-based simulation techniques to uncover the redistributive consequences of macroprudential policies and evaluate the most advantageous combination of these policies. The findings from this project complemented the insights from the European Central Bank’s (ECB) system-wide stress-testing exercises by providing valuable data on the rise of inequality in European Union (EU) countries because of the adoption of new financial regulations. (That project secured funding from the European Union’s Horizon 2020 Research and Innovation Programme through a Marie Skłodowska-Curie grant (no 101023445)).&lt;/p&gt; 
&lt;p&gt;The MACROPRU model&lt;sup&gt;5 &lt;/sup&gt;(outlined in Figure 1) is a distinctive economic framework that differentiates between heterogeneous economic agents: individuals that form households (consumers), versatile firms that operate across sectors, and banks within a financial sector.&lt;/p&gt; 
&lt;div id="attachment_3307" style="width: 843px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3307" loading="lazy" class="size-full wp-image-3307" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/12/CleanShot-2024-02-12-at-10.03.47.png" alt="Figure 1. The relationship between the agents in the MACROPRU model. " width="833" height="597"&gt;
 &lt;p id="caption-attachment-3307" class="wp-caption-text"&gt;Figure 1. The relationship between the agents in the MACROPRU model.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;This model stands apart from conventional economic approaches such as the Dynamic Stochastic General Equilibrium (DSGE) model due to its emphasis on the unique characteristics of the economic entities, which are derived from empirical data.&lt;/p&gt; 
&lt;p&gt;In the case of households, we calibrated their attributes and behaviors using data from the Eurosystem Household Finance and Consumption Survey (HFCS), from the European Central Bank (ECB). This data provides essential insights into how individuals and households function as consumers.&lt;/p&gt; 
&lt;p&gt;Although the model focuses on Poland as a small open economy, we could use it to simulate the economic scenarios in 22 different European countries where data is available (Eurozone, Poland, Hungary). A key aspect of the model is its ability to capture both the statistical equilibrium &lt;em&gt;and&lt;/em&gt; disequilibrium states, thus shedding light on the internal forces that drive the economic and financial cycles.&lt;/p&gt; 
&lt;p&gt;The model is made up from 58 modules, each governing the interactions and behaviors of the agents. These modules consider various factors, such as production, the procurement of goods, the sources of finance, demographic dynamics, investment decisions, and intergenerational wealth transfers.&lt;/p&gt; 
&lt;p&gt;Additionally, the model is capable of simulating changes in employment scenarios that result from macroeconomic shifts and transformations in different sectors. Such transitions occur in a probabilistic manner, effectively representing the intricate nature of occupational shifts in interconnected networks.&lt;/p&gt; 
&lt;p&gt;The model also analyses the impact of macroprudential policies on income and wealth inequalities. It departs from the conventional money multiplier approach by recognizing banks as creators of money.&lt;/p&gt; 
&lt;p&gt;The use of Agent-Based Modelling (ABM) in this study offers valuable insights into how different policy measures could affect the stability of a financial system and individual agents, including their income and wealth. Policy makers can use this approach to assess the effectiveness of macroprudential policies and to make informed decisions to promote financial stability and sustainable economic growth. ABM emphasizes the importance of considering the heterogeneity of agents and their interactions when designing and implementing policies. By incorporating individual behaviors and feedback loops into the model, policy makers gain a more accurate understanding of how different policy measures could affect both the stability of a financial system and the well-being of individual agents.&lt;/p&gt; 
&lt;p&gt;ABM provides flexibility, which enables the simulation of various scenarios and the exploration of different policy interventions. This helps policy makers to identify any potential risks and vulnerabilities within a system and to devise appropriate measures to mitigate them.&lt;/p&gt; 
&lt;h2&gt;AWS reference architecture&lt;/h2&gt; 
&lt;p&gt;Jagoda worked with the AWS Emerging Technologies team, to design an architecture with several key advantages. AWS services like Amazon Relational Database Service (RDS), Amazon Elastic Container Service (ECS) on Amazon Elastic Compute Cloud (EC2), AWS Batch, and Amazon Simple Storage Service (S3) played a pivotal role in calibrating and validating the model, optimizing the simulations, ensuring data integrity, and enabling scalability. The cloud-based infrastructure depicted in Figure 2 enhances the model’s analytical abilities and promotes transparency and accessibility in accordance with modern research practices.&lt;/p&gt; 
&lt;div id="attachment_3308" style="width: 719px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3308" loading="lazy" class="size-full wp-image-3308" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/12/CleanShot-2024-02-12-at-10.04.15.png" alt="Figure 2. Reference architecture for implementation of the ABMs in the AWS cloud." width="709" height="360"&gt;
 &lt;p id="caption-attachment-3308" class="wp-caption-text"&gt;Figure 2. Reference architecture for implementation of the ABMs in the AWS cloud.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We harnessed all these services to streamline the entire process and to optimize its efficiency. Amazon RDS for PostgreSQL stored the essential input data, ensuring data integrity and accessibility. Containerizing the applications let us leverage the cloud’s elasticity, so it became possible to dynamically scale the resources when we needed. Store the output data and logs in comma-delimited CSV files and other text formats helped us adhere to the principles of the open data policy advocated by the European Commission.&lt;/p&gt; 
&lt;p&gt;These choices simplified our analysis using various econometric packages but also maintained compatibility with other AWS services, including ECS on Amazon EC2, AWS Batch and Amazon S3. This approach not only improved the simulation’s analytical abilities but it also promoted transparency and accessibility in accordance with modern research practices.&lt;/p&gt; 
&lt;p&gt;Dr. Kaszowska-Mojsa also leveraged AWS Batch processes to facilitate the calibration and Monte Carlo analysis of the model, which ultimately improved the accuracy of the simulations. These AWS services provided an ideal environment for conducting complex, compute-intensive agent-based simulations to assess the impact of macroprudential policies on both an economy and society. This approach ensured scalability, reliability, security, and seamless integration, aligning perfectly with the project’s requirements.&lt;/p&gt; 
&lt;p&gt;Running the MACROPRU simulation on the AWS cloud infrastructure also proved to be a cost-effective and efficient choice. The cloud environment made it easier to store data securely, but also permitted the parallel execution of the same simulation with different calibrations – this significantly expedited the research process. Moreover, the scalability offered by AWS, combined with the use of services like Amazon RDS, ECS on Amazon EC2, AWS Batch and Amazon S3, ensured that the simulation ran smoothly and efficiently.&lt;/p&gt; 
&lt;p&gt;The ability to achieve these results in a secure, fast, and cost-efficient way underscores the advantages of leveraging cloud computing in research project like ours.&lt;/p&gt; 
&lt;h2&gt;Simulation results&lt;/h2&gt; 
&lt;p&gt;Our simulation comprehensively considered all the macroprudential instruments that were outlined in the CRR/CRD IV directive and subsequent European legislation and aligned with the principles of Basel III.&lt;/p&gt; 
&lt;p&gt;Practical testing within this framework involves diverse calibrations, including Capital Adequacy Ratios (CAR), Liquidity Coverage Ratio (LCR), Leverage Ratio (LR), as well as Sectorial Requirements and Large Exposures (LE). We also incorporated national requirements – specifically the Financial Stability Authority recommendations – which encompassed Debt Service to Income (DSTI), Loan to Value (LTV), Debt to Income (DTI) and Debt to Assets (DTA).&lt;/p&gt; 
&lt;p&gt;Creditworthiness evaluation for individual entities or companies remained contingent on individual banks in the simulation, which resulted in varying requirements based on a bank’s market strategy and risk assessment. In extending our model’s capabilities, we could also analyze additional elements such as the Capital Conservation Buffer, specific Countercyclical Capital Buffer (CCB), Systemic Risk Buffer (SRB), Global Systemically Important Banks (G-SIB) buffer and the buffers for other Systemically Important Banks (the “D-SIB” buffer).&lt;/p&gt; 
&lt;p&gt;Figure 3 illustrates the main message of the MACROPRU project: an inappropriate combination and poor calibration of macroprudential tools result in significant adverse redistributive outcomes, which can potentially undermine the positive effects of other public policy instruments – like fiscal, monetary, or social policies – on both an economy and wider society.&lt;/p&gt; 
&lt;div id="attachment_3309" style="width: 877px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3309" loading="lazy" class="size-full wp-image-3309" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/12/CleanShot-2024-02-12-at-10.04.49.png" alt="Figure 3 (a) &amp;amp; (b). Changes in (a) income and (b) indebtedness of households that result from adjustments to the calibration of macroprudential instruments." width="867" height="1035"&gt;
 &lt;p id="caption-attachment-3309" class="wp-caption-text"&gt;Figure 3 (a) &amp;amp; (b). Changes in (a) income and (b) indebtedness of households that result from adjustments to the calibration of macroprudential instruments.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;An aside:&lt;/em&gt; In Poland, the Debt Service to Income Ratio (DSTI) is calculated by dividing the sum of annual liabilities by the sum of annual income and then multiplying the result by 100%. DSTI serves as a crucial measure of household indebtedness in Poland. Each bank is required to calculate DSTI when evaluating the creditworthiness of households and ensuring compliance with regulatory requirements, specifically Recommendation S.&lt;/p&gt; 
&lt;p&gt;To demonstrate the effects of our analysis, we performed two scenarios where we adjusted the calibration parameters for DSTI and LTV while keeping the official calibrations for other macroprudential instruments. This allowed us to isolate the impacts of changes to DSTI and LTV, which affect the creditworthiness of businesses and their access to credit.&lt;/p&gt; 
&lt;p&gt;In the first counterfactual scenario, tightening DSTI and LTV led to a deterioration in the financial positions of populations in the lower percentiles, especially those with limited loans, compared to the base scenario. We validated this against empirical data. Populations in the higher percentiles who were accessing loans were minimally impacted – underscoring the differentiated effect of macroprudential policy changes across different income spectrums.&lt;/p&gt; 
&lt;p&gt;In each scenario, we performed a distinct calibration of macroprudential instruments which let us evaluate changes in inequality using the simulation’s output data, based on wellbeing economic KPIs such as the &lt;a href="https://ourworldindata.org/what-is-the-gini-coefficient"&gt;Gini Coefficient&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Naturally, more complex analyses are feasible, including calculating any uni- or multi- dimensional inequality measures. We could also conduct a time-based analysis to examine how policies influence individual agents, sectors and markets, along with a comprehensive assessment of risk transmission between economic sectors.&lt;/p&gt; 
&lt;p&gt;The quantification of the effects is of utmost significance to policy makers, central bankers, and regulators who shoulder the responsibility of maintaining a resilient economy &lt;em&gt;and&lt;/em&gt; for safeguarding the overall societal welfare. There’s more information about this on &lt;a href="https://monetaristinheels.com/project"&gt;the home page for the project&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Potential future applications and developments&lt;/h2&gt; 
&lt;p&gt;Agent-based modelling (ABM) has significant potential for improving our comprehension of macroprudential policies and their ramifications on the financial system, economy, and society. As researchers continue to explore and refine the approach, there are many future applications and developments that could further increase our understanding of public policies such as macroprudential policies, monetary policy, fiscal policy, environmental policy and other sectoral policies.&lt;/p&gt; 
&lt;p&gt;One potential application is the use of ABM to analyze the impact of macroprudential policies on the different sectors of an economy. Currently, most research is focused on the banking system, but there is also a need to understand how policies affect the housing market dynamics, consumer credit, Universal Basic Income, Circular Economy, climate adaptation and corporate lending. By expanding the scope of an analysis, policy makers can gain a more comprehensive understanding of the transmission channels and potential spillover effects of macroprudential policies.&lt;/p&gt; 
&lt;p&gt;Central bankers, regulators, and researchers may use DSGE and ABM models together in the future. This can address complex questions. It can also combine the strengths of each model type.&lt;/p&gt; 
&lt;p&gt;DSGE models are common for macroeconomics. But they have limits in showing diversity and change in finance. ABM models are better for this.&lt;/p&gt; 
&lt;p&gt;Using both models together gives more insights. Some central banks do this already. These include the Bank of England, Bank of Canada, Bank of Hungary, Bank of Spain, and Bank of Poland.&lt;/p&gt; 
&lt;p&gt;Furthermore, there is a pressing need for the increased real-world testing of ABM models. While ABM has shown its ability to capture the complexities of the financial system, we have to subject these models to rigorous testing using real-world data, validate them against stylized facts, and compare them to other economic models. This will help build confidence in ABMs and ensure their reliability and accuracy for policy analysis.&lt;/p&gt; 
&lt;p&gt;If you are a policy maker, business executive or financial services professional, please feel free to &lt;a href="mailto:ask-hpc@amazon.com"&gt;reach out to the AWS Emerging Technologies Computing team&lt;/a&gt; and we will help you get your ABM up and running on AWS.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;The potential future applications and developments of ABM for understanding macroprudential policies are vast and as the complexity and scale of these models increases, AWS services like AWS Batch are well positioned to serve the needs of ABM developers and researchers.&lt;/p&gt; 
&lt;p&gt;By expanding the scope of analysis, integrating ABM with other modelling approaches, validating models with real-world data, and receiving support from organizations like INET, policy makers can benefit from a more comprehensive and nuanced understanding of macroprudential policy and its implications for a financial system. As we continue to refine and advance ABM, we expect to uncover new insights and approaches that will contribute to the design and implementation of effective macroprudential, monetary and fiscal policies.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;References&lt;/h3&gt; 
&lt;p&gt;1 – Kaszowska-Mojsa, J., Pipień, M. 2020. Macroprudential Policy in a Heterogeneous Environment – An Application of Agent-Based Approach in Systemic Risk Modelling, Entropy, 22(2), p. 129.&lt;/p&gt; 
&lt;p&gt;2 – Pichler, A., Pangallo, M., del Rio-Chanona, R.M., Lafond, F. and Farmer, J.D., 2022. Forecasting the propagation of pandemic shocks with a dynamic input-output model. Journal of Economic Dynamics and Control, 144, p.104527.&lt;/p&gt; 
&lt;p&gt;3 – Clerc et al., 2015. Capital Regulation in a Macroeconomic Model with Three Layers of Default, International Journal of Central Banking, 11(3), p. 9-63.&lt;/p&gt; 
&lt;p&gt;4 – Kaszowska-Mojsa, J., Farmer, D., Bydlon, S., Gleiser, I., 2023. Cloud-Powered Insights: Unveiling the Effects of Macroprudential Policy in a Small Open Economy, available on: &lt;a href="https://monetaristinheels.com/project"&gt;https://monetaristinheels.com/project&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;5 – Kaszowska-Mojsa, J., Farmer, J.D., Włodarczyk, P., 2023. The details of the model, available on: &lt;a href="https://monetaristinheels.com/project"&gt;https://monetaristinheels.com/project&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This research uses data from the &lt;em&gt;Eurosystem Household Finance and Consumption Survey&lt;/em&gt;. The results published and the related observations and analysis may not correspond to results or analysis of the data producers.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Amazon’s renewable energy forecasting: continuous delivery with Jupyter Notebooks</title>
		<link>https://aws.amazon.com/blogs/hpc/amazons-renewable-energy-forecasting-continuous-delivery-with-jupyter-notebooks/</link>
		
		<dc:creator><![CDATA[Alec Hewitt]]></dc:creator>
		<pubDate>Tue, 13 Feb 2024 15:03:26 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">982561089e42b9831b7f993bd780876777cb7008</guid>

					<description>Interested in eliminating friction between data science and engineering teams? Read this post to learn how Amazon successfully transitioned Jupyter Notebooks from the lab to production.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3301" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/09/boofla88_a_wind_farms_of_turbines_across_a_rolling_green_hills__e8683f7d-f097-4513-af62-46cbd2b6ad88.png" alt="Amazon’s renewable energy forecasting- continuous delivery with Jupyter Notebooks copy" width="380" height="212"&gt;&lt;/em&gt;You might associate the phrase ‘Jupyter Notebooks in production’ with a scrappy startup short on engineers or a hobbyist tinkering in their free time. However, this story unfolds at Amazon, where a team transitioned from requiring software engineers to replicate scientists’ work for production, to enabling scientists to seamlessly deploy Jupyter Notebooks into production.&lt;/p&gt; 
&lt;p&gt;Amazon’s Renewable Energy Optimization team produces software to maximize the effectiveness of our portfolio of wind and solar farms. The team develops and runs machine learning models that forecast the state of the electricity grid in the next few days. As Amazon’s portfolio of wind and solar farms has expanded, the techniques that our scientists and software engineers use to create and run these machine learning models has evolved, too.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk you through a science-to-production workflow that’s probably familiar to you, explain the logic that supported our novel approach and – finally – let you benefit from what we learned implementing it all.&lt;/p&gt; 
&lt;h2&gt;The problem&lt;/h2&gt; 
&lt;p&gt;Initially, scientists and software engineers were separated. Scientists developed models and optimization techniques using Jupyter Notebooks. They’d pass these to a software engineer who would translate them into ‘production code’. Scientists &lt;em&gt;never&lt;/em&gt; wrote production code.&lt;/p&gt; 
&lt;p&gt;Although this worked for our initial deployments, the process had several limitations:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;It was slow.&lt;/strong&gt; Once a scientist settled on a model they wanted to use in production, it took two or three months for an engineer to translate that code into production. This created a bottleneck for the scientists and ran against the model of &lt;a href="https://aws.amazon.com/builders-library/going-faster-with-continuous-delivery/"&gt;continuous delivery&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Nobody understood the production code.&lt;/strong&gt; The engineer translating the notebook didn’t fully understand the science being used in the code. Once the code had been translated, it was difficult for the scientist to understand because the engineer changed the code structure. We ended up with code that no one understood. This made it difficult to debug errors and issues in the production code.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;There were inconsistencies between environments.&lt;/strong&gt; Scientists developed the notebooks using Amazon SageMaker or their local machine using dependencies they had installed manually. The engineer built the code using an Amazon internal build system. This frequently meant there were different versions of libraries in use. Subtle differences led to large differences between what scientists were expecting and what happened in production.&lt;/p&gt; 
&lt;p&gt;We needed engineers and scientists to use the same platform. This meant either getting scientists to write production-level code or we had to run Jupyter Notebooks in production (a little heretical).&lt;/p&gt; 
&lt;h2&gt;What’s a Jupyter Notebook and why do scientists like them?&lt;/h2&gt; 
&lt;p&gt;A &lt;a href="https://docs.jupyter.org/en/latest/projects/architecture/content-architecture.html#the-jupyter-notebook-interface"&gt;Jupyter Notebook&lt;/a&gt; is a JSON document that contains both the source code and output of the code execution. Users execute cells of the notebook and then the output of each cell run is saved into the JSON notebook itself. Every time a notebook is run, it gets mutated and overwritten with the latest run.&lt;/p&gt; 
&lt;div id="attachment_3292" style="width: 751px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3292" loading="lazy" class="size-full wp-image-3292" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/09/CleanShot-2024-02-09-at-14.21.13.png" alt="Figure 1 – Jupyter Server reads the JSON Notebook file on disk and displays it to the user. The user then requests commands to be executed which the Jupyter Server send to the Jupyter Kernel to perform. The JupyteServer receives the output of the commands from the Jupyter Kernal and updates the Notebook on disk." width="741" height="325"&gt;
 &lt;p id="caption-attachment-3292" class="wp-caption-text"&gt;Figure 1 – Jupyter Server reads the JSON Notebook file on disk and displays it to the user. The user then requests commands to be executed which the Jupyter Server send to the Jupyter Kernel to perform. The JupyteServer receives the output of the commands from the Jupyter Kernal and updates the Notebook on disk.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Jupyter Notebooks have a lot of advantages for scientists. They allow for quick iteration of ideas because a single operation can be run multiple times without having to run the entire program again. They’re also great for visualizing and analyzing the output from code. Tables, charts, or images can be displayed in the notebook, right next to the source code that generated them.&lt;/p&gt; 
&lt;div id="attachment_3293" style="width: 905px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3293" loading="lazy" class="size-full wp-image-3293" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/09/CleanShot-2024-02-09-at-14.21.42.png" alt="Figure 2 – Information displayed a graph in a Jupyter Notebook vs the same information in CloudWatch Logs." width="895" height="361"&gt;
 &lt;p id="caption-attachment-3293" class="wp-caption-text"&gt;Figure 2 – Information displayed a graph in a Jupyter Notebook vs the same information in CloudWatch Logs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;However, Jupyter Notebooks are not designed to run in production. There’s no built-in functionality to programmatically run a notebook with different parameters. They’re also not easy to test and they’re constantly evolving documents that change every time they are run which makes collaboration and reproducibility difficult. So despite Jupyter Notebooks offering desirable features, they weren’t compatible with our &lt;a href="https://docs.aws.amazon.com/whitepapers/latest/practicing-continuous-integration-continuous-delivery/summary-of-best-practices.html"&gt;software engineering best practices&lt;/a&gt; for production deployments.&lt;/p&gt; 
&lt;h2&gt;Why did we change our mind?&lt;/h2&gt; 
&lt;p&gt;Our views changed when we discovered the framework &lt;a href="https://github.com/nteract/papermill/tree/main"&gt;Papermill&lt;/a&gt; and its &lt;a href="https://netflixtechblog.com/scheduling-notebooks-348e6c14cfd6"&gt;usage at Netflix&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2209.09125"&gt;other industry leaders.&lt;/a&gt; Papermill is a library for executing notebooks programmatically. Specifically, it allowed us to turn a Jupyter Notebook into an immutable object, and then parametrize it.&lt;/p&gt; 
&lt;p&gt;Papermill replaces the user and UI. It acts as another client to the Jupyter Kernel using the same protocol as the Jupyter Server. However, rather than overriding the source notebook each time it runs, it will output a new notebook for each run. Therefore, each time Papermill runs a notebook, a new notebook is created. The notebook has gone from being a mutable document to immutable source code.&lt;/p&gt; 
&lt;p&gt;Additionally, Papermill allows each run to have different parameters. This allows a single notebook to perform a solar forecast for different locations based on a parameter that’s passed to the notebook.&lt;/p&gt; 
&lt;div id="attachment_3294" style="width: 851px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3294" loading="lazy" class="size-full wp-image-3294" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/09/CleanShot-2024-02-09-at-14.22.13.png" alt="Figure 3 – With Papermill, the Jupyter Server is replaced. Papermill is responsible for sending the commands to the Jupyter Kernel and reading and writing the output notebooks to disk. " width="841" height="396"&gt;
 &lt;p id="caption-attachment-3294" class="wp-caption-text"&gt;Figure 3 – With Papermill, the Jupyter Server is replaced. Papermill is responsible for sending the commands to the Jupyter Kernel and reading and writing the output notebooks to disk.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We now had a way to programmatically trigger a parametrized notebook. The next step was to ensure the scientists’ development environment matched the production environment.&lt;/p&gt; 
&lt;h2&gt;Keeping science environment consistent with production&lt;/h2&gt; 
&lt;p&gt;When we had to answer why one of our wind farms had been underperforming at one of Amazon’s infamous “&lt;a href="https://wa.aws.amazon.com/wat.concept.coe.en.html"&gt;Correction of Errors&lt;/a&gt;” meetings, it wasn’t the first time someone said, “&lt;em&gt;It worked on my machine!&lt;/em&gt;”. This was not a new problem and we knew existing technologies existed to solve it.&lt;/p&gt; 
&lt;p&gt;Initially, scientists used the pre-built &lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-images.html"&gt;SageMaker images&lt;/a&gt; to manage the dependencies for their notebooks. These images are useful for experimentation; however, &lt;em&gt;we didn’t have control of the exact version of each library that we were using&lt;/em&gt;. Specifically, some scientific Python libraries had the same version, but the systems used different &lt;a href="https://numpy.org/doc/stable/reference/routines.linalg.html"&gt;linear algebra libraries&lt;/a&gt; which affected the end result of our forecasts. We therefore decided to &lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-byoi.html"&gt;bring our own custom image into SageMaker&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-byoi.html"&gt;SageMaker custom images&lt;/a&gt; allows you to import an image from &lt;a href="https://aws.amazon.com/ecr/"&gt;Amazon Elastic Container Registry (Amazon ECR)&lt;/a&gt; that you have built yourself. This could be built in AWS CodeBuild or in another build system outside of AWS. We used this feature of SageMaker to build our own custom images with all the dependencies decided by our internal AWS Code Artifact Repository.&lt;/p&gt; 
&lt;p&gt;The same image being imported into SageMaker is exactly the same image that will be deployed to production and used on our development machines. This ensures when scientists are using a third party dependency, like &lt;a href="https://numpy.org/devdocs/index.html"&gt;Numpy&lt;/a&gt; or &lt;a href="https://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;, they have the confidence that it’s exactly the same version that will be used in production.&lt;/p&gt; 
&lt;p&gt;Once the scientist is happy with the notebook they’ve been experimenting with, they’ll submit it to our internal build system. The end result is a Docker image that contains the notebook as well as all of its dependencies.&lt;/p&gt; 
&lt;div id="attachment_3295" style="width: 909px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3295" loading="lazy" class="size-full wp-image-3295" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/09/CleanShot-2024-02-09-at-14.22.48.png" alt="Figure 4 – Scientists use Amazon SageMaker Studio for experimentation using the same docker image that is deployed into production." width="899" height="620"&gt;
 &lt;p id="caption-attachment-3295" class="wp-caption-text"&gt;Figure 4 – Scientists use Amazon SageMaker Studio for experimentation using the same docker image that is deployed into production.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;How do the notebooks run in production?&lt;/h2&gt; 
&lt;p&gt;Once a notebook has been built into the image, it can be run in production. The image has the notebook as well as a framework we built that runs the notebook using the Papermill library. An &lt;a href="https://aws.amazon.com/pm/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt; rule triggers the notebook to run at the appropriate time with the appropriate parameters.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; runs the notebook in &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon Elastic Container Service&lt;/a&gt; using the image that we pushed to the ECR repository. After the notebook finishes executing, we save the raw JSON notebook and the HTML representation of the notebook to &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Services (Amazon S3).&lt;/a&gt; These output notebooks can also be viewed in a web browser via an &lt;a href="https://aws.amazon.com/cloudfront/"&gt;Amazon CloudFront distribution&lt;/a&gt; in front of the Amazon S3 bucket.&lt;/p&gt; 
&lt;div id="attachment_3296" style="width: 834px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3296" loading="lazy" class="size-full wp-image-3296" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/09/CleanShot-2024-02-09-at-14.23.33.png" alt="Figure 5 – Notebooks are run on Amazon ECS using AWS Batch. The Batch jobs are triggered by schedules in Amazon EventBridge." width="824" height="813"&gt;
 &lt;p id="caption-attachment-3296" class="wp-caption-text"&gt;Figure 5 – Notebooks are run on Amazon ECS using AWS Batch. The Batch jobs are triggered by schedules in Amazon EventBridge.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We use &lt;a href="https://aws.amazon.com/dynamodb/"&gt;Amazon DynamoDB&lt;/a&gt; to store the status of each notebook run as well as additional metadata about the notebook run. Engineers and scientists can then use this information to debug the status of a notebook run. Even better, non-technical users can view that status of our forecasting and see key outputs in the notebook itself.&lt;/p&gt; 
&lt;div id="attachment_3315" style="width: 892px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3315" loading="lazy" class="size-full wp-image-3315" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/12/CleanShot-2024-02-12-at-10.57.02.png" alt="Figure 6 – Internal website that displays the list of notebook runs that have been saved into DynamoDB." width="882" height="406"&gt;
 &lt;p id="caption-attachment-3315" class="wp-caption-text"&gt;Figure 6 – Internal website that displays the list of notebook runs that have been saved into DynamoDB.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We set up the infrastructure and build pipeline using &lt;a href="https://aws.amazon.com/cdk/"&gt;AWS CDK&lt;/a&gt; which enables us to treat our &lt;a href="https://docs.aws.amazon.com/whitepapers/latest/introduction-devops-aws/infrastructure-as-code.html"&gt;infrastructure as code&lt;/a&gt;. This simplifies spinning up new environments, minimizes configuration errors, and leverages high-level constructs for faster, more reliable work.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;We started this project with the aim of allowing scientists to deploy more frequently without relying on engineers. We achieved that, but we &lt;em&gt;also&lt;/em&gt; improved how both scientists &lt;em&gt;and&lt;/em&gt; engineers interact with the production system. They can debug production notebooks through a web server that displays the output notebooks. This led to the use of these notebooks for reports and metrics presentations – accessible to others via shared links.&lt;/p&gt; 
&lt;p&gt;The infrastructure choices also increased our velocity. Previously our system was computationally intensive and it took two weeks to evaluate it sequentially on a very large machine. AWS Batch allows us to run more than a thousand &lt;em&gt;much smaller&lt;/em&gt; instances in parallel so our model evaluation went down to 3 hours. Solving these problems allowed our small &lt;a href="https://aws.amazon.com/executive-insights/content/amazon-two-pizza-team/"&gt;2-pizza team&lt;/a&gt; to dramatically increase our iteration speed and at the same time increased the reliability of our system.&lt;/p&gt; 
&lt;p&gt;At the outset of this work, scientists couldn’t tell their containers from their images – and the only thing engineers knew about Jupyter Notebooks was that they “&lt;em&gt;were not for production!&lt;/em&gt;”. But, by taking the time to understand each other’s tools and the problems that they were solving we built a system that let us to have consistent environments &lt;strong&gt;&lt;em&gt;and&lt;/em&gt;&lt;/strong&gt; continuous delivery of new science models.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
	</channel>
</rss>