<?xml version="1.0" encoding="UTF-8" standalone="no"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" version="2.0">

<channel>
	<title>AWS HPC Blog</title>
	<atom:link href="https://aws.amazon.com/blogs/hpc/feed/" rel="self" type="application/rss+xml"/>
	<link>https://aws.amazon.com/blogs/hpc/</link>
	<description>Just another Amazon Web Services site</description>
	<lastBuildDate>Thu, 09 Nov 2023 16:28:19 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>Deep-dive into Ansys Fluent performance on Ansys Gateway powered by AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/deep-dive-into-ansys-fluent-performance-on-ansys-gateway-powered-by-aws/</link>
		
		<dc:creator><![CDATA[Dnyanesh Digraskar]]></dc:creator>
		<pubDate>Thu, 09 Nov 2023 15:38:16 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">62218975828b3a44d88617555b4bff2d5132a214</guid>

					<description>In this post, we’ll show you the performance and price curves when Ansys Gateway, powered by AWS runs on different HPC instances - this should help you make the right hardware choices for running Fluent simulations in the cloud.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Dnyanesh Digraskar, Principal HPC Partner Solutions Architect, AWS, Ashwini Kumar, Senior Principal Engineer, Ansys, Nicole Diana, Director, Fluids Business Unit, Ansys.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Today, we’re going to deep-dive into the performance and associated cost of running computational fluid dynamics (CFD) simulations on AWS using Ansys Fluent through the &lt;a href="https://www.ansys.com/products/cloud/ansys-gateway"&gt;Ansys Gateway powered by AWS&lt;/a&gt; (or just “Ansys Gateway” for the rest of this post).&lt;/p&gt; 
&lt;p&gt;Ansys Gateway is an &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-ppiyshk2oin3i"&gt;AWS Marketplace&lt;/a&gt; hosted solution for users to manage their complete Ansys virtual desktop and HPC simulation workflows with more than fifty Ansys products in their AWS cloud environment. &lt;a href="https://www.ansys.com/resource-center/case-study/ansys-emirates-team-new-zealand"&gt;Emirates Team New Zealand&lt;/a&gt; and &lt;a href="https://www.ansys.com/resource-center/case-study/ansys-turntide-technologies"&gt;Turntide Technologies&lt;/a&gt; use Ansys Gateway to accelerate their design and engineering simulation cycles.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.ansys.com/products/fluids/ansys-fluent"&gt;Ansys Fluent&lt;/a&gt;, an advanced physics modeling simulation software is used by engineers and scientists across industries like automotive, aerospace, manufacturing, and energy to innovate and optimize product development.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll evaluate the performance and price characteristics for three test cases on various configurations of Amazon Elastic Compute Cloud (Amazon EC2) instance types. Using the results in this post, you’ll be able to make appropriate hardware choices for running Ansys Fluent simulations.&lt;/p&gt; 
&lt;h2&gt;Ansys Gateway recap&lt;/h2&gt; 
&lt;p&gt;Ansys Gateway is a secure, online platform that enables users to create, manage, and execute complete computer-aided engineering (CAE) workflows in their own AWS accounts. Earlier this year, we published a &lt;a href="https://aws.amazon.com/blogs/apn/accelerate-your-ansys-simulations-with-ansys-gateway-powered-by-aws/"&gt;blog post&lt;/a&gt; that described solution architecture components, security implementation, and typical end-user workflows for using Ansys Gateway.&lt;/p&gt; 
&lt;p&gt;The Ansys applications (solvers) available through Ansys Gateway are pre-configured, validated, and extensively benchmarked on various Amazon EC2 hardware for performance and price. Ansys users can refer to the &lt;a href="https://ansyshelp.ansys.com/account/secured?returnurl=/Views/Secured/gateway/v000/en/gateway_ru/bk_gateway_ru.html"&gt;recommended usage&lt;/a&gt; guidelines on the Ansys Gateway documentation in &lt;a href="https://www.ansys.com/support"&gt;Ansys Help&lt;/a&gt; to setup their virtual desktop infrastructure (VDI) or HPC environment with the recommended Amazon EC2 instance types. Users can then carry out simulations with solvers of their choice straight out-of-the-box without the need to manually setup and tune the solvers, simulation environment, and hardware parameters.&lt;/p&gt; 
&lt;h2&gt;Benchmark information and simulation environment&lt;/h2&gt; 
&lt;h3&gt;Benchmarks&lt;/h3&gt; 
&lt;p&gt;For the benchmarking purposes of this post, we’ve used the test cases from the standard &lt;a href="https://www.ansys.com/it-solutions/benchmarks-overview"&gt;Ansys Fluent Benchmarks&lt;/a&gt; suite. The model description of each test case, including the mesh size represented in terms of number of cells, turbulence model used, and the fluid-flow condition are shown in Table 1. These benchmarks, shown in Figures 1a – 1c for visual reference, represent the typical size and physics modeled by users. We used Ansys Fluent version 2023 R1 to run the simulations.&lt;/p&gt; 
&lt;div id="attachment_3037" style="width: 899px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3037" class="size-full wp-image-3037" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.06.26.png" alt="Table 1: Description of Ansys Fluent benchmarking test cases used for this post." width="889" height="198"&gt;
 &lt;p id="caption-attachment-3037" class="wp-caption-text"&gt;Table 1: Description of Ansys Fluent benchmarking test cases used for this post.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3038" style="width: 1107px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3038" loading="lazy" class="size-full wp-image-3038" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.10.32.png" alt="Figure 1a: Visual representation of Ansys Fluent benchmarking test cases used for this post – Steady-state simulation of flow through a vehicle exhaust system with 33 million cells. Figure 1b: Visual representation of Ansys Fluent benchmarking test cases used for this post – Transient simulation of flow through a combustor with 71 million cells. Figure 1c: Visual representation of Ansys Fluent benchmarking test cases used for this post – Steady-state simulation of external aerodynamics of Formula 1 race car with 140 million cells." width="1097" height="241"&gt;
 &lt;p id="caption-attachment-3038" class="wp-caption-text"&gt;Figure 1 – &lt;strong&gt;a&lt;/strong&gt;: Visual representation of Ansys Fluent benchmarking test cases used for this post – Steady-state simulation of flow through a vehicle exhaust system with 33 million cells. &lt;strong&gt;b&lt;/strong&gt;: Visual representation of Ansys Fluent benchmarking test cases used for this post – Transient simulation of flow through a combustor with 71 million cells. &lt;strong&gt;c&lt;/strong&gt;: Visual representation of Ansys Fluent benchmarking test cases used for this post – Steady-state simulation of external aerodynamics of Formula 1 race car with 140 million cells.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Simulation environment&lt;/h3&gt; 
&lt;p&gt;AWS recently announced the &lt;a href="https://aws.amazon.com/ec2/instance-types/hpc7a/"&gt;Amazon EC2 Hpc7a&lt;/a&gt; instance type, powered by 4&lt;sup&gt;th&lt;/sup&gt; generation AMD EPYC&lt;sup&gt;TM&lt;/sup&gt; (Genoa) processors with up to 192 physical cores and 300 Gbps Elastic Fabric Adapter (EFA) network bandwidth. We compared the performance of these benchmarks on Hpc7a and the previous generation &lt;a href="https://aws.amazon.com/ec2/instance-types/hpc6a/"&gt;Hpc6a&lt;/a&gt; instances.&lt;/p&gt; 
&lt;p&gt;The following table (Table 2) summarizes Amazon EC2 instance configurations used:&lt;/p&gt; 
&lt;div id="attachment_3039" style="width: 936px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3039" loading="lazy" class="size-full wp-image-3039" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.12.16.png" alt="Table 2: Amazon EC2 instances used for running the benchmarking test cases" width="926" height="362"&gt;
 &lt;p id="caption-attachment-3039" class="wp-caption-text"&gt;Table 2: Amazon EC2 instances used for running the benchmarking test cases&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Analysis methodology&lt;/h2&gt; 
&lt;p&gt;Our objective for running these benchmarks was to quantify Ansys Fluent performance, associated hardware, platform, and license costs, and thus be able to recommend the appropriate Amazon EC2 instance type to use on Ansys Gateway. With that in mind, we performed the analysis by measuring the following:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Solver Rating to represent the solver performance&lt;/li&gt; 
 &lt;li&gt;Ratio of performance to hardware configuration cost&lt;/li&gt; 
 &lt;li&gt;Ratio of performance to total job cost&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Solver Rating:&lt;/strong&gt; the &lt;em&gt;Solver Rating&lt;/em&gt; is defined as the number of times the benchmark can be run on a given machine in 24 hours. It’s computed by dividing the number of seconds in a day by the number of seconds required to run the benchmark. A higher &lt;em&gt;Solver Rating&lt;/em&gt; indicates better performance. &lt;em&gt;Solver Rating&lt;/em&gt; is the primary metric we used in this post to report the performance. We ran our simulations for 1000 iterations for steady-state flow or 1000 timesteps for transient flow.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Job cost:&lt;/strong&gt; The total job (or simulation) cost is comprised of three main components: the Amazon EC2 cost, Ansys Gateway charge of $0.25 per running Amazon EC2 instance per hour, and the Ansys Licensing&amp;nbsp;cost.&lt;/p&gt; 
&lt;p&gt;These cost representations can guide you to select the right HPC configuration to&amp;nbsp;meet any one of these three simulation goals:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;minimize job cost&lt;/li&gt; 
 &lt;li&gt;maximize performance&lt;/li&gt; 
 &lt;li&gt;achieve the best performance/cost ratio&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Note that for the purpose of this post, we used the Ansys Elastic Licensing to account for the licensing costs which might not be fully representative if you’re using leased or perpetual licenses. Also, we haven’t accounted for storage and networking charges in our calculations, simply because compute constitutes most (or nearly all) of the infrastructure costs for these simulations. We used Amazon EC2 on-demand costs from the us-east-2 (Ohio) region. You can take advantage of flexible Amazon EC2 pricing options like &lt;a href="https://aws.amazon.com/savingsplans/"&gt;Compute Savings Plan&lt;/a&gt; or &lt;a href="https://aws.amazon.com/ec2/pricing/reserved-instances/"&gt;Reserved Instances&lt;/a&gt; (RI) which provide a significant discount (up to 72%) compared to On-Demand pricing.&lt;/p&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;p&gt;To understand Ansys Fluent performance, we plotted the variation of the Solver Rating against the number of instance cores for each of our three test cases. These plots are in Figures 2 (a, b, c). Higher Solver Rating signifies better performance. The &lt;em&gt;Vehicle Exhaust&lt;/em&gt; model with 33 million cells was scaled to 1536 cores, while the &lt;em&gt;Combustor&lt;/em&gt; and &lt;em&gt;F1 Race Car&lt;/em&gt; models were scaled to over 6000 cores, respectively.&lt;/p&gt; 
&lt;div id="attachment_3040" style="width: 708px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3040" loading="lazy" class="size-full wp-image-3040" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.13.17.png" alt="Figure 2a: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Vehicle exhaust benchmark." width="698" height="451"&gt;
 &lt;p id="caption-attachment-3040" class="wp-caption-text"&gt;Figure 2a: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Vehicle exhaust benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3041" style="width: 697px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3041" loading="lazy" class="size-full wp-image-3041" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.13.53.png" alt="Figure 2b: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Combustor benchmark." width="687" height="436"&gt;
 &lt;p id="caption-attachment-3041" class="wp-caption-text"&gt;Figure 2b: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Combustor benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3042" style="width: 703px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3042" loading="lazy" class="size-full wp-image-3042" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.14.32.png" alt="Figure 2c: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Formula 1 car benchmark." width="693" height="442"&gt;
 &lt;p id="caption-attachment-3042" class="wp-caption-text"&gt;Figure 2c: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Formula 1 car benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Since the number of physical cores on Hpc6a and Hpc7a instance types are different, the relative performance between the two instance types on cores and node level differ, too. In Figures 3 (a, b, c) we plotted the variation of the Solver Rating with the number of instances (nodes).&lt;/p&gt; 
&lt;div id="attachment_3043" style="width: 706px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3043" loading="lazy" class="size-full wp-image-3043" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.15.25.png" alt="Figure 3a: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Vehicle Exhaust benchmark." width="696" height="398"&gt;
 &lt;p id="caption-attachment-3043" class="wp-caption-text"&gt;Figure 3a: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Vehicle Exhaust benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3044" style="width: 708px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3044" loading="lazy" class="size-full wp-image-3044" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.15.53.png" alt="Figure 3b: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Combustor benchmark." width="698" height="444"&gt;
 &lt;p id="caption-attachment-3044" class="wp-caption-text"&gt;Figure 3b: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Combustor benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3045" style="width: 748px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3045" loading="lazy" class="size-full wp-image-3045" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.16.19.png" alt="Figure 3c: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Formula 1 race car benchmark." width="738" height="470"&gt;
 &lt;p id="caption-attachment-3045" class="wp-caption-text"&gt;Figure 3c: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Formula 1 race car benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For the Vehicle Exhaust model with 33 million cells, the performance improvement with Hpc7a instances was 1.2x at 1536 cores on a &lt;em&gt;per-core &lt;/em&gt;basis, and 1.78x at 32 instances on a &lt;em&gt;per-instance&lt;/em&gt; basis. For the Combustor benchmark model with 71 million cells, the performance improvement with Hpc7a instances is 1.3x at 3072 cores on a &lt;em&gt;per-core&lt;/em&gt; basis, and was 1.83x at 32 instances on &lt;em&gt;per-instance&lt;/em&gt; basis.&lt;/p&gt; 
&lt;p&gt;As the benchmark model size increased, the instance topology and higher EFA networking bandwidth of Hpc7a instances helps to achieve better scaling. For the Formula 1 Race Car model with 140 million cells, the performance improvement &lt;em&gt;per-core&lt;/em&gt; with Hpc7a instances was 1.3x at 6144 cores, and was 2.1x at 32 instances on a &lt;em&gt;per-instance&lt;/em&gt; basis.&lt;/p&gt; 
&lt;h3&gt;Full cores vs partial cores&lt;/h3&gt; 
&lt;p&gt;These plots show performance results when running Ansys Fluent on the full available set of physical cores: 192 for hpc7a.96xlarge, and 96 for hpc6a.48xlarge. But it’s possible to manually disable certain numbers of physical cores – or use process pinning on each instance – to achieve better &lt;em&gt;per-core&lt;/em&gt; performance because of increased memory bandwidth per core. We call this under-subscribing.&lt;/p&gt; 
&lt;p&gt;When running simulations on the Hpc6a instance type, which is available in only one size, under-subscribing is implemented in Ansys Gateway via job setup scripts. Under-subscribing on Hpc7a isn’t required because it’s a feature of the different instance sizes.&lt;/p&gt; 
&lt;p&gt;In this section we’ll take a detailed look at the impact this under-subscribing technique has on Ansys Fluent performance. Figures 4a, 4b, 4c show variations of Solver Rating with the number of instance cores for 100%, 50%, and 25% cores enabled per-instance for Hpc6a and corresponding sizes of Hpc7a instance type.&lt;/p&gt; 
&lt;div id="attachment_3046" style="width: 694px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3046" loading="lazy" class="size-full wp-image-3046" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.17.08.png" alt="Figure 4a: Vehicle exhaust benchmark - comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance." width="684" height="413"&gt;
 &lt;p id="caption-attachment-3046" class="wp-caption-text"&gt;Figure 4a: Vehicle exhaust benchmark – comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3047" style="width: 689px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3047" loading="lazy" class="size-full wp-image-3047" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.17.34.png" alt="Figure 4a: Vehicle exhaust benchmark - comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance." width="679" height="463"&gt;
 &lt;p id="caption-attachment-3047" class="wp-caption-text"&gt;Figure 4a: Vehicle exhaust benchmark – comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3048" style="width: 718px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3048" loading="lazy" class="size-full wp-image-3048" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.17.57.png" alt="Figure 4c: F1 Race Car benchmark - comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. " width="708" height="454"&gt;
 &lt;p id="caption-attachment-3048" class="wp-caption-text"&gt;Figure 4c: F1 Race Car benchmark – comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;From Figures 4a – 4c, it’s clear that as we undersubscribe an instance (thus increasing the available memory bandwidth per core), the simulation performance improves. For the vehicle exhaust benchmark, the performance improvement with 50% and 25% cores enabled compared to 100% cores for both Hpc6a and Hpc7a instances was between 1.3x and 1.8x.&lt;/p&gt; 
&lt;p&gt;As the size of the simulation model increased, the performance improvement due to the under-subscribing became even more pronounced. For the Combustor model, the performance improvement with 50% and 25% cores enabled compared to 100% cores on both Hpc6a and Hpc7a was between 1.7x and 2.6x.&lt;/p&gt; 
&lt;p&gt;Finally, for the F1 Race Car model, the performance improvement with 50% and 25% cores enabled compared to 100% cores for both Hpc6a and Hpc7a instances was between 1.8x and 2.9x.&lt;/p&gt; 
&lt;h2&gt;Simulation costs&lt;/h2&gt; 
&lt;p&gt;Now let’s look at the cost impacts (based on cloud infrastructure and licensing) for running these benchmarks with our various instance configurations.&lt;/p&gt; 
&lt;p&gt;Figures 5a – 5b show variations of &lt;em&gt;performance to cloud-infrastructure-cost&lt;/em&gt; with across a range of &lt;em&gt;cores-enabled per-instance&lt;/em&gt; for Hpc6a and Hpc7a, for the smallest and the largest benchmark case.&lt;/p&gt; 
&lt;p&gt;The cloud infrastructure costs plotted here represent the total hardware cost based on Amazon EC2, and the Ansys Gateway hardware flat charge of $0.25 per-instance per-hour. This is a good metric to follow when you want to evaluate the instances for best performance while considering the cloud infrastructure costs only.&lt;/p&gt; 
&lt;div id="attachment_3049" style="width: 711px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3049" loading="lazy" class="size-full wp-image-3049" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.18.51.png" alt="Figure 5a: Vehicle exhaust benchmark – performance to cloud infrastructure cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Higher is better. Fully subscribed Hpc6a and Hpc7a instances provide the best performance to cost ratio. " width="701" height="416"&gt;
 &lt;p id="caption-attachment-3049" class="wp-caption-text"&gt;Figure 5a: Vehicle exhaust benchmark – performance to cloud infrastructure cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Higher is better. Fully subscribed Hpc6a and Hpc7a instances provide the best performance to cost ratio.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3051" style="width: 706px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3051" loading="lazy" class="size-full wp-image-3051" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.19.56.png" alt="Figure 5b: F1 race car benchmark – performance to cloud infrastructure cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Higher is better. Fully subscribed Hpc6a and Hpc7a instances provide the best performance to cost ratio. " width="696" height="433"&gt;
 &lt;p id="caption-attachment-3051" class="wp-caption-text"&gt;Figure 5b: F1 race car benchmark – performance to cloud infrastructure cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Higher is better. Fully subscribed Hpc6a and Hpc7a instances provide the best performance to cost ratio.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;From Figures 5a – 5b, you can tell that simulations run on the fully-subscribed Hpc6a and Hpc7a instances have the best performance &lt;em&gt;per hardware-configuration&lt;/em&gt; cost. At higher core counts, the cloud infrastructure costs start to increase, resulting in a drop of the &lt;em&gt;performance-to-cost&lt;/em&gt; ratio.&lt;/p&gt; 
&lt;p&gt;In Figure 6, we plotted the &lt;em&gt;performance to total-job-cost&lt;/em&gt; ratio, where the total job cost is the sum of cloud infrastructure &lt;em&gt;and&lt;/em&gt; the Ansys licensing cost. As the Ansys licensing costs can vary for each customer depending on their licensing agreement, for the purpose of this post we used the Ansys Elastic Currency (AEC) to represent the licensing cost, to keep it simple.&lt;/p&gt; 
&lt;div id="attachment_3052" style="width: 752px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3052" loading="lazy" class="size-full wp-image-3052" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.20.40.png" alt="Figure 6: F1 race car benchmark – performance to total cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Total cost includes Ansys licensing. Higher is better. Hpc6a with 25% subscribed and hpc7a.24xlarge instances provide the best performance to total cost ratio. " width="742" height="459"&gt;
 &lt;p id="caption-attachment-3052" class="wp-caption-text"&gt;Figure 6: F1 race car benchmark – performance to total cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Total cost includes Ansys licensing. Higher is better. Hpc6a with 25% subscribed and hpc7a.24xlarge instances provide the best performance to total cost ratio.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;From Figure 6, you can see that simulations run on Hpc6a instances with 25% cores enabled, and hpc7a.24xlarge show the best &lt;em&gt;performance to total-cost&lt;/em&gt; ratio. Running the simulations with 50% cores enabled on Hpc6a or on Amazon EC2 hpc7a.48xlarge is a good idea for customers with pre-existing Ansys licensing who are looking to balance performance and cloud infrastructure costs. All instance types provide better value at higher core counts because per-core license costs decrease when you use more cores.&lt;/p&gt; 
&lt;p&gt;In our plots, we focused on performance and cost comparisons, but customers are also interested in making the right Amazon EC2 choices based on &lt;strong&gt;simulation runtime and costs&lt;/strong&gt;. In Figures 7 and 8, we plotted the variation of cloud infrastructure and total costs with simulation runtime. By referring to these plots, you can choose the right instance for your preferred simulation runtime.&lt;/p&gt; 
&lt;div id="attachment_3053" style="width: 685px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3053" loading="lazy" class="size-full wp-image-3053" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.21.18.png" alt="Figure 7: F1 race car benchmark – variation of cloud infrastructure cost with simulation runtime on Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Lower cost and runtime are better. Fully subscribed Hpc6a and Hpc7a instances offer the lowest costs for a given simulation runtime. " width="675" height="441"&gt;
 &lt;p id="caption-attachment-3053" class="wp-caption-text"&gt;Figure 7: F1 race car benchmark – variation of cloud infrastructure cost with simulation runtime on Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Lower cost and runtime are better. Fully subscribed Hpc6a and Hpc7a instances offer the lowest costs for a given simulation runtime.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3054" style="width: 678px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3054" loading="lazy" class="size-full wp-image-3054" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.21.43.png" alt="Figure 8: F1 race car benchmark – variation of total cost i.e., cloud infrastructure + licensing cost with simulation runtime on Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Lower cost and runtime are better. Hpc6a with 25% subscribed and hpc7a.24xlarge instances offer the best total cost for a given simulation runtime. " width="668" height="435"&gt;
 &lt;p id="caption-attachment-3054" class="wp-caption-text"&gt;Figure 8: F1 race car benchmark – variation of total cost i.e., cloud infrastructure + licensing cost with simulation runtime on Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Lower cost and runtime are better. Hpc6a with 25% subscribed and hpc7a.24xlarge instances offer the best total cost for a given simulation runtime.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;From Figure 7, you can see that when prioritizing cloud infrastructure cost for a desired simulation runtime, the fully-subscribed instances offer the lowest cost because of utilizing maximum available cores on the instances.&lt;/p&gt; 
&lt;p&gt;When we add the software licensing costs – which can be high for larger simulations – the benefit of running the simulations on under-subscribed instances for performance gain helps to drive the total cost down. It’s clear from Figure 8 that under-subscribed instances offer the best total cost for a given simulation runtime.&lt;/p&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;Today we described the price-performance characteristics of Ansys Fluent CFD simulations on Ansys Gateway powered by AWS. We showed you some of our best practices for running simulations on different &lt;a href="https://aws.amazon.com/ec2/"&gt;Amazon EC2&lt;/a&gt; compute-optimized instances. And we described how total job costs comprise of infrastructure costs and Ansys licenses for the Ansys Fluent simulations. With these results you should be able to select the right instance types for running your simulations, whether your goal is to maximize performance or minimize costs.&lt;/p&gt; 
&lt;p&gt;You can get started with Ansys Gateway powered by AWS by subscribing through the &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-ppiyshk2oin3i"&gt;AWS Marketplace&lt;/a&gt;. Follow the &lt;a href="https://www.youtube.com/playlist?list=PL0lZXwHtV6Omw8rjdXmr4UiDj2WZksygA"&gt;Ansys Gateway YouTube channel&lt;/a&gt; for in-depth step-by-step tutorials and video guidelines to get started with setup and running Ansys applications. Finally, you can refer to the &lt;a href="https://forum.ansys.com/forums/forum/installation-and-licensing/ansys-gateway/"&gt;Ansys innovation Space learning forums&lt;/a&gt; for Ansys Gateway specific help.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.28.29.png" alt="Ashwini Kumar " width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Ashwini Kumar&lt;/h3&gt; 
  &lt;p&gt;Ashwini Kumar is a senior principal application engineer at Ansys, specializing in Ansys Cloud solutions, where he plays a pivotal role in assisting Ansys clients with optimizing their cloud investments. With over three decades of expertise in customer relationship management, cloud computing, technical support, and services related to Fluid Flow and Heat Transfer, he brings a wealth of knowledge to his role. His educational background includes a Ph.D. from the University of Minnesota in Minneapolis-St. Paul, Minnesota, a master’s degree from the University of Manitoba in Winnipeg, Canada, and a B.Tech from G.B. Pant University of Agriculture and Technology in Pantnagar, India.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.28.34.png" alt="Nicole Diana " width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Nicole Diana&lt;/h3&gt; 
  &lt;p&gt;Nicole Diana is Director of research and development Verification for the computational fluids dynamics (CFD) product line at Ansys. She has over twenty-five years of experience in the development, verification, and application of Ansys CFD products.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Lattice Boltzmann simulation with Palabos on AWS using Graviton-based Amazon EC2 Hpc7g instances</title>
		<link>https://aws.amazon.com/blogs/hpc/lattice-boltzmann-simulation-with-palabos-on-aws-using-graviton-based-amazon-ec2-hpc7g-instances/</link>
		
		<dc:creator><![CDATA[Jun Tang]]></dc:creator>
		<pubDate>Wed, 08 Nov 2023 13:47:14 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<guid isPermaLink="false">ecb740a48f70c44fb346183b25ed64dbe70baea3</guid>

					<description>In this post we’ll show you the performance when running the Parallel Lattice Boltzmann Solver (Palabos) on the latest generation of AWS Graviton CPUs in Hpc7g instances on AWS.</description>
										<content:encoded>&lt;p&gt;Computational fluid dynamics (CFD) has grown over several decades to become a widely used tool to study a broad range of important industrial and academic problems, ranging from automotive and aircraft design to the study of blood flow inside the human body.&lt;/p&gt; 
&lt;p&gt;Whilst traditional Navier-Stokes (NS) based codes using the finite-volume approach are still the most widely used, an alternative approach – the Lattice Boltzmann method (LBM) – has emerged in the last two decades. LBM methods are particularly attractive to both industry and academia.&lt;/p&gt; 
&lt;p&gt;In this post we’ll show you the performance and cost benefits of running the Parallel Lattice Boltzmann Solver (&lt;em&gt;Palabos&lt;/em&gt;) [1] – a specific LBM solver – on the latest generation of Amazon Elastic Compute Cloud instances based on the AWS Graviton.&lt;/p&gt; 
&lt;p&gt;Palabos is an open-source, C++ solver developed by the University of Geneva. It’s designed to run and scale efficiently on HPC clusters. Palabos is a highly versatile computational tool and has been widely used in the LBM community and is often a reference implementation for many Lattice Boltzmann models.&lt;/p&gt; 
&lt;h2&gt;Lattice Boltzmann models (LBMs)&lt;/h2&gt; 
&lt;p&gt;We mentioned LBMs are popular with both industry and academia. There are several reasons for this.&lt;/p&gt; 
&lt;p&gt;First, they’re flexible. You can model complex geometries whether they’re stationary or moving. Second, they suffer from minimal numerical dissipation, which makes them ideal for high-fidelity scale-resolving methods and aeroacoustics.&lt;/p&gt; 
&lt;p&gt;They also have superior computational performance, which stems from two key characteristics: data locality, and the fact that they’re easier to vectorize and set up for multithreading (on both CPUs and GPUs) compared to NS-based methods.&lt;/p&gt; 
&lt;p&gt;To illustrate these points, we’re going to show you how Palabos simulates blood flows [2][3] that can help researchers better understand vascular diseases and cancer cell movement through blood vessels. This can also provide a controlled environment to evaluate different treatment options.&lt;/p&gt; 
&lt;p&gt;There’s another reason LBMs are interesting. Given they’re highly scalable, researchers can run them on thousands or tens of thousands of cores to either get answers very quickly, or drive up the fidelity of the simulations themselves – or often both. But the availability of HPC resources can potentially become a bottleneck for many. As you can imagine, that causes a lot of researchers to look for large scale resources beyond their usual environments: enter AWS which can offer all the additional capacity they’re asking for. It’s also robust, secure by design, low cost and incredibly flexible.&lt;/p&gt; 
&lt;h2&gt;Introducing AWS Graviton3E&lt;/h2&gt; 
&lt;p&gt;Amazon EC2 offers hundreds of instance types optimized to fit different use cases. Instances vary based on CPU, memory, storage, and networking bandwidth giving customers the flexibility to choose the right mix of resources for their applications.&lt;/p&gt; 
&lt;p&gt;Amazon EC2 Hpc7g instances are the &lt;a href="https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/"&gt;latest addition&lt;/a&gt; to the extended family of Graviton-based EC2 instances. They carry DDR5 memory with an AWS Graviton3E processor, and 200Gbps low-latency networking delivered by Elastic Fabric Adapter – especially important when you’re scaling to a large number of instances.&lt;a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc7g-instances-powered-by-aws-graviton3e-processors-optimized-for-high-performance-computing-workloads/"&gt; They also consume up to 60% less energy&lt;/a&gt; for the same workloads than other comparable x86-based instances tailored for HPC applications – this is great for the planet.&lt;/p&gt; 
&lt;p&gt;In &lt;a href="https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/"&gt;an earlier&amp;nbsp;post&lt;/a&gt;, we published some performance results for real world workloads from CFD, finite-element analysis (FEA), molecular dynamics, and numerical weather prediction (NWP).&lt;/p&gt; 
&lt;p&gt;For this post, we compared Palabos performance across two HPC instance types offered by AWS and&amp;nbsp;we found that Hpc7g has competitive performance. In our tests, Hpc7g delivered up to&amp;nbsp;75% better performance and up to 3x better &lt;em&gt;price-performance&lt;/em&gt; compared to the previous generation Graviton instances (C6gn).&lt;/p&gt; 
&lt;h2&gt;Benchmark simulation result and performance&lt;/h2&gt; 
&lt;p&gt;For our tests, we used two instance types: Hpc7g.16xlarge (the latest processor in Graviton family customized for HPC applications) and C6gn.16xlarge (the previous generation processor in Graviton family).&amp;nbsp;We used AWS ParallelCluster to launch our environment, manage these fleets, and provide an Amazon FSx for Lustre (a popular parallel file system). ParallelCluster uses Slurm for its workload manager, making it familiar and easier to use. There’s an example ParallelCluster configuration file in &lt;a href="https://github.com/aws/aws-graviton-getting-started/blob/main/HPC/scripts-setup/hpc7g-ubuntu2004-useast1.yaml"&gt;the Graviton HPC best practices guide&lt;/a&gt;. You can also find a one-click launchable recipe in the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;HPC Recipes Library&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Cavity 3D benchmark&lt;/h3&gt; 
&lt;p&gt;In our first test, we used the “&lt;a href="https://gitlab.unige.ch/hpc/benchmark-aimp-2022/-/blob/main/Palabos/README.md"&gt;lid-driven cavity problem in a cuboid&lt;/a&gt;”, a three-dimension benchmark with 1 billion cells (1001x1001x1001). The top wall moves to the right with a constant velocity, while the other walls are stationary. Figure 1 is a snapshot of the simulated velocity field at 10.1 sec. We used Palabos v2.3.0, compiled with GNU Compiler v11.1.0 and Open MPI v4.1.4.&lt;/p&gt; 
&lt;div id="attachment_3081" style="width: 767px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3081" loading="lazy" class="size-full wp-image-3081" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.24.40.png" alt="Figure 1. A display of Palabos output velocity field at 10.1 sec for the three-dimension lid driven cavity problem. Arrow shows flow direction and speed." width="757" height="495"&gt;
 &lt;p id="caption-attachment-3081" class="wp-caption-text"&gt;Figure 1. A display of Palabos output velocity field at 10.1 sec for the three-dimension lid driven cavity problem. Arrow shows flow direction and speed.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We ran the same benchmark on both Hpc7g and C6gn scaling from 2 to 128 instances (8192cores) in both cases. We calculated throughput in &lt;em&gt;million site updates per second&lt;/em&gt; (MLUPS)&amp;nbsp;which we’ve shown in Table 1.&lt;/p&gt; 
&lt;p&gt;At 8192 cores, the solver maintains a strong scaling efficiency of 60% on Hpc7g (Figure 2), which also had 75% higher throughput than the previous generation Graviton 2 (C6gn) instance. The simulation cost on Hpc7g is close to 1/3 of the cost of doing it on C6gn (Figure 3).&lt;/p&gt; 
&lt;div id="attachment_3082" style="width: 564px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3082" loading="lazy" class="size-full wp-image-3082" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.25.27.png" alt="Table 1. Cavity 3D benchmark performance (higher is better)" width="554" height="306"&gt;
 &lt;p id="caption-attachment-3082" class="wp-caption-text"&gt;Table 1. Cavity 3D benchmark performance (higher is better)&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3083" style="width: 758px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3083" loading="lazy" class="size-full wp-image-3083" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.26.07.png" alt="Figure 2. Cavity 3D benchmark throughput (higher is better) on our two instance types; Hpc7g shows good efficiency at 8192 cores for this strong scaling test." width="748" height="561"&gt;
 &lt;p id="caption-attachment-3083" class="wp-caption-text"&gt;Figure 2. Cavity 3D benchmark throughput (higher is better) on our two instance types; Hpc7g shows good efficiency at 8192 cores for this strong scaling test.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3084" style="width: 760px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3084" loading="lazy" class="size-full wp-image-3084" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.26.33.png" alt="Figure 3. Cavity 3D benchmark cost for 10000 iterations on our two instance types, lower is better." width="750" height="564"&gt;
 &lt;p id="caption-attachment-3084" class="wp-caption-text"&gt;Figure 3. Cavity 3D benchmark cost for 10000 iterations on our two instance types, lower is better.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Cellular blood flow simulation benchmark&lt;/h3&gt; 
&lt;p&gt;Next, we studied the &lt;em&gt;cellular blood computation&lt;/em&gt;, which has three components: the &lt;em&gt;fluid solver&lt;/em&gt;, the &lt;em&gt;solid solver,&lt;/em&gt; and the &lt;em&gt;fluid-solid interaction&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;The fluid solver solves the weakly-compressible Navier-Stokes equations. The solid solver, based on nodal projective finite elements method (npFEM) resolves the trajectories and deformations of the blood cells. And the fluid-solid interaction is loosely coupled through an immersed boundary condition.&lt;/p&gt; 
&lt;p&gt;The result of this hybrid simulation is shown in Figure 4.&amp;nbsp;For &lt;a href="https://gitlab.com/unigespc/palabos/-/blob/master/examples/showCases/bloodFlowDefoBodies/README.md?ref_type=heads"&gt;this second benchmark&lt;/a&gt;, we simulated the blood flow in a 50x50x50um cube with 476 red blood cells (RBCs) and 95 platelets (PLTs).&lt;/p&gt; 
&lt;div id="attachment_3085" style="width: 767px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3085" loading="lazy" class="size-full wp-image-3085" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.27.17.png" alt="Figure 4. Hybrid simulation of blood plasma (Palabos) and deformable RBCs/PLTs (npFEM)." width="757" height="533"&gt;
 &lt;p id="caption-attachment-3085" class="wp-caption-text"&gt;Figure 4. Hybrid simulation of blood plasma (Palabos) and deformable RBCs/PLTs (npFEM).&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Table 2 and Figure 5 show the performance in the number of iterations we can simulate &lt;em&gt;per minute&lt;/em&gt; (so higher is better).&lt;/p&gt; 
&lt;p&gt;Hpc7g delivered up to 50% better performance and 2.45x better price-performance over C6gn for these tests.&lt;/p&gt; 
&lt;div id="attachment_3086" style="width: 678px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3086" loading="lazy" class="size-full wp-image-3086" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.27.55.png" alt="Table 2. Cellular bloodFlow benchmark throughput (higher is better)" width="668" height="211"&gt;
 &lt;p id="caption-attachment-3086" class="wp-caption-text"&gt;Table 2. Cellular bloodFlow benchmark throughput (higher is better)&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3087" style="width: 717px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3087" loading="lazy" class="size-full wp-image-3087" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.30.23.png" alt="Figure 5. Cellular blood flow benchmark throughput (higher is better) on our two instance types." width="707" height="552"&gt;
 &lt;p id="caption-attachment-3087" class="wp-caption-text"&gt;Figure 5. Cellular blood flow benchmark throughput (higher is better) on our two instance types.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusions&lt;/h2&gt; 
&lt;p&gt;In this post we showed you that a popular LBM code can be easily run on Graviton-based Amazon EC2 instances with up to 8192 cores. The Amazon EC2 Hpc7g instances showed up to 70% better performance and up to 3x better price-performance over the previous generation Graviton instances for Palabos.&lt;/p&gt; 
&lt;p&gt;The 3D Cavity benchmark scaled efficiently on Hpc7g instances up to 8192&amp;nbsp;cores and we think this illustrates how the combination of a computationally efficiency code, and large-scale cloud HPC facility can enable you to run larger cases than you could likely complete using limited on-prem HPC resources.&lt;/p&gt; 
&lt;p&gt;You can find the instructions to set up and run HPC applications on Graviton in our &lt;a href="https://github.com/aws/aws-graviton-getting-started/tree/main/HPC"&gt;best practice guide&lt;/a&gt;. And you can find easy to consume recipes for building clusters to suite your taste in the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;HPC Recipe Library&lt;/a&gt;. These are open source projects on GitHub so you can &lt;a href="https://github.com/aws/aws-graviton-getting-started/issues"&gt;let us know&lt;/a&gt; directly if you run into any technical issues. Let us know how you get on.&lt;/p&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;p&gt;[1] Latt, J., Malaspinas, O., Kontaxakis, D., Parmigiani, A., Lagrava, D., Brogi, F., Belgacem, M. B., Thorimbert, Y., Leclaire, S., Li, S., Marson, F., Lemus, J., Kotsalos, C., Conradin, R., Coreixas, C., Petkantchin, R., Raynaud, F., Beny, J., &amp;amp; Chopard, B. (2021). &lt;a href="https://www.sciencedirect.com/science/article/pii/S0898122120301267?via%3Dihub"&gt;Palabos: Parallel lattice boltzmann solver&lt;/a&gt;. Computers and Mathematics with Applications, 81, 334–350. https://doi.org/10.1016/j.camwa.2020.03.022&lt;/p&gt; 
&lt;p&gt;[2] Kotsalos, C., Latt, J., Beny, J., &amp;amp; Chopard, B. (2020). &lt;a href="https://arxiv.org/abs/1911.03062"&gt;Digital Blood in massively parallel CPU/GPU systems for the study of platelet transport.&lt;/a&gt; Interface Focus, 11(1), 20190116. https://doi.org/10.1098/rsfs.2019.0116&lt;/p&gt; 
&lt;p&gt;[3] Zavodszky, G., van Rooij, B., Azizi, V., Alowayyed, S., &amp;amp; Hoekstra, A. (2017). &lt;a href="https://www.sciencedirect.com/science/article/pii/S1877050917306245"&gt;Hemocell: A high-performance microscopic cellular library&lt;/a&gt;. &lt;em&gt;Procedia Computer Science&lt;/em&gt;, &lt;em&gt;108&lt;/em&gt;, 159–165. https://doi.org/10.1016/j.procs.2017.05.084&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Using Fleet Training to Improve Level 3 Digital Twin Virtual Sensors with Ansys on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/using-fleet-training-to-improve-l3-digital-twin-virtual-sensors-with-ansys-on-aws/</link>
		
		<dc:creator><![CDATA[Ross Pivovar]]></dc:creator>
		<pubDate>Tue, 07 Nov 2023 16:05:15 +0000</pubDate>
				<category><![CDATA[Amazon Neptune]]></category>
		<category><![CDATA[Amazon Timestream]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS IoT TwinMaker]]></category>
		<category><![CDATA[AWS Lambda]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Internet of Things]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">b2742a64559bd3c77020ec79c0838f5932a71ca8</guid>

					<description>AWS is developing new tools that enable easier and faster deployment of level 3/4 digital twins. This post discusses how a fleet calibrated level 3 digital twin can be cost effectively deployed on AWS Cloud.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post contrbuted by Ross Pivovar, Solution Architect, Autonomous Computing, and&amp;nbsp;&lt;/em&gt;&lt;em&gt;Adam Rasheed, Head of Autonomous Computing at AWS, and &lt;/em&gt;&lt;em&gt;Matt Adams, Lead Product Specialist at Ansys.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/blogs/hpc/deploying-a-level-3-digital-twin-virtual-sensor-with-ansys-on-aws/"&gt;In a previous post&lt;/a&gt;, we shared how to build and deploy a &lt;a href="https://aws.amazon.com/blogs/iot/l3-predictive-digital-twins/"&gt;Level 3 Digital Twin&lt;/a&gt; virtual sensor using an Ansys model on AWS. Virtual sensors are desirable when it is difficult to physically measure a quantity, expensive to measure, or difficult to maintain the sensor. In this post, we’ll discuss challenges with Level 3 (L3) virtual sensors and describe an improved approach using a fleet-trained L3 virtual sensor.&lt;/p&gt; 
&lt;p&gt;We used &lt;a href="https://www.ansys.com/products/digital-twin/ansys-twin-builder"&gt;Ansys Twin Builder and Twin Deployer&lt;/a&gt; to build the physics model and then &lt;a href="https://aws.amazon.com/iot-twinmaker/"&gt;AWS IoT TwinMaker&lt;/a&gt; and the &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow framework&lt;/a&gt; to deploy on AWS. To provide context, we use our &lt;a href="https://aws.amazon.com/blogs/iot/digital-twins-on-aws-unlocking-business-value-and-outcomes/"&gt;four-level Digital Twin leveling index&lt;/a&gt; to help customers understand their use cases and the technologies required to achieve their desired business value.&lt;/p&gt; 
&lt;h2&gt;Looking at challenges with L3 virtual sensor&lt;/h2&gt; 
&lt;p&gt;L3 virtual sensors are typically based on either machine-learning (ML) models or physics-based models. ML models require large training sets to cover the full domain of possible outcomes and extrapolation can be a safety concern. Comparatively, physics-based models only require small validation sets to confirm accuracy. However, these models do not reflect environmental changes like equipment degradation or deviation from the initial assumptions used to develop the physics models.&lt;/p&gt; 
&lt;p&gt;For example, Figure 1 compares the predicted value for pressure from the L3 Digital Twin virtual sensor (orange line, labeled Level 3 DT) from the prior post to the measured pressure sensor value (blue line, labeled Measured Value). The blue shaded regions around each line represent the uncertainty in the predicted and measured values. The virtual sensor is relying on the thermodynamic model of the ideal (new) compressor and initially (&amp;lt; 200 days) correctly predicts the discharge pressure of the fluid post-compression.&lt;/p&gt; 
&lt;div id="attachment_3007" style="width: 790px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3007" loading="lazy" class="size-full wp-image-3007" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure1.jpg" alt="Figure 1 Demonstration of L3 performance degradation due to lack of continual calibration" width="780" height="189"&gt;
 &lt;p id="caption-attachment-3007" class="wp-caption-text"&gt;Figure 1: Graph comparing L3 DT virtual sensor prediction (without periodic calibration) versus sensor data.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As the natural gas compressor train operates in a harsh environment, the overall compressor efficiency degrades over time. The virtual sensor does not know that the compressor efficiency has decreased, and falsely predicts a higher pressure. The L3 digital twin virtual sensor (orange line) has no way to update its model parameters to improve the predictions to account for the decrease in efficiency. As a result, as shown in the graph, after 400 days, the L3 digital twin virtual sensor prediction for pressure deviated well beyond the measured sensor values (even beyond the shaded region showing the uncertainty). An operator relying on the L3 digital twin virtual sensor would mistakenly think there is a safety issue and shutdown the compressor&lt;/p&gt; 
&lt;p&gt;To alleviate this issue, operators could perform maintenance at regularly scheduled intervals, for example, every 100 days. Regular maintenance keeps the compressors running approximately at the same efficiency as new compressor trains. However, frequent maintenance is costly requiring shut-down of the facility and needs to be compared to the cost of running a compressor train in a less efficient state. Additionally, the compressor degradation depends on the environmental and operating conditions, so in some cases, doing maintenance at 100 days is too frequent, resulting in unnecessary maintenance costs. In other cases, like after a major sandstorm, maintenance is required more frequently to maintain production efficiency.&lt;/p&gt; 
&lt;h2&gt;Using a fleet-trained L3 virtual sensor&lt;/h2&gt; 
&lt;p&gt;An alternative is to train the model using data that includes compressor degradation, like a drop in compressor efficiency. For equipment operators running fleets of compressors, such data can be obtained from a “fleet leader” compressor that is fully instrumented. This is a common situation where the operator identifies the “fleet leader” compressor which is the oldest compressor which they intentionally fully instrument and for which they collect the most data.&lt;/p&gt; 
&lt;p&gt;In practice, simply identifying the oldest compressor is insufficient, as age is often not an ideal predictor of equipment degradation. For example, a compressor train operating in a cold, dry arctic environment degrades very differently than a compressor train operating in a hot sandy desert despite being the same age. Therefore, we need an approach that combines a physics-based understanding of the efficiency degradation with an ML model that can adjust the predictions based on the environmental and operational history of the compressor&lt;/p&gt; 
&lt;p&gt;The exact features to use for this type of adaptation depends on the physical process. In our approach, we recognize from the thermodynamics that the compressor exit pressure (P2) is a function of the temperature ratio across the compressor (T2/T1). The temperature ratio increases as the compressor degrades (for a specific pressure ratio) from physics. By incorporating this crucial piece of physics as a feature into our predictor-corrector approach (described in detail in the prior post), we’re able to improve the prediction to incorporate the compressor efficiency degradation.&lt;/p&gt; 
&lt;p&gt;We also must perform an additional step to ensure that our ML corrector model is not extrapolating beyond its training data set. Figure 2 displays the workflow for this approach for a single stage compressor. As before, the predictor model is the physics-based thermodynamic model for an ideal compressor and this time the ML corrector model is trained including the temperature ratio (T2/T1) as a feature.&lt;/p&gt; 
&lt;div id="attachment_3008" style="width: 790px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3008" loading="lazy" class="size-full wp-image-3008" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure2.jpg" alt="Figure 2: Workflow of a self-calibrating virtual sensor based on fleet data." width="780" height="260"&gt;
 &lt;p id="caption-attachment-3008" class="wp-caption-text"&gt;Figure 2: Workflow of a self-calibrating virtual sensor based on fleet data.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For a production model, we first perform a statistical check to calculate the probability that the model is not extrapolating beyond its training data set (Input Data Deviation Detector Model). We use a predictor-corrector model, as described in &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-a-level-3-digital-twin-virtual-sensor-with-ansys-on-aws/"&gt;the prior post&lt;/a&gt;, if the model is not extrapolating. However, this time the ML corrector model includes the critical feature of temperature ratio. If we find the model is extrapolating, we trigger a model re-training by looking to the fleet for more data that covers the range of input data our compressor experiences.&lt;/p&gt; 
&lt;p&gt;As a first pass, we use data from the fully instrumented fleet leader, but with multiple fully instrumented compressors, we employ the same statistical algorithm to identify the fleet compressor with the highest probability of overlapping data. If there is no other available compressor with sufficient overlapping data, we employ a different approach of using an L4 self-calibrating virtual sensor, which we will describe in a future post.&lt;/p&gt; 
&lt;p&gt;Figure 3 shows the P2 prediction from the L3 virtual sensor model trained using the first 200 days of operation of the fully instrumented fleet leader compressor (green line). Note that this model is plotted against temperature ratio as we know it to be a better proxy for compressor performance compared to using age. For validation purposes, the orange circles show the measured P2 from our compressor during its first 200 days of operation. Normally, the orange circle data would not be available, as we would be relying on the green curve as the P2 virtual sensor prediction. As expected, we see good agreement between the measured values (orange circles) and the L3 virtual sensor.&lt;/p&gt; 
&lt;div id="attachment_3009" style="width: 634px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3009" loading="lazy" class="wp-image-3009 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure3.jpg" alt="Figure 3: Graph comparing L3 DT Fleet-Trained virtual sensor prediction versus sensor data for first 200 days of operation." width="624" height="164"&gt;
 &lt;p id="caption-attachment-3009" class="wp-caption-text"&gt;Figure 3: Graph comparing L3 DT Fleet-Trained virtual sensor prediction versus sensor data for first 200 days of operation.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;However, as the compressor degrades, a higher temperature ratio doesn’t necessarily indicate higher pressure. Figure 4 shows the complete data set (0 to 1500 days of operation) from Figure 3 as our compressor efficiency degrades.&lt;/p&gt; 
&lt;div id="attachment_3010" style="width: 790px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3010" loading="lazy" class="wp-image-3010 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure4.jpg" alt="Figure 4: Graph comparing L3 DT Fleet-Trained virtual sensor predictions versus sensor data for first 200 days and 500 days of operation." width="780" height="202"&gt;
 &lt;p id="caption-attachment-3010" class="wp-caption-text"&gt;Figure 4: Graph comparing L3 DT Fleet-Trained virtual sensor predictions versus sensor data for first 200 days and 500 days of operation.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The new compressor data (&amp;lt;200 days of operation, red circles) is separated from the rest of the data (0 days to 1500 days of operation, black circles). In this case, we trained the ML corrector model using data from the existing fleet leader compressor that has been operating for several years (up to a temperature ratio of ~1.26). We can visually see how the improved predictor-corrector model is able to predict the compressor exit pressure up to a compressor temperature ratio of ~1.26. Beyond that, we see the prediction again deviates from the measured values indicating the ML corrector model is now extrapolating and should be retrained with a different data set. By this time, the fleet leader compressor will have likely aged and degraded further, so its dataset would presumably be suitable for training for our virtual sensor.&lt;/p&gt; 
&lt;h2&gt;Identifying when the ML corrector model is extrapolating&lt;/h2&gt; 
&lt;p&gt;The workflow in Figure 2 describes a crucial step of determining when the fleet-trained L3 Digital Twin model is extrapolating and needs to be retrained. For this step, we developed a function that combines &lt;a href="https://en.wikipedia.org/wiki/Mixture_model"&gt;Gaussian Mixture Models&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration"&gt;Monte Carlo Integration&lt;/a&gt; to determine the probability that new sensor data collected by our sensors is supported by the training data set. With applying this method, we generate a training validity plot shown in Figure 5a, which shows the probability the sensor data is within the range of the training data (the first 200 days of the fleet-leader compressor). As expected, for less than 200 days, the measured sensor data exhibits a high probability of falling within the valid calibration range but then decreases over time. At approximately 200 days, the probability drops to near 0, indicating that the incoming sensor data deviated from the training set. We see this visually in Figure 5b where there is a strong overlap between the training data and the incoming sensor data for less than 100 days of operation. Conversely, Figure 5c shows poor overlap between the training data and the incoming sensor data for up to 500 days of operation.&lt;/p&gt; 
&lt;div id="attachment_3011" style="width: 1429px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3011" loading="lazy" class="wp-image-3011 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure5.jpg" alt="Figure 5: a) Probability new sensor data is supported by the training data using the first 200 days of data. b) Incoming sensor data overlaps the calibration data at 100 days. c) incoming sensor data does not overlap the calibration data at 500 days." width="1419" height="406"&gt;
 &lt;p id="caption-attachment-3011" class="wp-caption-text"&gt;Figure 5: a) Probability new sensor data is supported by the training data using the first 200 days of data. b) Incoming sensor data overlaps the calibration data at 100 days. c) incoming sensor data does not overlap the calibration data at 500 days.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;This behavior corresponds to Figure 1, where the predicted P2 values begin to deviate from the measured P2 values. Specifically, the uncertainty bands around the predicted and measured P2 no longer overlap. The Input Data Deviation model is not attempting to predict P2 or any other variable, rather it identifies when the incoming sensor data is different from the original training data.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deploying fleet trained L3 virtual sensor on AWS&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To build and deploy our L3 digital twins, we use our &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;AWS TwinFlow framework&lt;/a&gt;. TwinFlow deploys and orchestrates predictive models at scale across a distributed computing architecture. It also probabilistically updates model parameters using real-world data and calculates prediction uncertainties while maintaining a fully auditable history.&lt;/p&gt; 
&lt;p&gt;In our prior post, we used TwinFlow to deploy an L3 virtual sensor when new data is added to an &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service (Amazon S3)&lt;/a&gt; bucket specifying the existence of a new virtual sensor. The workflow for the prior L3 virtual sensor is shown in the “Auto-Deployment Fleet Size” and Virtual Sensors for each component” boxes in Figure 6.&lt;/p&gt; 
&lt;div id="attachment_3012" style="width: 985px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3012" loading="lazy" class="wp-image-3012 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure6.jpg" alt="Figure 6: TwinFlow workflow enabling digital twins that auto-deploy, self-calibrate, and perform IoT sensor data deviation checks." width="975" height="879"&gt;
 &lt;p id="caption-attachment-3012" class="wp-caption-text"&gt;Figure 6: TwinFlow workflow enabling digital twins that auto-deploy, self-calibrate, and perform IoT sensor data deviation checks.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The Auto-Deployment box represents the workflow for auto-deployment, and the Virtual Sensors box for the initial model training. One key difference between the prior L3 virtual sensor and the present L3 fleet-trained virtual sensor is the inclusion of temperature ratio in the training feature set.&lt;/p&gt; 
&lt;p&gt;For the present implementation of the L3 fleet-trained virtual sensor, we use TwinFlow to add the additional workflows for model validity check (IoT Deviation Check box) and ML corrector model retraining (Self-Calibration box) layers as depicted in Figure 6. The model validity check runs the input data deviation detection model and displays an alarm in the AWS IoT TwinMaker dashboard when we are extrapolating beyond the training data set of the ML corrector model. If the model is found to be extrapolating, additional data is obtained from the fleet leader compressor, and the ML corrector model is retrained. If the newly trained model is still extrapolating, we adopt a more advanced approach discussed in our next post.&lt;/p&gt; 
&lt;p&gt;Figure 7 shows the AWS architecture supporting this workflow. In step 1, you download and install TwinFlow from &lt;a href="https://github.com/aws-samples/twinflow"&gt;GitHub&lt;/a&gt;. From here (steps 2 and 3), you can customize the virtual sensor TwinFlow pipelines and embed their digital twins in containers that are stored in &lt;a href="https://aws.amazon.com/ecr/"&gt;Amazon Elastic Container Registry (Amazon ECR)&lt;/a&gt;. At step 4, calibration data can be cost effectively archived in an S3 Bucket. Live sensor data is streamed into &lt;a href="https://aws.amazon.com/timestream/"&gt;Amazon Timestream&lt;/a&gt;, which is a serverless database that supports real-time data. At step 5, an &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; compute environment scales to meet your virtual sensors needs and optimally selects &lt;a href="https://aws.amazon.com/pm/ec2/"&gt;Amazon Elastic Compute Cloud (Amazon EC2)&lt;/a&gt; instances based on your requirements. &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; supports both CPU and GPU requirements, and it elastically scales up or down based on load. In addition, all container output logs are automatically streamed to &lt;a href="https://aws.amazon.com/cloudwatch/"&gt;Amazon CloudWatch&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The TwinFlow graph orchestrator resides in an Amazon EC2 instance and offloads tasks to either &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; functions (which are serverless and perform short specific tasks) or directly to EC2 instances in AWS Batch. At step 6, a training task is run on Batch that will check for new calibration data in an Amazon S3 data lake and initiate ML training in AWS Batch if new data exists. A Lambda function (step 7) automatically deploys new virtual sensors or removes unused virtual sensors in AWS Batch after checking the IoT sensor database.&lt;/p&gt; 
&lt;p&gt;AWS IoT TwinMaker (step 8) shows the digital twin predictions, incoming sensor data, and 3D assets. For operational excellence, at step 9 and 10 additional diagnostic logging and archiving is performed. Since we could potentially use a large number of virtual sensors, we recommend using a Lambda function to archive logs from CloudWatch to an S3 bucket. Lastly, &lt;a href="https://aws.amazon.com/neptune/"&gt;Amazon Neptune&lt;/a&gt; is used with TwinFlow to log each task execution on a graph, which also records the relationship between parent and children tasks enabling root cause analysis if an error occurs in the future.&lt;/p&gt; 
&lt;div id="attachment_3013" style="width: 634px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3013" loading="lazy" class="size-full wp-image-3013" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure7.jpg" alt="Figure 7: AWS architecture for virtual sensors." width="624" height="364"&gt;
 &lt;p id="caption-attachment-3013" class="wp-caption-text"&gt;Figure 7: AWS architecture for virtual sensors.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we took an L3 digital twin virtual sensor workflow and added a fleet-calibration branch. We used data from the other fleet compressors to avoid having our virtual sensor extrapolate beyond its training data set. This L3 fleet-trained virtual sensor application is practical for operators running equipment fleets, like wind farms, compressor trains, oil wells, and other industrial applications. If you found this post interesting, then you should also read about &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/"&gt;a more advanced approach on deploying Level 4 digital twin self-calibrating virtual sensors&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to request a proof of concept or if you have feedback on the AWS tools, please reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Some of the content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/matt-adams-ansys-profile.png" alt="Matt Adams" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Matt Adams&lt;/h3&gt; 
  &lt;p&gt;Matt has 11 years of experience in developing machine learning methods to enhance physics-based simulations. Matt is a Lead Product Specialist at Ansys where he leads the development of software tools for building hybrid digital twins.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Accelerating green-urban planning simulations with AWS Batch</title>
		<link>https://aws.amazon.com/blogs/hpc/accelerating-green-urban-planning-simulations-with-aws-batch/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Wed, 01 Nov 2023 17:44:48 +0000</pubDate>
				<category><![CDATA[Amazon EC2 Container Registry]]></category>
		<category><![CDATA[Amazon EC2 Container Service]]></category>
		<category><![CDATA[Amazon Elastic Container Service]]></category>
		<category><![CDATA[AWS Fargate]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[HPC]]></category>
		<guid isPermaLink="false">1df35da35e864e989a1ccd4ad78a766bee85fee9</guid>

					<description>In this blog post, we’ll explore how Green Urban Scenarios simulator (GUS) helps urban planners explore the impact of green infrastructure on the urban environment using digital twins and simulations scaled using AWS Batch.</description>
										<content:encoded>&lt;p&gt;Despite the increasing risk of environmental threats to cities across the world, and the critical role the natural world plays in protecting cities and their residents from climate extremes, effective green urban planning receives little representation in city-planning and infrastructure decisions.&lt;/p&gt; 
&lt;p&gt;We know that urban forests [1][2] have a &lt;a href="https://www.researchgate.net/profile/David-Nowak-6/publication/312470680_Understanding_the_benefits_and_costs_of_urban_forest_ecosystems/links/5bf543bf299bf1124fe26670/Understanding-the-benefits-and-costs-of-urban-forest-ecosystems.pdf"&gt;wide range&lt;/a&gt; of economic and social benefits, and are one of the most effective ways to reduce climate risk in cities.&lt;/p&gt; 
&lt;p&gt;In this blog post, we’ll explore how the Netherlands-based organization &lt;em&gt;Lucidminds AI&lt;/em&gt; is addressing this challenge through their Green Urban Scenarios simulator (GUS). The team initially built GUS to power the TreesAI project, a collaboration between Lucidminds and Dark Matter Labs. It’s a tool built on AWS so that urban planners, researchers, green portfolio managers, and others can explore the impact of green infrastructure on the urban environment through the power of digital twins and numerical simulations scaled using AWS Batch.&lt;/p&gt; 
&lt;h2&gt;Trees – a natural city defender&lt;/h2&gt; 
&lt;p&gt;Trees are natural carbon sinks. As trees grow, they absorb the greenhouse gas CO2 and sequester carbon in their fibers until an event like combustion or decomposition releases the carbon back into the atmosphere as CO2. Even if a tree is harvested for wood, the carbon remains locked away and out of the atmosphere. Trees provide essential habitat for wildlife like birds and their canopies provide shade and shelter which reduces the amount of energy needed to cool buildings. Their roots help prevent soil erosion, and they improve air quality by removing and breaking down pollutants such as sulfur dioxide.&lt;/p&gt; 
&lt;p&gt;In short, planting trees is one of the most effective ways to combat climate change and promote biodiversity in cities. But urban planners and decision makers need tools to understand what the best strategies are for building or strengthening their urban forests, such as where trees should be placed, what types of trees to plant, and the optimal level of tree maintenance required. Critically, it’s also important to understand the impacts of proposed urban forest plans, in particular the estimated amount of carbon sequestered under different scenarios.&lt;/p&gt; 
&lt;p&gt;Nature-based solutions that use digital technologies, like Green Urban Scenarios (GUS), are making it easier for cities to understand how trees can benefit their communities. With a better understanding of how trees reduce emissions, governments and businesses can make more informed decisions about which trees to plant where, and how-to best care for them.&lt;/p&gt; 
&lt;h2&gt;How the “Green Urban Simulator” works&lt;/h2&gt; 
&lt;p&gt;To accurately simulate hypothetical green urban planning projects for a city, the simulations must reflect the unique characteristics of the city in question. Every city has its own built environment and geography that defines its associated urban forest.&lt;/p&gt; 
&lt;div id="attachment_3024" style="width: 825px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3024" loading="lazy" class="size-full wp-image-3024" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.21.58.png" alt="Figure 1 - Green Urban Scenarios simulator (GUS) is a tool built on AWS so that urban planners, researchers, green portfolio managers, and others can explore the impact of green infrastructure on the urban environment." width="815" height="495"&gt;
 &lt;p id="caption-attachment-3024" class="wp-caption-text"&gt;Figure 1 – Green Urban Scenarios simulator (GUS) is a tool built on AWS so that urban planners, researchers, green portfolio managers, and others can explore the impact of green infrastructure on the urban environment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For simulations to be useful, they must be initialized with data that reflects the real environment. Therefore, the first step in the process of green urban simulation is to capture and digitize the unique aspects of a city’s urban forest, namely the spatial coordinates, species, diameter, height, and current condition of individual trees across a city.&lt;/p&gt; 
&lt;p&gt;Lucidmind’s team uses a combination of available datasets including satellite imagery, street view images, and field surveys, to build digital urban forests. This data is collected into a CSV file where each line represents a single tree and columns that correspond to the tree characteristics we described. The CSV file is then fed into the simulation modules to initialize the computational domain of a green urban simulation.&lt;/p&gt; 
&lt;div id="attachment_3025" style="width: 827px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3025" loading="lazy" class="size-full wp-image-3025" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.22.39.png" alt="Figure 2 - Characteristics of Urban Forests Complex Systems consider specificity, heterogeneity and dynamic interaction between trees and their environment" width="817" height="487"&gt;
 &lt;p id="caption-attachment-3025" class="wp-caption-text"&gt;Figure 2 – Characteristics of Urban Forests Complex Systems consider specificity, heterogeneity and dynamic interaction between trees and their environment&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To illustrate the capabilities of GUS, the Lucidminds team performed simulations of the city of Amsterdam, the Dutch capital, that incorporated over 250,000 individual trees. That is just the trees currently growing in Amsterdam. One of the powerful features of simulations is the ability to examine hypothetical scenarios – such as the city of Amsterdam planting another 100,000 trees &amp;nbsp;– to understand how their urban forest might evolve and sequester carbon over the next 30 years.&lt;/p&gt; 
&lt;p&gt;There’s an endless number of possible arrangements for trees that can be simulated for any city in the world. Of course, as the number of trees in any simulation grows, so can the computational expense required to complete the simulation. To ensure that their simulations could scale along with the simulation ambitions, the Lucidminds looked to the power of AWS’s massive computing capacity and HPC services like AWS Batch.&lt;/p&gt; 
&lt;h2&gt;Running GUS on AWS&lt;/h2&gt; 
&lt;p&gt;The GUS engine is written in Python and is available as an &lt;a href="https://pypi.org/project/pygus/2.0.4/"&gt;open source Python package called ‘pygus’&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Like many simulations, there are phenomena represented in the simulations that behave probabilistically, such as the chance on any individual simulated day that a storm materializes resulting in precipitation that impacts some or all the trees in the city.&lt;/p&gt; 
&lt;p&gt;If you’re not familiar with simulations with probabilistic mechanisms, the important thing to understand is that if probabilities are embedded in a simulation engine, a user can run the exact same simulation code with the exact same simulation inputs (like tree data) and the two simulations will almost certainly produce different results. This is a perfectly acceptable outcome, one that comes with the territory of simulating scenarios where we can’t know for sure every single sub-event of a simulation, but we can assign a probability distribution to the occurrence of those events and draw from that distribution during the simulation.&lt;/p&gt; 
&lt;p&gt;The drawback to incorporating probabilities in simulations, however, is that it becomes necessary to run the simulations many times over to understand all the different possible outcomes. This mathematical technique is called &lt;a href="https://aws.amazon.com/what-is/monte-carlo-simulation/"&gt;Monte Carlo simulation&lt;/a&gt; and is widely used in simulations across the fields of physics, finance, biology, and engineering, to name a few. Depending on the complexity of the simulation and the number of probabilistic variables/events, hundreds or thousands of simulations might be necessary to understand the emerging behavior. And that’s just for one scenario or set of input parameters. If you want to simulate a different scenario, say where you replace a few dozen trees near the city center with trees of a different species, then you might need to run those thousands of simulations again.&lt;/p&gt; 
&lt;p&gt;If a single simulation iteration (a term used in this context to describe just one run of a simulation) takes a fraction of a second, then it’s not a big deal to run a thousand simulations one after the other to get an ensemble. That would only take a few minutes and you’d have a robust set of results to examine. Many simulations, however, take much longer to run. For instance, a single GUS iteration for Amsterdam’s 250k trees takes a little over an hour (~75 minutes) on a typical laptop or equivalent EC2 instance and the number of iterations needed to confidently simulate a single scenario is around a thousand.&lt;/p&gt; 
&lt;p&gt;Running each iteration back-to-back on a single laptop or a similar EC2 instance would take about 2 months, which is obviously not a feasible time frame to make these simulations useful for decision makers, especially considering that there are likely dozens or hundreds of scenarios that should be simulated when developing an urban planning project.&lt;/p&gt; 
&lt;p&gt;The solution to this problem is to embrace the highly parallel nature of Monte Carlo-based simulation. For the 1,000-simulation set described above, each iteration is independent of the other 999, meaning that we can split up the 1,000 simulations, run them separately and even at the same time, and then combine the results at the end. So, in theory we could spin up 1,000 EC2 instances, run our simulation code with the same inputs on each instance, dump the results to a shared storage location (say, an Amazon S3 bucket), and complete all of that in roughly the same 75 minutes that it would take to run a single simulation iteration.&lt;/p&gt; 
&lt;p&gt;But how do we go about orchestrating all those compute instances and getting our simulation code on each one? Enter AWS Batch.&lt;/p&gt; 
&lt;p&gt;AWS Batch is a fully managed batch computing service that plans, schedules, and runs containerized batches or machine learning (ML) workloads across the full range of AWS compute offerings, such as&amp;nbsp;Amazon ECS,&amp;nbsp;Amazon EKS,&amp;nbsp;AWS Fargate, using Spot or On-Demand Amazon EC2 instances.&lt;/p&gt; 
&lt;p&gt;Batch is a great solution for performing Monte Carlo solutions because it handles a lot of the undifferentiated heavy lifting of setting up the compute resources needed to run simulations leaving you more time to focus more on developing the &lt;em&gt;actual simulation&lt;/em&gt; code and analyzing it’s results.&lt;/p&gt; 
&lt;p&gt;To run a GUS of Amsterdam, Lucidminds took simplicity a step further by using AWS Fargate in combination with AWS Batch. AWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers or choosing instance types. With Fargate you simply package your application in containers, specify the CPU and memory requirements, define networking and IAM policies, and launch the application. Fargate then launches and scales the compute to closely match the resource requirements that you specify for the container.&lt;/p&gt; 
&lt;p&gt;To illustrate what running Monte Carlo simulations using AWS Batch looks like in practice, let’s look at the steps that Lucidminds took to get their Python-based simulation application running at scale using AWS resources. We’ll break down the steps into two distinct phases: 1) containerizing and testing the simulation application and; 2) scaling the simulation application using AWS Batch.&lt;/p&gt; 
&lt;p&gt;As a helpful guide to understand how the various AWS services used in this post (simulation application and scaling) fit together, refer to the reference architecture diagram in Figure 3.&lt;/p&gt; 
&lt;div id="attachment_3026" style="width: 857px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3026" loading="lazy" class="size-full wp-image-3026" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.24.02.png" alt="Figure 3 - Reference architecture diagram for running Monte Carlo simulations." width="847" height="632"&gt;
 &lt;p id="caption-attachment-3026" class="wp-caption-text"&gt;Figure 3 – Reference architecture diagram for running Monte Carlo simulations.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Phase 1: containerize, set up, and test simulation application on Amazon Elastic Container Service (Amazon ECS)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Containerize the simulation application&lt;/strong&gt; – AWS Batch runs jobs using Docker container images. In this post we’re not going to cover how to containerize applications, just mention that is a part of the process. If you’re new to containers, check out this quick primer on the subject: &lt;a href="https://aws.amazon.com/what-is/containerization/"&gt;AWS: What is Containerization?&lt;/a&gt; You can containerize applications using Docker technology on your local machine or using AWS services like AWS Cloud9, depending on where you are developing your applications. Lucidminds built and tested their containers on their local machines.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Push container image to Amazon Elastic Container Registry (Amazon ECR) – &lt;/strong&gt;Once you have a containerized application, we need to get it to an image repository. Lucidminds used ECR, which is a fully managed container registry offering high-performance hosting, so you can reliably deploy application images and artifacts anywhere. The first step is to &lt;a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/repository-create.html"&gt;create an image repository&lt;/a&gt;, which can be done via the AWS Command Line Interface (CLI) or using the AWS console. If you’re unsure about what commands are needed to tag and push images to an ECR repository, simply create a repository through the AWS console, navigate to the repository and click &lt;strong&gt;View push commands&lt;/strong&gt; to view the steps to push an image to your new repository.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_3027" style="width: 854px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3027" loading="lazy" class="size-full wp-image-3027" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.25.58.png" alt="Figure 4 - Creating an Image repository for your containerized application on Amazon Elastic Container Registry" width="844" height="277"&gt;
 &lt;p id="caption-attachment-3027" class="wp-caption-text"&gt;Figure 4 – Creating an Image repository for your containerized application on Amazon Elastic Container Registry&lt;/p&gt;
&lt;/div&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;strong&gt;Set up databases that connect to the simulation container&lt;/strong&gt; – The GUS application connects to two databases to store data needed during the simulations (Amazon MemoryDB for Redis) and outputs of simulation runs for caching purposes (Amazon RDS for PostgreSQL). Caching simulation outputs allows Lucidminds to reduce their need for compute when users enter a set of simulation parameters that have already been computed in the past. Not all simulations require a database connection and it is not a requirement for running Monte Carlo simulations using AWS Batch, that’s just how the Green Urban Simulator application works. GUS also writes simulation results to an S3 bucket. It’s strongly recommended that container instances are launched within a Virtual Private Cloud (VPC), and required when using managed compute instances (discussed later), so for the simulation containers to output results to S3 buckets, you’ll need a VPC endpoint. For more information, see &lt;a href="https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"&gt;Gateway endpoints for Amazon S3&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test application on Amazon Elastic Container Service (ECS) cluster&lt;/strong&gt; – Make sure to test that the application is working as anticipated before trying to scale. Lucidminds tested their simulation application using an ECS cluster, to ensure that the container could be loaded and run, and that the application can successfully connect to and read/write results to the associated data stores. Once GUS was performing sas expected, it was time to get running with AWS Batch.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Phase 2: set up and run simulations using AWS Batch&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Create a compute environment (CE) &lt;/strong&gt;– A compute environment is a collection of compute resources used to run batch jobs. Before you can run jobs in AWS Batch, you need a CE. You can set up a managed CE which is managed by AWS, or an unmanaged CE that you manage yourself. You can configure managed CEs to use Fargate or On-demand Amazon EC2 instances. For each of those options you can choose to use Spot Instances at a deep discount, however they can be stopped suddenly and restarted at any time with a 2-minute warning (potentially from a checkpoint). If you’re not familiar with Spot Instances, check out the &lt;a href="https://aws.amazon.com/ec2/spot/"&gt;Amazon EC2 Spot Instances product page&lt;/a&gt;. For the simulations of Amsterdam, Lucidminds chose to use a managed CE, configured with AWS Fargate. During setup of a Fargate-configured CE, you choose a name for the environment, select whether you want to run with Spot Instances, set the maximum number of vCPUs the environment can use, and set up the network configuration by choosing (or creating) a VPC, subnets into which resources are launched, and the security groups to be associated with the launched instances.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create a job queue, associate to CE &lt;/strong&gt;– Jobs queues are the place that submitted jobs sit until they are scheduled to run and are associated with CEs. You can set up multiple job queues and set a priority order to determine what jobs get run first in an environment where multiple types of jobs are being submitted. For example, you can create a queue that uses Amazon EC2 On-Demand instances for high priority jobs and another queue that uses Amazon EC2 Spot Instances for low-priority jobs. For additional details on setting up job queues, see the &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/job_queues.html"&gt;AWS Batch job queues&lt;/a&gt;&lt;u&gt; documentation&lt;/u&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create and register a job definition&lt;/strong&gt; – Job definitions describe the job to be executed, including parameters, environment variables, and compute requirements. Job definitions are how you tell AWS Batch the location of the container image you’re running, the number of vCPUs and amount of memory the container should use with the container, IAM roles the job might need, and any commands the container should run when starting. Once you create a job definition, it can be reused or shared by multiple jobs. In the Lucidminds case for Amsterdam, the GUS container used 1 vCPU (it’s a single-threaded application) and 2 GB per container.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Submit an AWS Batch array job&lt;/strong&gt; – Now that all AWS Batch resources are set up, we’re ready to run simulations. You can submit jobs using the AWS Console or the AWS CLI. When submitting a job, you select the CE, job queue, and job definition. If needed, you can override many of the parameters specified in the job definition at runtime. One additional parameter we can set that is particularly useful in the case of the GUS (and Monte Carlo simulations in general) is: &lt;strong&gt;Array Size&lt;/strong&gt;. This parameter can be set between 2 and 10,000 and is used to run &lt;em&gt;array jobs&lt;/em&gt;: jobs that share common parameters, like the job definition, vCPUs, and memory. These jobs run as a collection of related, yet separate, basic jobs that might be distributed across multiple hosts and might run concurrently. Array jobs are the most efficient way to run extremely-parallel jobs like Monte Carlo simulations or parametric sweeps.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;As we mentioned earlier, running an ensemble of simulations for a green urban scenario does not require any simulation input parameters to be different, because the inherent probabilistic nature of the simulations provides different results for the same set of inputs. If you have an application or simulation to which you’d like to pass a sequence of different input parameters, such as for parameter sweeps or grid search cases, you can use the AWS_BATCH_JOB_ARRAY_INDEX environment variable to differentiate the child jobs. For a quick, simple, and yet highly-illustrative tutorial that demonstrates this concept, see &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/array_index_example.html"&gt;Tutorial: Using the array job index to control job differentiation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Once you submit your job, you can track it on the AWS Batch dashboard. In Figure 5, we’ve shown a screenshot of an example array job with 100 child jobs submitted and mid-run. As the jobs get scheduled and processed, they move across the dashboard from &lt;strong&gt;Submitted&lt;/strong&gt; to either &lt;strong&gt;Succeeded&lt;/strong&gt; or &lt;strong&gt;Failed&lt;/strong&gt;.&lt;/p&gt; 
&lt;div id="attachment_3028" style="width: 854px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3028" loading="lazy" class="size-full wp-image-3028" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.27.20.png" alt="Figure 5 - Reviewing results from induvidual jobs on Amazon CloudWatch" width="844" height="363"&gt;
 &lt;p id="caption-attachment-3028" class="wp-caption-text"&gt;Figure 5 – Reviewing results from induvidual jobs on Amazon CloudWatch&lt;/p&gt;
&lt;/div&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;&lt;strong&gt;Consolidate and view results&lt;/strong&gt; – Once the job completes, it’s time to review the results. You can navigate into individual jobs and view the logs via CloudWatch. In the array job screenshot shown in Figure 4, you’d get there by clicking one of the hyperlinked numbers in the column &lt;strong&gt;Job Index&lt;/strong&gt; and by clicking the link under &lt;strong&gt;Log stream name&lt;/strong&gt;. If your application writes results to an Amazon S3 bucket as the GUS does, you might need to include an extra step where the results are consolidated to make for easier analysis.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;GUS in action&lt;/h2&gt; 
&lt;p&gt;Lucidmind’s pilot project with the City of Glasgow, aimed at supporting the city’s climate targets by growing canopy cover in deprived neighborhoods, has yielded promising results. Soon the city of Stuttgart (in Germany) will adopt the GUS Framework. With GUS, these cities can run city-scale simulations and explore multiple scenarios to identify optimal projects that align with their net-zero climate targets. By leveraging the GUS framework, they can harness the numerous benefits of trees, including carbon sequestration and storage, storm-water retention, and mitigating heatwave effects.&lt;/p&gt; 
&lt;p&gt;The GUS framework, consisting of a set of meticulously designed microservices, serves as a powerful tool for various organizations worldwide. VAIV, a South Korean company specializing in AI and big data solutions, will utilize the GUS API to create digital twins of their projects, specifically leveraging the heatwave effects module to address their unique use-case. This demonstrates the versatility and adaptability of the GUS framework, catering to diverse requirements across different regions and sectors. Stuttgart is another example where GUS is used for long term urban deforestation planning and decision making.&lt;/p&gt; 
&lt;p&gt;A GUS demo is now available for anyone who’d like to run city-scale simulations and explore multiple scenarios, gaining valuable insights into the impact of your decisions on urban environments. You can visit &lt;a href="http://run.greenurbanscenarios.com/"&gt;run.greenurbanscenarios.com/&lt;/a&gt;, set up an example scenario, and click “run on AWS” to run a simulation and view the results. The science behind GUS is explained in a peer-reviewed journal article [3].&lt;/p&gt; 
&lt;div id="attachment_3029" style="width: 767px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3029" loading="lazy" class="size-full wp-image-3029" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.27.55.png" alt="Figure 6 - GUS is available to run online" width="757" height="628"&gt;
 &lt;p id="caption-attachment-3029" class="wp-caption-text"&gt;Figure 6 – GUS is available to run online&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3030" style="width: 851px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3030" loading="lazy" class="size-full wp-image-3030" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.29.25.png" alt="Figure 7 - Insights generated by GUS Simulations ran on AWS HPC. Impact Analysis Dashboard for Carbon, Water retention, Air Quality and Canopy Cover" width="841" height="522"&gt;
 &lt;p id="caption-attachment-3030" class="wp-caption-text"&gt;Figure 7 – Insights generated by GUS Simulations ran on AWS HPC. Impact Analysis Dashboard for Carbon, Water retention, Air Quality and Canopy Cover&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The GUS dashboard’s standardized outputs are generated through Monte Carlo experiments, considering the probabilistic nature of climate and tree growth dynamics influenced by their surroundings.&lt;/p&gt; 
&lt;p&gt;GUS leverages AWS computing services to perform Monte Carlo experiments. In Figure 8, we illustrate a simulation ensemble of fifty different simulations for three different maintenance scenarios (high, medium, and low maintenance). The solid line represents the mean carbon sequestration of the fifty forest growth trajectories. The high-maintenance scenario assumes state-of-the-art pruning, replanting, and disease treatment, while lower maintenance scenarios involve reduced or nonexistent care provisions.&lt;/p&gt; 
&lt;p&gt;These simulation results would tell a decision maker the amount of carbon a proposed green urban project could be expected to sequester over time, and the decision maker could use that information in combination with cost estimates for each maintenance scenario specific to their area to better understand the cost-benefit tradeoffs associated with green urban planning.&lt;/p&gt; 
&lt;div id="attachment_3031" style="width: 854px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3031" loading="lazy" class="size-full wp-image-3031" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.30.16.png" alt="Figure 8 - Monte Carlo simulations of sequestration showing that the higher the tree maintenance, the more carbon will be captured over time." width="844" height="493"&gt;
 &lt;p id="caption-attachment-3031" class="wp-caption-text"&gt;Figure 8 – Monte Carlo simulations of sequestration showing that the higher the tree maintenance, the more carbon will be captured over time.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Urban forests are an important part of the fight against climate change. Digital technologies such as scenario analysis, digital twins, agent-based modeling, and high performance computing on AWS using services like AWS Batch can help us simulate urban forests and examine how different potential green urban projects can have an impact on our cities.&lt;/p&gt; 
&lt;p&gt;In this post, we’ve not only examined why simulation techniques like Lucidmind’s Green Urban Simulator (GUS) are important tools for the future or urban planning and green cities, but also how AWS Batch can be used to scale simulations and drastically reduce the amount of time to get results.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;References&lt;/h3&gt; 
&lt;p&gt;[1] Journal of Arboriculture 18(5): September 1992 227 ASSESSING THE BENEFITS AND COSTS OF THE URBAN FOREST by John F. Dwyer, E. Gregory McPherson, Herbert W. Schroeder, and Rowan A. Rowntree&lt;/p&gt; 
&lt;p&gt;[2] David J. Nowak and John F. Dwyer, Understanding the Benefits and Costs of Urban Forest Ecosystems. &amp;nbsp;&lt;a href="https://www.researchgate.net/publication/321619207_Urban_and_Community_Forestry_in_the_Northeast"&gt;Urban and Community Forestry in the Northeast.&amp;nbsp;&lt;/a&gt;Doi: 10.1007/978-1-4020-4289-8_2.&lt;/p&gt; 
&lt;p&gt;[3] Bulent Ozel and Marko Petrovic. 2023. Green Urban Scenarios: A framework for digital twin representation and simulation for urban forests and their impact analysis, &lt;a href="https://joa.isa-arbor.com/index.asp"&gt;Journal of Arboriculture &amp;amp; Urban Forestry &lt;/a&gt;(Forthcoming).&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-17.40.16.png" alt="Oguzhan Yayla" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Oguzhan Yayla&lt;/h3&gt; 
  &lt;p&gt;Oguzhan Yayla is Co-Founder of Lucidminds, a driving force behind the transition towards regenerative economies. With a strong background as a Complex Systems Engineer and Digital Infrastructure Architect, Oguzhan is dedicated to building the necessary infrastructure and tools that will shape the transition toward a more sustainable, prosperous future and break out of the carbon tunnel vision.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-17.40.20.png" alt="Roni Bulent Ozel" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Roni Bulent Ozel&lt;/h3&gt; 
  &lt;p&gt;Roni Bulent Ozel is the co-founder and CEO of Lucidminds AI, with a double PhD in complex systems, economics, and computer science. He excels in bridging business, technology, science, and policy-making, leading innovations published in peer-reviewed articles, books, patents, and software packages. His mission is to drive systemic change for an equitable planet, benefiting humans, non-humans, and future generations.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-17.40.27.png" alt="Jake Doran" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Jake Doran&lt;/h3&gt; 
  &lt;p&gt;Jake Doran originally studied theoretical physics before transitioning to a career in software. Most recently he works as a researcher and engineer at LucidMinds, hoping to build and promote systems of societal good through technology.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Deploying Level 4 Digital Twin Self-Calibrating Virtual Sensors on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/</link>
		
		<dc:creator><![CDATA[Ross Pivovar]]></dc:creator>
		<pubDate>Mon, 30 Oct 2023 15:04:39 +0000</pubDate>
				<category><![CDATA[AWS IoT TwinMaker]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Internet of Things]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<guid isPermaLink="false">13884cdb21d9f5d41adb23e1c678c023812d8ed6</guid>

					<description>Digital twins can be hard if they deviate from real-world behavior as real systems degrade and change over time. Today we’ll show digital twins that calibrate on operational data, using TwinFlow on AWS.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Ross Pivovar, Solution Architect, Autonomous Computing, and Adam Rasheed, Head of Autonomous Computing at AWS; and Kayla Rossi, Application Engineer, and Orang Vahid, Director of Engineering Services at Maplesoft.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;In a &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-a-level-3-digital-twin-virtual-sensor-with-ansys-on-aws/"&gt;previous post&lt;/a&gt; we shared the common customer use case of deploying &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-a-level-3-digital-twin-virtual-sensor-with-ansys-on-aws/"&gt;Level 3 Digital Twin (L3 DT) virtual sensors&lt;/a&gt; to help operators make more informed decisions in situations where using physical sensors is difficult, expensive, or impractical. One of the challenges is that L3 DT virtual sensors use pre-trained models that deviate from real-world behavior as the physical system degrades and changes over time. Operators then become hesitant to base operational decisions solely on the L3 DT virtual sensor predictions.&lt;/p&gt; 
&lt;p&gt;Today we’ll describe how to build and deploy L4 DT self-calibrating virtual sensors where operational data is used to automatically calibrate the virtual sensor. This automated self-calibration allows the virtual sensor predictions to adapt and more closely match the real-world, allowing the operators to take proactive corrective actions to prevent failures and optimize performance. For more discussion on the different levels for digital twins, check out &lt;a href="https://aws.amazon.com/blogs/iot/digital-twins-on-aws-unlocking-business-value-and-outcomes/"&gt;our previous post describing the AWS L1-L4 Digital Twin framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In this post, we use a&lt;a href="https://en.wikipedia.org/wiki/Modelica"&gt; Modelica&lt;/a&gt;-based model created using &lt;a href="https://www.maplesoft.com/products/maplesim/"&gt;Maplesoft’s MapleSim simulation and engineering design software&lt;/a&gt;. MapleSim provides the tools to create engineering simulation models of machine equipment and exports them as Functional Mockup Units (FMUs) which is an industry standard file format for simulation models. We then use &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow&lt;/a&gt; to deploy the model on AWS and calibrate the FMU using probabilistic Bayesian estimation techniques to statistically infer the unmeasured model coefficients – using the incoming sensor data to calibrate against. TwinFlow is an AWS open-source framework for building and deploying predictive models at scale.&lt;/p&gt; 
&lt;h2&gt;Roll-to-roll manufacturing&lt;/h2&gt; 
&lt;p&gt;For our use case, we’ll consider the web-handling process in roll-to-roll manufacturing (depicted in Figure 1) used for continuous materials such as paper, film, and textiles. The web-handling process involves unwinding the material from a spool, guiding it through various treatments such as printing or coating, and then winding it onto individual rolls. Precise control of tension, alignment, and speed is essential to ensure smooth processing and maintain product quality.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Web Line Perspective 2023 07 20" width="500" height="281" src="https://www.youtube-nocookie.com/embed/gouyAMuY6fs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;div class="wp-caption aligncenter" style="width: 945px"&gt; 
 &lt;p id="caption-attachment-2964" class="wp-caption-text"&gt;Figure 1: A short movie showing the dynamics of the web material handling process in roll to roll manufacturing. The web material is the sheet passing through the rollers which control the tension and speed of the manufacturing process. The driven rollers set the web speed and tension.&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;There are many different failure mechanisms, however for this post, we’ll focus on two failure modes: &lt;strong&gt;high tension failures&lt;/strong&gt; and &lt;strong&gt;slip failures&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Figure 2 shows a schematic diagram of the web-handling equipment with the material spans between the rollers labeled from S1 through S12. Figure 2b shows a screenshot of the MapleSim web-handling simulation model of the roll-to-roll manufacturing line.&lt;/p&gt; 
&lt;div id="attachment_2964" style="width: 945px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2964" loading="lazy" class="size-full wp-image-2964" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure2.jpg" alt="Figure 2: a) Schematic diagram of the web handling equipment in which each span is labeled, and b) Screenshot of the MapleSim simulation model of the web handling equipment." width="935" height="560"&gt;
 &lt;p id="caption-attachment-2964" class="wp-caption-text"&gt;Figure 2: a) Schematic diagram of the web handling equipment in which each span is labeled, and b) Screenshot of the MapleSim simulation model of the web handling equipment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Referring to Figure 2, tension failures occur when the tension within the material at a particular span exceeds a threshold (175 Newtons in this example) resulting in web deformities like wrinkling and troughing. Slip failures are more subtle and occur when the relative movement between the web material and rollers isn’t in sync, causing the web to be dragged across the roller. Slip failures are quantified by measuring the slip velocity which is the difference between the linear velocity of the web material and the tangential velocity of the roller. We consider it a slip failure when the slip velocity exceeds a threshold (0.0001 m/s in this case). It can result in difficult-to-detect defects, misalignment, and reduced product quality if not controlled.&lt;/p&gt; 
&lt;p&gt;Tension and slip are measurable variables, but it’s expensive and intrusive to add these sensors at every location along the manufacturing line. It’s more common to have a limited number of these sensors at key locations while measuring the angular velocity (rotation speed, often colloquially referred to as RPM) of each of the rollers and relying on best practices to control the manufacturing process. This results in sub-optimal process control and relies heavily on operator experience. A digital twin of this process can be used to calculate the tension and slip velocity when combined with the proper IoT data.&lt;/p&gt; 
&lt;p&gt;Both failure mechanisms are the direct result of the rollers becoming more difficult to turn because of dirt and grime fouling the bearings, increasing the viscous friction. In our example, the viscous friction of the rollers is accounted for in the MapleSim model. The model solves the equations of motion, but requires estimates of the viscous friction coefficients to use for the rollers in the model. Viscous friction (damping) is not a directly measurable variable but can be inferred by calibrating the model using the measured angular velocities.&lt;/p&gt; 
&lt;p&gt;In essence, we’re seeking to infer the viscous friction coefficient by leveraging statistics, the FMU model, and our probabilistic Bayesian estimation methods.&lt;/p&gt; 
&lt;p&gt;We use the calibrated L4 Digital Twin to create self-calibrating virtual sensors of tension and slip velocity. These L4 DT self-calibrating virtual sensors can then be used by operators to support operational decisions to control the manufacturing process.&lt;/p&gt; 
&lt;h2&gt;Example failure scenario&lt;/h2&gt; 
&lt;p&gt;To understand the process of combining IoT data streams with the FMU model to create an L4 Digital Twin, we’ll examine a failure scenario that we simulated. In our synthetic failure scenario, we introduced defects for rollers 3 and 9 in the form of contamination – leading to increased friction. Here, we linearly increased the bearing viscous damping coefficients from 0 on Day 15 to 0.2 on Day 23 (which we’ve shown in Figure 3).&lt;/p&gt; 
&lt;div id="attachment_2965" style="width: 896px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2965" loading="lazy" class="size-full wp-image-2965" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure3.jpg" alt="Figure 3: Plot showing the artificially induced increase in viscous damping for rollers 3 and 9 starting from 0 on Day 15 to 0.2 on Day 23." width="886" height="480"&gt;
 &lt;p id="caption-attachment-2965" class="wp-caption-text"&gt;Figure 3: Plot showing the artificially induced increase in viscous damping for rollers 3 and 9 starting from 0 on Day 15 to 0.2 on Day 23.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 4 shows the angular velocity measurements (both plots show the same data with different y-axis scale) for each of the rollers. Figure 4b shows that – as expected – at Day 15, the angular velocity for rollers 3 and 9 begin to change because of the increasing viscous damping we introduced to simulate dirt build-up. Since the entire web line is linked via the material, a single roller increasing in resistance impacts the angular velocities of the surrounding rollers. In our case, we see that rollers 1,2,3,7,8,9, and 10 all experience changes in angular velocity.&lt;/p&gt; 
&lt;p&gt;By Day 21, the dirt build-up is sufficient to cause a very large change in the angular velocity of roller 9 as we’ve shown in Figure 4.&lt;/p&gt; 
&lt;div id="attachment_2966" style="width: 1384px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2966" loading="lazy" class="size-full wp-image-2966" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure4.jpg" alt="Figure 4: Incoming IoT data streams for angular velocity. Left and right figures are the same data with the scaling of the y-axis changed to enable observing tiny differences." width="1374" height="442"&gt;
 &lt;p id="caption-attachment-2966" class="wp-caption-text"&gt;Figure 4: Incoming IoT data streams for angular velocity. Left and right figures are the same data with the scaling of the y-axis changed to enable observing tiny differences.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 5 shows the corresponding span tensions and slip velocities (that we would measure with IoT sensors if they were installed) leading to tension failures in spans 4, 5, and 6 and slip failure at roller 9. Figure 5a shows spans 4, 5, and 6 exceeding the 175N threshold and Figure 5b shows roller 9 with non-zero slip velocity.&lt;/p&gt; 
&lt;div id="attachment_2967" style="width: 1351px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2967" loading="lazy" class="size-full wp-image-2967" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure5.jpg" alt="Figure 5: a) Measured span tensions during the dirt build up scenario in which spans 4,5, and 6 exceed the failure threshold for the webbing material, and b) measured roller slip velocities during the dirt build up scenario in which roller 9 eventually exhibits significant slippage around day 21." width="1341" height="399"&gt;
 &lt;p id="caption-attachment-2967" class="wp-caption-text"&gt;Figure 5: a) Measured span tensions during the dirt build up scenario in which spans 4,5, and 6 exceed the failure threshold for the webbing material, and b) measured roller slip velocities during the dirt build up scenario in which roller 9 eventually exhibits significant slippage around day 21.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As mentioned already, in practice, operators typically only measure the angular velocity of the rollers – and span tension might be measured using a load cell for only one span across the entire production line.&lt;/p&gt; 
&lt;p&gt;This example shows the importance of calibrating the system model to use the correct viscous friction coefficients. Without regular calibrations, the model predictions for span tension and roller slip velocity will be incorrect, resulting in the operator being unaware of potential, impending failures.&lt;/p&gt; 
&lt;h2&gt;Using TwinFlow to calibrate the L4 Digital Twin&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow&lt;/a&gt; is the AWS open-source framework for building and deploying millions of predictive models at scale on a distributed, heterogeneous architecture. TwinFlow incorporates the &lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt; module to orchestrate the model deployment and the &lt;a href="https://github.com/aws-samples/twinstat"&gt;TwinStat&lt;/a&gt; module which includes statistical methods for building and deploying L4 Digital Twins. These methods include techniques to build quick-execution (seconds or less) response surface models (RSMs) from complex simulation models that could take hours to run and would be too costly and too slow to support operational decisions.&lt;/p&gt; 
&lt;p&gt;The TwinStat module also includes methods to probabilistically update and calibrate L4 Digital Twins. In our case, the MapleSim model is exported as an FMU which already has a very quick execution time on the order of a few seconds. Since the degradation effect due to roller dirt accumulation is gradual (hours to days), our FMU execution time is more than sufficient – and cost-efficient.&lt;/p&gt; 
&lt;p&gt;The model calibration techniques in TwinStat include probabilistic Bayesian estimation methods like &lt;a href="https://en.wikipedia.org/wiki/Kalman_filter"&gt;Unscented Kalman Filters (UKF)&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Particle_filter"&gt;Particle Filters (PF)&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Gaussian_process"&gt;Gaussian Processes&lt;/a&gt;. Each of the methods have pros and cons and preferred scenarios, which we’ll discuss in a future post. For our present use case, UKF provides the necessary accuracy while minimizing the compute time. We used UKF in TwinFlow with the MapleSim FMU model to calibrate the viscous coefficients using the measured angular velocity IoT data.&lt;/p&gt; 
&lt;p&gt;To use TwinFlow, we needed an appropriately-sized Amazon EC2 instance. For our specific scenario (fast FMU with low memory requirements) we wanted to minimize the network overhead, hardware procurement time, and container download and activation time needed to run UKF. We know that UKF scales with the number of FMU executions by 2*D+1, where D is the number of variables included in the UKF. For our example, we have 9 measured angular velocities and 9 unmeasured viscous damping coefficients for a total of 18 variables. For each incoming IoT data point, UKF will run the FMU 37 times. Thus, we select an EC2 instance with around 37 cores for optimal runtime performance. In this scenario, the FMU is single-threaded (you’d need to increase your instance size if you used a multi-threaded FMU with UKF). TwinFlow parallelizes the UKF execution with multi-processing, thus the more CPUs, the shorter the runtime.&lt;/p&gt; 
&lt;p&gt;In our calibration example, we used the synthetic dataset for the angular velocity plotted in Figure 4 to represent the real-world measured sensor data. We used UKF to iterate and determine the correct viscous damping coefficients in the FMU model to match the measured angular velocities. Figure 6a shows the values determined by UKF for the viscous damping coefficients, as well as the estimates of uncertainty (shaded regions around the lines).&lt;/p&gt; 
&lt;p&gt;We see that UKF correctly estimates the coefficients to be approximately zero for all rollers &lt;em&gt;except 3 and 9&lt;/em&gt;, which increased to a value of approximately 0.20, giving us confidence that UKF is successfully calibrating the L4 Digital Twin to model the evolving behavior of the web-handling system.&lt;/p&gt; 
&lt;p&gt;Figure 6b shows how the UKF estimates for angular velocity lies directly on top of the IoT sensor data (open circle data points). The shaded region represents uncertainty.&lt;/p&gt; 
&lt;div id="attachment_2968" style="width: 634px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2968" loading="lazy" class="size-full wp-image-2968" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure6.jpg" alt="Figure 6: a) UKF estimates for a) viscous damping coefficients (commonly denoted with letter ”b” ) and b) angular velocity of each roller with uncertainty estimations (shaded region). We see that the viscous damping coefficients for rollers 3 and 9 increase from 0 to 0.2, corresponding to the failure scenario we manually introduced when creating the synthetic data, and we see the UKF angular velocity estimates matching the IoT synthetic data." width="624" height="244"&gt;
 &lt;p id="caption-attachment-2968" class="wp-caption-text"&gt;Figure 6: a) UKF estimates for a) viscous damping coefficients (commonly denoted with letter ”b” ) and b) angular velocity of each roller with uncertainty estimations (shaded region). We see that the viscous damping coefficients for rollers 3 and 9 increase from 0 to 0.2, corresponding to the failure scenario we manually introduced when creating the synthetic data, and we see the UKF angular velocity estimates matching the IoT synthetic data.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We can now examine the performance of the L4 Digital Twin self-calibrated virtual sensor for tension and slip by checking the residuals are close to zero. The &lt;em&gt;residual&lt;/em&gt; is the difference between the virtual sensor predicted value and the actual IoT measured data value (in this case our synthetic dataset).&lt;/p&gt; 
&lt;p&gt;Figure 7 shows the residuals for tension and slip velocity are near zero, meaning that the virtual sensor predicted values closely match the true values, and we can eliminate the costly physical sensors for measuring tension and slip velocity.&lt;/p&gt; 
&lt;div id="attachment_2969" style="width: 768px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2969" loading="lazy" class="size-full wp-image-2969" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure7.jpg" alt="Figure 7: Residuals of the digital twin output comparing tension on the left and slip velocity on the right. Due to the example scenario using synthetic data that is noiseless, the residuals are almost perfectly zero." width="758" height="312"&gt;
 &lt;p id="caption-attachment-2969" class="wp-caption-text"&gt;Figure 7: Residuals of the digital twin output comparing tension on the left and slip velocity on the right. Due to the example scenario using synthetic data that is noiseless, the residuals are almost perfectly zero.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Now, we compare the results of using an uncalibrated L3 virtual sensor model with the L4 self-calibrating virtual sensor.&lt;/p&gt; 
&lt;p&gt;Figure 8a shows that the L3 digital twin initially matches the measured angular velocity, but then misses the changes as the system performance degrades due to dirt accumulation, whereas the L4 self-calibrated digital twin virtual sensor evolves with the real physical system and matches the measured data closely. Similarly, Figure 8b and Figure 8c show that the L3 (uncalibrated) virtual sensors initially match the measured data, but then underpredict the roller 9 slip velocity and the span 6 tension as the system degrades and the operator is completely unaware of the impending failure.&lt;/p&gt; 
&lt;p&gt;The L4 (self-calibrating) virtual sensor predictions, however, closely match the measured data, allowing the operator to take corrective actions &lt;em&gt;prior to failure&lt;/em&gt;. The slip failure is particularly noteworthy since it induces difficult-to-detect material imperfections that can make their way into the final product. We see that the L3 uncalibrated virtual sensor initially makes reasonable predictions, but as the web-handling system behavior evolves over time (due to real-world degradation), a self-calibrating L4 virtual sensor is required to adapt the model to the changing real-world conditions.&lt;/p&gt; 
&lt;div id="attachment_2970" style="width: 634px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2970" loading="lazy" class="size-full wp-image-2970" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure8.jpg" alt="Figure 8: Comparison of a digital twins that are either calibrated or uncalibrated relative to measured data." width="624" height="204"&gt;
 &lt;p id="caption-attachment-2970" class="wp-caption-text"&gt;Figure 8: Comparison of a digital twins that are either calibrated or uncalibrated relative to measured data.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;AWS Architecture&lt;/h2&gt; 
&lt;p&gt;These are great results, so it’s worth taking a moment to explain what architectural decisions we made that allowed us to get here.&lt;/p&gt; 
&lt;p&gt;The AWS architecture used for the L4 Digital Twin virtual sensor calibration is shown in Figure 9. In our scenario, we periodically collected sensor data every 10-20 mins – a sufficient time resolution to capture the failure phenomena of interest.&lt;/p&gt; 
&lt;p&gt;It’s important to balance the trade-off of increased time resolution versus what is needed for the use case, as gathering a lot of data at high frequency results in unnecessary data storage and additional compute costs. We used an &lt;a href="https://aws.amazon.com/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt; scheduler to enable periodic calibration. We could have alternatively added logic to the container code to first calculate the error of the digital twin and decided to only calibrate if an error threshold is violated. And since Amazon EventBridge can only handle a maximum of 100 scheduling rules, we’d need to modify the architecture to use a Lambda function between Amazon EventBridge and &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, if we needed to scale to millions of tasks.&lt;/p&gt; 
&lt;p&gt;In Step 1 in Figure 9, we’ve shown how you can download &lt;a href="https://github.com/aws-samples/twinflow"&gt;TwinFlow&lt;/a&gt; to a temporary EC2 instance where they can customize, build, and push containers to cloud repositories, and then deploy your infrastructure as code (IaC).&lt;/p&gt; 
&lt;p&gt;Next, you can modify the example container and insert your own model into the container. The container gets pushed up to Amazon Elastic Container Registry (Amazon ECR) where it’s now available to all AWS services.&lt;/p&gt; 
&lt;p&gt;At step 4, you connect your IoT data from an edge location into a cloud database like &lt;a href="https://aws.amazon.com/iot-sitewise/"&gt;AWS IoT SiteWise&lt;/a&gt;, which is a serverless database designed to handle sensors with user defined attributes.&lt;/p&gt; 
&lt;p&gt;At this point, the Amazon EventBridge scheduler calls tasks in an &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; compute environment which loads the customized container, pulls data from &lt;a href="https://aws.amazon.com/iot-sitewise/"&gt;AWS IoT SiteWise&lt;/a&gt;, calibrates the L4 Digital Twin, saves the calibration to an S3 bucket, makes physics predictions, and saves them back in AWS IoT SiteWise.&lt;/p&gt; 
&lt;p&gt;AWS Batch selects the optimal EC2 instance type, auto-scales up or out, and logs all task output in Amazon CloudWatch. Finally, we used &lt;a href="https://aws.amazon.com/iot-twinmaker/"&gt;AWS IoT TwinMaker&lt;/a&gt; to create dashboards in Grafana so operators can review the measured and predicted data in near real time – and make operational decisions.&lt;/p&gt; 
&lt;div id="attachment_2971" style="width: 987px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2971" loading="lazy" class="size-full wp-image-2971" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/HPCBlog-244-fig9.png" alt="Figure 9: AWS Cloud architecture needed to achieve digital twin periodic-calibration" width="977" height="550"&gt;
 &lt;p id="caption-attachment-2971" class="wp-caption-text"&gt;Figure 9: AWS Cloud architecture needed to achieve digital twin periodic-calibration.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we showed how to build an L4 Digital Twin self-calibrating virtual sensor on AWS using an FMU model created by MapleSim.&lt;/p&gt; 
&lt;p&gt;MapleSim provides the physics model in the form of an FMU, and TwinFlow allows us to use incoming IoT data to probabilistically calibrate the L4 Digital Twin virtual sensor for &lt;em&gt;span tension&lt;/em&gt; and &lt;em&gt;roller slip velocity&lt;/em&gt;. In future posts, we’ll discuss how to use the calibrated L4 Digital Twin to perform scenario analysis, risk assessment, and process optimization.&lt;/p&gt; 
&lt;p&gt;If you want to request a proof of concept or if you have feedback on the AWS tools, reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Some of the content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/kayla-rossi-profile.png" alt="Kayla Rossi" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Kayla Rossi&lt;/h3&gt; 
  &lt;p&gt;Kayla is an Application Engineer at Maplesoft providing customers with advanced engineering support for the digital modeling of their complex production machines. Her project experience includes the simulation and analysis of web handling and converting systems across various industrial applications, including printing and battery production.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/orang-vahid-profile.png" alt="Dr. Orang Vahid" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Dr. Orang Vahid&lt;/h3&gt; 
  &lt;p&gt;Orang has over 20 years of experience in system-level modeling, advanced dynamic systems, frictional vibration and control, automotive noise and vibration, and mechanical engineering design. He is a frequent invited speaker and has published numerous papers on various topics in engineering design. At Maplesoft he is Director of Engineering Services, with a focus on the use and development of MapleSim solutions.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>EFA: how fixing one thing, led to an improvement for … everyone</title>
		<link>https://aws.amazon.com/blogs/hpc/efa-how-fixing-one-thing-lead-to-an-improvement-for-everyone/</link>
		
		<dc:creator><![CDATA[Shi Jin]]></dc:creator>
		<pubDate>Thu, 26 Oct 2023 14:30:25 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[Finite Element Analysis]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[MPI]]></category>
		<category><![CDATA[Protein Folding]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">ba7892a9031f1d2a300dccb5eeca53894380222e</guid>

					<description>Today, we're diving deep into the open-source frameworks that move MPI messages around, and showing you how work we did in the Open MPI and libfabrics community lead to an improvement for EFA users - and everyone else, too.</description>
										<content:encoded>&lt;p&gt;We routinely test a lot of HPC applications to make sure they work well on AWS, because we think that’s the only benchmark that matters. Hardware improvements from generation to generation are always important, but software is more frequently the place we find gains for customers.&lt;/p&gt; 
&lt;p&gt;Last summer, our performance teams alerted us to a discrepancy in runtimes when a single application ran with several different MPIs, while on the same hardware. At a high level, codes running with Open MPI were coming in a little slower than when using other commercial or open-source MPIs. This concerned us, because Open MPI is the most popular MPI option for customers using AWS Graviton (our home-grown Arm64 CPU) and we don’t want customers working on that processor to be disadvantaged.&lt;/p&gt; 
&lt;p&gt;In today’s post, we’ll explain how deep our engineers had to go to solve this puzzle (spoiler: it didn’t end with Open MPI). We’ll show you the outcomes from that work with some benchmark results including a real workload.&lt;/p&gt; 
&lt;p&gt;Along the way, we’ll give you a peek at how our engineers work in the open-source community to improve performance for everyone, and you’ll get a sense of the valuable contributions from the people in those communities who support these MPIs.&lt;/p&gt; 
&lt;p&gt;And – of course – we’ll show you how to get your hands on this so your code just runs faster – on every CPU (not just Graviton).&lt;/p&gt; 
&lt;h2&gt;First, some architecture&lt;/h2&gt; 
&lt;p&gt;If you’ve been paying &lt;em&gt;close attention&lt;/em&gt; to the posts on this channel&lt;em&gt;,&lt;/em&gt; you’ll know that the Elastic Fabric Adapter (EFA) is key to much of the performance you see in our posts. EFA works underneath your MPI and is often (we hope) just plain invisible to you and your application. EFA’s responsibility is to move messages securely – and rapidly – across our fabric between the instances that form parts of your cluster job. Doing this in a cloud environment adds an element or two to the task, because the cloud is both always under construction, and truly &lt;a href="https://youtu.be/JIQETrFC_SQ?t=1484"&gt;massive in scale&lt;/a&gt;. Scale isn’t the enemy however, and &lt;a href="https://aws.amazon.com/blogs/hpc/in-the-search-for-performance-theres-more-than-one-way-to-build-a-network/"&gt;we’ve exploited that fact&lt;/a&gt; to pull off &lt;a href="https://aws.amazon.com/blogs/hpc/second-generation-efa-improving-hpc-and-ml-application-performance-in-the-cloud/"&gt;great performance for HPC and ML codes&lt;/a&gt; with an interconnect built on the existing AWS network from existing technologies like Ethernet.&lt;/p&gt; 
&lt;p&gt;EFA remains largely invisible, because we chose to expose it through libfabric, which is also known as Open Fabrics Interfaces (OFI). This is a communications API for lots of parallel and distributed computing applications. By design, it’s lower-level than, say, MPI, and so allows lots of hardware makers (like AWS) to abstract their networking fabrics, so application programmers don’t need to pay attention to too many details of how they’re built.&lt;/p&gt; 
&lt;div id="attachment_2860" style="width: 976px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2860" loading="lazy" class="size-full wp-image-2860" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/13/CleanShot-2023-09-13-at-09.04.03.png" alt="Figure 1 - Open Fabrics Interfaces (OFI), or libfabric is the abstraction we've chosen to expose EFA to MPIs and NCCL. This allows application programmers to be productive without needing to know too much about how the fabric underneath does its work." width="966" height="466"&gt;
 &lt;p id="caption-attachment-2860" class="wp-caption-text"&gt;Figure 1 – Open Fabrics Interfaces (OFI), or libfabric is the abstraction we’ve chosen to expose EFA to MPIs and NCCL. This allows application programmers to be productive without needing to know too much about how the fabric underneath does its work.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As Figure 1 shows, there are several components and APIs in libfabric that MPI interacts with. While it’s impossible to explain them all in this post, let’s take a minute to work through a simple API flow for MPI that sends and receives a message between two ranks – using libfabric.&lt;/p&gt; 
&lt;h3&gt;An aside: a simple data flow example&lt;/h3&gt; 
&lt;p&gt;There are 3 groups of libfabric APIs involved: a &lt;em&gt;data sending&lt;/em&gt; API, a &lt;em&gt;data receiving&lt;/em&gt; API, and a &lt;em&gt;completion polling&lt;/em&gt; API. We’ve illustrated their connections in Figure 2.&lt;/p&gt; 
&lt;div id="attachment_2861" style="width: 553px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2861" loading="lazy" class="size-full wp-image-2861" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/13/CleanShot-2023-09-13-at-09.05.04.png" alt="Figure 2 – API flow for MPI to send and receive a message between two ranks using libfabric." width="543" height="474"&gt;
 &lt;p id="caption-attachment-2861" class="wp-caption-text"&gt;Figure 2 – API flow for MPI to send and receive a message between two ranks using libfabric.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;On the sender side, MPI initiates a message sending via the &lt;code&gt;fi_tsend&lt;/code&gt; function (&lt;a href="https://ofiwg.github.io/libfabric/main/man/fi_tagged.3.html"&gt;or its variants&lt;/a&gt;). This allows MPI to post a sending request that includes the address, length, and tag of the &lt;em&gt;sending&lt;/em&gt; buffer. On the receiver side, MPI posts a receiving request via the &lt;code&gt;fi_trecv&lt;/code&gt; call. This includes the address, length, and tag of the &lt;em&gt;receiving&lt;/em&gt; buffer. The tags are used for the receiver to filter what messages should be received from one or more senders.&lt;/p&gt; 
&lt;p&gt;Both sending and receiving APIs return &lt;em&gt;asynchronously&lt;/em&gt;, which means the sending and receiving operations don’t necessarily complete when the calls return. Except for some special cases, MPI needs to poll a completion queue (CQ) to get the completion events for sending and receiving operations, through &lt;code&gt;fi_cq_read&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Hopefully this gives you a picture of what goes on under the hood when MPI ranks want to communicate with each other. Anyhow, back to our main story now …&lt;/p&gt; 
&lt;h2&gt;Intra-node comms&lt;/h2&gt; 
&lt;p&gt;Digging deeper, the other MPIs seemed to have better &lt;em&gt;intra-node performance&lt;/em&gt; than Open MPI. This might sound a little surprising – you could be forgiven for presuming that &lt;em&gt;intra&lt;/em&gt;-node performance would be … more of a &lt;em&gt;solved problem&lt;/em&gt;. It’s literally cores sending messages to other cores &lt;em&gt;inside the same compute node&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Open MPI is designed to use a layer called the &lt;strong&gt;matching transport layer&lt;/strong&gt; (MTL) whenever libfabric is used to manage the two-sided tagged message sending and receiving operations (there &lt;em&gt;is&lt;/em&gt; another layer called the &lt;em&gt;byte transfer layer&lt;/em&gt; that uses libfabric for one-sided operations, like write and read, but we’re not going to discuss it in this post). This layer offloads &lt;em&gt;all communications&lt;/em&gt;, whether they’re intra-node or inter-node, to the libfabric layer. This is different from how the others do it, because they use their own shared-memory implementations for intra-node communications, &lt;em&gt;by default&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Other MPIs allow you to define which underlying libfabric provider they should use for shared memory operations (usually by tweaking some environment variables). Open MPI &lt;em&gt;also&lt;/em&gt; has its own shared-memory implementation in its BTL (Byte Transfer Layer) component (BTL/vader in Open MPI 4, and BTL/sm in Open MPI 5). This allowed us to explore several pathways to narrow down the specific portions of code where trouble was occurring.&lt;/p&gt; 
&lt;p&gt;We noticed that when running on a single instance, Open MPI with BTL/SM had similar performance to the others when they were using their native routines. As we noted above, due to low-level implementation details in Open MPI, when libfabric is being used, BTL/SM &lt;em&gt;cannot&lt;/em&gt; be used.&lt;/p&gt; 
&lt;p&gt;This is where some subtle effects can have quite an impact on outcomes. When Open MPI sends an intra-node message through the libfabric EFA provider, the EFA provider actually hands over the message to the libfabric SHM provider, which is yet another provider in libfabric – &lt;em&gt;that supports pure intra-node communications&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;When we put this to the test with &lt;a href="http://mvapich.cse.ohio-state.edu/benchmarks/"&gt;OSU’s benchmarking suite&lt;/a&gt; and some instrumented code, the high level performance gap we noticed between Open MPI with libfabric EFA and the other MPI’s native implementations could be decomposed into – mainly – two parts:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The gap between libfabric’s SHM provider itself and the others’ shared memory implementation.&lt;/li&gt; 
 &lt;li&gt;The gap between libfabric with (EFA + SHM)&amp;nbsp;and libfabric SHM on its own.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For a sense of scale, for small message sizes under 64 bytes (using a c5n.18xlarge instance) these gaps were adding around 0.4-0.5 µsec extra ping-pong latency. Given these are messages travelling &lt;em&gt;inside&lt;/em&gt; a machine and never making it to a PCI bus (let alone a network cable), this was a big penalty.&lt;/p&gt; 
&lt;h2&gt;Community efforts&lt;/h2&gt; 
&lt;p&gt;To address these two gaps, we needed to pursue directions of effort, in parallel.&lt;/p&gt; 
&lt;p&gt;Addressing the first gap involving the libfabric SHM provider performance became an effort with the help of the libfabric maintainers to optimize the locking mechanisms in the queues, and some other details – which we’ll dive into in a future post.&lt;/p&gt; 
&lt;p&gt;For the second gap involving the handoff between the EFA provider and the SHM provider, the AWS team worked to onboard the EFA provider using a newly-developed libfabric interface called&amp;nbsp;the &lt;a href="https://ofiwg.github.io/libfabric/main/man/fi_peer.3.html"&gt;Peer API&lt;/a&gt; so we could use the SHM provider as a &lt;em&gt;peer provider&lt;/em&gt;.&amp;nbsp;Peer providers are a way for independently developed providers to be used together in a tight fashion, which helps everyone (a) avoid layering overhead; and (b) resist the urge to duplicate each other’s’ provider functionality.&lt;/p&gt; 
&lt;div id="attachment_2862" style="width: 1178px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2862" loading="lazy" class="size-full wp-image-2862" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/13/CleanShot-2023-09-13-at-09.06.37.png" alt="Figure 3 - The base case before our improvement work began, characterized by a long pathway through all the layers of the stack, even though intra-node traffic should be able to short circuit much of it." width="1168" height="489"&gt;
 &lt;p id="caption-attachment-2862" class="wp-caption-text"&gt;Figure 3 – The base case before our improvement work began, characterized by a long pathway through all the layers of the stack, even though intra-node traffic should be able to short circuit much of it.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We’ve illustrated the contrasting pathways that small messages traverse when using the Peer API. Figures 3 and 4 depict these two paradigms – before and after we introduced peering through this new API. We’ll summarize it from two angles: the data movement and the completion-events population.&lt;/p&gt; 
&lt;div id="attachment_2863" style="width: 1151px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2863" loading="lazy" class="size-full wp-image-2863" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/13/CleanShot-2023-09-13-at-09.08.18.png" alt="Figure 4 - After we introduced the Peer API, which effectively short-circuits the pathways an intra-node message needs to travel." width="1141" height="594"&gt;
 &lt;p id="caption-attachment-2863" class="wp-caption-text"&gt;Figure 4 – After we introduced the Peer API, which effectively short-circuits the pathways an intra-node message needs to travel.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Data movement&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Before using the Peer API&lt;/strong&gt;, when MPI sends a message using the libfabric EFA provider, it cside, MPI posts a receiving requestalls &lt;code&gt;fi_tsend()&lt;/code&gt;(&lt;a href="https://ofiwg.github.io/libfabric/main/man/fi_tagged.3.html"&gt;or its variants&lt;/a&gt;) through the EFA endpoint. The EFA provider performs protocol selection logic and adds EFA headers to the message body before posting the message again though the SHM endpoint. After getting the send request from EFA, SHM does its &lt;em&gt;own &lt;/em&gt;protocol selection and message packaging &lt;em&gt;again,&lt;/em&gt; before copying the message into a ‘bounce’ buffer, which is shared in both the sender’s and receiver’s memory spaces. Over on the receiver’s side, SHM will copy the received message to a bounce buffer in the EFA provider, before delivering the message to the application’s buffer via another memory copy. That’s a lot of memory copies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;After using the Peer API&lt;/strong&gt;, on the sender’s side, EFA will check to see if SHM is enabled, and if the destination is on the same instance. If that’s true, EFA will immediately forward the &lt;code&gt;fi_tsend()&lt;/code&gt; request from MPI to SHM. On the receiver side, SHM copies the received message directly to the application buffer without the extra forwarding through the bounce buffer in the EFA layer.&lt;/p&gt; 
&lt;h3&gt;Completion events population&lt;/h3&gt; 
&lt;p&gt;The improvement on the completion events population is almost as dramatic. MPI gets a notification for the send/receive completion events by polling the completion queues.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Before using the Peer API&lt;/strong&gt;, the completion events generated by SHM when the actual send/receive operation completed needed another forwarding &lt;em&gt;inside&lt;/em&gt; the EFA provider before populating to the completion queues that the MPI polls from.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;After using the Peer API&lt;/strong&gt;, EFA and SHM just share the same completion queue allocated by MPI, and SHM can write the completion events directly to the queue.&lt;/p&gt; 
&lt;h2&gt;Performance improvements&lt;/h2&gt; 
&lt;p&gt;Let’s show you the performance improvements we saw when we ran both micro-benchmarks and real applications – the ultimate test.&lt;/p&gt; 
&lt;p&gt;For micro benchmarks, we use the &lt;a href="https://mvapich.cse.ohio-state.edu/benchmarks/"&gt;OSU Latency benchmark&lt;/a&gt; to measure the latency of point-to-point comms for two ranks, and OSU’s &lt;code&gt;MPI_Alltoallw&lt;/code&gt; test to measure the latency for collective comms.&lt;/p&gt; 
&lt;p&gt;For a real-world application, we ran &lt;a href="https://openfoam.org/"&gt;OpenFOAM&lt;/a&gt; with a 4M cell motorbike model.&lt;/p&gt; 
&lt;p&gt;In our testing, we used Open MPI 4.1.5 with two different versions of libfabric: 1.18.1 (&lt;em&gt;before&lt;/em&gt; the Peer API), and 1.19.0 (&lt;em&gt;after&lt;/em&gt; implementing Peer API).&lt;/p&gt; 
&lt;h3&gt;OSU latency&lt;/h3&gt; 
&lt;p&gt;This is where the improvements from our combined efforts are most visible – by measuring the point-to-point (two-rank) MPI send and receive ping-pong latency.&lt;/p&gt; 
&lt;p&gt;Figure 5 shows the OSU latency benchmark when running Open MPI with libfabric 1.18.1 (before using Peer API), and again with the newer libfabric 1.19.0 (after implementing the Peer API). We ran all these tests on two ranks on two different instance types: An hpc6a.48xlarge built around an x86 architecture, and the hpc7g.32xlarge, which is built using the AWS Graviton 3E – an Arm64 architecture.&lt;/p&gt; 
&lt;div id="attachment_2988" style="width: 1090px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2988" loading="lazy" class="size-full wp-image-2988" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/23/CleanShot-2023-10-23-at-18.41.29.png" alt="Figure 5 - Peer to peer intra-node MPI ping-pong latency measurements, before and after we implemented the Peering API in our libfabric providers. Latency approximately halved, which is a dramatic improvement." width="1080" height="463"&gt;
 &lt;p id="caption-attachment-2988" class="wp-caption-text"&gt;Figure 5 – Peer to peer intra-node MPI ping-pong latency measurements, before and after we implemented the Peering API in our libfabric providers. Latency approximately halved, which is a dramatic improvement.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The latency we measured approximately halved – a dramatic result, but in line with what we should expect when we removed all the unnecessary memory copies and method calls by using the Peer API, and the improvements in SHM itself.&lt;/p&gt; 
&lt;p&gt;The work continues in this area: the gap between libfabric 1.19.0 and the other private implementations will likely be reduced further – our teams are working together with the libfabric community to optimize the SHM provider further, with a goal of making it comparable to MPICH. This is a space to watch.&lt;/p&gt; 
&lt;h3&gt;OSU all-to-all communication&lt;/h3&gt; 
&lt;p&gt;To see the impact on MPI &lt;em&gt;collective operations&lt;/em&gt;, we benchmarked &lt;code&gt;MPI_Alltoallw&lt;/code&gt; – well known for stressing MPI communications.&lt;/p&gt; 
&lt;div id="attachment_2989" style="width: 1057px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2989" loading="lazy" class="size-full wp-image-2989" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/23/CleanShot-2023-10-23-at-18.42.06.png" alt="Figure 6 - The impact on collective operations. Latency dropped by between a third (in the case of the Hpc6a nodes) and a half (for Hpc7g)." width="1047" height="464"&gt;
 &lt;p id="caption-attachment-2989" class="wp-caption-text"&gt;Figure 6 – The impact on collective operations. Latency dropped by between a third (in the case of the Hpc6a nodes) and a half (for Hpc7g).&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 6 shows the results again for running Open MPI with libfabric 1.18.1 (before Peer API), libfabric 1.19.0 (after Peer API). This time we used 64 ranks and conducted the tests as before: using Hpc6a and Hpc7g instances.&lt;/p&gt; 
&lt;p&gt;The impact was significant. On Hpc6a, the latency fell generally by about a third. For Hpc7g, it fell by half.&lt;/p&gt; 
&lt;h3&gt;OpenFOAM motorbike 4M case&lt;/h3&gt; 
&lt;p&gt;The real test of our work is always an actual application that a customer runs: micro-benchmarks can only tell you so much. We ran the same &lt;em&gt;OpenFOAM motorBike 4M cell&lt;/em&gt; case on 4 x hpc6a.48xlarge (96 ranks per node) and 6 x hpc7g.16xlarge (64 ranks per node) – 386 total ranks in both cases. And in this case, we only tested with Open MPI, but with both our newer and older libfabric versions.&lt;/p&gt; 
&lt;p&gt;We measured &lt;em&gt;runs per day&lt;/em&gt; to focus on what a customer will care about – the pace at which they can results from different CFD models. The runs showed around a 10% improvement from libfabric 1.18.1 to libfabric 1.19.0 across both instance types. We’ve graphed this in Figure 7.&lt;/p&gt; 
&lt;div id="attachment_2990" style="width: 642px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2990" loading="lazy" class="size-full wp-image-2990" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/23/CleanShot-2023-10-23-at-18.42.58.png" alt="Figure 7 - Runs per day for our OpenFOAM workload across two instance types, both before and after Peer API was used. Both show around 10% improvement from the new methods." width="632" height="483"&gt;
 &lt;p id="caption-attachment-2990" class="wp-caption-text"&gt;Figure 7 – Runs per day for our OpenFOAM workload across two instance types, both before and after Peer API was used. Both show around 10% improvement from the new methods.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;How to get this new code for your workloads&lt;/h2&gt; 
&lt;p&gt;The improvements we’ve shown here are already in the main branch of the &lt;a href="https://github.com/ofiwg/libfabric"&gt;OFIWG/libfabric GitHub repo&lt;/a&gt; and they’re part of libfabric 1.19, which was released at the end of August. We ingested that into EFA installer 1.27.0, which also shipped recently. You can always &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start.html"&gt;check our documentation&lt;/a&gt; to find out how to get the latest version.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;The enhancements we’ve described in this post should make everyone’s lives easier. In most cases, the intra-node latency between ranks using Open MPI will drop by around a half – without any code changes in the end user application. In our real-world test using a CFD benchmark, we saw a 10% performance gain.&lt;/p&gt; 
&lt;p&gt;We think there are two significant takeaways from this for the HPC community.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The first&lt;/strong&gt; is that our engineering teams are always looking for ways to boost performance. Sometimes that happens in hardware, very often firmware, and – like today’s post – software, too. When we find those ways, we’ll roll them out as quick as we can, and they’ll form part of the pattern that cloud HPC users have come to expect: things just get faster (and easier) over time, without compromising security.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The second&lt;/strong&gt; is that those same engineers work collaboratively with their counterparts at partners (and competitors, too) to make things better, and faster – for everyone. This collaborative angle, underscores a truth that innovation thrives in shared effort rather than isolated silos. We’re thankful to have been able to work with so many dedicated engineers from around the world to bring these results to you today.&lt;/p&gt; 
&lt;p&gt;Reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt; if you have any questions – or if you want to find a way to contribute, too.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Unpacking the power of agent-based models for environmental and socio-economic impact</title>
		<link>https://aws.amazon.com/blogs/hpc/unpacking-the-power-of-agent-based-models-for-environmental-and-socio-economic-impact/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Tue, 24 Oct 2023 13:16:52 +0000</pubDate>
				<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">7d5dc1a9167d60992b36e694dc5e534206fd493f</guid>

					<description>Today we’ll help you understand agent-based models and their potential to impact the environment and socio-economic systems. We'll explore a few use cases and show how ABMs can improve our comprehension of environmental and social decision making.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="size-full wp-image-2848 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/08/boofla88_conways_game_of_life_as_a_concept_3D_close_zoom_photo__61aa6916-7a48-466e-93c7-00b102db121d.png" alt="" width="380" height="212"&gt;&lt;em&gt;This post was contributed by Namid Stillman, Quantitative Researcher at Simudyne, and Sam Bydlon and Ilan Gleiser from the Global Impact Computing team at AWS.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Agent-based modeling (ABM) is a powerful computational modeling approach that centers on the simulation of interactions between autonomous agents to understand the behavior of the broader system and its governing principles. Agent-based simulation can be traced back all the way to the mid-20&lt;sup&gt;th&lt;/sup&gt; century and John von Neumann’s universal constructor, but due to the large computational requirements of running agent-based simulations widespread development and application did not come until the 1990s.&lt;/p&gt; 
&lt;p&gt;Since then, ABMs have evolved and gained prominence in helping us model and understand complex systems by representing the behaviors and interactions between agents. With the emergence of cloud technology, in particular high-performance computing services on AWS, agent-based simulation is poised to enter a new era of scale and accessibility. By developing simulations that mimic systems such as economies and scaling the number of agents to be comparable to reality, we can explore hypothetical scenarios that would not be possible, or might be too risky, to test in the real world.&lt;/p&gt; 
&lt;p&gt;With the drop in computing costs and the democratization of HPC on AWS, ABMs have become a significant tool for analyzing the complexities of the environmental and socio-economic impacts of our decisions. By simulating interactions between individual ‘agents’, like households, governments, firms, trees, and vehicles, for example, ABMs can grant a deeper insight into effects of economic policy decisions, changes to supply chains, urban plastics flow, and climate risks, among others.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll help you understand agent-based models and their potential to impact the environment and socio-economic systems. We’ll explore a few use cases of ABMs and show how they can improve our comprehension of environmental and social decision making.&lt;/p&gt; 
&lt;div id="attachment_2837" style="width: 989px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2837" loading="lazy" class="size-full wp-image-2837" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/08/CleanShot-2023-09-08-at-11.29.07.png" alt="Fig 1: Examples of Agent-Based Model simulations available from Simudyne SDK on AWS HPC " width="979" height="332"&gt;
 &lt;p id="caption-attachment-2837" class="wp-caption-text"&gt;Fig 1: Examples of Agent-Based Model simulations available from Simudyne SDK on AWS HPC&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Economic Policy Simulation&lt;/h2&gt; 
&lt;p&gt;Economic policy simulation is a crucial application of ABMs for understanding the complex dynamics of social and economic systems. ABMs provide a unique approach to modeling the interactions between individuals and institutions within an economy, allowing for a more realistic representation of economic behaviors and policy outcomes.&lt;/p&gt; 
&lt;p&gt;In stark contrast to traditional methods of economic modeling, ABMs revolutionize the way we analyze and model economic scenarios. Unlike classical approaches that analyze macroeconomic trends and assume the homogeneity of individuals, ABMs are built as bottom-up models that capture the intricate heterogeneity and complexity of human decision-making processes. This approach enables businesses, policymakers, and economists to simulate a broad range of economic models and analyze their potential impacts, giving them an edge over classical economic methods. ABMs are also useful for evaluating the outcomes of different policy interventions and projecting probable future scenarios, thereby improving policymaking in many domains.&lt;/p&gt; 
&lt;p&gt;One area where ABMs have been particularly valuable is in the study of macroeconomic policy, which we described in a previous &lt;a href="https://aws.amazon.com/blogs/hpc/rigor-and-flexibility-the-benefits-of-agent-based-computational-economics/"&gt;post&lt;/a&gt;. ABMs can simulate the effects of fiscal and monetary policy – like changes in tax rates, government spending, or interest rates – on key economic variables like income distribution, inflation, and GDP growth. This allows policymakers to simulate the potential impacts of policy changes before implementing them in the real world, making for more informed decisions. For instance, policy makers can study the impact of introducing a universal basic income on the distribution of wealth among agents in an economy.&lt;/p&gt; 
&lt;p&gt;ABMs can also capture the dynamics of financial markets and banking systems, providing insights into the effects of regulatory policies on financial stability and systemic risk. By simulating the interactions between banks, investors, and borrowers, ABMs can shed light on the transmission channels through which shocks (like climate events) propagate within the financial system, helping policymakers design more robust regulatory frameworks.&lt;/p&gt; 
&lt;p&gt;As the science of economic agent-based modeling advances, incorporating reinforcement learning (RL) agents into simulations is becoming increasingly popular because the technique allows agents to learn from experience and make better decisions. In an RL framework, agents initially operate by trial and error but are rewarded or punished based on their actions. This means the agents learn to make optimal decisions under uncertain conditions, mimicking the way humans learn. Incorporating reinforcement learning into agent-based simulations can enhance the predictive power of the simulations through these feedback loops, where agents can learn and adjust their behavior based on past outcomes and their own &amp;nbsp;&amp;nbsp;mistakes. With each iteration, agents become more adept at predicting future outcomes and choosing the best course of action to achieve their goals.&lt;/p&gt; 
&lt;p&gt;The scalability and computational power of cloud computing platforms like AWS have made it possible to build ABM simulations for economic policy research. AWS HPC services – AWS Batch and AWS ParallelCluster – allow researchers to run complex and computationally-intensive ABMs. This means researchers can simulate populations with millions of individuals and firms, where individuals can have differences in preferences, access to information and learning rates. They can also leverage these services to more efficiently explore a broader range of policy scenarios, conduct sensitivity analysis, and perform parameter calibration. Since they can distribute simulations across multiple virtual machines, researchers can reduce the time required for running simulations, accelerating the research process and enabling faster policy evaluation.&lt;/p&gt; 
&lt;h2&gt;Supply-Chain Decarbonization&lt;/h2&gt; 
&lt;p&gt;Supply-chain decarbonization is a critical aspect of sustainability efforts of businesses and policymakers who are aiming to reduce the greenhouse gas emissions associated with the production, transportation, and distribution of goods and services. The transition to a low-carbon economy requires significant changes in supply-chain operations and practices. In fact, many firms and governments are aiming for net-zero carbon emissions by 2040, so ABMs can play a crucial role supporting these firms to accurately model and optimize their supply chains and perhaps even achieving those targets sooner.&lt;/p&gt; 
&lt;p&gt;ABMs enable businesses and policymakers to simulate the complex interactions between the various actors within a supply chain: manufacturers, suppliers, logistics providers, and consumers. By modeling the behavior and decision-making processes of these actors, ABMs can help identify opportunities and strategies for decarbonizing supply chains while minimizing disruptions and maximizing economic efficiency.&lt;/p&gt; 
&lt;p&gt;One of the key benefits of ABMs for supply-chain decarbonization is their ability to capture the heterogeneity and dynamics of &lt;em&gt;real-world&lt;/em&gt; supply chains. Each agent in a supply chain has different characteristics, capabilities, and goals, and ABMs can represent this diversity. This lets companies analyze the impact of different agents’ behaviors and decision-making processes on the overall carbon footprint of the supply chain. For example, &amp;nbsp;Amazon can model their entire supply chain so that every facility (like inbound cross-docking facilities, or fulfillment centers) are modelled as agents. Each facility agent will have its own unique parameters – just like in the real world. Millions, or billions of products are then tracked moving through the supply chain. Various scenarios and policy changes that are aimed at reducing costs or carbon footprint can be simulated safely to identify the policies that meet the desired targets without sacrificing key business metrics, like on time delivery.&lt;/p&gt; 
&lt;p&gt;For example, ABMs can simulate the effects of adopting more sustainable manufacturing processes or using renewable energy sources in transportation. By modeling the interactions between manufacturers, suppliers, and logistics providers, supply chain optimization teams can use their supply chain simulator to identify potential bottlenecks, trade-offs, and synergies that arise from different decarbonization strategies.&lt;/p&gt; 
&lt;p&gt;ABMs can also capture the influence of external factors on supply-chain decarbonization, like government tax policies, consumer preferences, and technological advancements. They can simulate the impact of policy interventions, too – such as carbon pricing, or subsidies for renewable energy – on supply-chain emissions. By incorporating these factors into the simulation, ABMs can provide valuable insights into the potential effectiveness and unintended consequences of different policy options.&lt;/p&gt; 
&lt;p&gt;To effectively run large scale ABM simulations for supply-chain decarbonization, HPC capabilities, such as those provided by AWS, are essential. The vast scale and intricacy of supply chains necessitate the use of large-scale simulations, which enable researchers to analyze data at various levels of granularity, ranging from individual warehouses up to the national and even international distribution networks. This allows for a more comprehensive evaluation of different decarbonization strategies, identifying those that are most effective and practical in different contexts. At the national and international level, the use of HPC on AWS is essential for providing the necessary scale of compute power to integrate these granular details and achieve accurate and actionable insights. These technologies are empowering supply chain optimization teams to make critical advances towards more sustainable and resilient supply chains that benefit society, and the planet, too.&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;strong&gt;&lt;em&gt;“Overall, ABMs along with HPC is a novel approach to solving supply chain related problems and to decarbonize the supply chain by optimizing it.”&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;strong&gt;&lt;em&gt;Siva Veluchamy, &lt;/em&gt;&lt;/strong&gt;Sr. Simulation Scientist, Amazon World-Wide Design Engineering&lt;/p&gt; 
&lt;h2&gt;Plastic Flow Simulations&lt;/h2&gt; 
&lt;p&gt;Plastic flow simulations are an important tool in understanding the movement and distribution of plastic waste in the environment. According to the UN, “&lt;em&gt;every year 19-23 million tonnes of plastic waste leaks into aquatic ecosystems, &lt;/em&gt;&lt;a href="mailto:https://www.unep.org/plastic-pollution"&gt;&lt;em&gt;polluting lakes, rivers and seas&lt;/em&gt;&lt;/a&gt;“. There is increasing concern about plastic pollution with 175 countries agreeing to implement legally binding guidelines by 2024. Given this, and plastic’s impact on ecosystems and human health, it’s crucial to develop strategies and policies that effectively manage and reduce plastic waste.&lt;/p&gt; 
&lt;p&gt;ABMs offer a unique approach to simulating plastic flow by modeling the behavior and interactions of various agents involved in the plastic lifecycle. These agents can include manufacturers, consumers, waste management facilities, and natural processes like weather patterns and ocean currents.&lt;/p&gt; 
&lt;p&gt;According to researchers at &lt;a href="mailto:mailto:https://www.iaria.org/conferences2021/filesSIMUL21/50033_SIMUL.pdf"&gt;Purdue University&lt;/a&gt; and the &lt;a href="mailto:https://doi.org/10.1016/j.procir.2020.01.083"&gt;National University of Singapore&lt;/a&gt;, one of the key benefits of ABMs for plastic flow simulations is their ability to capture the complex and dynamic nature of plastic waste movement. Plastic waste can travel through multiple pathways, like rivers, wind, and ocean currents, and can accumulate in place like beaches, riversides, or ocean gyres. ABMs can simulate these movement patterns and help identify areas of high plastic concentration, letting policymakers target their efforts for maximum impact.&lt;/p&gt; 
&lt;p&gt;As Purdue researchers have shown, using data from the American Chemistry Council and the National Association for PET Container Resources, ABMs can also simulate the behavior of different actors within the plastic supply chain and waste management system. This includes modeling consumer behavior, like plastic consumption patterns and recycling habits, and the operations of waste management facilities and their capacity to handle plastic waste. By incorporating these factors into the simulation, ABMs can provide insights into the effectiveness of different interventions, such as implementing recycling programs or reducing single-use plastic consumption.&lt;/p&gt; 
&lt;h2&gt;Green Urban Planning Simulation&lt;/h2&gt; 
&lt;p&gt;The Green Urban Scenarios simulator (&lt;a href="https://run.greenurbanscenarios.com/"&gt;GUS&lt;/a&gt;), powered by AWS HPC, is a digital twin of a city that can be used to run environmental scenarios. The GUS simulator was launched by Lucid Minds, a start-up company from Germany, and uses agent-based modeling on top of a digital twin of proposed green urban projects, such as tree planting and maintenance in a city, to understand the effects of a project before it is implemented. The simulators allows urban planners to explore potential impacts of proposed projects, including the amount of carbon sequestered, or the quantity of pollution that might be removed from the local environment, or the ability of tree planting to retain water and mitigate the impacts of storms.&lt;/p&gt; 
&lt;h2&gt;Circular Economy Simulations&lt;/h2&gt; 
&lt;p&gt;A circular economy is a system where the goal is to minimize waste and pollution by keeping materials in use for as long as possible. This involves a shift away from the traditional linear economic model of “take, make, use, and dispose” and towards a more circular approach. As outlined by the University of Cambridge Judge Business School, the goal of a circular economy “is to put back into the system everything relating to production, distribution and consumption, in order to extract as much value as possible from the resources and materials we utilize.”&lt;/p&gt; 
&lt;p&gt;Circular economy simulations are a critical tool in understanding the potential impact of circular economy strategies on social and environmental systems. Using ABMs, businesses and policymakers can analyze the effects of different circular economy strategies on the overall sustainability of a system.&lt;/p&gt; 
&lt;p&gt;ABMs let organizations model the interactions between various actors and factors involved in circular economy initiatives, like businesses, consumers, waste management systems, and policymakers. By simulating the behavior and decision-making processes of these actors, ABMs can help evaluate the effectiveness of different circular economy strategies in reducing resource consumption, minimizing waste generation, and promoting sustainable economic growth. This allows researchers to explore a wider range of circular economy scenarios, evaluate different strategies, and identify the most effective interventions for transitioning to a sustainable and regenerative economic system.&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;strong&gt;&lt;em&gt;“Agent Based Modeling (ABM) has been instrumental in several circular economy analysis projects at NREL. It has helped us to identify the potential barriers and enablers to &lt;/em&gt;&lt;/strong&gt;&lt;a href="https://www.nature.com/articles/s41560-021-00888-5"&gt;&lt;strong&gt;&lt;em&gt;increasing the circularity of renewable energy technologies&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;em&gt;, such as &lt;/em&gt;&lt;/strong&gt;&lt;a href="mailto:https://www.nrel.gov/docs/fy23osti/84141.pdf"&gt;&lt;strong&gt;&lt;em&gt;wind turbines and photovoltaic panels&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;em&gt;.”&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;strong&gt;Julien Walzberg&lt;/strong&gt;, Ph.D., Researcher, National Renewable Energy Laboratory&lt;/p&gt; 
&lt;h2&gt;Simulating climate risk scenarios&lt;/h2&gt; 
&lt;p&gt;As the world experiences increasingly severe and frequent climate events, all businesses will be affected to some degree. Simulations provide a powerful tool for decision makers to evaluate the range of possible scenarios they may face and prepare accordingly. More importantly, simulations give insights into the cost of inaction, like failing to invest in supply chain resilience or promoting recycling schemes. &amp;nbsp;Agent-based models extend the impact of simulations to human-dependent systems like logistic networks, waste management facilities, or even entire economies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Simudyne&lt;/strong&gt;, a provider of simulation technologies, is a company leading the way in this space. They have developed a platform and software development kit (SDK) that uses patented graph-based computing to allow businesses to simulate different aspects of their operations using ABMs. These models range from national supply chains to firm-specific exposure to national climate shocks. Simulations of the supply chain are designed to account for various factors like different suppliers, transportation routes, and inventory levels. This granular view provides a comprehensive understanding of how different sections of the network are affected by extreme weather events and can be used to identify steps towards decarbonisation in line with net zero goals.&lt;/p&gt; 
&lt;p&gt;Alternatively, Simudyne can model shocks at the scale of an entire economy, the kind caused by events like flooding or heatwaves. They can also account for government incentive schemes that reward early adopters in green innovation to develop new production processes and technologies with the explicit aim of reducing environmental risks. These models give insights into key vulnerabilities at the scale of the sector or an individual firm and options for offsetting these risks. This provides a valuable tool for business leaders and policy makers alike.&lt;/p&gt; 
&lt;p&gt;By simulating climate risk events with ABMs, companies like Deloitte can make better-informed decisions on every aspect of their business, quickly assess the financial consequences of a risk scenario, and respond in a more coordinated and efficient manner. This means that companies can develop contingency plans for managing unexpected climate events rather than be caught off guard.&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;strong&gt;&lt;em&gt;“Agent-based modelling gives us a clear view of the interplay between climate change and our economies. It’s a tool that sharpens our understanding, helping us pinpoint inefficiencies in supply chains, reduce costs, and cut carbon emissions. With it, we can make better decisions for a safer world.”&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;strong&gt;Justin Lyon&lt;/strong&gt;, CEO, Simudyne&lt;/p&gt; 
&lt;h2&gt;Leveraging the Simudyne SDK with AWS HPC services&lt;/h2&gt; 
&lt;p&gt;Developing agent-based models can be difficult both in terms of model construction and deployment. The Simudyne SDK is a Java-based simulation engine that makes model development easy and accessible, meaning businesses can spend more time understanding the results rather than developing them. Importantly, the Simudyne SDK leverages patented graph-computation to efficiently scale ABMs from tens to millions of agents. This allows for huge and complex systems to be modelled and forecast, such as simulating three weeks of nationwide supply chains with hundreds of millions of products flowing through the supply chain every week at the resolution of a single item in less than an hour.&lt;/p&gt; 
&lt;p&gt;One of the central benefits of the Simudyne SDK is its flexibility. The SDK can be used to simulate supply chains, customer networks, fast moving consumer goods, or even entire economies. However, not all simulations are alike and identifying the right computational architecture depends on understanding&amp;nbsp;the specifics to each model. The number of agents in the simulation, the type of data that is passed between agents, the size or amount of data being passed, how the simulation is used by other processes such as ML workflows, and how both input and output data is accessed, are all factors that impact compute requirements. For now, let’s go through some of these considerations in turn.&lt;/p&gt; 
&lt;p&gt;One of biggest considerations when building agent-based models is understanding how many agents will be in the simulation and how they interact. This relates to the agent interaction network, the communication framework for messages to be passed between agents. Some interaction networks can be very simple, such as financial exchanges which have a `star-like’ interaction network (all agents link to the market and not to each other) or they can be very complex, like a social network, where links between agents are dynamic, changing over time. The Simudyne SDK’s graph-based computational approach efficiently simulates millions of agents, allowing users to easily simulate real-world systems.&lt;/p&gt; 
&lt;p&gt;Just as critical as the number of agents in the simulations is the type of data that passes between them. The Simudyne SDK uses a message passing framework to distribute information across the simulations. The size of the messages drives the amount of memory needed to run the simulations. Whether there are several million agents passing a small amount of data or tens of agents passing large volumes, memory consideration is key to enabling efficient scaling of the simulation. AWS supports high-performance high-memory computing resources. When deciding on which EC2 instances to use, it is critical that there is sufficient memory for each instance. Simulation profiling and benchmarking can help determine the amount of memory required.&lt;/p&gt; 
&lt;p&gt;A simulation is only as useful as how it is used. Some simulations, like models of an entire economy, are standalone. They can be used for economic forecasting, highlighting the impact of shocks to the economy such as rising inflation. Other simulations, like supply chain models, can be used to chart the best course of action. By running multiple simulations with different supply chain structures (for example, choosing many small distributors or a few large ones), strategies can be compared, and an optimal structure can be identified. Optimization modules can find optimal strategies in an efficient way.&lt;/p&gt; 
&lt;p&gt;When deciding on how to structure a simulation pipeline, it’s imperative that the data fed into the simulation is well described and understood. Related considerations include the format, frequency and mechanism by which data is passed to the simulation. For example, is the data a static source that is kept in an Amazon S3 bucket? Or is it continually updated, using real-time data streaming from &lt;a href="https://aws.amazon.com/kinesis/"&gt;Amazon Kinesis? &lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Similarly, the output of the data must be properly structured to fit into other downstream tasks. If the data is required for reports, then a simple CSV or text file might suffice. More complex data pipelines might require parquet or H5 files which can be easily read by other code sources, such as in Python or C/C++ scripts.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;With the rise of HPC on AWS, agent-based modeling has become an even more powerful tool for understanding complex systems and their impact on our environment and society. The ability to simulate the behaviors and interactions of individual agents has led to deeper insights into the effects of economic policy decisions, urban plastics flow, and climate risks. ABMs have democratized the process of analyzing environmental and socio-economic impacts, making it easier and more affordable to understand the complex interactions that drive our world. AWS’ HPC services and wide selection of instance types make it an excellent tool to harness the power of ABMs and help us make better decisions for the future.&lt;/p&gt; 
&lt;p&gt;Looking to learn more about how to use Simudyne’s SDK and AWS HPC resources to solve your current and future business problems? Simudyne has a team of expert simulation engineers ready to help. Reach out today to find out more. For an evaluation of your ABM simulation workloads, please &lt;a href="mailto:ask-hpc@amazon.com"&gt;reach out to&lt;/a&gt; the AWS Global Impact Computing Team and we will schedule a discovery session with you.&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/08/CleanShot-2023-09-08-at-11.52.13.png" alt="Namid Stillman" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Namid Stillman&lt;/h3&gt; 
  &lt;p&gt;Dr. Namid Stillman is a Quantitative Researcher at Simudyne. He has a background in multi-disciplinary research, including in fields of material science, cell migration and nanotechnology, with a focus on developing interpretable AI method for scientific research. He is the author of the GNNs in Action textbook from Manning Press and co-organizer of the Simulation-based Science interest group at the Alan Turing Institute.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Why you should use Fargate with AWS Batch for your serverless batch architectures</title>
		<link>https://aws.amazon.com/blogs/hpc/why-use-fargate-with-aws-batch-for-serverless-batch-compute/</link>
		
		<dc:creator><![CDATA[Angel Pizarro]]></dc:creator>
		<pubDate>Thu, 19 Oct 2023 16:35:41 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS Fargate]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Serverless]]></category>
		<guid isPermaLink="false">ed6ef3c81c5346f412fd19dd442308e952baeb55</guid>

					<description>AWS Batch recently added support for Graviton and Windows containers on Fargate. Read about how these and other features like large task sizes and configurable local storage make AWS Batch on Fargate a fantastic serverless solution for your batch workloads.</description>
										<content:encoded>&lt;p&gt;The AWS Batch team recently &lt;a href="https://aws.amazon.com/about-aws/whats-new/2023/08/aws-batch-fargate-linux-arm64-windows-x86-containers-console/"&gt;launched&lt;/a&gt; support for Graviton and Windows containers running on AWS Fargate resources. Combine that announcement with other recent additions such as larger task sizes and configurable local storage and Fargate becomes an &lt;em&gt;even better&lt;/em&gt; serverless solution for your batch and asynchronous workloads.&lt;/p&gt; 
&lt;p&gt;Let’s take a look at the features that make Fargate a great choice for your small to medium scale AWS Batch environments, starting with the most recent announcements.&lt;/p&gt; 
&lt;h2&gt;Graviton resources&lt;/h2&gt; 
&lt;p&gt;Previously, Batch on Fargate only supported running containers that leverage an x86 CPU architecture, even though Fargate itself was able to run tasks using Linux containers built for the Graviton2 Arm architecture. We closed that gap with this release and you can now leverage Graviton2 which delivers 2-3.5 times better CPU performance per watt than any other processors of the same generation for your Batch jobs, making them a sustainable choice. They also have very capable price/performance characteristics and so are idea for analyzing data, logs, or other batch processing. Fargate does not yet support Graviton3.&lt;/p&gt; 
&lt;p&gt;The best way to find out whether x86 or aarch64 resources give you better price/performance is to try them out on your own application. You can leverage &lt;a href="https://aws.amazon.com/codepipeline/"&gt;AWS CodePipeline&lt;/a&gt; to build multi-architecture container images and store them in &lt;a href="https://aws.amazon.com/blogs/containers/introducing-multi-architecture-container-images-for-amazon-ecr/"&gt;Amazon Elastic Container Registry (Amazon ECR)&lt;/a&gt;. If you need some guidance for building multi-architecture container images, this AWS samples &lt;a href="https://github.com/aws-samples/aws-multiarch-container-build-pipeline"&gt;GitHub repository&lt;/a&gt; is a good place to start.&lt;/p&gt; 
&lt;p&gt;To leverage Fargate Graviton resources, you set the job definition’s runtimePlatform.cpuArchitecture parameter to ARM64. Then submit a job to a job queue that is configured to use Fargate resources. Graviton with Fargate is available in most AWS Regions, but there are some limitations for you should consider such as being limited to Linux containers. The full set of considerations are outlined in the documentation in &lt;a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-arm64.html"&gt;&lt;em&gt;Working with 64-bit ARM workloads on Amazon ECS&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Windows containers&lt;/h2&gt; 
&lt;p&gt;Since its release, AWS Batch has only supported running containers on top of the Linux operating system. Customers could build against the .NET Core for Linux, but not native Windows containers. Those containers are required to run Windows-only applications. Windows Batch jobs can also automate tasks that would benefit from a Windows environment, for example when you want to integrate with Microsoft Active Directory. Now customers can leverage the runtimePlatform.operatingSystemFamily parameter to designate that the container should run on a variety of &lt;a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform-windows-fargate.html"&gt;Windows Server releases supported by Fargate&lt;/a&gt;. With Fargate taking care of the licensing, running batch processing jobs on Windows is simpler than managing your own licensing for the ephemeral resources.&lt;/p&gt; 
&lt;p&gt;One thing to keep in mind when working with Windows containers is the larger size of the windows base layer compared to most Linux versions. Even with the recent Server Core image size reduction introduced with Windows Core 2022, at the time of writing it’s still 3.91GB. Additionally, you need to ensure &lt;a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform-windows-fargate.html"&gt;compatibility&lt;/a&gt; between the Windows version of the job and the Windows version used on the Batch compute environment. For a full set of considerations, refer to the documentation on using &lt;a href="https://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html#windows-considerations"&gt;Windows containers on AWS Fargate&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Lastly, we recommend that you map Windows and Linux compute environments to platform-specific job queues to avoid trying to run jobs using the wrong operating system.&lt;/p&gt; 
&lt;h2&gt;Larger task sizes&lt;/h2&gt; 
&lt;p&gt;Last October &lt;a href="https://aws.amazon.com/about-aws/whats-new/2022/10/aws-batch-increases-compute-memory-resource-configurations-fargate-type-jobs-4x/"&gt;we announced&lt;/a&gt; the ability to launch larger Fargate type jobs that use up to 16 vCPUs and up to 120 GiB of memory. This is approximately a 4x increase from previous limits.&lt;/p&gt; 
&lt;p&gt;These larger task sizes enable you to run more compute-heavy and/or memory-intensive applications like machine learning inference, scientific modeling, and distributed analytics on Fargate. Larger vCPU and memory options may also make migration to serverless container compute simpler for jobs that need more compute resources and cannot be easily re-architected into smaller sized containers.&lt;/p&gt; 
&lt;p&gt;To take advantage of these larger task sizes, set the appropriate values for the resourceRequirements parameter of your containerProperties. You’ll want to pay attention to the &lt;a href="https://docs.aws.amazon.com/batch/latest/APIReference/API_ResourceRequirement.html"&gt;API documentation&lt;/a&gt; about the valid combination of values for Fargate for VCPU and MEMORY. For example, if your value for VCPU is &lt;em&gt;1&lt;/em&gt;, then the valid values for MEMORY (in MiB) are &lt;em&gt;2048, 3072, 4096, 5120, 6144, 7168&lt;/em&gt;, or &lt;em&gt;8192&lt;/em&gt;.&lt;/p&gt; 
&lt;h2&gt;Configurable local storage volumes&lt;/h2&gt; 
&lt;p&gt;More recently, the AWS Batch team &lt;a href="https://aws.amazon.com/about-aws/whats-new/2023/03/aws-batch-configurable-ephemeral-storage-fargate/"&gt;released a feature&lt;/a&gt; that allows you to configure ephemeral storage up to 200 GiB in size on Fargate job definitions. The previous default was 20 GiB, which was not nearly enough for data-heavy processes like genomics or video processing. If you need even more storage space, you still have the option to configure a shared Amazon Elastic File System (Amazon EFS) mountpoint as part of the job definition.&lt;/p&gt; 
&lt;p&gt;To define the size of an ephemeral volume for a job, use the ephemeralStorage &lt;a href="https://docs.aws.amazon.com/batch/latest/APIReference/API_EphemeralStorage.html"&gt;parameter&lt;/a&gt; in the job definition’s containerProperties. You can set a value from 21 up to 200 GiB. Not specifying this parameter results in the default value of 20 GiB for local storage. This parameter is available only when you’re using Fargate.&lt;/p&gt; 
&lt;h2&gt;Other great Fargate features for batch workloads&lt;/h2&gt; 
&lt;p&gt;Even before adding support for larger tasks, larger local storage, and Graviton and Windows containers, Fargate had some nice advantages for running batch workloads.&lt;/p&gt; 
&lt;h3&gt;Job level cost allocation&lt;/h3&gt; 
&lt;p&gt;When running Batch jobs on EC2 resources, it can take some effort to accurately determine the compute cost of any given job that ran on the shared instance. Since Fargate jobs are metered individually, you have an easier time determining the cost of jobs as opposed to joining the metered usage against &lt;a href="https://docs.aws.amazon.com/AmazonECS/latest/userguide/usage-reports.html#task-cur"&gt;split cost allocation data for tasks&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Fargate with Amazon EC2 Spot capacity provider&lt;/h3&gt; 
&lt;p&gt;Fargate resources can leverage EC2 Spot capacity to run jobs at a discounted rate compared to the on-demand price. The usual Spot criteria apply — meaning that your jobs could be interrupted but you’ll get two-minute warning. For Fargate, the warning is sent as a task state change event to Amazon EventBridge and as a SIGTERM signal to the running task.&lt;/p&gt; 
&lt;p&gt;You can take advantage of the SIGTERM signal in your application code to gracefully exit the process, possibly checkpointing data to shared storage so that the task can start at a later time when capacity becomes available. The SIGTERM signal must be received from within the container to perform any cleanup actions. Failure to process this signal results in the task receiving a SIGKILL signal and that, in turn, may result in data loss or corruption.&lt;/p&gt; 
&lt;p&gt;Please note that Fargate Spot is not supported for&amp;nbsp;ARM64&amp;nbsp;and Windows-based containers on Fargate, but we still think it is a great option for your other Batch on Fargate workloads.&lt;/p&gt; 
&lt;h3&gt;Shared storage across jobs using Amazon Elastic File System&lt;/h3&gt; 
&lt;p&gt;Expanded local storage is great, but sometimes you need to send the output of one task as the input to another, and copying all that data around can take time (and hence money). Sometimes it is easier and faster to share data across tasks directly using a shared file system. A shared filesystem would also be advantageous for checkpointing processes for later restart, as in the case of Spot interruptions.&lt;/p&gt; 
&lt;p&gt;You can define mountpoints Amazon Elastic File System (EFS) at the job definition level. If you define two job definitions with the same mount point, they effectively have a common place to read and write data. You can read more about how to do that &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-support-for-per-job-amazon-efs-volumes-in-aws-batch/"&gt;in our “how to use EFS with AWS Batch” blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Finally, whenever you leverage shared storage across jobs and applications, you should always pay attention to how they may interact with each other. Specifically you should identify and address any security boundaries or data overwriting scenarios that may occur when separate job definitions mount the same volume.&lt;/p&gt; 
&lt;h2&gt;“Is Fargate right for me?”&lt;/h2&gt; 
&lt;p&gt;If you’re thinking about leveraging Fargate with AWS Batch, it’s worth taking a moment to consider your overall scale and throughput needs.&lt;/p&gt; 
&lt;p&gt;Batch maps a single job request to a single Fargate resource. This means that your maximum jobs-per-minute dispatch rate is limited at 500 task launches per minute. Alternatively, if you use Batch with EC2, multiple tasks are placed on running instances resulting in faster job placement.&lt;/p&gt; 
&lt;p&gt;Fargate also has a service limit defining the total number of concurrent Fargate vCPUs you can launch in a Region. Depending on the size of your jobs, it defines the number of concurrent jobs you can run. You can find your Fargate service limits using the &lt;a href="https://console.aws.amazon.com/servicequotas/home/services/fargate/quotas"&gt;AWS Service Quotas management console&lt;/a&gt;. You’ll also find a mechanism there for requesting an increase, should you need it.&lt;/p&gt; 
&lt;p&gt;Depending on your workload (e.g. the size of the tasks, duration of each job, and frequency of the jobs), it’s recommended to reach out to AWS Support in advance if you’re planning on running very large workloads using&amp;nbsp;Batch&amp;nbsp;and Fargate.&amp;nbsp;For more information on how to choose the underlying compute model, see our best practices documentation on &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/best-practices.html#bestpractice4"&gt;how to choose the right compute environment resource&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;With the release of Graviton and Windows container support, AWS Batch with Fargate has become a very capable serverless batch computing solution. It’s a great fit if you’re using AWS Batch for background, asynchronous tasks or for data processing.&lt;/p&gt; 
&lt;p&gt;If you want to try out the features mentioned in this post, log into the &lt;a href="https://console.aws.amazon.com/batch/home?"&gt;AWS Batch management console&lt;/a&gt; and try out the features yourself!&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Introducing login nodes in AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/introducing-login-nodes-in-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Austin Cherian]]></dc:creator>
		<pubDate>Wed, 18 Oct 2023 11:17:52 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[Slurm]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">b47b51c49022a980aec35361efa3990537c5c3a1</guid>

					<description>AWS ParallelCluster 3.7 now supports adding login nodes to your cluster, out of the box. Here, we'll show you how to set this up, and highlight some important tunable options for tweaking the experience.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright wp-image-2940 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/12/AdobeStock_593057386-expanded.png" alt="Introducing login nodes in AWS ParallelCluster" width="380" height="212"&gt;If you’re a user of a Slurm-based HPC cluster, it’s likely you interact with your cluster using a &lt;em&gt;login node&lt;/em&gt;. It’s the portal through which you access your cluster’s vast computational resources. You’ve probably used one to browse your files, submit jobs (and check on them) and compile your code.&lt;/p&gt; 
&lt;p&gt;You can do all these things using the headnode, too, but when a cluster is shared among multiple users in an enterprise or lab, someone compiling their code on the headnode can hamper other users trying to submit jobs, or just doing their own work. Some AWS ParallelCluster customers have worked around this limitation by manually creating login nodes for their users, but this involved a lot of undocumented steps and forced their admins to know about ParallelCluster’s internals.&lt;/p&gt; 
&lt;p&gt;So we’re happy to announce that AWS ParallelCluster 3.7 now supports adding login nodes to your cluster, out of the box. In this post we’ll show you an example of setting this up for a cluster, and highlight some of the more important tunable options for tweaking the experience.&lt;/p&gt; 
&lt;h2&gt;Getting started with AWS ParallelCluster Login Nodes&lt;/h2&gt; 
&lt;p&gt;Login nodes are specified in a similar way to compute nodes: as a ‘pool of nodes’ which in this case have single purpose. You can specify one pool of login nodes with as many instances as you would like to configure for your cluster.&lt;/p&gt; 
&lt;p&gt;If you want to try this feature out and explore how it works &lt;em&gt;without having to first design a cluster&lt;/em&gt;, you can use our &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/login_nodes"&gt;one-click launchable stack&lt;/a&gt; from the HPC Recipes Library (a &lt;em&gt;super useful&lt;/em&gt; resource which we described in a &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;recent post on this channel&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Or, you can follow the steps here to augment an existing configuration file you have on hand, to enable login nodes on a new cluster, or to make an update to an existing one.&lt;/p&gt; 
&lt;h3&gt;Step 1: Configure ParallelCluster with login nodes&lt;/h3&gt; 
&lt;p&gt;First, ensure that you’re using ParallelCluster 3.7.0. &amp;nbsp;(or later) which introduces this new feature. You can then create a new cluster or update an existing one with login nodes.&lt;/p&gt; 
&lt;p&gt;Enabling login nodes starts by configuring them in the YAML configuration file that describes your ParallelCluster. You can always retrieve the YAML config for a running cluster using the &lt;code&gt;pcluster-describe&lt;/code&gt; command (there’s an example in &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcluster.describe-cluster-v3.html"&gt;our documentation&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;When you defined your ComputeResources as part of &lt;code&gt;SlurmQueues&lt;/code&gt;, you specified the instance type, security groups, and several other details.&lt;/p&gt; 
&lt;p&gt;It’s similar for login nodes where you now define these parameters in a new section called &lt;code&gt;LoginNodes&lt;/code&gt;. There are a few more settings unique to login nodes like: &lt;code&gt;Count&lt;/code&gt;, which specifies the number of nodes; and the &lt;code&gt;GraceTimePeriod&lt;/code&gt;, which lets you set a &lt;em&gt;countdown timer&lt;/em&gt; for logged in users on a node when ParallelCluster is planning to stop it. The full spread of options is in our &lt;code&gt;LoginNodes&lt;/code&gt; &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/LoginNodes-v3.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The following snippet (which you can add to the end of your config file) shows how to setup a cluster with three login nodes, as part of a pool named &lt;code&gt;CFDCluster&lt;/code&gt; that act as a front door to users to login and submit fluid dynamics jobs on your cluster.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;LoginNodes:
   Pools:
    - Name: CFDCluster
      Count: 3  # Specify the number of login nodes you need
      InstanceType: t2.micro  # Choose an appropriate instance type
      Ssh:
         KeyName: CFSClusterKey # The key pair setup in your AWS account
      Networking: 
        SubnetId: 
          - subnet-XXXXXXXXX  # Specify the subnet for your login nodes
        SecurityGroups:
          - sg-XXXXXXXXX  # security groups your EC2 Login nodes will be within
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can customize the settings according to your requirements, including the instance type, subnet, and security groups. There are some additional settings you can specify like IAM roles and policies, and additional security groups. You can also define custom AMIs if you want to be more opinionated about the experience for your users when they login (for instance, by giving them access to compilers or licensed debuggers). You’ll find all these details in the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/LoginNodes-v3.html#LoginNodes-v3.properties"&gt;ParallelCluster documentation on LoginNodes properties&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 2: Update your cluster&lt;/h3&gt; 
&lt;p&gt;After you’ve finished editing your ParallelCluster config file, you’re ready to update your running cluster, or create a new one.&lt;/p&gt; 
&lt;p&gt;If you’re setting up a &lt;em&gt;new cluster&lt;/em&gt; with login nodes, use this command along with the configuration file that contains your settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster —cluster-configuration your_config_file_name —cluster-name your_cluster_name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you’re updating an &lt;em&gt;existing cluster&lt;/em&gt; then it’s just:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-cluster --cluster-configuration your_config_file_name --cluster-name your_cluster_name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Of course, replace &lt;code&gt;your_config_file_name&lt;/code&gt; with the name of your configuration file and &lt;code&gt;your_cluster_name&lt;/code&gt; with the name of your cluster.&lt;/p&gt; 
&lt;h3&gt;Step 3: Access your login nodes&lt;/h3&gt; 
&lt;p&gt;Once your cluster is ready, users with appropriate permissions can access the login nodes using SSH. But first they’ll need to find the address of the nodes they want to connect to.&lt;/p&gt; 
&lt;p&gt;Login nodes are provisioned with a single connection address to a Network Load Balancer (which is a feature of Elastic Load Balancer), specifically configured for the pool of login nodes. The exact address depends on the type of subnet you specified in the &lt;code&gt;LoginNodes&lt;/code&gt; pool configuration. All connection requests are managed by the Network Load Balancer using &lt;em&gt;round-robin routing&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;To retrieve the address of the single connection provisioned to access the login nodes, you can run the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcluster.describe-cluster-v3.html"&gt;pcluster describe-cluster&lt;/a&gt; command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster describe-cluster --cluster-name your_cluster_name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will also provide more information about the &lt;em&gt;status&lt;/em&gt; of the login nodes. Here’s an example what it returns for our login nodes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;"loginNodes": 
{ "status": "active", 
  "address": "8af2145440569xyz.us-east-1.amazonaws.com", 
  "scheme": "internet-facing|internal", 
  "healthyNodes": 3, 
  "unhealthyNodes": 0 },
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now it’s just a simple matter of SSH’ing to that address:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;ssh username@8af2145440569xyz.us-east-1.amazonaws.com&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Users can submit and manage jobs, and they’ll have access to any shared storage the cluster uses, too, so they can manage their files and see the output from their jobs.&lt;/p&gt; 
&lt;p&gt;You’ll notice that the &lt;code&gt;describe-cluster&lt;/code&gt; output also gave you some information on the status and health of your login nodes. You can get more granular information on the state, IP address, and launch time for individual login nodes in the pool, by using the &lt;code&gt;pcluster describe-cluster-instances&lt;/code&gt; command, specifying the node-type as &lt;code&gt;LoginNodes&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster describe-cluster-instances --node-type LoginNode --cluster-name your_cluster_name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check our &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcluster.describe-cluster-instances-v3.html"&gt;documentation&lt;/a&gt; for more about this.&lt;/p&gt; 
&lt;h2&gt;Controlling the population of login nodes&lt;/h2&gt; 
&lt;p&gt;Once configured, your login nodes will keep running until you remove them from the pool.&lt;/p&gt; 
&lt;h3&gt;Adding and removing login nodes&lt;/h3&gt; 
&lt;p&gt;Adding and removing login nodes from a pool is straight forward. You set the Count parameter of the &lt;code&gt;LoginNodes&lt;/code&gt; configuration in the ParallelCluster YAML file to your desired number. Then, update your running cluster configuration using the &lt;code&gt;pcluster update-cluster command&lt;/code&gt;, as before:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-cluster --cluster-configuration your_config_file_name --cluster-name your_cluster_name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To remove all login nodes you can just set the &lt;code&gt;Count&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt; and update the cluster.&lt;/p&gt; 
&lt;h3&gt;Setting a grace-time message for users when terminating Login Nodes&lt;/h3&gt; 
&lt;p&gt;When you remove login nodes from the pool using the &lt;code&gt;Count&lt;/code&gt; parameter we just described, ParallelCluster will terminate the Amazon EC2 instances powering them. During a node’s termination, logged in users will receive terminal notifications in their SSH windows alerting them about the impending shutdown. The message will specify a grace-time period during which no new connections will be allowed, except for those from the cluster’s default user. The message is customizable by the cluster administrator from the headnode or from a login node by editing the file &lt;code&gt;/opt/parallelcluster/shared_login_nodes/loginmgtd_config.json&lt;/code&gt; . Here’s what that looks like, by default:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;{
  "termination_script_path": "/opt/parallelcluster/shared_login_nodes/loginmgtd_on_termination.sh",
  "termination_message": "The system will be terminated within 10 minutes.",
  "gracetime_period": "10"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;AWS ParallelCluster 3.7 introduced login nodes – a powerful and flexible way to manage access and more carefully define the user experience for your HPC cluster users.&lt;/p&gt; 
&lt;p&gt;By distributing interactive user sessions across a pool of dedicated login nodes, you can improve your cluster’s performance, expand the tools available to your end users, and streamline everyone’s data access. With careful configuration and management, login nodes can become an integral part of your cloud-based HPC infrastructure, enabling efficient and scalable computing for your organization.&lt;/p&gt; 
&lt;p&gt;If you want to get started right away, you can use our &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/login_nodes"&gt;one-click launchable stack&lt;/a&gt; from the HPC Recipes Library to get a complete test environment launched quickly, which you can customize (or delete) later. This comes from the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;HPC Recipe Library&lt;/a&gt; which can help you quickly achieve feature-rich,&amp;nbsp;&lt;em&gt;reliable&lt;/em&gt;&amp;nbsp;HPC deployments that are ready to run a diverse range of workloads – regardless of where you’re starting from.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Financial services industry HPC migrations using AWS ParallelCluster with Slurm</title>
		<link>https://aws.amazon.com/blogs/hpc/financial-services-industry-hpc-migrations-using-aws-parallelcluster-with-slurm/</link>
		
		<dc:creator><![CDATA[Vinay Arora]]></dc:creator>
		<pubDate>Tue, 10 Oct 2023 13:14:45 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[FSI]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Slurm]]></category>
		<guid isPermaLink="false">38335ed4c953ad6c7ef375c45681146e51ae92b4</guid>

					<description>In this post, we’ll walk you through how banks and other financial services firms migrate or burst their grid workloads onto AWS using AWS ParallelCluster and the Slurm scheduler.</description>
										<content:encoded>&lt;p&gt;If you’re reading this post, you’ll know that HPC is a way of solving hard problems by slicing the problem space up amongst a lot of computers. You might have also heard the term ‘&lt;em&gt;grid computing’&lt;/em&gt; in the context of financial services. This refers to centralized systems which are typically used to provide ‘utility computing’ to support HPC workloads.&lt;/p&gt; 
&lt;p&gt;In financial services these types of jobs use complex algorithms to calculate as many risk or trading scenarios in parallel as possible, bound only by the amount of compute available. Financial services firms are always keen to reduce their operational costs and look to the cloud as a way to provide the elasticity and cost benefits to do this, but also: the ability to respond rapidly to changing economic conditions and market volatility.&lt;/p&gt; 
&lt;p&gt;Customers from all the major Financial Services Industry (FSI) verticals (including banking, capital markets, and insurance) are moving their on-premises grid workloads partially, or entirely, to the cloud. These workloads typically include high volumes of short-running tasks.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk through how these firms can migrate or burst their grid workloads onto AWS using AWS ParallelCluster and the Slurm scheduler – and we’ll give you an introduction to those two packages, if you’ve not met them before.&lt;/p&gt; 
&lt;h2&gt;Loosely-coupled Grid scenarios&lt;/h2&gt; 
&lt;p&gt;Grid workloads often require hundreds of thousands, or millions, of parallel processes to complete a calculation or simulation. Generally, these jobs run on a single node, consuming one process or multiple processes with shared memory parallelization (SMP) for parallelization &lt;em&gt;within&lt;/em&gt; the node. The parallel processes, or the iterations in the simulation, are post-processed to create one solution or discovery from the simulation.&lt;/p&gt; 
&lt;p&gt;During the simulation, the operations can take place in any order, and the loss of any one node or job in a loosely coupled workload usually doesn’t delay the entire calculation. The lost work can be picked up later or omitted altogether. The nodes involved in the calculation can vary in specification and power. This gives us some hints about the types of compute that we need to run these loosely-connected simulations.&lt;/p&gt; 
&lt;h2&gt;Architectural considerations&lt;/h2&gt; 
&lt;h3&gt;Networking&lt;/h3&gt; 
&lt;p&gt;Grid processes run in parallel and don’t communicate with each other (at all) to exchange information. This means the performance of jobs isn’t impacted by latency between the nodes. This is great because it means that instead of optimizing for latency, we can instead optimize for &lt;em&gt;instance availability&lt;/em&gt;: we can spread the job out over several Availability Zones (AZs), each of which have their own capacity pools. This is especially useful when using the Amazon Elastic Compute Cloud (Amazon EC2) Spot purchasing model because the availability of Spot instances in any given AZ can be limited.&lt;/p&gt; 
&lt;p&gt;By providing AWS ParallelCluster with options about where to place the compute, we can achieve an architecture that can scale up to tens or hundreds of thousands of cores.&lt;/p&gt; 
&lt;h3&gt;Storage&lt;/h3&gt; 
&lt;p&gt;Loosely coupled jobs also have a great characteristic when working with shared storage. Typically, they need to read data in, compute on it, and then write it out all without interfering with any other job. This is the perfect use case for &lt;em&gt;object storage&lt;/em&gt;. In this post, we’ll focus on Amazon Simple Storage Service (Amazon S3) as the storage layer because it provides internet-scale storage, it’s low cost, and it scales up to hundreds of thousands of tasks working simultaneously.&lt;/p&gt; 
&lt;h3&gt;Compute&lt;/h3&gt; 
&lt;p&gt;Choosing instance types means looking at the job’s requirement for memory:vCPU ratio, availability, and architecture support for the workload. (ie x86 or aarch64). In this post, we’ll stick with x86, to provide the greatest instance availability &lt;em&gt;and compatibility&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;ParallelCluster can request instances based on what’s available and will &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/slurm-short-capacity-fail-mode-v3.html"&gt;quickly “fail-over”&lt;/a&gt; if the instances can’t be launched. For the purposes of testing we chose instances from both x86 manufacturers’ families because they share the same instruction set and by choosing both we have a larger pool to draw from.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll use &lt;em&gt;vCPUs&lt;/em&gt; instead of &lt;em&gt;cores&lt;/em&gt; because most x86 instances onAWS have hyperthreading enabled. Hyperthreading allows two virtual cores to share the same arithmetic/logic unit (ALU). This increases overall throughput by doubling the core count but deceases the individual core’s &lt;em&gt;solve-time&lt;/em&gt;. This is sometimes a controversial topic in HPC, but for grid workloads in FSI, we’ve seen that hyperthreading is a net benefit, so we’re leaving it enabled.&lt;/p&gt; 
&lt;h3&gt;Scheduling&lt;/h3&gt; 
&lt;p&gt;Running a few tasks on a single instance is easy to manage by hand, but once you scale up the number of tasks, you’ll need a scheduler to manage their lifecycle. &lt;strong&gt;Slurm&lt;/strong&gt; is an open-source job scheduler that’s optimized for scheduling both tightly-coupled &lt;em&gt;and&lt;/em&gt; loosely-coupled tasks across multiple instances. It handles queuing, execution, retries, and accounting. ParallelCluster sets up Slurm using the &lt;a href="https://slurm.schedmd.com/elastic_computing.html"&gt;Slurm cloud bursting plugin&lt;/a&gt; and manages the scaling of Amazon EC2 instances.&lt;/p&gt; 
&lt;p&gt;We recommend enabling Slurm’s &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/slurm-mem-based-scheduling-v3.html"&gt;memory based scheduling&lt;/a&gt; feature. This provides an easy way to request specific memory amounts and compute directly in the job submission, i.e. 1 vCPU and 4 GB of memory could be set in the Slurm &lt;code&gt;sbatch&lt;/code&gt; file. It also helps Slurm maximize the CPU utilization of all the cores across the fleet.&lt;/p&gt; 
&lt;h2&gt;AWS ParallelCluster&lt;/h2&gt; 
&lt;p&gt;AWS ParallelCluster is an open-source cluster management tool that makes it straightforward for you to deploy, manage, and scale Slurm-based clusters on AWS. ParallelCluster allows you to leverage the elasticity of AWS by providing an easy way to expand and contract compute queues that you define. You can use multiple instance types, in multiple Availability Zones, and create job submission queues with a lot of creative freedom.&lt;/p&gt; 
&lt;p&gt;You can also quickly spin up clusters for experimenting and prototyping. Since ParallelCluster uses a simple YAML file to define all the resources you need, the process of standing up additional clusters – for production, dev, or testing – is an automated (and secure) process.&lt;/p&gt; 
&lt;p&gt;Let’s look at a reference architecture and a tutorial that you can use to deploy this in your own AWS account.&lt;/p&gt; 
&lt;div id="attachment_2873" style="width: 1077px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2873" loading="lazy" class="size-full wp-image-2873" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/18/CleanShot-2023-09-18-at-13.16.09.png" alt="Figure 1 –The reference architecture for FSI grid-style computing on AWS using AWS ParallelCluster. The queues span over all the Availability Zones in the region and read and write data from an Amazon S3 bucket in that region." width="1067" height="490"&gt;
 &lt;p id="caption-attachment-2873" class="wp-caption-text"&gt;Figure 1 –The reference architecture for FSI grid-style computing on AWS using AWS ParallelCluster. The queues span over all the Availability Zones in the region and read and write data from an Amazon S3 bucket in that region.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Implementation details&lt;/h2&gt; 
&lt;p&gt;In the following steps, we’ll show you how to setup a cluster with a &lt;strong&gt;headnode&lt;/strong&gt; and two &lt;strong&gt;compute queues&lt;/strong&gt;. The queues span over all the Availability Zones in the region and read and write data from an Amazon S3 bucket in that region. The Slurm scheduler process (&lt;code&gt;slurmctld&lt;/code&gt;) runs on the &lt;code&gt;HeadNode&lt;/code&gt; and users can login there to submit jobs via AWS Systems Manager (SSM), using SSH, or with a remote desktop connection using DCV as shown in the reference architecture in Figure 1.&lt;/p&gt; 
&lt;p&gt;You can follow along with these steps in the &lt;a href="https://catalog.workshops.aws/fsi-slurm-aws-parallelcluster"&gt;FSI Tutorial&lt;/a&gt; hands-on lab.&lt;/p&gt; 
&lt;p&gt;First, install AWS ParallelCluster CLI or UI. We’ll assume you’re using the CLI for now. In ParallelCluster, you initiate the creation of a cluster through the CLI by specifying the location of your configuration file, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster -c cluster.yaml -n cluster-name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Your config file might look something like the this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;HeadNode:
  InstanceType: c5a.xlarge
  Networking:
    SubnetId: subnet-846f1aff
  LocalStorage:
    RootVolume:
      VolumeType: gp3
  Iam:
    AdditionalIamPolicies:
      - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
  Dcv:
    Enabled: true
  CustomActions:
    OnNodeConfigured:
      Sequence:
        - Script: &amp;gt;-
            &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh" rel="noopener noreferrer"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh&lt;/a&gt;
        - Script: &amp;gt;-
            &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/pyxis/postinstall.sh" rel="noopener noreferrer"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/pyxis/postinstall.sh&lt;/a&gt;
          Args:
            - /fsx
  Imds:
    Secured: true
Scheduling:
  Scheduler: slurm
  SlurmQueues:
    - Name: c6i
      AllocationStrategy: capacity-optimized
      ComputeResources:
        - Name: spot
          Instances:
            - InstanceType: c6i.32xlarge
            - InstanceType: c6a.32xlarge
            - InstanceType: m6i.32xlarge
            - InstanceType: m6a.32xlarge
            - InstanceType: r6i.32xlarge
            - InstanceType: r6a.32xlarge
          MinCount: 0
          MaxCount: 100
      CustomActions:
        OnNodeConfigured:
          Sequence:
            - Script: &amp;gt;-
                &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/pyxis/postinstall.sh" rel="noopener noreferrer"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/pyxis/postinstall.sh&lt;/a&gt;
              Args:
                - /fsx
      ComputeSettings:
        LocalStorage:
          RootVolume:
            VolumeType: gp3
      Networking:
        SubnetIds:
          - subnet-8b15a7c6
          - subnet-04f44e9dc1c8ee425
          - subnet-91ab80f8
        PlacementGroup:
          Enabled: false
      CapacityType: SPOT
      Iam:
        AdditionalIamPolicies:
          - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
Region: us-east-2
Imds:
  ImdsSupport: v2.0
Image:
  Os: alinux2
SharedStorage:
  - Name: Ebs0
    StorageType: Ebs
    MountDir: /shared
    EbsSettings:
      VolumeType: gp3
      DeletionPolicy: Retain
      Size: '100'
Tags:
  - Key: parallelcluster-ui
    Value: 'true'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This config has a few key sections:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;HeadNode&lt;/code&gt; – this is the Amazon EC2 instance that runs the Slurm scheduler processes (&lt;code&gt;slurmctld&lt;/code&gt;, for example). Because this instance is responsible for scaling up the cluster, scheduling jobs, and serving the config files, we recommend going with an instance like the c6i.2xlarge that has sufficient resources (8 vCPUs and 16 GB memory) to run Slurm comfortably.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;SlurmQueues&lt;/code&gt; – these will depend on your resource requirements. For example, if you have jobs that requires 1 vCPUs and 2 GB memory, set up a queue with c6i instances. These instances all meet the memory to core ratio and specifying multiples different sizes of them will ensure you’re most likely to get your desired capacity. If you require more memory, the m6i and r6i instances offer 8 GB per vCPU and 16 GB per vCPU, respectively. We recommend you offer your users a choice by putting each instance family in their own &lt;code&gt;SlurmQueue&lt;/code&gt;. In addition to diversifying the instance types by providing multiple from each family, we also recommend you provide multiple Availability Zones, because this expands the number of pools you’re drawing from and is especially important for using Amazon EC2 Spot.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Storage&lt;/code&gt; – for grid workloads there’s rarely a need for a parallel filesystem – more important is a filesystem that can serve traffic in multiple Availability Zones and scale up to the thousands of jobs that can execute concurrently. We recommend using Amazon S3 or if there’s a need for a POSIX compliant filesystem, then Amazon FSx for OpenZFS. The creation of the filesystem is out of scope for this blog, but you can learn more by reading about &lt;a href="https://aws.amazon.com/blogs/hpc/expanded-filesystems-support-in-aws-parallelcluster-3-2/"&gt;Filesystem Support in AWS ParallelCluster&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When ParallelCluster builds your cluster, it’ll attach any existing filesystems you specified in the config file. Those filesystems are typically used for applications, libraries, and users’ data. It’s important to have a separation between the filesystem and cluster configuration because it allows for easy upgrades later when ParallelCluster release a new version which you want to take advantage of. In the config file we showed, it calls for ParallelCluster itself to create an EBS-based shared file system, mounted as &lt;code&gt;/shared&lt;/code&gt;. This will be NFS-exported to the compute nodes in the cluster when they’re created.&lt;/p&gt; 
&lt;p&gt;Once your cluster is up and running, you have several ways to interact with it:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;You can login to the head node: you can use normal SSH, or connect through SSM using the attached &lt;code&gt;SSMInstanceCore&lt;/code&gt; SSM allows access to instances that are not routable through SSH, like those in a private subnet.&lt;/li&gt; 
 &lt;li&gt;You can submit jobs using the default Slurm commands like &lt;code&gt;sbatch&lt;/code&gt;, &lt;code&gt;srun&lt;/code&gt;, and then monitor with &lt;code&gt;squeue&lt;/code&gt; and &lt;code&gt;sinfo&lt;/code&gt;. See Slurm &lt;a href="https://slurm.schedmd.com/sbatch.html"&gt;sbatch documentation&lt;/a&gt; for more details on options for submitting jobs, and the syntax for the run scripts.&lt;/li&gt; 
 &lt;li&gt;When Slurm detects new jobs in the queue, it uses the &lt;a href="https://slurm.schedmd.com/elastic_computing.html"&gt;Cloud Scheduling&lt;/a&gt; plugin, which calls a &lt;code&gt;ResumeProgram&lt;/code&gt; script that ParallelCluster manages. This script, in turn, calls the Amazon EC2 Fleet API to allocate instances to the queue. The jobs run soon after the instances bootstrap (typically 2-3 minutes).&lt;/li&gt; 
 &lt;li&gt;Once all jobs are complete, the instances are left running for a short ScaleDownIdleTime, in case more jobs arrive. This defaults to 10 minutes, but you can configure it yourself by specifying a &lt;code&gt;ScaleDownIdleTime&lt;/code&gt; &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/multiple-queue-mode-slurm-user-guide-v3.html"&gt;parameter in the ParallelCluster config file&lt;/a&gt;. After this time expires, ParallelCluster will terminate the instances, and wait again for new jobs to arrive in the queues.&lt;/li&gt; 
 &lt;li&gt;If the cluster is deleted, all instances &lt;em&gt;including the head node&lt;/em&gt; will be terminated.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There’s an &lt;a href="https://catalog.workshops.aws/fsi-slurm-aws-parallelcluster/en-US"&gt;AWS Workshop available online&lt;/a&gt; to show you how to calculate the price of a financial &lt;em&gt;auto-callable option&lt;/em&gt; based on this ParallelCluster architecture. The workshop provides step-by-step guidance to create and configure ParallelCluster and to deploy the containerized workload.&lt;/p&gt; 
&lt;h2&gt;Best Practices when using ParallelCluster with Slurm&lt;/h2&gt; 
&lt;h3&gt;Smaller and heterogeneous job duration&lt;/h3&gt; 
&lt;p&gt;If your job durations are small, for example in seconds and somewhat heterogeneous, we suggest following &lt;a href="https://slurm.schedmd.com/high_throughput.html"&gt;the recommendations from SchedMD&lt;/a&gt; (the makers of Slurm) to tune the cluster for the best job throughput – measured in tasks per second. It’s possible for Slurm to scale to 500 jobs/sec – under the right conditions and with the right architecture.&lt;/p&gt; 
&lt;h3&gt;Large Scale Clusters&lt;/h3&gt; 
&lt;p&gt;If your cluster exceeds 1024 instances, we recommend that following the &lt;a href="https://slurm.schedmd.com/big_sys.html"&gt;Slurm Large Cluster Administration guide.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You’ll also want to consider your &lt;strong&gt;HeadNode&lt;/strong&gt; architecture. Using a c6i.2xlarge won’t be sufficient for the network bandwidth demanded by slurmctld when it’s communicating with all the child nodes. We recommend using a network optimized instance with &lt;em&gt;at least&lt;/em&gt; 50 Gbps of dedicated bandwidth, like the c6in.8xlarge. Remember, the HeadNode instance communicates with the compute instances &lt;em&gt;and&lt;/em&gt; serves the Slurm config file, and application binaries via NFS.&lt;/p&gt; 
&lt;p&gt;You’ll also need to review your AWS account limits, (which, we remind you, are set on a per-region basis):&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Amazon EC2 Limits&lt;/strong&gt; – make sure you increase the Amazon EC2 limits for the compute nodes you plan to use. You can review these, and request limit increases via the AWS Service Quotas console.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;EBS limits&lt;/strong&gt; – each compute node mounts a 35 GB root volume (or larger) of gp3 EBS storage. If you plan to launch more instances than your quota (which defaults to 50 TiB), you should &lt;a href="https://console.aws.amazon.com/servicequotas/home/services/ebs/quotas"&gt;increase this limit&lt;/a&gt;, too.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Route53 limit&lt;/strong&gt; – each compute node is added to a Route53 Private Hosted Zone. This has a default limit of 10,000 records. If you plan to exceed this limit, make sure to &lt;a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/DNSLimitations.html"&gt;request a limit increase&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Acadian did this – so you can you&lt;/h3&gt; 
&lt;p&gt;Acadian chose to use Slurm in AWS ParallelCluster to execute thousands of these heterogeneous jobs. This cloud-native grid solution enables them to benefit from faster and seamless compute capacity provisioning &lt;em&gt;and&lt;/em&gt; on-demand auto scaling features, resulting in optimal results.&lt;/p&gt; 
&lt;p&gt;According to Jian Pan, Head of Quantitative Systems at Acadian, “&lt;em&gt;we are able to reap immediate benefit such as maintaining consistent 4hr model runtime in AWS, comparing to 20~40hr runtime on-premise when resource is under stress&lt;/em&gt;”.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Slurm can handle a cluster that grows and shrinks – driven by demand. It’s able to this dynamically by scaling compute resources from Amazon EC2. Compute fleets can be spun up to complete the workload in the queue. When the jobs are complete, ParallelCluster scales those same fleets back down to the minimum level you set (usually zero).&lt;/p&gt; 
&lt;p&gt;If you want to try this yourself, you can follow the steps in the &lt;a href="https://catalog.workshops.aws/fsi-slurm-aws-parallelcluster"&gt;FSI Tutorial&lt;/a&gt; hands-on lab, and when you’ve done that, you can try a real application to &lt;a href="https://catalog.workshops.aws/fsi-slurm-aws-parallelcluster/en-US/02-application/03-job-submit"&gt;calculate the price&lt;/a&gt; of a financial &lt;em&gt;auto-callable option&lt;/em&gt;. Let us know how to get on – you can reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Conceptual design using generative AI and CFD simulations on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/conceptual-design-using-generative-ai-and-cfd-simulations-on-aws/</link>
		
		<dc:creator><![CDATA[Dr. Vidyasagar Ananthan]]></dc:creator>
		<pubDate>Mon, 02 Oct 2023 14:19:33 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">ebf53bce2be2ea1f1b340e195b5361045a4f1f7f</guid>

					<description>In this post we’ll show how generative AI, combined with conventional physics-based CFD can create a rapid design process to explore new design concepts in automotive and aerospace from just a single image.</description>
										<content:encoded>&lt;p&gt;In this post we’ll demonstrate how &lt;a href="https://aws.amazon.com/generative-ai/"&gt;generative AI&lt;/a&gt; techniques can be combined with conventional physics-based&amp;nbsp;&lt;a href="https://aws.amazon.com/hpc/cfd/"&gt;computational fluid dynamics&lt;/a&gt;&amp;nbsp;(CFD) simulations to create a rapid conceptual design process that can be used to explore new design concepts in the automotive, motorsport, and aerospace sectors from just a single image.&lt;/p&gt; 
&lt;p&gt;Thanks to AWS services like &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, and the open-source&amp;nbsp;&lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt;, this can all be combined into an event-driven workflow on AWS that could scale to explore millions of possible scenarios. TwinGraph is the model orchestration module within the open-source&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow framework&lt;/a&gt;&amp;nbsp;that enables deploying predictive modeling, simulation, and&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;Level 4 Digital Twin&lt;/a&gt;&amp;nbsp;workflows at scale.&lt;/p&gt; 
&lt;p&gt;The generative capability of machine learning (ML) algorithms holds significant promise across diverse industries. Generative AI techniques are powered by large machine learning models, pre-trained on very large amounts of data. We’re seeing the impact of these models in several areas, including transformer models for natural language processing, text-to-image models like &lt;a href="https://en.wikipedia.org/wiki/Stable_Diffusion"&gt;Stable Diffusion&lt;/a&gt; for image manipulation, and generative adversarial networks for zero-shot classifiers.&lt;/p&gt; 
&lt;h2&gt;Why is this important?&lt;/h2&gt; 
&lt;p&gt;Today, an AI image generator like Stable Diffusion can be employed to generate conceptual designs for cars, planes, and other vehicles. However, these methods lack a foundation in understanding performance factors like aerodynamic drag because they don’t consider the underlying physical laws and the design constraints like noise levels and the extra energy usage they drive. In an era of increased emphasis on energy efficiency and sustainability, conceptual designs must extend beyond just adhering to style guidelines.&lt;/p&gt; 
&lt;p&gt;Over past decades, CFD-driven design optimization grew significantly across a number of industries. A general workflow typically involves simulating complex geometries using conventional physics-based solvers for different sets of individual parameters (e.g. &lt;em&gt;wing chord length&lt;/em&gt;, or &lt;em&gt;rear window angle&lt;/em&gt;) and finding the optimal setting across an entire parameter space.&lt;/p&gt; 
&lt;p&gt;While these solvers offer a lot of accuracy, they’re computationally intensive and time-consuming, which slows down the pace of engineering design. There’s been a growing interest in combining conventional CFD models with ML approaches to try to overcome these computational challenges. Using generative AI in the design process allows efficient sweeping of the parameter space in a non-parametric manner, based on physically meaningful design configurations of the system being examined.&lt;/p&gt; 
&lt;p&gt;We want to show you how generative AI can be applied to design optimization. In this post we’ll focus on its effectiveness in finding better solutions for drag reduction. Our approach combines generative AI for the initial design phase with &lt;a href="https://www.openfoam.com/"&gt;OpenFOAM&lt;/a&gt; CFD simulations for the evaluation of vehicle aerodynamics.&lt;/p&gt; 
&lt;p&gt;Through this process, we’ve developed a workflow that empowers users to define a non-parametric design optimization problem as an algorithm suitable for execution on AWS – at scale with robust infrastructure. This is underpinned by&amp;nbsp; &lt;a href="https://github.com/aws-samples/twingraph"&gt;Twingraph&lt;/a&gt; which does the undifferentiated heavy lifting of dynamic task orchestration, gathering provenance information, and scaling.&lt;/p&gt; 
&lt;div id="attachment_2896" style="width: 1860px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-2896" loading="lazy" class="wp-image-2896 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.25.21@2x.png" alt="Figure 1: Overall workflow for iterative design optimization of car aerodynamics by combining Generative AI techniques and Computational Fluid Dynamics simulations" width="1850" height="576"&gt;
 &lt;p id="caption-attachment-2896" class="wp-caption-text"&gt;Figure 1: Overall workflow for iterative design optimization of car aerodynamics by combining Generative AI techniques and Computational Fluid Dynamics simulations&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Design iterations through Stable Diffusion&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/CompVis/stable-diffusion"&gt;Stable Diffusion&amp;nbsp;&lt;/a&gt;is a generative AI model which enables image generation through text-driven manipulation. The underlying architecture of Stable Diffusion comprises of three key phases:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;obtaining a latent representation of the image, which captures the meaning of objects/people depicted in the image&lt;/li&gt; 
 &lt;li&gt;progressively adding Gaussian noise to this representation&lt;/li&gt; 
 &lt;li&gt;reconstructing the image through removing the noise, resulting in a modified variant of the original image which reflects the semantics of the text prompt.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As an example, in Figure 2 we show the results from using Stable Diffusion to modify an automotive design, starting from a stock image of a sedan to convert it into a sporty aerodynamic design. This resulted in a series of transformations from the original to a modified image. For this exercise, the pre-trained Stable Diffusion model was used for the image generation, but this can be fine-tuned to match the design philosophy of the individual car manufacturer.&lt;/p&gt; 
&lt;div id="attachment_2906" style="width: 490px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2906" loading="lazy" class="wp-image-2906 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/Figure2-converted.gif" alt="Figure 2: Sequential transformation of a car design, given an appropriate prompt to improve aerodynamics and make the car sportier, using an image-to-image pipeline with Stable Diffusion 2.1" width="480" height="360"&gt;
 &lt;p id="caption-attachment-2906" class="wp-caption-text"&gt;Figure 2: Sequential transformation of a car design, given an appropriate prompt to improve aerodynamics and make the car sportier, using an image-to-image pipeline with Stable Diffusion 2.1&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;But this transformation &lt;em&gt;isn’t based on the underlying physics&lt;/em&gt; – it’s an interpretation of the prompts alongside the latent space embedding, which is a condensed mathematical representation of the image, within the Stable Diffusion algorithm. In turn, this interpretation is really driven by training data that exposed the model to sports cars, leading to a predisposition for similar looking designs. To accurately evaluate if the transformation path is an improvement in the aerodynamics of the vehicle, the natural step would be to convert the image into a 3D representation that can be used &amp;nbsp;for high-fidelity CFD simulations.&lt;/p&gt; 
&lt;h2&gt;Generation of point-cloud using neural radiance fields&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2012.02190"&gt;Neural radiance fields&lt;/a&gt;&amp;nbsp;(NeRF) are algorithms showing great promise for converting one or more images into full 3D representations. Combining&amp;nbsp;&lt;a href="https://arxiv.org/abs/2211.11674"&gt;bootstrapped NeRF&amp;nbsp;&lt;/a&gt;with &lt;em&gt;generative adversarial networks&lt;/em&gt; (GANs), we can reconstruct multiple poses of objects to augment and improve the predictions.&lt;/p&gt; 
&lt;p&gt;To make this work, we feed images of the car into NeRFs to obtain &lt;em&gt;signed-distance functions&lt;/em&gt; (SDFs) and construct point-cloud representations like we’ve shown in Figure 3. We fine-tuned the NeRF model using the &lt;a href="https://cvgl.stanford.edu/projects/pascal3d.html"&gt;Pascal3D&lt;/a&gt; data set for 3D object reconstruction.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 3: Point-cloud of car (bottom) obtained using NeRF." width="500" height="375" src="https://www.youtube-nocookie.com/embed/3NtY7eTLVTs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 3: Point-cloud of car (bottom) obtained using NeRF by using the base image (top), transforming the image into a 3D structure representing the car.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Reconstruction of surface topology from point-cloud&lt;/h2&gt; 
&lt;p&gt;The point-cloud representation lacks the crucial connectivity or surface topology information that’s required for understanding the behavior of air flow around the vehicle.&lt;/p&gt; 
&lt;p&gt;To reconstruct the surface from the point-cloud, we first generated an &lt;em&gt;unstructured polygonal Alpha shape mesh&lt;/em&gt; (for a non-convex hull). We achieved this through a coarse Delaunay triangulation using the &lt;a href="http://www.open3d.org/"&gt;Open3D&lt;/a&gt; library. This computational geometric technique identifies the encompassing surface including the points presented in the point-cloud, generated from NeRF.&lt;/p&gt; 
&lt;p&gt;To further refine the mesh, we extracted the points generated on the surface (nodes of the initial triangulation) together with estimated normals from the Alpha shapes. This surface point-cloud is ingested into a&amp;nbsp;&lt;a href="https://research.nvidia.com/labs/toronto-ai/NKSR/"&gt;Neural Kernel Surface Reconstruction&lt;/a&gt;&amp;nbsp;(NKSR) algorithm, which is a machine learning technique that can perform fast surface reconstructions from sparse data. The final result is displayed in Figure 4. While this technique doesn’t capture finer surface details, the overall shape of the car is approximately modeled by the resulting mesh.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 4: Surface mesh generated using Neural Kernel Surface Reconstruction" width="500" height="375" src="https://www.youtube-nocookie.com/embed/sgHBGi5kMN0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 4: Surface mesh generated using Neural Kernel Surface Reconstruction, with the original point cloud (top) and the triangulated mesh (bottom) showing a good match in general topographic features.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Running CFD simulations on OpenFOAM&lt;/h2&gt; 
&lt;p&gt;We used &lt;a href="https://www.openfoam.com/"&gt;OpenFoam&lt;/a&gt; to compute the flow field around the vehicle. We build an unstructured hex-dominate mesh with prismatic boundary layer cells using blockMesh and SnappyHexMesh from the .obj file we generated in the previous step.&lt;/p&gt; 
&lt;p&gt;For this post, we intentionally opted for much lower refinement levels than what is typically employed in the industry (we can increase these levels as required). Our mesh count was approximately one million cells, on average – this changes slightly depending on the geometry itself. To accelerate the CFD part of the process we restricted ourselves to steady-state RANS simulations using the k-omega SST model (for industrial applications you could extend this to use hybrid RANS-LES or WMLES methods, which have a higher fidelity).&lt;/p&gt; 
&lt;p&gt;Finally, in our setup we used the &lt;em&gt;simpleFoam&lt;/em&gt; solver based upon the semi-implicit method for pressure-linked equations (SIMPLE) algorithm. Table 1 shows the parameters.&lt;/p&gt; 
&lt;div id="attachment_2924" style="width: 410px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2924" loading="lazy" class="wp-image-2924 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.37.31@2x-1.png" alt="Table 1: Constants used for computational fluid dynamics simulations" width="400" height="155"&gt;
 &lt;p id="caption-attachment-2924" class="wp-caption-text"&gt;Table 1: Constants used for computational fluid dynamics simulations&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The image in Figure 5 displays the streamlines as well as surface pressure over the initial car design. For this illustrative simulation, we used 425,045 cells for the mesh.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 5: Streamlines visualizing flow field around a generated car mesh." width="500" height="281" src="https://www.youtube-nocookie.com/embed/EUzvYkivaZo?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 5: Streamlines visualizing flow field around a generated car mesh – these are indicative of the fluid velocities, while the colors on the car surface represent the surface pressure magnitude.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;To compute the drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;values during post-processing, we derived a reference wheelbase length and frontal surface areas based on the initial designs – these reference values remain relatively constant across all observations. We used the final drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;values in the pipeline to evaluate and rank the generated design options to find the best intermediate choices.&lt;/p&gt; 
&lt;h2&gt;Integration into Simulation-Guided Design Workflow on AWS&lt;/h2&gt; 
&lt;p&gt;The overall workflow has five key components: image generation, Stable Diffusion, point-cloud with NeRF, mesh generation with Open3d and NKSR, and finally the OpenFoam CFD simulation. Each of these are containerized and orchestrated by the&amp;nbsp;&lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt; orchestration module within the &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We deployed this workflow by using&amp;nbsp;&lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;&amp;nbsp;and scaled it as needed to find the optimal designs. We achieved the necessary scale by leveraging both CPU &lt;em&gt;and&lt;/em&gt; GPU architectures depending on the specific requirements of each workflow component.&lt;/p&gt; 
&lt;div id="attachment_2899" style="width: 1720px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-2899" loading="lazy" class="size-full wp-image-2899" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.32.19@2x.png" alt="Figure 6: AWS Architecture diagram for a running workflow from connecting with a remote client, to launching the algorithmic pipeline on TwinGraph and executing jobs on AWS Batch, and visualizing results." width="1710" height="974"&gt;
 &lt;p id="caption-attachment-2899" class="wp-caption-text"&gt;Figure 6: AWS Architecture diagram for a running workflow from connecting with a remote client, to launching the algorithmic pipeline on TwinGraph and executing jobs on AWS Batch, and visualizing results.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We repeated the experiment for a number of generated images across multiple cycles, and uploaded the results from each experiment automatically to&amp;nbsp;&lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service (Amazon S3)&lt;/a&gt;. This ensured persistent storage of our results. The necessary meta-data provenance information from each experiment was automatically uploaded to an&amp;nbsp;&lt;a href="https://aws.amazon.com/neptune/"&gt;Amazon Neptune&lt;/a&gt;&amp;nbsp;graph database for the subsequent analysis.&lt;/p&gt; 
&lt;p&gt;Once the results were generated, we could retrieve the specific outcomes we were interested in from Amazon S3 using a GPU instance – we ran our visualization interface thought a high-performance remote desktop protocol called &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;NICE DCV&lt;/a&gt; (an AWS product).&lt;/p&gt; 
&lt;p&gt;Overall,&amp;nbsp;&lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt;&amp;nbsp;orchestrates tasks in an asynchronous manner, which means we can execute multiple experiments concurrently, at scale using AWS Batch.&lt;/p&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;p&gt;As part of the numerical experiments, we ran 10 different instances (10 &lt;em&gt;variants&lt;/em&gt;) of the Stable Diffusion image-to-image generation, surface reconstruction, &lt;em&gt;and&lt;/em&gt; CFD simulations with the same initialization as in Figure 1 with a generic sedan.&lt;/p&gt; 
&lt;p&gt;We used the variant image corresponding to the lowest drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;value, to seed the &lt;em&gt;next&lt;/em&gt; image generation sequence cycle. To guide the design iteration process at a fine level, we reduced the strength of image-to-image changes significantly, compared to Figure 1.&lt;br&gt; We repeated this cycle 20 times (or 20 &lt;em&gt;generations&lt;/em&gt;) and the results are plotted in Figure 7 as the minimum drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;per generation and in Figure 8 as a heat map of the drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;&amp;nbsp;values for each of the generated designs.&lt;/p&gt; 
&lt;div id="attachment_2900" style="width: 1608px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-2900" loading="lazy" class="size-full wp-image-2900" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.33.34@2x.png" alt="Figure 7: Minimum drag coefficients per generation, corresponding to the image variants used to create the subsequent generation of variants. The general downward trend, though non-monotonic due to intermediate car configurations, is indicative that the pipeline improves the drag performance of the best variant through the generations." width="1598" height="968"&gt;
 &lt;p id="caption-attachment-2900" class="wp-caption-text"&gt;Figure 7: Minimum drag coefficients per generation, corresponding to the image variants used to create the subsequent generation of variants. The general downward trend, though non-monotonic due to intermediate car configurations, is indicative that the pipeline improves the drag performance of the best variant through the generations.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 7 shows that the averaged drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) decreases gradually from an initial value of 0.46 to approximately 0.4. During the sequential generations, the decrease is non-monotonic. This is due to the non-parametric and non-linear nature of the optimization procedure – allowing the image generation process to arbitrarily morph the car design with a final goal of reducing drag.&lt;/p&gt; 
&lt;p&gt;Moreover, the intermediate design configurations have incomplete components which result in increased drag for several generations until the image generation process resolves the features. We investigated this further through drag coefficients corresponding to &lt;em&gt;each variant&lt;/em&gt; in the 20 generations we show in Figure 8. The average drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) slightly &lt;em&gt;increases&lt;/em&gt; in the intermediate generations but then gradually &lt;em&gt;decreases&lt;/em&gt; towards the end of the 20 generations.&lt;/p&gt; 
&lt;div id="attachment_2901" style="width: 1632px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-2901" loading="lazy" class="size-full wp-image-2901" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.34.33@2x.png" alt="Figure 8: Illustration of drag coefficients associated with each of 10 variants (vertical axes) during each of generations (horizontal axes, sequences consisting of image generation, mesh reconstruction and simulation). The blue regions indicate lower drag, and we can observe a trend of increased blue regions going across the generations." width="1622" height="904"&gt;
 &lt;p id="caption-attachment-2901" class="wp-caption-text"&gt;Figure 8: Illustration of drag coefficients associated with each of 10 variants (vertical axes) during each of generations (horizontal axes, sequences consisting of image generation, mesh reconstruction and simulation). The blue regions indicate lower drag, and we can observe a trend of increased blue regions going across the generations.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2918" style="width: 482px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2918" loading="lazy" class="size-full wp-image-2918" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/Figure9-converted.gif" alt="Figure 9: Sequence of transitions through the generations for the optimal variant in drag coefficient, demonstrating a smoothing of the hood of the car, a reduced angle of windshield." width="472" height="346"&gt;
 &lt;p id="caption-attachment-2918" class="wp-caption-text"&gt;Figure 9: Sequence of transitions through the generations for the optimal variant in drag coefficient, demonstrating a smoothing of the hood of the car, a reduced angle of windshield.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 9 provides insights into the evolution of car design across generations. The hood of the car adapts to a curved shape with considerable removal of material. Also, the angle of the windshield to the horizon reduces and there are slight changes in the curvature of the car’s rear section. Overall these are subtle, yet significant, changes driven by a generative AI process demonstrating the potential for guiding and making informed design choices through an automated &lt;em&gt;physics-informed&lt;/em&gt; pipeline.&lt;/p&gt; 
&lt;h2&gt;Limitations and future work&lt;/h2&gt; 
&lt;p&gt;The approaches presented here hold promise for accelerating aesthetically and sustainability-focused non-parametric design optimization. But there are limits at each stage that will need to be overcome.&lt;/p&gt; 
&lt;p&gt;As pre-trained Stable Diffusion network weights change and evolve (due to further training), the predictions will change in an unpredictable manner – making repeatability an issue. Also, capturing surface topology and roughness accurately using this method is complex due to the lossy reconstruction from point-clouds compared to full resolution computer-aided design (CAD) meshes. This is important for accurate drag calculations.&lt;/p&gt; 
&lt;p&gt;However, with improvements in generative AI algorithms, we can expect workflows that couple machine learning to classic physics-based simulations to provide practical benefits. We can integrate multiple components into a single algorithmic pipeline that can scale agnostically with respect to the underlying infrastructure, computer architecture and choice of programming models. This provides a template to deploy &lt;em&gt;future&lt;/em&gt; design optimization concepts across multiple domains more easily and reliably.&lt;/p&gt; 
&lt;h2&gt;Conclusions&lt;/h2&gt; 
&lt;p&gt;In this post, we discussed the potential for integrating generative AI techniques with physics-based CFD simulations. We demonstrated a methodology that has the capability to guide the image generation process with physics-informed drag coefficients (C&lt;sub&gt;d&lt;/sub&gt;) using CFD simulations.&lt;/p&gt; 
&lt;p&gt;We also showcased how to turn these images into 3D meshes. These meshes were used for the CFD simulations, but can &lt;em&gt;also &lt;/em&gt;be imported into CAD programs so they can be used in real design processes.&lt;/p&gt; 
&lt;p&gt;Best of all, we combined this into a single event-driven workflow thanks to &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; and &lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt; – which allows for scaling out machine learning and simulation tasks.&lt;/p&gt; 
&lt;p&gt;This work has been focused on running inference using generative AI models, but we could use &lt;a href="https://aws.amazon.com/bedrock/"&gt;Amazon Bedrock&lt;/a&gt; and &lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html"&gt;Amazon SageMaker JumpStart&lt;/a&gt; to improve the developer experience when fine-tuning the &amp;nbsp;models. Amazon Bedrock is a fully managed service that provides foundation models from a collection of leading AI startups (and from Amazon itself) via an API. Bedrock allows you to speed up your development of generative AI applications that you can privately customize and scale using secure and reliable infrastructure in AWS. SageMaker Jumpstart offers you the capability to train and fine tune those foundation models in a managed environment.&lt;/p&gt; 
&lt;p&gt;This approach still requires further development to be applicable to industry, but it demonstrates the potential of integrating generative AI techniques with physics-based simulations. This potential extends beyond automotive CFD design and holds promise to a lot of other scientific and engineering fields.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Implementing AWS ParallelCluster in a Shared VPC</title>
		<link>https://aws.amazon.com/blogs/hpc/implementing-aws-parallelcluster-in-a-shared-vpc/</link>
		
		<dc:creator><![CDATA[Pedro Gil]]></dc:creator>
		<pubDate>Tue, 26 Sep 2023 15:11:47 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">dbe82324fb2c3a9277531ded24917622b6e0ec25</guid>

					<description>In this post we’ll show you how to deploy ParallelCluster in a shared VPC environment so you can separate infrastructure management, cluster operations, and help segregate costs, too.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was controbuted by Pedro Gil, Solutions Architect, and Ryan Anderson, Software Engineer HPC Engineering&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;AWS Shared Virtual Private Cloud (VPC) is a feature that allows multiple AWS accounts to share a single VPC, enabling them to collaborate on resources within the same network. This helps in managing and sharing network resources within an organization, and allows teams to work independently without compromising the security of the VPC.&lt;/p&gt; 
&lt;p&gt;AWS ParallelCluster is an open source cluster management tool that makes it easy for you to deploy and manage high performance computing (HPC) clusters on AWS.&lt;/p&gt; 
&lt;p&gt;Installing ParallelCluster in a shared VPC – when using Slurm as the scheduler – is often a challenge because ParallelCluster assumes that the Amazon Route53 Hosted Zone and the VPC belongs to the same account where the cluster is being created.&lt;/p&gt; 
&lt;p&gt;In this post we’ll show you a solution that gets ParallelCluster up and running in a shared VPC environment where the VPC belongs to one account and it is shared to another account for resource deployment operations.&lt;/p&gt; 
&lt;h2&gt;Overview of our solution&lt;/h2&gt; 
&lt;p&gt;We’ll show you how to deploy ParallelCluster into Account B using a shared VPC from infrastructure in Account A.&lt;/p&gt; 
&lt;div id="attachment_2808" style="width: 1624px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2808" loading="lazy" class="size-full wp-image-2808" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-16.41.52@2x.png" alt="Figure 1 – VPC Resource Share created on Account A (VPC infrastructure account) and shared to Account B (ParallelCluster creation account) using AWS Resource Access Manager." width="1614" height="1400"&gt;
 &lt;p id="caption-attachment-2808" class="wp-caption-text"&gt;Figure 1 – VPC Resource Share created on Account A (VPC infrastructure account) and shared to Account B (ParallelCluster creation account) using AWS Resource Access Manager.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;These are the steps we’ll take for the solution:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Step 1- Create a Route 53 Private Hosted Zone and associate it with shared VPC&lt;/li&gt; 
 &lt;li&gt;Step 2- Create an additional AWS Identity and Access Management&lt;/li&gt; 
 &lt;li&gt;(IAM) Policy&lt;/li&gt; 
 &lt;li&gt;Step 3- Install AWS ParallelCluster and its configuration file&lt;/li&gt; 
 &lt;li&gt;Step 4- Modify the configuration file to add the additional policy and to include the &lt;code&gt;Hosted Zone ID&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Step 5- Create a cluster using this configuration file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;For this walkthrough, you should have the following prerequisites:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An AWS account B (cluster creation account) where the VPC was shared and an AWS account A (VPC account owner) where the VPC was created (like in Figure 1).&lt;/li&gt; 
 &lt;li&gt;A user with sufficient privileges to create the IAM Policy, Amazon Route53 Private Zone and to install ParallelCluster.&lt;/li&gt; 
 &lt;li&gt;AWS resources: AWS console access, AWS CLI using AWS Cloud9&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Step 1 – Create Amazon Route53 Private Hosted Zone&lt;/h2&gt; 
&lt;p&gt;ParallelCluster uses Amazon Route53 Private Hosted Zone to resolve cluster nodes and creating one across accounts requires the following specific procedure.&lt;/p&gt; 
&lt;p&gt;First, log in to Account B and create a Private Hosted Zone using the Route53 service console. Associate it with any existing VPC in Account B (you’ll remove this association at the end of this section). Take note of the Private Hosted Zone id you just created.&lt;/p&gt; 
&lt;p&gt;Next, using AWS Cloud9 in Account B, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws route53 create-vpc-association-authorization --hosted-zone-id &amp;lt;hosted-zone-id&amp;gt; --vpc VPCRegion=&amp;lt;region&amp;gt;,VPCId=&amp;lt;vpc-id&amp;gt; --region &amp;lt;region&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command authorizes the VPC association between the private hosted zone you just created and the VPC from Account A. Use the Hosted Zone ID that you obtained in previous step. Use the AWS region and ID of the shared VPC.&lt;/p&gt; 
&lt;p&gt;Now, using AWS Cloud9 instance in Account A, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws route53 associate-vpc-with-hosted-zone --hosted-zone-id &amp;lt;hosted-zone-id&amp;gt; --vpc VPCRegion=&amp;lt;region&amp;gt;,VPCId=&amp;lt;vpc-id&amp;gt; --region &amp;lt;region&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command creates the association between the private hosted zone in Account B and the VPC in Account A. Use the Hosted Zone ID from earlier, and the Region and ID of the VPC in Account A.&lt;/p&gt; 
&lt;p&gt;Finally, go back to the Route 53 service console on Account B and verify that the shared VPC association with the Private Hosted Zone is listed. Delete the association of the local VPC done in step A.&lt;/p&gt; 
&lt;h2&gt;Step 2 – Create the IAM policy&lt;/h2&gt; 
&lt;p&gt;We need to create an additional policy for the head node to have permissions to create cluster nodes in the shared VPC.&lt;/p&gt; 
&lt;p&gt;First, login to Account B and create a new IAM Policy using the following template. Use ManageHeadnodePermissions as the name of new policy. Use ID for Account A and the subnet ID from the Shared VPC where the compute nodes will be created.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;{
    "Version": "2012-10-17",
   "Statement": [
        {
            "Sid": "SharedSubnets",
            "Effect": "Allow",
            "Action": [
                "ec2:CreateTags",
                "ec2:RunInstances",
                "ec2:CreateFleet"
            ],
    "Resource": "arn:aws:ec2:&amp;lt;region&amp;gt;:&amp;lt;Account A ID&amp;gt;:subnet/&amp;lt;Subnet ID&amp;gt;"
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Step 3 – Install and configure ParallelCluster&lt;/h2&gt; 
&lt;p&gt;You should follow the steps to &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-configuring.html"&gt;install ParallelCluster&lt;/a&gt; on a suitable instance or laptop. After doing so, you’ll need to create a config file for ParallelCluster to use. The pcluster configure will step you through this process, asking you some questions, and creating a new config file at the end.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster configure –config config-file.yaml&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Choose the AWS region ID where your shared VPC is located.&lt;/li&gt; 
 &lt;li&gt;Choose your Amazon Elastic Compute Cloud (Amazon EC2) key pair – you’ll need to have one already.&lt;/li&gt; 
 &lt;li&gt;Choose Slurm as your scheduler&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;&amp;lt;n&amp;gt;&lt;/strong&gt; for VPC creation and select the existing shared VPC&lt;/li&gt; 
 &lt;li&gt;Choose an appropriate operating system&lt;/li&gt; 
 &lt;li&gt;Select appropriate instance types and queue configurations for your workload&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The next step would usually be to run pcluster create to build the cluster using the choices and parameters you just entered, however before we do that, we need to delve into the configuration file that this process produced and make some changes.&lt;/p&gt; 
&lt;h2&gt;Step 4 – Modify ParallelCluster configuration file&lt;/h2&gt; 
&lt;p&gt;Modify your ParallelCluster configuration file to include the following using your own Hosted Zone ID and the new Policy Name you created in the previous steps &lt;code&gt;ManageHeadnodePermissions&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;HeadNode:
  Iam:
    AdditionalIamPolicies:
      - Policy: arn:aws:iam::&amp;lt;Account B ID&amp;gt;:policy/ManageHeadnodePermissions
Scheduling:
  Scheduler: slurm
  SlurmSettings:
    Dns:
      HostedZoneId: &amp;lt;hosted-zone-id&amp;gt; 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 5 – Create your cluster&lt;/h4&gt; 
&lt;p&gt;It’s time to create your cluster. If you need to, you can find more details &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-configuring.html"&gt;in our documentation&lt;/a&gt;. But for now, you just need to run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster –cluster-name test-cluster –cluster-configuration cluster-config.yaml&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Congratulations, you have finished the creation of your cluster using AWS ParallelCluster in a shared VPC.&lt;/p&gt; 
&lt;h2&gt;Cleaning up&lt;/h2&gt; 
&lt;p&gt;It is a best practice to delete the association authorization after you create the association. This step prevents you from recreating the same association later and will not prevent you to create new ParallelCluster instances later. To delete the authorization, reconnect to Account A. Then, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws route53 delete-vpc-association-authorization --hosted-zone-id &amp;lt;hosted-zone-id&amp;gt; --vpc VPCRegion=&amp;lt;region&amp;gt;,VPCId=&amp;lt;vpc-id&amp;gt; --region &amp;lt;region&amp;gt;&amp;nbsp;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You might also want to delete cluster resources after you are done with your workload by running the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster delete-cluster –region &amp;lt;region&amp;gt; --cluster-name test-cluster&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;We’ve shown you how to install AWS ParallelCluster in a shared VPC environment, which means you can use a common VPC between AWS accounts inside an organization, while keeping billing and ownership separate for the users of the cluster.&lt;/p&gt; 
&lt;p&gt;When creating other clusters all you need to do is include the additional policy in the &lt;code&gt;headnode&lt;/code&gt; section of the configuration file and make sure you use the proper &lt;code&gt;Hosted Zone ID&lt;/code&gt;. Using AWS batch in ParallelCluster does &lt;em&gt;not&lt;/em&gt; require any changes to the cluster configuration or Route53 entry since it relies on its own internal mechanism to resolve hostnames.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Introducing a community recipe library for HPC infrastructure on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/</link>
		
		<dc:creator><![CDATA[Matt Vaughn]]></dc:creator>
		<pubDate>Mon, 25 Sep 2023 15:57:26 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Amazon FSx]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">7fc9c3693f241025a5266e1ccaa748626ae48563</guid>

					<description>Today we’re showing you our community library of HPC Recipes for AWS. It's a public repo @github that will help you achieve feature-rich, reliable HPC deployments ready to run your workloads no matter where you're starting from.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-2887" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/20/boofla88_a_tree_of_human_knowledge_e32ef8b1-ed02-4bf2-8ba4-523ccfdb89f2-1.png" alt="" width="380" height="212"&gt;We want to make it easier for customers to extend and build on AWS using tools like AWS ParallelCluster, Amazon FSx for Lustre, and some of the hundreds of other AWS services that customers often use to make discoveries from their data or simulations.&lt;/p&gt; 
&lt;p&gt;Recently, we introduced you to new capabilities in ParallelCluster that provide a neat way to &lt;a href="https://aws.amazon.com/blogs/hpc/automate-your-clusters-by-creating-self-documenting-hpc-with-aws-parallelcluster/"&gt;create self-documenting infrastructure&lt;/a&gt; – mainly by first writing the specs into AWS CloudFormation templates, then letting CloudFormation build the infrastructure for you behind the scenes. This lets you version control your cluster by putting your templates under source control. You can even embed their management in continuous integration systems.&lt;/p&gt; 
&lt;p&gt;Today we’re making available a community library of patterns that build on that functionality. &lt;strong&gt;HPC Recipes for AWS &lt;/strong&gt;is a &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/"&gt;public repository at GitHub&lt;/a&gt; that hosts interoperable CloudFormation templates designed to work together to build complete HPC environments. We believe this library will help customers achieve feature-rich, &lt;em&gt;reliable&lt;/em&gt; HPC deployments that are ready to run diverse workloads – regardless of where they’re starting from.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll provide some background about this new project, and take you for a tour of some of its components. Finally, we’ll show you how to cook up a cluster from these recipes in just a few minutes.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;By design, ParallelCluster makes it straightforward to create and manage HPC clusters. It handles the undifferentiated heavy lifting to orchestrate the compute, networking, and storage resources you need (assembled from around a dozen AWS services) into a coherent whole. Customers tell us that most common paths through cluster creation and management are well-served by ParallelCluster through its declarative &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/cluster-configuration-file-v3.html"&gt;cluster template file&lt;/a&gt;, reliable resource management, and an optional &lt;a href="https://www.youtube.com/watch?v=FAfVbiTVk24"&gt;web user interface&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;However, customers &lt;em&gt;also&lt;/em&gt; told us about two papercuts that they needed help with.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;First:&lt;/strong&gt; it can be complicated to incorporate external resources, like user authentication resources, or existing filesystems, or relational databases for job accounting – into ParallelCluster. The “right way” usually includes custom scripts, home-grown CloudFormation templates, or documented steps and workarounds.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Second: &lt;/strong&gt;Most of these problems have been solved before, sometimes by AWS solution architects and sometimes by the community at large. Why aren’t these solutions discoverable and reusable? ParallelCluster has made the cluster part of HPC easier, but there can be a steep learning curve involved to stand up the related parts of the infrastructure.&lt;/p&gt; 
&lt;p&gt;We agreed.&lt;/p&gt; 
&lt;p&gt;So, in ParallelCluster 3.6, we &lt;a href="https://youtu.be/dj2ZDmNOJps"&gt;built support for CloudFormation&lt;/a&gt;, so anyone could write templates to create and manage ParallelCluster clusters. These templates could sit beside other templates that launch surrounding and supporting resources.&lt;/p&gt; 
&lt;p&gt;But this opened up a new challenge: There are many ways to set up these resources, all quite valid. We wanted builders to be comfortable leveraging each other’s work, but without continuously reinventing each other’s wheels. We needed some mechanisms for making CloudFormation templates modular, interoperable, and shareable. And that led us to the work we’re sharing today.&lt;/p&gt; 
&lt;h2&gt;Introducing… HPC Recipes for AWS&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;HPC Recipes for AWS&lt;/strong&gt; is a public GitHub repo with over 20 recipes, organized into themes like networking, storage, user environments, and vertical integrations. Each recipe features downloadable assets, along with documentation and metadata to assist with discovery and attribution. The repository is managed by the HPC engineering team, with contributions from AWS solutions architects and our customers.&lt;/p&gt; 
&lt;p&gt;In the repo, you’ll find two general kinds of recipe:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Modular templates&lt;/strong&gt; – these launch specific services or enable particular configurations, and can be to stitched together with other templates. To that end, they share a common naming convention for their &lt;em&gt;parameters&lt;/em&gt; (inputs) and &lt;strong&gt;&lt;em&gt;outputs&lt;/em&gt;. &lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;One-click launchable stacks&lt;/strong&gt; – these combine multiple building blocks from the modular templates catalog, and often launch complete working clusters that you can adopt, tweak, or make your own.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Modular templates are great for standing up pieces of functionality that might be used by several other services or clusters. A great example is the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/db/slurm_accounting_db/"&gt;serverless SQL database&lt;/a&gt; you can use to power Slurm’s accounting features. Or it could be larger: setting up an &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/fsx_lustre"&gt;Amazon FSx for Lustre filesystem&lt;/a&gt; that’s going to be shared between multiple clusters.&lt;/p&gt; 
&lt;p&gt;One-click launchable stacks are more &lt;em&gt;opinionated&lt;/em&gt; assemblies of the modular components. They can be launched quickly, usually after you make some minor customization choices. For instance, you can launch &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/pcui/"&gt;AWS ParallelCluster UI&lt;/a&gt; with an HPC cluster ready-to-go. Or, you can bring up a specifically-tuned HPC cluster to try out the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/try_hpc7g/"&gt;latest AWS Graviton3E CPUs in our Hpc7g instances&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We think these recipes will help you quickly find a pattern that most closely resembles your own to &amp;nbsp;get started with. Afterwards, you can take a stack and customize it to make it your own. In any case, you can make tactical changes as your needs evolve over time, because … it’s the cloud.&lt;/p&gt; 
&lt;h3&gt;Fairness is nice, and useful too.&lt;/h3&gt; 
&lt;p&gt;Our repo is designed to be &lt;a href="https://www.nature.com/articles/s41597-022-01710-x"&gt;F.A.I.R.&lt;/a&gt; software to help ensure it is broadly useful:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Findable&lt;/strong&gt;: the recipes are organized into categories and clearly named, and are tagged by their relevant technologies and attributions so they’re easier to discover.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accessible&lt;/strong&gt;: the collection is hosted in a public GitHub repository under a MIT-0 license, which is standard for many AWS open-source projects. We make it even more accessible by mirroring assets from its recipes to Amazon S3, which gives every file an HTTPS URL that can be used with CloudFormation and other services. This means they can be imported into other recipes or embedded in quick-create links.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interoperable&lt;/strong&gt;: recipes are written in AWS CloudFormation or AWS CDK. They follow standards (where available) and best practices for naming and design. There is a good-faith effort to use clear, standard names for parameters, outputs, and exports.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reusable&lt;/strong&gt;: there are a growing number of modular infrastructure recipes. We intend that these can be used directly, but also imported into other recipes (even outside this collection). Furthermore, each recipe is clearly documented towards being a teaching instrument to promote modification and adaptation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It turns out that being FAIR can also make a project quite useful, as we’ll see next, as we explore how some of its recipes (and the repo’s design) can align to simplify HPC on AWS.&lt;/p&gt; 
&lt;h2&gt;Off to the Kitchen&lt;/h2&gt; 
&lt;p&gt;Let’s head to a virtual test kitchen to compare three recipes for cooking up a cluster with a persistent Lustre filesystem backed by Amazon Simple Storage Service (Amazon S3).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The first&lt;/strong&gt; shows a common case, where you want to use resources provisioned separately from your cluster. Using this, you’ll need to look up several values and input them into a launch template. This is the most versatile option if you’re a frequent CloudFormation user, but can lead to some repetitive tasks. It’s useful to understand how this works – especially if you’re likely to borrow stacks from other repos from time to time.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The second&lt;/strong&gt; shows what can be done with infrastructure stacks that are written to work together – like the ones in the recipe library.&lt;/p&gt; 
&lt;p&gt;Finally, &lt;strong&gt;the third&lt;/strong&gt; recipe demonstrates how to connect separate stacks into a single streamlined launch template. This is the fastest way to get started with a complete environment, but you’ll probably want to customize these to adapt to your needs before you get too serious.&lt;/p&gt; 
&lt;p&gt;As we go through the recipes, we’ll call out how key features of HPC Recipes on AWS and AWS CloudFormation enable these designs.&lt;/p&gt; 
&lt;h2&gt;Recipe 1: cluster by hand&lt;/h2&gt; 
&lt;p&gt;Our first recipe uses &lt;strong&gt;modular templates&lt;/strong&gt; that create supporting infrastructure, then has us configure the cluster by hand with their outputs. Found at &lt;strong&gt;training/try_recipes_1&lt;/strong&gt;, it involves several steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create an HPC-ready VPC using the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/net/hpc_basic/"&gt;&lt;strong&gt;net/hpc_basic&lt;/strong&gt;&lt;/a&gt; There are several fields in this template, but the only one you &lt;em&gt;need&lt;/em&gt; to set is &lt;strong&gt;Availability Zone&lt;/strong&gt;. When the stack has been created, look up the VPC and subnet IDs in its outputs. They will be named &lt;strong&gt;VPC&lt;/strong&gt;, &lt;strong&gt;DefaultPublicSubnet&lt;/strong&gt;, and &lt;strong&gt;DefaultPrivateSubnet&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Provision an Amazon S3 bucket to back the FSx for Lustre filesystem with &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/s3_demo/"&gt;&lt;strong&gt;storage/s3_demo&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Now, create a persistent Lustre filesystem with a data repository association using &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/fsx_lustre_s3_dra/"&gt;&lt;strong&gt;storage/fsx_lustre_s3_dra&lt;/strong&gt;.&lt;/a&gt; Put the Amazon S3 bucket name in &lt;strong&gt;DataRepositoryPath&lt;/strong&gt;, formatted as an S3 URL. Select the networking stack VPC in &lt;strong&gt;VpcId&lt;/strong&gt; and its private subnet for &lt;strong&gt;SubnetId&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Once the filesystem and other resources are created, go the cluster recipe, and choose &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/training/try_recipes_1/"&gt;&lt;strong&gt;Launch stack&lt;/strong&gt;&lt;/a&gt; to create a an HPC system using outputs from all the supporting stacks.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To accomplish this last step, you’ll need to do additional output-to-parameter mappings (Figure 1):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Select the public subnet from your network stack for This is where the head node will launch. The private subnet goes in &lt;strong&gt;ComputeNodeSubnetId&lt;/strong&gt;. This is for the compute nodes.&lt;/li&gt; 
 &lt;li&gt;Go to the Outputs tab in the filesystem stack. Find the value for &lt;strong&gt;FSxLustreFilesystemId&lt;/strong&gt; and use it for your cluster’s &lt;strong&gt;FilesystemId&lt;/strong&gt; Use the value for &lt;strong&gt;FSxLustreSecurityGroupId&lt;/strong&gt; for the &lt;strong&gt;FilesystemSecurityGroupId&lt;/strong&gt; setting.&lt;/li&gt; 
 &lt;li&gt;Finally, choose the operating system, architecture, number of compute instances, and Lustre filesystem size and finish launching the stack.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div id="attachment_2880" style="width: 767px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2880" loading="lazy" class="size-full wp-image-2880" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/20/CleanShot-2023-09-20-at-17.01.08.png" alt="Figure 1. Cluster launch template requests outputs from other CloudFormation stacks" width="757" height="643"&gt;
 &lt;p id="caption-attachment-2880" class="wp-caption-text"&gt;Figure 1. Cluster launch template requests outputs from other CloudFormation stacks&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In around 10-15 minutes, the cluster will be ready to use.&lt;/p&gt; 
&lt;p&gt;Go to stack outputs and choose &lt;strong&gt;SystemManagerUrl&lt;/strong&gt; to log into the cluster with &lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/what-is-systems-manager.html"&gt;Amazon SSM&lt;/a&gt;. Once you’re in, you can view queues and run jobs. If you upload some data to the S3 bucket, it’ll &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data-repo.html"&gt;show up in the Lustre shared filesystem&lt;/a&gt; (and vice versa).&lt;/p&gt; 
&lt;p&gt;To shut down the cluster and its resources, delete the stacks in reverse order from when they were created. Start with the cluster stack, then the FSx storage stack, followed by the Amazon S3 stack, and finally, the network stack.&lt;/p&gt; 
&lt;p&gt;You might also notice that each recipe has a quick-create link that launches the AWS CloudFormation console. These are enabled by the automatic mirror of recipe assets to the Amazon S3 bucket we mentioned earlier.&lt;/p&gt; 
&lt;p&gt;Here is a template for creating a quick-create link:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-html"&gt;https://console.aws.amazon.com/cloudformation/home?region=REGION#/stacks/create/review?stackName=OPTIONAL-STACK-NAME&amp;amp;templateURL=HPC-RECIPES-S3-URL&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;All recipes that include a CloudFormation template can be embedded this way. You can learn more about quick-create links in the &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-stacks-quick-create-links.html"&gt;CloudFormation User Guide&lt;/a&gt; and the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes#incorporating-recipe-assets"&gt;documentation for this repo&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_2881" style="width: 900px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2881" loading="lazy" class="size-full wp-image-2881" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/20/CleanShot-2023-09-20-at-17.01.35.png" alt="Figure 2. Complex outputs-to-parameter mappings between CloudFormation stacks." width="890" height="523"&gt;
 &lt;p id="caption-attachment-2881" class="wp-caption-text"&gt;Figure 2. Complex outputs-to-parameter mappings between CloudFormation stacks.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Turning to the actual cluster recipe, there is a challenge with its design – the configuration is pretty simple yet we &lt;em&gt;still&lt;/em&gt; had to specify VPC and subnet two times and consult multiple stack outputs to launch it (Figure 2).&lt;/p&gt; 
&lt;p&gt;What if we need to integrate multi-user support or Slurm accounting (each their own stack with more sophisticated networking needs)? What if we also have additional filesystems, security groups, or IAM policies? We’d probably need a pen and paper to keep all the parameter mappings straight! It might also be challenging to remember how resources from the various stacks are related to one another when we need to make updates or delete them.&lt;/p&gt; 
&lt;h2&gt;Recipe 2: Cluster using Imports&lt;/h2&gt; 
&lt;p&gt;The next approach, &lt;strong&gt;training/try_recipes_2,&lt;/strong&gt; improves on the first by using CloudFormation’s &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html"&gt;ImportValue intrinsic function&lt;/a&gt; to bring in information from existing stacks. With this design, you provide the names of the &lt;em&gt;stacks&lt;/em&gt; that provide networking and filesystem support (Figure 3). Then the cluster CloudWatch template imports values &lt;em&gt;from their outputs&lt;/em&gt;.&lt;/p&gt; 
&lt;div id="attachment_2882" style="width: 868px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2882" loading="lazy" class="size-full wp-image-2882" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/20/CleanShot-2023-09-20-at-17.02.10.png" alt="Figure 3. Simplified templates with CloudFormation imports" width="858" height="517"&gt;
 &lt;p id="caption-attachment-2882" class="wp-caption-text"&gt;Figure 3. Simplified templates with CloudFormation imports&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Let’s see how it works in practice:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a HPC network stack with &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/net/hpc_basic/"&gt;&lt;strong&gt;net/hpc_basic&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Next, create an Amazon S3 bucket using the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/s3_demo/"&gt;&lt;strong&gt;storage/s3_demo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Stand up an Amazon FSx for Lustre filesystem using the “&lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/fsx_lustre_s3_dra#alternative-import-stack"&gt;&lt;strong&gt;alternative import stack&lt;/strong&gt;&lt;/a&gt;” in the &lt;strong&gt;storage/fsx_lustre_s3_dra&lt;/strong&gt; recipe&lt;strong&gt;. &lt;/strong&gt;Instead of picking a VPC and subnet, just input the name of the networking stack from step 1 into &lt;strong&gt;NetworkStackNameParameter&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;When the FSx for Lustre stack is ready, go to the cluster recipe and choose &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/training/try_recipes_2/"&gt;&lt;strong&gt;Launch stack&lt;/strong&gt;&lt;/a&gt;. It will ask for the names of your &lt;em&gt;networking &lt;/em&gt;and &lt;em&gt;storage&lt;/em&gt; Then, it’ll use them to automatically import key configuration details like the VPC, subnets, and filesystem ID. That’s a lot less typing!&lt;/li&gt; 
 &lt;li&gt;Last, choose your operating system, architecture, number of compute instances, and size of the Lustre filesystem and complete the stack launch.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;After a few minutes, the cluster will be accessible via AWS Systems Manager, just like in the first recipe. To shut down the system and delete its dependencies, delete the stacks in reverse order from when they were created, beginning with the cluster stack.&lt;/p&gt; 
&lt;p&gt;As you can see, this approach streamlines the cluster creation process because you don’t have to look up parameter values – instead they are simply imported when the stack launches.&lt;/p&gt; 
&lt;p&gt;Besides providing a simplified end-user experience, there are two other benefits to this design. First, you can change out implementations of the modular stacks with your own CloudFormation templates. They just have to follow the parameter and export naming conventions expected by the other stacks. Second, this design helps promote sustainable infrastructure practices by organizing your deployment by logical units – like we recommend in the AWS Cloud Development Kit (CDK) &lt;a href="https://docs.aws.amazon.com/cdk/v2/guide/best-practices.html"&gt;best practices guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To get started with CloudFormation imports, read over the &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-hpc-recipes/main/recipes/training/try_recipes_2/assets/import.yaml"&gt;template for this recipe&lt;/a&gt;, as well as that of dependencies like the &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-hpc-recipes/main/recipes/net/hpc_basic/assets/public-private.yaml"&gt;networking recipe&lt;/a&gt;. Notice how exports from the dependency stacks get imported by name into the cluster template using the &lt;code&gt;Sub&lt;/code&gt; and &lt;code&gt;ImportValue&lt;/code&gt; intrinsic functions.&lt;/p&gt; 
&lt;h2&gt;Recipe 3: Automatic Cluster&lt;/h2&gt; 
&lt;p&gt;In our final recipe, &lt;strong&gt;training/try_recipes_3,&lt;/strong&gt; we demonstrate a &lt;strong&gt;one-click launchable stack&lt;/strong&gt;. The only required input is the Availability Zone you want to use. Everything else is automatically configured (Figure 4).&lt;/p&gt; 
&lt;p&gt;Using it is much simpler:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the recipe and choose &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/training/try_recipes_3/"&gt;&lt;strong&gt;Launch stack&lt;/strong&gt;&lt;/a&gt;. You will be asked to select an Availability Zone, then choose the operating system, architecture, number of compute instances, and Lustre filesystem size. In a few minutes, the cluster will be ready to use. Go to stack outputs and navigate to &lt;strong&gt;SystemManagerUrl &lt;/strong&gt;to log in using a web console.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Getting rid of the various HPC resources is just as straightforward. Delete the main stack and CloudFormation will take care of shutting everything down in the correct order. If you want to keep some of the provisioned resources, you can go to the CloudFormation console, find the relevant component stack, and &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html"&gt;enable termination protection&lt;/a&gt; before deleting the parent stack (s).&lt;/p&gt; 
&lt;p&gt;This approach relies on an important CloudFormation capability called “&lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html"&gt;nested stacks&lt;/a&gt;“. With these, you can create resources by loading CloudFormation code from an S3 URL. In the case of this recipe, code for those resources comes from &lt;em&gt;other&lt;/em&gt; HPC recipes. It is quite an opinionated way of doing things, but provides a direct path for anyone to offer reproducible deployments of complex infrastructure for demonstrations, proofs of concept, or training.&lt;/p&gt; 
&lt;div id="attachment_2883" style="width: 894px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2883" loading="lazy" class="size-full wp-image-2883" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/20/CleanShot-2023-09-20-at-17.03.02.png" alt="Figure 4. Nested stacks can enable 1-step deployments of complex infrastructure" width="884" height="296"&gt;
 &lt;p id="caption-attachment-2883" class="wp-caption-text"&gt;Figure 4. Nested stacks can enable 1-step deployments of complex infrastructure&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To learn more about nested stacks, have a look at this &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-hpc-recipes/main/recipes/training/try_recipes_3/assets/nested.yaml"&gt;recipe’s template file&lt;/a&gt;. Pay special attention to how &lt;code&gt;TemplateURL&lt;/code&gt; is used to import other HPC Recipes for AWS, and how &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html"&gt;dynamic references&lt;/a&gt; are used to link stack outputs and parameters.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;AWS HPC systems often depend on other AWS resources, like filesystems, networking, databases, or directory services. It can be complicated to set up all the dependencies and ensure they work together.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;HPC Recipes for AWS &lt;/strong&gt;is growing collection of modular and all-in-one infrastructure recipes that helps you accomplish that. You can learn from them, directly use them to configure and launch infrastructure, and extend or remix them to your own ends.&lt;/p&gt; 
&lt;p&gt;We invite you to try &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/latest/"&gt;launching an HPC cluster&lt;/a&gt; with one of these recipes today, &lt;a href="https://github.com/aws-samples/aws-hpc-recipes"&gt;explore the repository&lt;/a&gt; in greater detail, and contribute new recipes or improvements. You might also consider &lt;a href="https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars"&gt;starring the repository&lt;/a&gt; on GitHub so you can be informed of updates and new recipes.&lt;/p&gt; 
&lt;p&gt;Finally, if this new resource improves your workflow or helps you solve harder problems than you were before, reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt; and let us know about it.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Real-time quant trading on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/real-time-quant-trading-on-aws/</link>
		
		<dc:creator><![CDATA[Sam Farber]]></dc:creator>
		<pubDate>Tue, 19 Sep 2023 12:37:14 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[Financial Services]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[FSI]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">619fceb5fee43814924c6ba69d59f90e0b03f32b</guid>

					<description>In this post, we’ll show you an open-source solution for a real-time quant trading system that you can deploy on AWS. We’ll go over the challenges brought on by monitoring portfolios, the solution, and its components. We’ll finish with the installation and configuration process and show you how to use it.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-2756" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/02/boofla88_quantitative_trading_as_a_concept._blue_sky_background_b26f2fe6-a229-4eff-8e93-6e306b0ded60.png" alt="Real-time quant trading on AWS" width="380" height="212"&gt;This post was contributed by Boris Litvin, Financial Services SA; Sam Farber, Startup SA; Ronny Rodriguez, Senior Financial Services TPM; Adeleke Coker, Global Solutions Architect&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;There are a variety of vendors already providing real-time quant trading systems, the question is: “do we need another one?” We believe the answer is: “yes.” Quant trading is a never-ending arms race to extract alpha – the excess return a trader is able to attain, relative to the market. This is fueled by a flywheel of new compute capabilities, data analytics, and AI/ML. These made short-term (intraday) and mid-term (intraday to one week) alphas not only easier to discover, but also more feasible to execute.&lt;/p&gt; 
&lt;p&gt;As a result, we see an increased demand for a real-time quant trading cloud-native system to research and execute short/mid-term alpha, a popular strategy due to the higher risk/adjusted returns when compared to traditional ones.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll show you an open-source solution for a real-time quant trading system that you can deploy on AWS. We’ll go over the challenges brought on by monitoring portfolios, the solution, and its components. We’ll finish with the installation and configuration process and show you how to use it.&lt;/p&gt; 
&lt;p&gt;The nature of these short to mid-term trading strategies is very elastic, causing the timing of trading signals to be unpredictable. Portfolios change often, frequently overlap, and often contain hundreds to thousands of different positions. This in turn generates unpredictable and uneven demands for the compute capacity needed to manage these short-term portfolios throughout their lifecycle in real-time. Moreover, cost optimization is an important factor, influencing the feasibility of specific opportunities – in other words, price/performance matters.&lt;/p&gt; 
&lt;p&gt;The solution to the challenges described above is an AWS-native trading system capable of scaling up or down with near-zero operational overhead.&lt;/p&gt; 
&lt;div id="attachment_2739" style="width: 981px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2739" loading="lazy" class="size-full wp-image-2739" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/01/CleanShot-2023-08-01-at-16.39.53.png" alt="Figure 1 – This architecture describes the one-time installation process of the solution. It first details the use of the AWS Cloud Development Kit (AWS CDK) to deploy the solution into your AWS account. It then looks at the method of uploading certain values like API keys in AWS Secrets Manager. The diagram also shows the different application stacks that are created through AWS CloudFormation, like AWS Batch, different databases, as well as AWS Lambda. Keep in mind that for any specific customizations, stacks will need to be redeployed for application coding changes." width="971" height="495"&gt;
 &lt;p id="caption-attachment-2739" class="wp-caption-text"&gt;Figure 1 – This architecture describes the one-time installation process of the solution. It first details the use of the AWS Cloud Development Kit (AWS CDK) to deploy the solution into your AWS account. It then looks at the method of uploading certain values like API keys in AWS Secrets Manager. The diagram also shows the different application stacks that are created through AWS CloudFormation, like AWS Batch, different databases, as well as AWS Lambda. Keep in mind that for any specific customizations, stacks will need to be redeployed for application coding changes.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2740" style="width: 997px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2740" loading="lazy" class="size-full wp-image-2740" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/01/CleanShot-2023-08-01-at-16.40.26.png" alt="Figure 2 – This architecture describes the operational aspect of the solution and first shows the process of inserting the portfolio into Amazon DynamoDB. It then looks at the AWS Batch job that is triggered through AWS Lambda whenever there is a change in the portfolio. Finally, we have AWS Batch writing market data to Amazon Timestream as well as the use of Amazon Managed Grafana to produce real-time visualizations. Amazon EventBridge is also utilized to automatically trigger events based on trading hours." width="987" height="503"&gt;
 &lt;p id="caption-attachment-2740" class="wp-caption-text"&gt;Figure 2 – This architecture describes the operational aspect of the solution and first shows the process of inserting the portfolio into Amazon DynamoDB. It then looks at the AWS Batch job that is triggered through AWS Lambda whenever there is a change in the portfolio. Finally, we have AWS Batch writing market data to Amazon Timestream as well as the use of Amazon Managed Grafana to produce real-time visualizations. Amazon EventBridge is also utilized to automatically trigger events based on trading hours.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;AWS Batch is our mechanism to achieve system elasticity. It allows the flexibility to run on any Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS) compute fleet, including EC2 Spot instances without operational and DevOps overhead. The event-driven design addresses the unpredictable nature of information arrival (i.e., signal generation, portfolio creation, news).&lt;/p&gt; 
&lt;p&gt;Amazon Timestream enables developers to achieve better productivity by eliminating undifferentiated heavy lifting. Most notably: a schema-less design with automatic duplicate detection on inserts. In addition, the service comes with time series functionality such as interpolation and smoothing. Amazon Managed Grafana automatically connects to Amazon Timestream and other data sources for both real-time and historical visualization/dashboards. Finally, we did the work to assemble disparate components into a single, one-click, deployable stack to ensure smooth initial installation and ongoing SDLC.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deploying the solution&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;These instructions will guide you to set up a real-time market portfolio application on AWS through the &lt;a href="https://aws.amazon.com/cdk/"&gt;AWS CDK&lt;/a&gt;. The deployed CDK infrastructure comes with an example portfolio of the S&amp;amp;P 500 based on &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2440866"&gt;Market Intraday Momentum&lt;/a&gt;. The intraday momentum pattern says that the first half-hour returns on the market since the previous day’s market close will predict the last half-hour returns. This predictability is typically stronger on more volatile days, on higher volume days, on recession days, and on major macroeconomic news release days.&lt;/p&gt; 
&lt;p&gt;Note: You will need a subscription and an API key to a market data feed like B-PIPE or IEX for this solution to fully work.&lt;/p&gt; 
&lt;p&gt;Initial Setup&lt;br&gt; You will use &lt;a href="https://aws.amazon.com/cloud9/"&gt;AWS Cloud9&lt;/a&gt;&amp;nbsp;as the IDE to setup the code and deploy the CDK environment. You can also use a different IDE if you’d prefer.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to the &lt;strong&gt;AWS Cloud9 &lt;/strong&gt;&lt;a href="https://console.aws.amazon.com/cloud9/"&gt;console&lt;/a&gt; and press &lt;strong&gt;Create environment&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Enter a name –&amp;nbsp;&lt;strong&gt;MarketPortfolioEnv.&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Use a &lt;strong&gt;t2.micro&lt;/strong&gt; instance type.&lt;/li&gt; 
 &lt;li&gt;Leave all other settings as default and choose &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;After a few minutes, the environment should be created. Under &lt;strong&gt;Cloud9 IDE&lt;/strong&gt;, press&amp;nbsp;&lt;strong&gt;Open.&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;In the command line at the bottom, clone the &lt;a href="https://github.com/aws-samples/quant-trading"&gt;Git repository&lt;/a&gt; using the following command:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;git clone https://github.com/aws-samples/quant-trading.git&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;CDK Deployment&lt;/h3&gt; 
&lt;p&gt;Now that the environment is setup, let’s deploy the application. You’ll need to run a few commands to get everything set up and this will allow for the entire application to be spun up through the CDK.&lt;/p&gt; 
&lt;p&gt;In the &lt;strong&gt;Cloud9 CLI&lt;/strong&gt;, navigate to the repository by entering the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd quant-trading&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, type in the following command to install the necessary dependencies, bootstrap the environment, and deploy the application using the CDK code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;./deployment.sh&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you get an error saying the docker build failed and says no space left on device run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;chmod +x aws-quant-infra/src/utils/resize_root.sh &amp;amp;&amp;amp;
aws-quant-infra/src/utils/resize_root.sh 50
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you get an error from creating the DynamoDB replica instance in the DB stack, you’ll need to go to the DynamoDB console and delete the replica from the console and delete the DB stack, then redeploy the CDK stack.&lt;/p&gt; 
&lt;h3&gt;Adding an API key to Begin Data Flow&lt;/h3&gt; 
&lt;p&gt;You can have data come in from either &lt;a href="https://iexcloud.io/"&gt;IEX&lt;/a&gt; or &lt;a href="https://www.bloomberg.com/professional/product/market-data-cloud/"&gt;B-PIPE&lt;/a&gt; (Bloomberg Market Data Feed). In this section, you’ll enter the API key in AWS Secrets Manager and that will enable the Intraday Momentum application to start working.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to the &lt;strong&gt;AWS Secrets Manager&lt;/strong&gt; console.&lt;/li&gt; 
 &lt;li&gt;You should see two secrets created: &lt;code&gt;api_token_pk_sandbox&lt;/code&gt; and &lt;code&gt;api_token_pk&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_2742" style="width: 813px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2742" loading="lazy" class="size-full wp-image-2742" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/01/CleanShot-2023-08-01-at-16.48.09.png" alt="Figure 2 - Using AWS Secrets Manager to store your API keys." width="803" height="147"&gt;
 &lt;p id="caption-attachment-2742" class="wp-caption-text"&gt;Figure 2 – Using AWS Secrets Manager to store your API keys.&lt;/p&gt;
&lt;/div&gt; 
&lt;ol&gt; 
 &lt;li&gt;Select &lt;code&gt;api_token_pk&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Scroll down to the section that says &lt;strong&gt;Secret value&lt;/strong&gt;&amp;nbsp;and towards the right, select &lt;strong&gt;Retrieve secret value.&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ol&gt; 
 &lt;li&gt;Then, choose &lt;strong&gt;Edit&lt;/strong&gt;&amp;nbsp;and paste in your IEX or B-PIPE API key.&lt;/li&gt; 
 &lt;li&gt;Press &lt;strong&gt;Save&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Looking at the Results&lt;/h3&gt; 
&lt;p&gt;You can view the results of the Intraday Momentum application after the day end by going to the DynamoDB table.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to the &lt;strong&gt;AWS DynamoDB&lt;/strong&gt; console.&lt;/li&gt; 
 &lt;li&gt;On the left, select &lt;strong&gt;Tables &lt;/strong&gt;and then choose the table called &lt;code&gt;MvpPortfolioMonitoringPortfolioTable&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Then, press the orange button in the top right that says &lt;strong&gt;Explore table items&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_2741" style="width: 818px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2741" loading="lazy" class="size-full wp-image-2741" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/01/CleanShot-2023-08-01-at-16.44.40.png" alt="Using the AWS DynamoDB console to look at your results." width="808" height="291"&gt;
 &lt;p id="caption-attachment-2741" class="wp-caption-text"&gt;Figure 3- Using the AWS DynamoDB console to look at your results.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;You should then see&amp;nbsp;data populated at the bottom under &lt;strong&gt;Items returned.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Note: If you don’t see any data, select the orange &lt;strong&gt;Run &lt;/strong&gt;button to scan the table and retrieve the data.&lt;/p&gt; 
&lt;p&gt;If you’d like to analyze this data further, you can download it in CSV format by selecting &lt;strong&gt;Actions&lt;/strong&gt;, then &lt;strong&gt;Download results to CSV. &lt;/strong&gt;You can also use Amazon Managed Grafana to visualize the results.&lt;/p&gt; 
&lt;h2&gt;Clean Up&lt;/h2&gt; 
&lt;p&gt;You can delete the entire stack using the CDK.&lt;/p&gt; 
&lt;p&gt;Using the CLI where you deployed the stack, enter the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cdk destroy --all&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Quant trading, given its scale, short-term portfolio lifetime, and unpredictable nature of information arrival, uniquely benefits from cloud elasticity. However, designing the system to harvest the elasticity requires expert level knowledge (both, trading and cloud) and is resource intensive.&lt;/p&gt; 
&lt;p&gt;In this article, we described the system we built to jump-start quant trading on AWS and how to get set up by deploying the stack from the &lt;a href="https://github.com/aws-samples/quant-trading"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>How Maxar builds short duration ‘bursty’ HPC workloads on AWS at scale</title>
		<link>https://aws.amazon.com/blogs/hpc/how-maxar-builds-short-duration-bursty-hpc-workloads-on-aws-at-scale/</link>
		
		<dc:creator><![CDATA[Scott Ma]]></dc:creator>
		<pubDate>Tue, 12 Sep 2023 17:27:44 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">bfe6723330cc5ed1c5a36e9edb71bce3e6688c0b</guid>

					<description>In this post, we hear from Maxar's WeatherDesk team on how they deploy their HPC workloads using a “fail fast” software development technique so they can be sure of meeting customer deadlines for their business.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed&amp;nbsp;by Christopher Cassidy, Maxar Principal DevOps Engineer, &lt;/em&gt;&lt;em&gt;Stefan Cecelski, PhD, Maxar Principal Data Scientist, &lt;/em&gt;&lt;em&gt;Travis Hartman, Maxar Director of Weather and Climate&lt;/em&gt;&lt;em&gt;, Scott Ma, AWS Sr Solutions Architect, &lt;/em&gt;&lt;em&gt;Luke Wells, AWS Sr Technical Account Manager&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;High performance computing (HPC) has been key to solving the most complex problems in every industry and has been steadily changing the way we work and live. From weather forecasting to genome mapping to the search for extraterrestrial intelligence, HPC is helping to push the boundaries of what’s possible with advanced computing technologies.&lt;/p&gt; 
&lt;p&gt;Maxar’s WeatherDesk&lt;sup&gt;SM &lt;/sup&gt;leverages these advanced computing technologies to deliver weather forecasts faster to customers, enabling them to make better informed business decisions. &lt;a href="https://www.maxar.com/products/weatherdesk"&gt;WeatherDesk&lt;/a&gt;&amp;nbsp;builds HPC solutions on AWS to provide access to global numerical weather forecasts to stay ahead of emerging conditions that affect agriculture production, commodity trading, and financial markets. These forecasts are also vital for protecting critical infrastructure like power grids around the world, energy exploration and production, and even transportation.&amp;nbsp;The WeatherDesk platform provides access to data services, web applications, and information reports to customers around the clock via a software-as-a-service (SaaS) suite of offerings designed for specific personas – data scientists and developers, researchers, and executives and operators, respectively.&lt;/p&gt; 
&lt;p&gt;Maxar uses a number of HPC services like &lt;a href="https://aws.amazon.com/hpc/efa/"&gt;Elastic Fabric Adapter&lt;/a&gt; (EFA), the &lt;a href="https://aws.amazon.com/ec2/nitro/"&gt;AWS Nitro System&lt;/a&gt; and &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; to deliver their solutions to their customers. All of this allows Maxar to scale HPC applications to tens of thousands of CPUs with the reliability, scalability, and agility of AWS that would otherwise be extremely difficult to achieve.&lt;/p&gt; 
&lt;p&gt;In this post, we will discuss how Maxar deploys all these tools to run short duration HPC workloads using the “&lt;a href="https://www.martinfowler.com/ieeeSoftware/failFast.pdf"&gt;fail fast&lt;/a&gt;” software development technique.&lt;/p&gt; 
&lt;h2&gt;Constraints&lt;/h2&gt; 
&lt;p&gt;HPC workloads come in all different shapes and sizes, but can generally be divided into two categories based on the degree of interaction between the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/scenarios.html"&gt;concurrently running parallel processes&lt;/a&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Loosely-coupled&lt;/strong&gt; are those where the multiple processes don’t strongly interact with each other in the course of a simulation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tightly coupled&lt;/strong&gt; are those where the parallel processes are simultaneously running and regularly exchanging information between cooperating processes at each step of a simulation.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Solutions like Maxar’s WeatherDesk platform using HPC for numerical weather prediction are tightly coupled due to the complexity of the calculations that go into making a numerical weather forecast. Mainly, the codependency of global weather parameters and computing algorithms require over two billion calculations per second spread across hundreds of Amazon Elastic Compute Cloud (Amazon EC2) instances within an AWS HPC cluster environment. Each of these calculations depends on each other – so the reliable exchange of lots of data in the least time, is important.&lt;/p&gt; 
&lt;p&gt;Additionally, HPC workloads – including Maxar’s WeatherDesk – are often constrained in other ways that can impact the final solution:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HPC workloads are often very bursty&lt;/strong&gt;, performing computations only a few hours per day. This requires a large number of cores for a short time when they run.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Time-bound workloads&lt;/strong&gt; must complete on a specific schedule with the exact number of instances or physical cores available throughout that time period.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Spot instances&lt;/strong&gt; require workloads to “checkpoint” to handle interruptions, but not all can. Typically, tightly coupled applications, like weather simulation, find this hard.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Workloads often prefer homogeneous instance types or instances with the same underlying architecture and, as we discussed, they need reliable, fast, high-throughput connectivity between instances.&lt;/p&gt; 
&lt;p&gt;Maxar needs to ensure they’re able to launch the HPC clusters when needed, using the suite of architectures given the problem set, and they must dynamically adjust (or &lt;em&gt;fail fast&lt;/em&gt;) to minimize downstream impact.&lt;/p&gt; 
&lt;h2&gt;Solution overview&lt;/h2&gt; 
&lt;p&gt;With these constraints in mind, Maxar built a solution on AWS that uses &lt;a href="https://aws.amazon.com/ec2/instance-types/hpc6/"&gt;hpc6a&lt;/a&gt; instances. These instances are powered by 3rd generation AMD EPYC processors and offer up to 65% better price performance over comparable Amazon EC2 x86 based compute-optimized instances.&lt;/p&gt; 
&lt;p&gt;Their solution uses AWS ParallelCluster is an open-source cluster management tool that makes it easy for you to deploy and manage HPC clusters on AWS. ParallelCluster uses EFA,&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"&gt;cluster placement groups&lt;/a&gt;, and supports &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html"&gt;On-Demand Capacity Reservations (ODCR)&lt;/a&gt; – important technologies and techniques for creating a highly performant &lt;em&gt;and&lt;/em&gt; flexible&amp;nbsp;HPC solution.&lt;/p&gt; 
&lt;p&gt;They also use AWS CloudFormation to provision the head node and then use shell scripts to dynamically adjust input parameters to an AWS ParallelCluster CLI invocation &lt;em&gt;which builds a cluster&lt;/em&gt; of over 25,000 physical cores … and they do this &lt;em&gt;several times a day&lt;/em&gt;!&lt;/p&gt; 
&lt;p&gt;They manage the cluster provisioning workflow using CloudFormation but controlled via CI/CD pipelines like &lt;a href="https://aws.amazon.com/codecommit/"&gt;AWS CodeCommit &lt;/a&gt;and &lt;a href="https://aws.amazon.com/codebuild/"&gt;AWS CodeBuild&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The ODCR allows Maxar to reserve large numbers of a specific instance type in a specific availability zone (AZ) and using a cluster placement group for the duration of their workload. This can happen at any time of any day and without entering into a one-year or three-year commitment.&lt;/p&gt; 
&lt;p&gt;Based on the response from the synchronous &lt;code&gt;&lt;a href="https://docs.aws.amazon.com/cli/latest/reference/ec2/create-capacity-reservation.html"&gt;CreateCapacityReservation&lt;/a&gt;&lt;/code&gt; API call, Maxar can pivot to other instance types, AZs, or even other AWS Regions based on the proximity to the datasets and other variables. For Maxar, pivoting to a similar instance type (or two) in the same AZ, in the same region provided enough flexibility (and resiliency) to boost both performance &lt;em&gt;and&lt;/em&gt; confidence.&lt;/p&gt; 
&lt;h2&gt;Walkthrough&lt;/h2&gt; 
&lt;p&gt;Let’s walk through a sample CloudFormation template that incorporates the newly supported AWS ParallelCluster resources.&lt;/p&gt; 
&lt;p&gt;AWS CloudFormation provisions and configures AWS resources for you, so that you don’t have to individually create and configure them and determine resource dependencies. In our solution we’ll use c6g instances powered by Arm-based &lt;a href="https://aws.amazon.com/ec2/graviton/"&gt;AWS Graviton Processors&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_2641" style="width: 1018px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2641" loading="lazy" class="size-full wp-image-2641" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.42.09.png" alt="Figure 1: Solution architecture to outline the steps" width="1008" height="863"&gt;
 &lt;p id="caption-attachment-2641" class="wp-caption-text"&gt;Figure 1: Solution architecture to outline the steps&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In this walkthrough we’ll take you through six parts, including one that’s optional:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Prerequisites&lt;/li&gt; 
 &lt;li&gt;Deploying the solution&lt;/li&gt; 
 &lt;li&gt;Running the workload&lt;/li&gt; 
 &lt;li&gt;Results of the workload (&lt;em&gt;optional&lt;/em&gt;)&lt;/li&gt; 
 &lt;li&gt;Cluster cleanup&lt;/li&gt; 
 &lt;li&gt;Conclusion&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;For this to work in your own AWS account, you’ll need:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A list of instance types suitable for the workload (e.g. if the workload requires a GPU, g5, g5g, or g4dn might be suitable). In this example, we will use c6g.8xlarge and c6g.16xlarge.&lt;/li&gt; 
 &lt;li&gt;A list of availability zones that would meet the proximity requirement to the data for your workload.&lt;/li&gt; 
 &lt;li&gt;The duration of the workload and the number of instances required.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Deploying the solution&lt;/h3&gt; 
&lt;p&gt;You can find our one-click &lt;a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=pcluster-odcr-demo&amp;amp;templateURL=https://hpc-odcr.s3.amazonaws.com/3.6.0/primary-cfn.yaml"&gt;Launch Stack&lt;/a&gt; to deploy the CloudFormation template with all the necessary components for this solution.&amp;nbsp;The solution is made up of:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;CloudFormation custom resources to create ODCRs&lt;/li&gt; 
 &lt;li&gt;A ParallelCluster head node with Slurm Workload Manager&lt;/li&gt; 
 &lt;li&gt;Security groups&lt;/li&gt; 
 &lt;li&gt;Cluster placement group&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The input parameter &lt;strong&gt;NodeInstanceTypes&lt;/strong&gt; is in comma delimited format used to specify the instance types you want to try to request On-demand Capacity Reservation (ODCR).&lt;/p&gt; 
&lt;p&gt;The input parameter &lt;strong&gt;NodeInstanceDurations&lt;/strong&gt; is in comma delimited format used to specify the durations you want to reserve for in minutes corresponding to the number of &lt;strong&gt;NodeInstanceTypes.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The input parameter &lt;strong&gt;NodeInstanceCounts&lt;/strong&gt; is in comma delimited formation used to specify the number of nodes you want to reserve for corresponding to the number of &lt;strong&gt;NodeInstanceTypes.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The input parameter &lt;strong&gt;KeyName&lt;/strong&gt; specifies the EC2 key pair you would like to use.&lt;/p&gt; 
&lt;p&gt;Be careful to ensure you specify a key pair you can access as you’ll need this later on!&lt;/p&gt; 
&lt;div id="attachment_2642" style="width: 999px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2642" loading="lazy" class="size-full wp-image-2642" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.43.19.png" alt="Figure 2: CloudFormation Input Parameters" width="989" height="822"&gt;
 &lt;p id="caption-attachment-2642" class="wp-caption-text"&gt;Figure 2: CloudFormation Input Parameters&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2643" style="width: 1063px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2643" loading="lazy" class="size-full wp-image-2643" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.43.34.png" alt="Figure 3: CloudFormation Deployment Stacks" width="1053" height="743"&gt;
 &lt;p id="caption-attachment-2643" class="wp-caption-text"&gt;Figure 3: CloudFormation Deployment Stacks&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Running the workload&lt;/h2&gt; 
&lt;p&gt;The workload is based on the &lt;a href="https://github.com/aws-samples/hpc-workshop-wrf"&gt;WRF on AWS HPC workshop&lt;/a&gt;. This uses the Weather Research and Forecasting (WRF) model – a mesoscale numerical weather prediction system designed for both atmospheric research and operational forecasting applications. It runs on AWS Graviton processors with AWS ParallelCluster.&lt;/p&gt; 
&lt;p&gt;To run the workload, login to the HPC head node created by the CloudFormation template you deployed above&amp;nbsp;using &lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html"&gt;AWS Systems Manager Session Manager&lt;/a&gt;&amp;nbsp;by following &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/session-manager.html"&gt;these instruction&lt;/a&gt;s.&lt;/p&gt; 
&lt;p&gt;Once you’ve logged in to the head node, you can execute some commands to start a weather forecast simulation. &lt;em&gt;Note that this will take approximately 2 hours to complete: one hour to download and prepare the data and software, and another hour to complete the simulation:&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;sudo -i
cd /shared
touch /var/log/user-data.log
./rundemo.sh &amp;amp;
tail -f /var/log/user-data.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The shell script implements the ODCR logic by using the &lt;code&gt;create-capacity-reservation&lt;/code&gt; API through the AWS Command Line Interface (CLI) to reserve the capacity you need. The script reserves a specific instance type in a specific AZ, which is tied to the cluster placement group. With ODCRs, the capacity becomes available and billing starts as soon as Amazon EC2 provisions the Capacity Reservation.&lt;/p&gt; 
&lt;p&gt;If the ODCR request fails, the script will pivot to alternative instance types until the ODCR request is successful. It’s also possible to loop through different AZs and regions if those are viable options for your workload.&lt;/p&gt; 
&lt;p&gt;Once the ODCR is successful, the script updates the instance type and then kicks off the HPC workload.&lt;/p&gt; 
&lt;p&gt;Navigating to the &lt;a href="https://us-east-1.console.aws.amazon.com/ec2/home?region=us-east-1#CapacityReservations:"&gt;Capacity Reservations&lt;/a&gt; section of the &amp;nbsp;Amazon EC2 Console you can see the Capacity Reservations that are actively being used by the solution.&lt;/p&gt; 
&lt;div id="attachment_2644" style="width: 997px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2644" loading="lazy" class="size-full wp-image-2644" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.45.01.png" alt="Figure 4: On-Demand Capacity Reservation" width="987" height="190"&gt;
 &lt;p id="caption-attachment-2644" class="wp-caption-text"&gt;Figure 4: On-Demand Capacity Reservation&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Visualize the results of the workload (optional)&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/dcv/"&gt;NICE DCV&lt;/a&gt; is a remote visualization technology that enables you to securely connect to graphic-intensive 3D applications hosted on a remote server.&lt;/p&gt; 
&lt;p&gt;To see the results,&amp;nbsp;you’ll need to connect to the head node through NICE DCV &lt;strong&gt;remotely&lt;/strong&gt; using &lt;a href="https://pypi.org/project/aws-parallelcluster/"&gt;AWS ParallelCluster python cluster management tool&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To do this, open a terminal from your local machine and run these commands to install the AWS ParallelCluster CLI and connect to the head node. You’ll need to specify the PEM file that corresponds to the EC2 key-pair you selected as an input parameter to your CloudFormation stack:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;# run pip3 command if you do not already have pcluster cluster management tool installed.
pip3 install aws-parallelcluster
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once you’re connected to the head node, launch a terminal, and run the following command to start ncview to visualize your results:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd $WRFWORK
ncview wrfout*
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here’s a couple of screenshots from the simulation results.&lt;/p&gt; 
&lt;div id="attachment_2645" style="width: 653px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2645" loading="lazy" class="size-full wp-image-2645" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.46.16.png" alt="Figure 5: ncview graphic application to visualize WRF output" width="643" height="581"&gt;
 &lt;p id="caption-attachment-2645" class="wp-caption-text"&gt;Figure 5: ncview graphic application to visualize WRF output&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2646" style="width: 634px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2646" loading="lazy" class="size-full wp-image-2646" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.46.22.png" alt="Figure 6: QVAPOR – Water Vapor Mixing Ratio" width="624" height="415"&gt;
 &lt;p id="caption-attachment-2646" class="wp-caption-text"&gt;Figure 6: QVAPOR – Water Vapor Mixing Ratio&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Cluster Cleanup&lt;/h2&gt; 
&lt;p&gt;To remove the cluster, go to&lt;a href="https://us-east-1.console.aws.amazon.com/cloudformation/home?region=us-east-1"&gt; AWS CloudFormation console&lt;/a&gt; and delete the CloudFormation stack that you created in this solution.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;With this solution, an ODCR reserves sufficient Amazon EC2 instances for the burstiness of the Maxar WeatherDesk workloads that run – at most – a few hours a day. They’re able to do this efficiently and cost-effectively without the interruptions from EC2 Spot or the need to sign up to a one-year or three-year commitment.&lt;/p&gt; 
&lt;p&gt;This method also introduces the concept of&amp;nbsp;&lt;em&gt;failing fast&lt;/em&gt; – pivoting to other HPC-optimized EC2 instance types &lt;em&gt;as needed&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;In the case of Maxar’s WeatherDesk, speed is everything…but so is dependability. If a workload can’t run because resources aren’t available, a quick pivot is essential so that Maxar’s customers still receive the critical Earth Intelligence information they need to operate their business, mission, or operation.&lt;/p&gt; 
&lt;p&gt;With a few lines of code, layering in ODCR workflows into their solution enabled the flexibility they need at the scale necessary for timeliness requirements.&lt;/p&gt; 
&lt;p&gt;Building Maxar’s WeatherDesk on AWS means they can rely on foundational concepts of reliability, scalability, and agility to address the rigid and demanding needs of weather workflows. By leveraging hpc6a instances, Maxar further reduced the runtime of its &lt;a href="https://blog.maxar.com/leading-the-industry/2020/maxar-wins-aws-2020-public-sector-partner-award-for-best-high-performance-computing-solution"&gt;award-winning numerical weather prediction HPC workload&lt;/a&gt; by 35% while also creating efficiency gains. This results in a solution that is more attractive to more users, particularly in terms of speed, ease of access and dependability, all of which are critical for supporting our customers’ decision making for their business, mission, and operations.&lt;/p&gt; 
&lt;p&gt;Using ODCRs gave them peace of mind regarding capacity, because they can dynamically shift EC2 instance types and still be on time, and keep their compute costs low. Since Maxar implemented this solution on AWS, their cost-to-run shrank by more than 50% and they’ve been able to take advantage of numerous AWS offerings that help reduce even further – while also increasing resiliency.&lt;/p&gt; 
&lt;p&gt;These efficiency gains result in a solution that is more attractive to more users, particularly in terms of cost, ease of access, and dependability – all of which are critical for supporting our customers’ decision making for their business, mission, and operation and delivering on Maxar’s purpose, For A Better World.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.57.16.png" alt="Christopher Cassidy" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Christopher Cassidy&lt;/h3&gt; 
  &lt;p&gt;Christopher Cassidy is a Principal DevOps Engineer at Maxar, working with the WeatherDesk team since 2006. He started as a data engineer before shifting gears to lead the initial cloud migration and ongoing cloud deployments. He is constantly looking for ways to optimize and improve how the organization utilizes the cloud. Outside of work, he enjoys watching Boston sports and Penn State University football, biking, gardening, and spending time with family.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.57.21.png" alt="Stefan Cecelski" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Stefan Cecelski&lt;/h3&gt; 
  &lt;p&gt;Stefan Cecelski is a Principal Data Scientist and Engineer at Maxar. As the technical lead for HPC solutions on the WeatherDesk team, he has a passion for applying innovative technologies that help Maxar better understand the weather and its impacts on customers. Beyond always keeping an eye on the sky, he enjoys hiking, birding, traveling internationally, and spending time with his family.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.57.24.png" alt="Travis Hartman" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Travis Hartman&lt;/h3&gt; 
  &lt;p&gt;Travis Hartman is the Director of Weather and Climate at Maxar. He has been with the organization since the early 2000s, beginning his career as an operational meteorologist and now leading and managing the Maxar portfolio of Weather and Climate products and programs. While he is always learning more about why the wind blows the way it does and why clouds do what they do, he also enjoys spending time with his family and trying to keep up with their busy, youth sports schedules.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>How Amazon’s Search M5 team optimizes compute resources and cost with fair-share scheduling on AWS Batch</title>
		<link>https://aws.amazon.com/blogs/hpc/how-amazons-search-m5-team-optimizes-compute-resources-and-cost-with-fair-share-scheduling-on-aws-batch/</link>
		
		<dc:creator><![CDATA[Kamalakannan Hari Krishna Moorthy]]></dc:creator>
		<pubDate>Tue, 05 Sep 2023 14:16:09 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Customer Solutions]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Thought Leadership]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<guid isPermaLink="false">fe8b346b20cec9af7046fccc16492d8c76275437</guid>

					<description>In this post, we share how Amazon Search optimizes their use of accelerated compute resources using AWS Batch fair-share scheduling to schedule distributed deep learning workloads.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed&amp;nbsp;by: Kamalakannan Hari Krishna Moorthy (SDE), Ameeta Muralidharan (SDE), Vijay Rajakumar (Sr SDE), R J (Principal Engineer), James Park (Sr Solutions Architect)&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The M5 program within Amazon Search owns the discovery learning strategy for Amazon and builds large-scale models across modalities: multilingual, multi-entity, and multitask. To build and train models with billions of parameters at scale, M5 uses accelerated compute such as Amazon Elastic Compute Cloud (Amazon EC2) instances with GPUs and AWS Trainium. One of our central tenets is to keep the infrastructure and operational costs under control.&lt;/p&gt; 
&lt;p&gt;In this post, we focus on how we evolved our systems to manage accelerated compute resources efficiently, and schedule the distributed deep learning workloads by leveraging AWS Batch &lt;strong&gt;fair-share scheduling&lt;/strong&gt;. By continuously improving the approach to managing compute resources and scheduling, we have: 1) reduced idle resources by 14%; 2) increased GPU utilization of our fleet by 19%; and 3) eliminated downtime during reallocation of compute.&lt;/p&gt; 
&lt;h2&gt;The evolution of our queue system over time&lt;/h2&gt; 
&lt;p&gt;Initially, M5 started as a one-pizza team and each developer was assigned some compute resources to run their experiments (Figure 1). However, this approach led to idle resources whenever a developer wasn’t actively running an experiment. When the team size increased, the number of experiments we ran also increased and eventually the compute required by the experiments exceeded the available compute. Due to this constraint, we needed a job queue to manage experiment lifecycle with compute that was available, in a scalable and reproducible manner.&lt;/p&gt; 
&lt;div id="attachment_2791" style="width: 460px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2791" loading="lazy" class="size-full wp-image-2791" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-12.36.50.png" alt="Figure 1: Our initial approach was to have each M5 developer be assigned a dedicated set of compute resources." width="450" height="279"&gt;
 &lt;p id="caption-attachment-2791" class="wp-caption-text"&gt;Figure 1: Our initial approach was to have each M5 developer be assigned a dedicated set of compute resources.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We chose AWS Batch as the solution to this requirement and were successful in efficiently sharing our compute resources across different AWS Regions, boosting our experiment velocity (Figure 2). Batch is a fully managed service that plans, schedules, and executes containerized ML workloads on different AWS compute offerings such as Amazon EC2 (both Spot and On-Demand Instances), Amazon ECS, Amazon EKS, AWS Fargate. Distributed training jobs in M5 are executed using a cluster of accelerated compute resources supported by multi-node-parallel jobs in Batch.&lt;/p&gt; 
&lt;div id="attachment_2792" style="width: 953px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2792" loading="lazy" class="size-full wp-image-2792" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-12.38.08.png" alt="Figure 2: As the team grew, we introduced AWS Batch job queues to manage the jobs on the compute resources using a first-in-first-out (FIFO) strategy." width="943" height="287"&gt;
 &lt;p id="caption-attachment-2792" class="wp-caption-text"&gt;Figure 2: As the team grew, we introduced AWS Batch job queues to manage the jobs on the compute resources using a first-in-first-out (FIFO) strategy.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As time progressed and our team size (and experiments and workloads) increased, we encountered another scheduling challenge. The Batch queue employed a first-in-first-out (FIFO) strategy for job placement, which sometimes resulted in a series of long-running jobs occupying compute resources for extended periods and dominating the queue. As a consequence, other jobs had to wait longer than we wanted before they could access the required compute resources and certain internal teams faced difficulties in obtaining compute resources promptly for their experiments. The single FIFO queue strategy was impacting their ability to meet business-driven timelines (Figure 3).&lt;/p&gt; 
&lt;div id="attachment_2793" style="width: 969px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2793" loading="lazy" class="size-full wp-image-2793" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-12.38.52.png" alt="Figure 3. First-in-first-out queue shown above is shared by 2 teams (blue and green) where order of submitting jobs and running duration of jobs favor team blue which acquired more compute resources resulting in an unfair allocation to team green, simply because blue submitted their jobs earlier in time." width="959" height="289"&gt;
 &lt;p id="caption-attachment-2793" class="wp-caption-text"&gt;Figure 3. First-in-first-out queue shown above is shared by 2 teams (blue and green) where order of submitting jobs and running duration of jobs favor team blue which acquired more compute resources resulting in an unfair allocation to team green, simply because blue submitted their jobs earlier in time.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To enable experiments from all teams to proceed at an equal pace, the total compute was divided and allocated to multiple new FIFO queues such that each team receives a part of the total compute dedicated to their experiments and decide prioritization.&lt;/p&gt; 
&lt;p&gt;Team-specific FIFO queues worked well for teams to schedule their jobs in a timely manner. However, we observed two main disadvantages over time:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Reallocation of resources across queues involved downtime.&lt;/strong&gt; Changes to resource allocation were carried out by scaling down compute from a source queue and scaling up in a target queue. In the background, this involved termination of EC2 instances from Batch compute environment linked to the source queue and re-provisioning them in the compute environment linked to the target queue. Due to limited availability of accelerated compute resources, the re-provisioning step was not predictably able to complete in a deterministic timeframe. This is because when you terminate an EC2 instance it becomes available to numerous other AWS accounts. In our experience, capacity reallocations ranged from few days to sometimes weeks, incurring cost and time during which no queue was able to use those instances.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Inefficient use of resources became a problem again.&lt;/strong&gt; We started observing unused/idle resources in the team specific FIFO queues. This was mainly coming from three factors: 1) random traffic of jobs across different teams led to under-saturated job queues (less jobs than available resources), 2) scheduling overhead —&amp;nbsp;which is the time the scheduler takes to prepare N instances to start a N-node job when all N nodes are not readily available, 3) fragmentation —&amp;nbsp;which refers to idle instances resulting across 20 queues. Though the fragmented instances could be from the same AWS Availability Zone (AZ), they cannot be utilized to schedule jobs if they belong to different Batch compute environments (CEs). CEs are the compute pools backing job queues. The total unused compute resources contributed to about 23.6% of our total fleet size, which is more than enough cause to explore solutions to reduce idle resources across our queues.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_2794" style="width: 913px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2794" loading="lazy" class="size-full wp-image-2794" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-12.39.31.png" alt="Figure 4. Next, we created dedicated FIFO job queues for each team, which provided dedicated compute reservations but leads to inefficient utilization. In the example with two queues shown, jobs wait for resources in first queue while there are idle resources and no pending jobs in the latter." width="903" height="604"&gt;
 &lt;p id="caption-attachment-2794" class="wp-caption-text"&gt;Figure 4. Next, we created dedicated FIFO job queues for each team, which provided dedicated compute reservations but leads to inefficient utilization. In the example with two queues shown, jobs wait for resources in first queue while there are idle resources and no pending jobs in the latter.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To overcome these disadvantages, we evaluated then newly-announced &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-fair-share-scheduling-for-aws-batch/"&gt;fair share scheduling policies&lt;/a&gt; (FSS) for Batch job queues. With FSS, Batch offers configurable parameters like share identifiers, weights for share identifiers, priority for jobs, fairness duration, etc., to ensure fair allocation of compute resources to several users while managing them in a single queue. The following is how we used these parameters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;shareIdentifier&lt;/code&gt;: represent internal teams that share the compute&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;weightFactor&lt;/code&gt;: share of compute allocated for each &lt;code&gt;shareIdentifier&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;shareDecaySeconds&lt;/code&gt;: period over which Batch calculates fairness of usage by a &lt;code&gt;shareIdentifier&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;computeReservation&lt;/code&gt;: share of compute to be set aside for inactive &lt;code&gt;shareIdentifiers&lt;/code&gt;. i.e., share identifiers which do not have active jobs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jobPriority&lt;/code&gt;: determines priority of jobs within a &lt;code&gt;shareIdentifier&lt;/code&gt;. i.e., a job with higher priority will be scheduled ahead of a job with lower priority irrespective of their creation times&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The first four parameters are configured using a scheduling policy which is attached to the job queue. Once configured, the Batch scheduler will keep track of compute usage and schedule jobs such that resources are allocated fairly. The &lt;code&gt;jobPriority&lt;/code&gt; parameter is set either within the job definition, or at the time of submitting a job. Using this information, we created a plan to move from team-specific FIFO queues to a unified queue with FSS where teams share capacity &lt;em&gt;while retaining the advantages of having team specific queues. &lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;As a first step, we converted team specific FIFO queues into FSS queues to reduce idle resources caused by scheduling overhead (~3% of total fleet) within individual queues. This was enabled by allowing jobs to be submitted with different priorities where developers could submit jobs with higher priorities to take advantage of instances that are idle due to scheduling overhead. For example, if there is a job needs 24 instances to become available and get scheduled while the queue only has 16 available, a new incoming job with higher priority can get scheduled immediately if it requires 16 nodes or less. Since CEs can be attached to more than one queue, we successfully switched over from FIFO to FSS in ~20 job queues without any downtime.&lt;/p&gt; 
&lt;p&gt;Next, we consolidated our capacity pools that were partitioned logically as several On-Demand Capacity Reservations (ODCR) into fewer ODCRs grouped by AZ. We then carried out the consolidation of 20 FSS queues in 2 phases. In the first phase, we reduced 20 FSS queues to 3 based on project boundaries. The CEs from 20 queues were retained and attached as-is to one of the 3 FSS queues to avoid downtime. This phase gave us an opportunity to dry run the new scheduling approach while limiting our blast radius from an unforeseen outage. At the end of phase 1, we reduced the idle compute resources from 23.6% to 20% while the GPU utilization of the fleet was at 69%.&lt;/p&gt; 
&lt;p&gt;In the second phase, we created the unified FSS queue with fewer CEs to reduce fragmentation and migrated the compute resources from staging FSS queues. Teams using the unified queue received a share of compute from our fleet which was configured using &lt;code&gt;shareIdentifiers&lt;/code&gt; and &lt;code&gt;weightFactors&lt;/code&gt; on the scheduling policy. Values for &lt;code&gt;weightFactor&lt;/code&gt; were based on the level of compute a team needed in the near future. Changes to resource allocation are handled by simply updating the weights, which eliminated any downtime while moving capacity between teams. After completion of phase 2, we saw a reduction in idle resources from 20% to 9.5% while our fleet’s aggregate GPU utilization increased from 69% to 88.7% due to reduction in idle compute resources. (If you are interested in how we measure GPU utilization, check out the AWS Deep Learning AMI &lt;a href="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-gpu-monitoring-gpumon.html"&gt;documentation&lt;/a&gt;).&lt;/p&gt; 
&lt;div id="attachment_2795" style="width: 892px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2795" loading="lazy" class="size-full wp-image-2795" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-12.42.04.png" alt="Figure 5. Compute managed and share by teams with a single FSS queue improving utilization while retaining fairness in allocation " width="882" height="591"&gt;
 &lt;p id="caption-attachment-2795" class="wp-caption-text"&gt;Figure 5. Compute managed and share by teams with a single FSS queue improving utilization while retaining fairness in allocation&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;Figure 5. Compute managed and share by teams with a single FSS queue improving utilization while retaining fairness in allocation&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we described how Amazon’s Search M5 team continuously optimized their compute resources while owning and operating one of the largest accelerated compute clusters at Amazon. By moving to a unified fair-share scheduling queue on AWS Batch, M5 met its requirements with 14% less resources, and improved GPU utilization by 19% while also eliminating downtime during capacity allocation changes. We also published a self-paced workshop on how to leverage AWS Batch multi-node processing jobs to train deep learning models, and you can try it out in your own account &lt;a href="https://frameworks.hpcworkshops.com/05-batch-mnp-train-gpu.html"&gt;here&lt;/a&gt;. If you are interested in more detailed information about fair share, read the previous &lt;a href="https://aws.amazon.com/blogs/hpc/deep-dive-on-fair-share-scheduling-in-aws-batch/"&gt;deep dive blog post&lt;/a&gt; on fair share scheduling policy parameters.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Bursting your HPC applications to AWS is now easier with Amazon File Cache and AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/bursting-your-hpc-applications-to-aws-is-now-easier-with-amazon-file-cache-and-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Shun Utsui]]></dc:creator>
		<pubDate>Thu, 31 Aug 2023 14:35:36 +0000</pubDate>
				<category><![CDATA[Amazon File Cache]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Hybrid Cloud Management]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[hybrid]]></category>
		<guid isPermaLink="false">e9507e0b872282e4bb0b29af1c43a2a0ce6e8277</guid>

					<description>Today we're announcing the integration between Amazon File Cache and AWS ParallelCluster - super important for hybrid scenarios. We'll show you how it works and how to deploy it.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright wp-image-2829 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/30/File-Cache-and-ParallelCluster-1.png" alt="Bursting your HPC applications to AWS is now easier with Amazon File Cache and AWS ParallelCluster" width="380" height="212"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;This post was contributed by Shun Utsui, Senior HPC Solutions Architect, Austin Cherian, Senior Product Manager&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Combining on-premises and cloud&amp;nbsp;computing resources has emerged as a powerful solution for organizations seeking to meet their&amp;nbsp;computing needs. While you can target compute to be on-premises &lt;em&gt;or&lt;/em&gt; in the cloud, your workloads’ accessibility to data in a fast and secure manner is critical to its performance.&lt;/p&gt; 
&lt;p&gt;Customers have asked for more flexibility to send their workloads to AWS while keeping some data primarily on-premises, so today we’re announcing support for Amazon File Cache in ParallelCluster 3.7.&lt;/p&gt; 
&lt;p&gt;Amazon File Cache offers a high-speed cache on AWS to facilitate efficient file data processing, regardless of its storage location. File Cache serves as temporary, high-performance cache layer in the cloud, for data stored on-premises. Once configured, workload data moves from on-premises storage to AWS as the workload accesses it via File Cache, which appears to the compute client as a Lustre file system.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk you through the features of File Cache that are important for HPC environments, and show you, step-by-step, how you can quickly deploy this and try it out for yourself.&lt;/p&gt; 
&lt;h2&gt;Introducing Amazon File Cache&lt;/h2&gt; 
&lt;p&gt;Previous versions of ParallelCluster already supported mounting multiple FSx file systems. A File Cache can be attached to a cluster using the same familiar mechanisms. These configurations are useful for building “cloud bursting” scenarios: you can attach a File Cache to your ParallelCluster and submit jobs to the cluster either through a federated cluster on-premises like a stretch cluster in Slurm, or by logging into the ParallelCluster directly.&lt;/p&gt; 
&lt;p&gt;But beyond just data accessibility, there are other considerations that make Amazon File Cache an effective choice for your hybrid HPC use cases.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Scalability &amp;amp; Performance&lt;/strong&gt;: Amazon File Cache is built on Lustre, the popular, high-performance file system, and provides scale-out performance that &lt;em&gt;increases linearly with the cache storage capacity&lt;/em&gt;. File Cache is designed to accelerate cloud bursting workloads using cache storage designed to deliver sub-millisecond latencies, up to hundreds of GB/s of throughput, and millions of IOPS.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Data Management &amp;amp; Security:&lt;/strong&gt; Data encryption, access controls, and backup mechanisms ensure data integrity and confidentiality with robust storage solutions. File Cache provides data protection by encrypting data at rest and in-transit between the cache and Amazon Elastic Compute Cloud (Amazon EC2) instances. You can use AWS Key Management Service (KMS) to manage the encryption keys that are used to encrypt your data. File Cache also supports the use of AWS PrivateLink to access your cache resources without going over the internet. You can use Amazon Virtual Private Cloud (VPC) to control access to your cache resources by configuring the VPC Network ACLs to control access to your cache resources. Network ACLs enable you to create a firewall that controls inbound and outbound traffic at the subnet level.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Cost optimization&lt;/strong&gt;: &amp;nbsp;File Cache offers a flexible pricing model so you only pay for the resources you use and there are no minimum fees or setup charges. While pricing is quoted on a monthly basis, billing is &lt;em&gt;per-second&lt;/em&gt;.&lt;/p&gt; 
&lt;h2&gt;Reference Architecture&lt;/h2&gt; 
&lt;p&gt;Amazon File Cache can be associated with Amazon S3 buckets or Network File System (NFS) file systems that support NFS v3. Figure 1 shows a typical architecture for a hybrid HPC environment.&lt;/p&gt; 
&lt;p&gt;Customers with large-scale HPC resources in their own data centers often already have a parallel file system for storage, like Lustre which can be exported using NFS.&lt;/p&gt; 
&lt;p&gt;To deploy a hybrid infrastructure pattern like this, you’ll need to connect your corporate data center to your VPC using either an &lt;a href="https://aws.amazon.com/vpn/site-to-site-vpn/"&gt;AWS Site-to-Site VPN&lt;/a&gt;, or an &lt;a href="https://aws.amazon.com/directconnect/"&gt;AWS Direct Connect&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_2818" style="width: 1026px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2818" loading="lazy" class="size-full wp-image-2818" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/30/CleanShot-2023-08-30-at-13.30.08.png" alt="Figure 1 – High-level architecture of Amazon File Cache in AWS ParallelCluster with a data repository association (DRA) to an on-premises HPC storage system. To link an on-premises parallel file system to File Cache you can export the file system over NFS v3. " width="1016" height="450"&gt;
 &lt;p id="caption-attachment-2818" class="wp-caption-text"&gt;Figure 1 – High-level architecture of Amazon File Cache in AWS ParallelCluster with a data repository association (DRA) to an on-premises HPC storage system. To link an on-premises parallel file system to File Cache you can export the file system over NFS v3.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Deployment details&lt;/h2&gt; 
&lt;p&gt;Attaching an Amazon File Cache to a ParallelCluster is straightforward. Let’s walk through a typical work flow so you get the idea. We’ll assume you already have an operational ParallelCluster.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 1.&lt;/strong&gt; Create a Security Group for Amazon File Cache following the “&lt;a href="https://docs.aws.amazon.com/fsx/latest/FileCacheGuide/limit-access-security-groups.html"&gt;Cache access control with Amazon VPC&lt;/a&gt;” section of the File Cache user guide.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 2.&lt;/strong&gt; Create a cache following the “&lt;a href="https://docs.aws.amazon.com/fsx/latest/FileCacheGuide/getting-started-step1.html"&gt;Create your cache&lt;/a&gt;” section of the File Cache User Guide. Copy the &lt;strong&gt;File Cache ID&lt;/strong&gt; to the clipboard.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 3.&lt;/strong&gt; Now we’ll modify the ParallelCluster configuration file. If you have an existing cluster deployed through ParallelCluster, you should have a YAML file that you specified during the deployment.&lt;/p&gt; 
&lt;p&gt;The ParallelCluster configuration file is written in YAML and it defines the resources for your HPC cluster. You can refer to the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/configuration-v3.html"&gt;ParallelCluster User Guide&lt;/a&gt; to find out more about the details the file format.&lt;/p&gt; 
&lt;p&gt;Under the &lt;code&gt;SharedStorage&lt;/code&gt; section of the configuration file, add &lt;code&gt;StorageType: FileCache&lt;/code&gt; with the name of the file system, the mount directory, and the File Cache ID that you copied onto your clipboard in the previous step.&lt;/p&gt; 
&lt;p&gt;In this example, we’re configuring the cache to be mounted under the &lt;code&gt;/cache&lt;/code&gt; directory.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;SharedStorage:
  - Name: FileCache0
    MountDir: /cache
    StorageType: FileCache
    FileCacheSettings:
      FileCacheId: &amp;lt;your_file_cache_ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You also need to specify the security group associated with the File Cache. Under the &lt;code&gt;HeadNode&lt;/code&gt; section and the &lt;code&gt;Scheduling&lt;/code&gt; section, enter the security group ID you just created in Step 1 (note that this entry has to be defined under &lt;strong&gt;both&lt;/strong&gt; the &lt;code&gt;HeadNode&lt;/code&gt; &lt;strong&gt;and&lt;/strong&gt; the &lt;code&gt;Scheduling&lt;/code&gt; section, otherwise the cache won’t be mounted on the entire cluster).&lt;/p&gt; 
&lt;p&gt;If you’re configuring multiple queues, you’ll need to specify the &lt;code&gt;AdditionalSecurityGroups&lt;/code&gt; string under the Networking section of &lt;em&gt;all your queue definitions&lt;/em&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Networking:
  SubnetId: &amp;lt;your_subnet_ID&amp;gt;
  AdditionalSecurityGroups:
    - &amp;lt;file_cache_security_group&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step 4.&lt;/strong&gt; Update your running cluster.&lt;/p&gt; 
&lt;p&gt;First, you’ll need to stop the compute fleet so you can update the cluster.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-compute-fleet --status STOP_REQUESTED -n &amp;lt;cluster name&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the compute fleet is stopped, update your cluster with the update-cluster sub-command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-cluster -c &amp;lt;configuration file name&amp;gt; -n &amp;lt;cluster name&amp;gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will take about 10 minutes for the cluster update to complete. Once the update is complete, start the compute fleet again:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-compute-fleet --status START_REQUESTED -n &amp;lt;cluster name&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Confirm that your compute fleet’s status is “RUNNING” with the describe-compute-fleet sub-command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster describe-compute-fleet -n &amp;lt;cluster name&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now login to the head node and check that the cache is mounted under the directory you specified. In our example, we mounted the cache under the directory &lt;code&gt;/cache&lt;/code&gt;, which we’ve shown in Figure 2. You should now be able to access your cache like any other file system on a Linux machine.&lt;/p&gt; 
&lt;div id="attachment_2819" style="width: 974px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2819" loading="lazy" class="size-full wp-image-2819" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/30/CleanShot-2023-08-30-at-13.34.36.png" alt="Figure 2 – Amazon File Cache mounted on /cache in AWS ParallelCluster " width="964" height="294"&gt;
 &lt;p id="caption-attachment-2819" class="wp-caption-text"&gt;Figure 2 – Amazon File Cache mounted on /cache in AWS ParallelCluster&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Step 5.&lt;/strong&gt; Now run your HPC workload on the cluster, using your new cache, and then export results to the data repository. You should be able to list files that you have already uploaded into your data repository.&lt;/p&gt; 
&lt;p&gt;When a file is uploaded onto the data repository, Amazon File Cache will only fetch the metadata. At this point, the data itself is kept in the origin repository, and is only retrieved to the cache when accessed from an Amazon EC2 instance. This means the file access latency will be slightly higher for the first access, but will drop to a sub-millisecond levels once the content has cached. This is called lazy loading, and saves a lot of bandwidth between sites when you’re mounting a cache connected to a large remote file system.&lt;/p&gt; 
&lt;p&gt;Similarly, you can also release a file from the cache without removing the metadata on it. Once your job is completed and the cache is no longer needed, we recommend you export the data onto the data repository and clear the cache. To force exporting of the data from the cache, you can run a command like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;nohup find &amp;lt;target directory on cache&amp;gt; -type f -print0 | xargs -0 -n 1 sudo lfs hsm_archive &amp;amp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the data export task is finished, you can evict the cache to free up capacity. To do that, run the following command on the files you want released:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;lfs hsm_release &amp;lt;file&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Find out more about &lt;a href="https://docs.aws.amazon.com/fsx/latest/FileCacheGuide/export-changed-data.html"&gt;exporting changes to the data repository&lt;/a&gt; and &lt;a href="https://docs.aws.amazon.com/fsx/latest/FileCacheGuide/cache-eviction.html"&gt;cache eviction&lt;/a&gt; in the official Amazon File Cache user guide.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 6. (optional) &lt;/strong&gt;Remove the cache from the cluster and delete it. Once files are exported onto the data repository and the cache is cleared, you can remove the cache. To do that, open your ParallelCluster configuration file and remove the parts that you added in Step 3. Then, execute the exact same commands as in Step 4 to update the cluster. When the cluster is updated and the cache is not mounted anymore, you can safely delete it (follow &lt;a href="https://docs.aws.amazon.com/fsx/latest/FileCacheGuide/getting-started-step4.html"&gt;these steps&lt;/a&gt; for more detail) to save cost.&lt;/p&gt; 
&lt;h2&gt;Verifying the effects of Amazon File Cache and lazy loading&lt;/h2&gt; 
&lt;p&gt;To highlight the effect of Amazon File Cache and its lazy loading feature, we ran the &lt;a href="https://github.com/hpc/ior"&gt;IOR parallel I/O benchmarks&lt;/a&gt;. We created a multi-Availability Zone (AZ) environment to mimic a hybrid HPC set up. AZs are physically separated by a meaningful distance from other AZs in the same AWS Region, but they are all within 60 miles (~ 100 kilometers) of each other.&lt;/p&gt; 
&lt;p&gt;First, we created a ParallelCluster in an AZ that we randomly selected in the North Virginia Region (us-east-1c). Then, we created an EC2 instance with an Amazon EBS volume attached to it in the same region, but in another AZ (us-east-1b).&lt;/p&gt; 
&lt;p&gt;Next, we configured that EC2 instance to become an NFS server that exports the EBS volume we attached.&lt;/p&gt; 
&lt;p&gt;Back in us-east-1c, we created a cache in the same AZ as our ParallelCluster. Then, we linked the cache to the NFS serving EC2 instance in us-east-1c as its data repository.&lt;/p&gt; 
&lt;p&gt;Figure 3 illustrates this configuration.&lt;/p&gt; 
&lt;div id="attachment_2832" style="width: 1287px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2832" loading="lazy" class="size-full wp-image-2832" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/30/IOR-diagram-1.png" alt="Figure 3 – High-level architecture for IOR Parallel I/O benchmarks. The EC2 instance in us-east-1b is an NFS server that exports the EBS volume over NFS v3, which is then linked to Amazon File Cache in us-east-1c as a data repository." width="1277" height="606"&gt;
 &lt;p id="caption-attachment-2832" class="wp-caption-text"&gt;Figure 3 – High-level architecture for IOR Parallel I/O benchmarks. The EC2 instance in us-east-1b is an NFS server that exports the EBS volume over NFS v3, which is then linked to Amazon File Cache in us-east-1c as a data repository.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We submitted our first IOR job onto the cluster to see the write performance to the cache. For this job we used 8x hpc7g.16xlarge instances (which have 64 physical cores per instance), and using all 512 cores in parallel. Each core runs a process which writes a 3.2 GiB file onto the cache, 1.56 TiB of data in aggregate. We instructed the job to run 5 times, so we could see a consistent trend.&lt;/p&gt; 
&lt;p&gt;On a cache that we sized at 4.8 TiB (corresponding to a baseline performance of 4.8 GiB/s) we saw consistent write performance at around 4.48 GiB/s in all 5 iterations of the job. Results of the write performance test are shown in the graph in Figure 4.&lt;/p&gt; 
&lt;div id="attachment_2821" style="width: 729px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2821" loading="lazy" class="size-full wp-image-2821" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/30/CleanShot-2023-08-30-at-13.37.14.png" alt="Figure 4 – A bar graph of IOR write performance tests. Performance is consistent across all 5 iterations. " width="719" height="420"&gt;
 &lt;p id="caption-attachment-2821" class="wp-caption-text"&gt;Figure 4 – A bar graph of IOR write performance tests. Performance is consistent across all 5 iterations.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We wanted to see the difference in read performance for the first and subsequent reads, so next we ran a &lt;em&gt;read-performance&lt;/em&gt; test on the data set that we generated during our &lt;em&gt;write-performance&lt;/em&gt; test.&lt;/p&gt; 
&lt;p&gt;To avoid reading the data from the cache for the first iteration, we cleared the cache following the instructions in Step 5.&lt;/p&gt; 
&lt;p&gt;We also used the same compute resources to perform the write test that we used for the read test. Like before, we instructed the job to run 5 iterations so that during the first iteration, the files are ingested from the data repository into the cache. For the subsequent iterations, the job will read the files from the cache. Results of the read-performance tests are shown in the graph in Figure 5.&lt;/p&gt; 
&lt;div id="attachment_2822" style="width: 745px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2822" loading="lazy" class="size-full wp-image-2822" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/30/CleanShot-2023-08-30-at-13.37.37.png" alt="Figure 5 – A bar graph of IOR read performance tests. The first iteration is slower because the data is retrieved from the data repository through the lazy loading mechanism. The subsequent iterations are consistently fast because data is read from cache. " width="735" height="447"&gt;
 &lt;p id="caption-attachment-2822" class="wp-caption-text"&gt;Figure 5 – A bar graph of IOR read performance tests. The first iteration is slower because the data is retrieved from the data repository through the lazy loading mechanism. The subsequent iterations are consistently fast because data is read from cache.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The results show that the first iteration achieved around 0.12 GiB/s throughput, and in all the subsequent iterations it was consistently around 4.44 GiB/s (around 37 times better!).&lt;/p&gt; 
&lt;p&gt;During the first read, the workload loaded the entire data set of 1.56 TiB onto the cache from the data repository, which resides in another AZ, a physically distant location. Hence the performance was slower. From the second read onwards, the workload read the data only from the cache.&lt;/p&gt; 
&lt;p&gt;Of course, the performance gap between the first and subsequent iterations is dependent on a lot of factors including aggregate size of the data being transferred, some features of the data set (e.g. many small files vs one big file), the network bandwidth, and the NFS server specification. Your mileage &lt;em&gt;will&lt;/em&gt; vary, but you can test it quite quickly yourself, using the steps we’ve outlined in this post.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post we introduced you to the new Amazon File Cache integration feature with AWS ParallelCluster 3.7 to create a hybrid HPC environment with low latency to access your data.&lt;/p&gt; 
&lt;p&gt;We walked you through the details of how to set up File Cache with ParallelCluster and showed you some benchmarks to highlight how lazy-loading can drastically improve IO performance in a hybrid scenario.&lt;/p&gt; 
&lt;p&gt;When you consider Amazon File Cache as a solution for your hybrid workloads, you should consider the trade-offs between cost, performance, and (of course) business value. As we’ve seen with the IOR benchmarks, for data intensive workloads that read large data sets from the repository for the first time, it’ll take more time for a job to finish due to file access latency.&lt;/p&gt; 
&lt;p&gt;Customers who require predictability in performance for &lt;em&gt;every single job&lt;/em&gt;, might want to consider other AWS services. &lt;a href="https://aws.amazon.com/datasync/"&gt;AWS DataSync&lt;/a&gt;, for example, can sync data between your site and AWS to maintain a constantly up to date mirror. This will ensure the workload you run on AWS will access its input data with low latency.&lt;/p&gt; 
&lt;p&gt;Amazon File Cache is a powerful tool that enables customers to access their data on a cache with sub-millisecond latency. Support for Amazon File Cache is a great addition to AWS ParallelCluster for hybrid HPC workloads, making it easy for customers to deploy an HPC environment on AWS in a matter of minutes with a cache linked to data in their own data centers.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>The plumbing: best-practice infrastructure to facilitate HPC on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/the-plumbing-best-practice-infrastructure-to-facilitate-hpc-on-aws/</link>
		
		<dc:creator><![CDATA[Evan Bollig]]></dc:creator>
		<pubDate>Tue, 29 Aug 2023 15:02:56 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Best Practices]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<guid isPermaLink="false">53a6f1dcc9fb5f53203f89e0b63b36a7e608a879</guid>

					<description>If you want to build enterprise-grade HPC on AWS, what’s the best path to get started? Should you create a new AWS account and build from scratch? In this post we'll walk you through the best practices for getting setup cleanly from the start.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-2748" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/02/boofla88_complicate_plumbing_as_a_rube_golberg_machine_with_an__dc6d5623-255a-4f26-9e08-245491e934d8.png" alt="The plumbing: best-practice infrastructure to facilitate HPC on AWS" width="380" height="212"&gt;If you want to build enterprise-grade high performance computing on AWS, what’s the best path to get started? Should you create a new AWS account and build from scratch?&lt;/p&gt; 
&lt;p&gt;To answer those questions, consider an analogy of someone washing their hands at a sink. Many focus on a person washing their hands, but that act is an outcome or result. We’ll argue that the plumbing – the water, the sink, and the drain pipes – is critical to facilitate that outcome.&lt;/p&gt; 
&lt;p&gt;The plumbing often fades to the background, its value under-appreciated until something fails. Yet it provides a best-practices foundation for countless use-cases. In similar fashion, enterprise-grade HPC depends on a lot of critical plumbing.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll identify the hallmarks and best-practices for building a solid HPC foundation so you can get the plumbing right. We’ll assume you’re an HPC facilitator: a system admin, operator, or you work at an HPC center. Deep experience with AWS is &lt;strong&gt;not&lt;/strong&gt; required.&lt;/p&gt; 
&lt;p&gt;Our plan is that you come away with an appreciation for shared responsibilities when building on AWS, and aware of solutions that make enterprise-grade HPC a good deal easier than you might have expected.&lt;/p&gt; 
&lt;h2&gt;Shared responsibility&lt;/h2&gt; 
&lt;p&gt;Many customers, especially individuals or small teams assume that operating on the cloud requires bearing the entire burden of their architecture. The weight of compliance and security, the complexity of networking and identity – those burdens are much larger than one person or one team.&lt;/p&gt; 
&lt;p&gt;Even more HPC customers stumble because they fear that once they’ve entered their credit card to create an account, they own every single decision involved in building HPC on the cloud.&lt;/p&gt; 
&lt;p&gt;If you’re a small business – say a startup – you wouldn’t break ground for a new facility on day one, run network, water, and power lines, build an office building, and then populate it. Instead, you might rent an office – or go to a coffee shop table – where network, water, and power exist for you to tap into. Good plumbing allows you to focus first on building what matters most: your business.&lt;/p&gt; 
&lt;p&gt;At AWS, we often talk about the “&lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/shared-responsibility.html"&gt;shared responsibility model&lt;/a&gt;“ as core to our well-architected framework, a six-pillar framework for best-practices in building secure, high-performing, resilient and efficient infrastructures. Most customers understand shared responsibility regarding the security pillar: &amp;nbsp;customer responsibility &lt;em&gt;in the cloud&lt;/em&gt; vs. AWS responsibility &lt;em&gt;of the cloud&lt;/em&gt;. But shared responsibility also applies elsewhere – like &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/the-shared-responsibility-model.html"&gt;sustainability&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;It can also be multi-layered and differentiate between roles inside your own organization. For example, you as an HPC provider (a &lt;em&gt;facilitator&lt;/em&gt;) might focus on building an HPC cluster, while other teams manage incident response, account guardrails and boundaries, or identity and access management.&lt;/p&gt; 
&lt;p&gt;To make it clear, let’s say you represent an HPC center, and you serve end-users campus-wide at a university or research institution. As an HPC provider, you might be embedded within the campus-wide IT team, or maybe you operate with autonomy while pieces of your infrastructure are tied back through the central IT services. On-premises, you already have the concept of “&lt;em&gt;the plumbing&lt;/em&gt;” that you build on top of. You’re not operating alone. You focus effort on HPC, but your HPC system depends on enterprise shared services (like Active Directory where your users’ IDs and passwords live), your campus network, compliance reporting, and so on. This same pattern repeats itself whether you’re at a university, a national lab, or a non-profit organization. It’s true whether you’re public-sector or a company.&lt;/p&gt; 
&lt;p&gt;As a best-practice, HPC customers (especially those intending to operate enterprise-wide HPC clusters or HPC as a service offerings) leverage shared responsibility to build on top of good &lt;em&gt;plumbing&lt;/em&gt; infrastructure on AWS. The most successful collaborate with their central IT services to layer HPC on top of a broader enterprise cloud strategy.&lt;/p&gt; 
&lt;h2&gt;The plumbing&lt;/h2&gt; 
&lt;p&gt;So what is foundational &lt;em&gt;plumbing,&lt;/em&gt; and how does HPC attach to it?&lt;/p&gt; 
&lt;p&gt;Most enterprise customers establish an &lt;a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-migration/aws-landing-zone.html"&gt;AWS Landing Zone&lt;/a&gt;. This is a well-architected, pre-defined architecture for enterprise infrastructure. A Landing Zone is the foundation for multi-account architecture, Identity and Access Management (IAM), governance, data security, network design, and logging. It’s typically defined by central IT within an enterprise, and helps align accounts to AWS best-practices so they can meet compliance frameworks. Any infrastructure built within an enterprise, including HPC, affixes to the Landing Zone.&lt;/p&gt; 
&lt;p&gt;Therefore, in your role as an HPC facilitator, step one should be to ask your central IT team whether a Landing Zone exists, and how to get started. Share responsibility with that team, and let them guide your HPC build.&lt;/p&gt; 
&lt;p&gt;The fastest path to establish a Landing Zone is through the &lt;a href="https://aws.amazon.com/solutions/implementations/landing-zone-accelerator-on-aws/"&gt;Landing Zone Accelerator&lt;/a&gt; (LZA). This is an open source &lt;a href="https://docs.aws.amazon.com/whitepapers/latest/introduction-devops-aws/infrastructure-as-code.html"&gt;infrastructure as code&lt;/a&gt; solution that enables repeatable configuration and modification of a Landing Zone through low-code YAML files. &lt;a href="https://aws.amazon.com/professional-services/"&gt;AWS Professional Services&lt;/a&gt; and &lt;a href="https://aws.amazon.com/controltower/partners/"&gt;AWS Partners&lt;/a&gt; also offer fast-track options to build and support Landing Zones through LZA deployment and alternative solutions.&lt;/p&gt; 
&lt;p&gt;There is no one-size-fits-all Landing Zone configuration for HPC customers because HPC lives within every industry and compliance framework. However, Figure 1 shows a best-practices architecture pattern that’s repeated across reference LZA implementations from regulated industries like &lt;a href="https://aws.amazon.com/blogs/industries/introducing-landing-zone-accelerator-for-healthcare/"&gt;Health Care (e.g., HIPAA, C5, etc.)&lt;/a&gt;, and country/regional-specific compliance requirements like &lt;a href="https://aws.amazon.com/blogs/publicsector/data-security-governance-best-practices-education-state-local-government/"&gt;State and Local Governments (e.g., FISMA)&lt;/a&gt;, &lt;a href="https://github.com/awslabs/landing-zone-accelerator-on-aws/tree/main/reference/sample-configurations/aws-best-practices-education"&gt;Education (e.g., NIST 800-53, NIST 800-171, ITAR, etc.)&lt;/a&gt;, and &lt;a href="https://github.com/awslabs/landing-zone-accelerator-on-aws/tree/main/reference/sample-configurations/aws-best-practices-govcloud-us"&gt;US Federal and Department of Defense (e.g., ITAR, FedRAMP, CMMC, etc.)&lt;/a&gt;. Other reference implementations for LZAs (including territories outside the USA) are &lt;a href="https://aws.amazon.com/solutions/implementations/landing-zone-accelerator-on-aws/"&gt;inventoried here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;As a word of caution: linking to core infrastructure and adhering to constraints/guardrails imposed by a Landing Zone moves infrastructure closer to meeting compliance requirements, but may not be a complete solution for compliance. Just attaching an HPC cluster to a FedRAMP Landing Zone configuration doesn’t make it FedRAMP-compliant. Proper documentation of controls and internal audits (and in some cases, 3&lt;sup&gt;rd&lt;/sup&gt; party audits) might be required. Here again, share responsibility with central IT services and your enterprise security and compliance office to satisfy enterprise requirements.&lt;/p&gt; 
&lt;div id="attachment_2749" style="width: 990px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2749" loading="lazy" class="size-full wp-image-2749" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/02/CleanShot-2023-08-02-at-08.22.02.png" alt="Figure 1 - Landing Zone Accelerator provides the best-practices multi-account structure (the Plumbing for HPC). Shown here: an example configuration with shared services and shared networking for an enterprise. HPC is built within one or more Workload Accounts." width="980" height="550"&gt;
 &lt;p id="caption-attachment-2749" class="wp-caption-text"&gt;Figure 1 – Landing Zone Accelerator provides the best-practices multi-account structure (the Plumbing for HPC). Shown here: an example configuration with shared services and shared networking for an enterprise. HPC is built within one or more Workload Accounts.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Commonly, a Landing Zone is comprised of a root, or management, organizational unit (OU) with an account that centrally steers the multi-account structure through mandatory and preventative guard rails and other requirements for how accounts are provisioned.&lt;/p&gt; 
&lt;p&gt;The root account may define Service Control Policies and/or a Permissions Boundary to govern how accounts/roles in the organization tree provision resources, call APIs, etc. While it’s possible to self-build and manage a Landing Zone, best-practices (including for Landing Zone Accelerator) &lt;a href="https://aws.amazon.com/blogs/mt/scaling-landing-zone-with-aws-control-towers/"&gt;leverage AWS Control Tower&lt;/a&gt;, which is a managed service purpose-built for this task.&lt;/p&gt; 
&lt;p&gt;Below the top-level OU, a Security OU with Audit and Logging accounts manages organizational-wide services for security (like threat detection) and centralized logs (such as archive and forensics).&lt;/p&gt; 
&lt;p&gt;A third OU dedicated to shared Infrastructure might contain services like Active Directory, as well as a core Network account for firewall and/or packet inspection appliances, and a shared &lt;a href="https://aws.amazon.com/directconnect/"&gt;Direct Connect&lt;/a&gt; attachment.&lt;/p&gt; 
&lt;p&gt;Lastly, the final OU is dedicated to workload accounts – this is where an HPC team can operate to build dev, test, and production clusters and link to the shared infrastructure.&lt;/p&gt; 
&lt;h2&gt;HPC within a Landing Zone&lt;/h2&gt; 
&lt;p&gt;In practice, HPC teams build on top of a Landing Zone with minimal awareness of how the plumbing infrastructure is built. They only need to know when to tie in shared services and how to build HPC infrastructure for themselves.&lt;/p&gt; 
&lt;p&gt;For a deeper look at common HPC scenarios and key elements to ensure architectural best-practices, you can refer to the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/welcome.html"&gt;HPC Lens for the AWS Well-Architected Framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The most significant opportunity for HPC facilitators when they’re building on AWS is to augment their HPC design to exercise the elasticity and flexibility of the cloud. Compute nodes spin up as needed, and down when not. The compute node memory size, core-count, accelerators (and more) can be adjusted in minutes rather than the cluster hardware staying set in stone for a three to five year lifecycle.&lt;/p&gt; 
&lt;p&gt;Your HPC architecture might be a single monolithic HPC system, reproducing the on-premises cluster experience. Or you might be extending it (like in a hybrid HPC scenario) so end-users share a single scheduler and common storage resources. As an alternative, tools and services like &lt;a href="https://aws.amazon.com/blogs/hpc/choosing-between-batch-or-parallelcluster-for-hpc/"&gt;AWS ParallelCluster and AWS Batch&lt;/a&gt; present an opportunity to build smaller HPC solutions right-sized for workloads on a per-project or per-team basis – or even for individual users.&lt;/p&gt; 
&lt;p&gt;Whether you build one cluster or dozens, elasticity and flexibility are critical to conserve cost and evolve the end-user experience from “&lt;em&gt;what questions can I ask with the cores I’ve got?&lt;/em&gt;” (science limited by scale), to “&lt;em&gt;what do I need to do to get more cores?&lt;/em&gt;” (science is priority). The latter implies greater productivity and ability to innovate.&lt;/p&gt; 
&lt;p&gt;A Landing Zone amplifies flexibility in the case of multiple HPC clusters. For example, mapping users and workloads onto separate accounts in the multi-account structure is useful to partition data for residency and access constraints.&lt;/p&gt; 
&lt;p&gt;Second, a Landing Zone configuration dictates how new accounts are provisioned on creation, helping facilitators pre-warm dependencies for launching clusters (for example, required IAM roles, or a database that’s needed to enable accounting in the scheduler).&lt;/p&gt; 
&lt;p&gt;Third, Landing Zones can integrate &lt;a href="https://aws.amazon.com/servicecatalog/"&gt;AWS Service Catalog&lt;/a&gt;, which is a vending service for infrastructure as code. This creates a controlled mechanism for end-users to self-service, provision, and delete infrastructure in their account. Service Catalog Products define template configurations (a website, a database, or an HPC cluster) that are reviewed and approved by HPC facilitators or central IT services to satisfy compliance requirements or other guardrails. Optional parameters on Service Catalog Products give end-users control of facets of HPC relevant to their work, like instance-type selection, storage quota, or cost-center allocation. Through Service Catalog, facilitators distill the complexity of enterprise HPC design and management, but empower their users to innovate autonomously.&lt;/p&gt; 
&lt;p&gt;Landing Zones also support HPC facilitators with cost and billing strategies. Typically, central IT services receive &lt;a href="https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html"&gt;consolidated bills&lt;/a&gt; and reconcile costs with workload accounts through internal cost recovery and charge-back processes. With consolidated billing, organizations combine usage across all workload accounts to more effectively scale &lt;a href="https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html"&gt;Savings Plans&lt;/a&gt; or &lt;a href="https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/useconsolidatedbilling-effective.html#useconsolidatedbilling-discounts"&gt;Volume discounts&lt;/a&gt;. Whereas one HPC cluster with episodic utilization might not benefit from a Savings Plan, utilization &lt;em&gt;in the aggregate&lt;/em&gt; across &lt;em&gt;all&lt;/em&gt; HPC accounts might look sustained – and therefore justify making a commitment with a Savings Plan, and thus saving up to 72% on the organization’s overall compute bill.&lt;/p&gt; 
&lt;p&gt;The same logic applies when utilization is aggregated for HPC and &lt;em&gt;non-HPC&lt;/em&gt; workloads.&lt;/p&gt; 
&lt;p&gt;Consolidated billing doesn’t preclude charge-back to individuals (per-project, team, or user). Separate workload accounts provide a simple segmentation to track spend in the consolidated bill. Alternatively, cost allocation tags can be applied to resources within or across accounts, and differentiate between cost centers for utilization. Again, consult with your central IT services. They should be able to advise on how to best handle cost, billing, and tagging strategies for your HPC workload accounts.&lt;/p&gt; 
&lt;p&gt;If you need more information, review this &lt;a href="https://aws.amazon.com/blogs/apn/reducing-the-cost-of-managing-multiple-aws-accounts-using-aws-control-tower/"&gt;blog post related to Landing Zone best-practices&lt;/a&gt; for cost and billing, and this blog post about &lt;a href="https://aws.amazon.com/blogs/aws-cloud-financial-management/cost-tagging-and-reporting-with-aws-organizations/"&gt;tagging and reporting strategy for organizations&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Landing Zones are a best-practice “&lt;em&gt;plumbing&lt;/em&gt;” for enterprise-grade HPC and the most natural starting point for most HPC customers. Whether you intend to offer end-users flexible HPC as a service, or build a single monolithic HPC cluster shared by all users, we recommend you exercise &lt;strong&gt;shared responsibility&lt;/strong&gt; and collaborate with central IT services teams in your enterprise so you can layer HPC as a workload within the broader cloud strategy.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Deep-dive into Hpc7a, the newest AMD-powered member of the HPC instance family</title>
		<link>https://aws.amazon.com/blogs/hpc/deep-dive-into-hpc7a-the-newest-amd-powered-member-of-the-hpc-instance-family/</link>
		
		<dc:creator><![CDATA[Stephen Sachs]]></dc:creator>
		<pubDate>Tue, 22 Aug 2023 14:54:19 +0000</pubDate>
				<category><![CDATA[*Post Types]]></category>
		<category><![CDATA[Announcements]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[MPI]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">3edbe371730c2bf0764bb5a4d71c99ec5a30d5c5</guid>

					<description>Today we discuss the performance results we saw from the new hpc7a instance, running HPC workloads like CFD, molecular dynamics, and weather prediction codes.</description>
										<content:encoded>&lt;p&gt;Last week, &lt;a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc7a-instances-powered-by-4th-gen-amd-epyc-processors-optimized-for-high-performance-computing/"&gt;we announced the Hpc7a instance type&lt;/a&gt;, the latest generation AMD-based HPC instance, purpose-built for tightly-coupled high performance computing workloads. This joins our family of HPC instance types in the Amazon Elastic Compute Cloud (Amazon EC2) which began with Hpc7a’s predecessor — the Hpc6a in January 2022.&lt;/p&gt; 
&lt;p&gt;Amazon EC2 Hpc7a instances, powered by 4th generation AMD EPYC processors, deliver up to 2.5x better performance compared to Amazon EC2 Hpc6a instances.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll discuss details of the new instance and show you some of the performance metrics we’ve gathered by running HPC workloads like computational fluid dynamics (CFD), molecular dynamics (MD) and numerical weather prediction (NWP).&lt;/p&gt; 
&lt;h2&gt;Introducing the Hpc7a instance&lt;/h2&gt; 
&lt;p&gt;We launched Hpc6a last year for customers to efficiently run their compute-bound HPC workloads on AWS. As their jobs grow in complexity, customers have asked for more cores with more compute performance, as well as more memory and network performance to reduce their time to results. The Hpc7a instances deliver on these asks, providing twice the number of cores, 1.5x the number of memory channels, and three-times the network bandwidth compared to the previous generation.&lt;/p&gt; 
&lt;p&gt;It’s based on the 4th Generation AMD EPYC (code name Genoa) processor with up to 192 physical cores, an&amp;nbsp;all-core turbo frequency of 3.7 GHz,&amp;nbsp;768 GiB of memory, and 300 Gbps of Elastic Fabric Adapter (EFA) network performance. This is all possible because of the &lt;a href="https://aws.amazon.com/ec2/nitro/"&gt;AWS Nitro System&lt;/a&gt;, a combination of dedicated hardware and a lightweight hypervisor&amp;nbsp;that offloads many of the traditional virtualization functions to dedicated hardware, result in performance that’s &lt;a href="https://aws.amazon.com/blogs/hpc/bare-metal-performance-with-the-aws-nitro-system/"&gt;virtually indistinguishable&lt;/a&gt; from bare metal.&lt;/p&gt; 
&lt;h2&gt;HPC instances sizes&lt;/h2&gt; 
&lt;p&gt;Many off the shelf HPC applications use per-core commercial licensing that is often much greater in price per core than the cores themselves. Those customers have asked for more available memory bandwidth, and more network throughput available &lt;em&gt;per-core&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Starting today, Hpc7a (along with other 7&lt;sup&gt;th&lt;/sup&gt; generation HPC instances) will be available in different sizes. Usually in Amazon EC2, a smaller instance size reflects a smaller slice of the underlying hardware. However, for the HPC instances — starting with Hpc7g and Hpc7a — each size option &lt;em&gt;will have the same engineering specs and price&lt;/em&gt;, and will differ only by the number of cores enabled.&lt;/p&gt; 
&lt;p&gt;You have always been able to manually disable cores or use process pinning (affinity) to carefully place threads around the CPUs. But doing this optimally needs an in-depth knowledge of the chip architecture – like the number of NUMA domains and the memory layout. It also means you have to know MPI well, and have a clear sight to what your job submission scripts do when the scheduler receives them.&lt;/p&gt; 
&lt;p&gt;By offering instance sizes that already have the &lt;em&gt;right pattern of cores turned off&lt;/em&gt;, you’ll be able to maximize the performance of your code with less work. This will be a boost for customers who need to achieve the absolute best performance &lt;em&gt;per core&lt;/em&gt; for their workloads, which often includes commercially-licensed ISV applications. In this case, customers are driven by pricing concerns to get the best possible results from each &lt;em&gt;per-core&lt;/em&gt; license they buy. You can find a &lt;a href="https://aws.amazon.com/blogs/hpc/instance-sizes-in-the-amazon-ec2-hpc7-family-a-different-experience/"&gt;detailed explanation in our post on this topic&lt;/a&gt;, along with performance comparisons that will help you understand our methodology.&lt;/p&gt; 
&lt;p&gt;With the HPC instance sizes, the cost stays the same for all sizes because you still get access to the entire node’s memory and network, with selected cores turned off to leave extra memory bandwidth available for the remaining cores. We encourage you to benchmark your own codes and find the right balance for your specific needs.&lt;/p&gt; 
&lt;p&gt;Hpc7a will be available in &lt;strong&gt;four different instances sizes&lt;/strong&gt;. Table 1 describes these in detail.&lt;/p&gt; 
&lt;div id="attachment_2788" style="width: 901px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2788" loading="lazy" class="wp-image-2788 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-14.18.26.png" alt="Table 1 - Available instance sizes for Hpc7a. Customers choosing a smaller instance type will still have access to the full memory and network performance of the largest instance size but a different number of CPU cores." width="891" height="263"&gt;
 &lt;p id="caption-attachment-2788" class="wp-caption-text"&gt;Table 1 – Available instance sizes for Hpc7a. Customers choosing a smaller instance type will still have access to the full memory and network performance of the largest instance size but a different number of CPU cores.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;Hpc7a shows significant performance gains over previous generations. While doubling the number of cores per instance, the performance, network, and memory bandwidth per-core have all increased. In many cases, this is leading to a doubling and more in overall simulation speed.&lt;/p&gt; 
&lt;p&gt;To illustrate this, we’ll look at the relative performance improvement of five common HPC codes across two generations of instance. We’ll look at:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Siemens Simcenter STAR-CCM+ for CFD&lt;/li&gt; 
 &lt;li&gt;Ansys Fluent for CFD&lt;/li&gt; 
 &lt;li&gt;OpenFOAM for CFD&lt;/li&gt; 
 &lt;li&gt;GROMACS for MD&lt;/li&gt; 
 &lt;li&gt;Weather Research and Forecasting Model (WRF) for NWP&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Let’s take a detailed look at each code with performance comparisons to the previous generation Hpc6a instance. For the readers convenience, the previous generations’ specs are Hpc6a.48xlarge: 96 physical cores (3&lt;sup&gt;rd&lt;/sup&gt; Generation AMD EPYC, code name Milan), 384 GB memory, 4 GB memory per core, 3.6 GHz CPU Frequency, and 100 Gbps EFA Network Performance.&lt;/p&gt; 
&lt;h2&gt;Siemens Simcenter STAR-CCM+&lt;/h2&gt; 
&lt;p&gt;First, we take a look at Siemens Simcenter STAR-CCM+. We chose the AeroSUV 320M cell automotive test case – a useful public case with similar characteristics to production automotive external aerodynamics models. We ran this with Siemens Simcenter STAR-CCM+ 2306, using Intel MPI 2021.6. As this is an ISV application, no further tuning to match the architecture is necessary. The graphs below show the &lt;em&gt;iterations per minute&lt;/em&gt; as a metric for performance.&lt;/p&gt; 
&lt;p&gt;We saw an up to 2.7x speed-up between Hpc7a and Hpc6a at 16 instances and similar scaling all the way to 12k cores, that’s around 26k cells per core.&lt;/p&gt; 
&lt;div id="attachment_2768" style="width: 980px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2768" loading="lazy" class="size-full wp-image-2768" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.36.02.png" alt="Figure 1 – A graph of performance of Siemens Simcenter STAR-CCM+ on the AeroSUV 320M cell dataset. The figure shows that Hpc7a outperforms Hpc6a up to 3.2x on a per instance basis." width="970" height="538"&gt;
 &lt;p id="caption-attachment-2768" class="wp-caption-text"&gt;Figure 1 – A graph of performance of Siemens Simcenter STAR-CCM+ on the AeroSUV 320M cell dataset. The figure shows that Hpc7a outperforms Hpc6a up to 2.7x on a per instance basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2786" style="width: 978px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2786" loading="lazy" class="wp-image-2786 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-14.14.12.png" alt="Figure 2 – A graph of performance of Siemens Simcenter STAR-CCM+ on the AeroSUV 320M cell dataset. Hpc7a outperforms Hpc6a up to 1.29x on a per core basis." width="968" height="537"&gt;
 &lt;p id="caption-attachment-2786" class="wp-caption-text"&gt;Figure 2 – A graph of performance of Siemens Simcenter STAR-CCM+ on the AeroSUV 320M cell dataset. Hpc7a outperforms Hpc6a up to 1.29x on a per core basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Ansys Fluent&lt;/h2&gt; 
&lt;p&gt;Next we took a look at ANSYS Fluent 2023R1 – where we ran the common public dataset &lt;a href="https://www.ansys.com/en-in/it-solutions/benchmarks-overview/ansys-fluent-benchmarks/ansys-fluent-benchmarks-release-19/external-flow-over-a-formula-1-race-car"&gt;External flow over a Formula-1 race car&lt;/a&gt; . The case has around 140-million cells and uses the realizable k-e turbulence model and the pressure-based coupled solver, least squares cell-based, pseudo transient solver. We ran it to over 9,000 cores, which is still well within the parallel scaling of this particular test case.&lt;/p&gt; 
&lt;p&gt;The graphs show Solver rating as defined by Ansys as the number of benchmarks that can be run on a given machine (in sequence) in a 24-hour period. We compute this by dividing the number of seconds in a day (86,400 seconds) by the number of seconds required to run the benchmark.&lt;/p&gt; 
&lt;p&gt;On a per instance basis Hpc7a exhibits up to 2.48x better performance than Hpc6a at 6 instances. The iso-core benefit to Hpc7as favor peaks with 1.29x at 6144 cores.&lt;/p&gt; 
&lt;div id="attachment_2770" style="width: 981px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2770" loading="lazy" class="size-full wp-image-2770" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.37.23.png" alt="Figure 3 – A graph of performance of Ansys Fluent on the F1 race car 140M dataset. Hpc7a outperforms Hpc6a up to 2.48x on a per instance basis." width="971" height="537"&gt;
 &lt;p id="caption-attachment-2770" class="wp-caption-text"&gt;Figure 3 – A graph of performance of Ansys Fluent on the F1 race car 140M dataset. Hpc7a outperforms Hpc6a up to 2.48x on a per instance basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2771" style="width: 979px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2771" loading="lazy" class="size-full wp-image-2771" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.37.43.png" alt="Figure 4 – A graph of performance of Ansys Fluent on the F1 race car 140M dataset. Hpc7a outperforms Hpc6a up to 1.29x on a per core basis." width="969" height="538"&gt;
 &lt;p id="caption-attachment-2771" class="wp-caption-text"&gt;Figure 4 – A graph of performance of Ansys Fluent on the F1 race car 140M dataset. Hpc7a outperforms Hpc6a up to 1.29x on a per core basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;OpenFOAM&lt;/h2&gt; 
&lt;p&gt;Next, we tested OpenFOAM with the &lt;em&gt;DrivAer fastback vehicle&lt;/em&gt; (from the &lt;a href="http://autocfd.eng.ox.ac.uk/"&gt;AutoCFD workshop&lt;/a&gt;) with 128M cells (generated using the ANSA preprocessing software by BETA-CAE Systems), and ran the case in hybrid RANS-LES mode using the pimpleFoam solver. We used OpenFOAM v2206 compiled with GNU C++ Compiler v12.3.0 and Open MPI v4.1.5. We use the architecture specific flags to tune the compilation for each instance type (“-march=x86-64-v4 -mtune=x86-64-v4” for Hpc7a and “-march=znver3 -mtune=znver3” for Hpc6a). We ran the case using 192 and 96 MPI ranks per instance, respectively, (fully-populated) and scaled to 3072 cores. The graphs below show the &lt;em&gt;iterations per minute&lt;/em&gt; as a metric for performance.&lt;/p&gt; 
&lt;p&gt;We saw an up to 2.7x speed-up at 4 instances as this is the core count where Hpc7a is showing super linear scaling curve. Super linear scaling stops at 1536 cores for both instances types and further decreases towards 3072. This is 42k cores per MPI rank which is the expected scaling limit for OpenFOAM.&lt;/p&gt; 
&lt;div id="attachment_2772" style="width: 977px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2772" loading="lazy" class="size-full wp-image-2772" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.38.19.png" alt="Figure 5 – A graph of performance of OpenFOAM on the DrivAer 128M dataset. Hpc7a outperforms Hpc6a by up 2.7x on a per instance basis." width="967" height="537"&gt;
 &lt;p id="caption-attachment-2772" class="wp-caption-text"&gt;Figure 5 – A graph of performance of OpenFOAM on the DrivAer 128M dataset. Hpc7a outperforms Hpc6a by up 2.7x on a per instance basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2773" style="width: 979px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2773" loading="lazy" class="size-full wp-image-2773" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.38.49.png" alt="Figure 6 – A graph of performance of OpenFOAM on the DrivAer 128M dataset. Hpc7a outperforms Hpc6a by up 1.3x on a per core basis." width="969" height="540"&gt;
 &lt;p id="caption-attachment-2773" class="wp-caption-text"&gt;Figure 6 – A graph of performance of OpenFOAM on the DrivAer 128M dataset. Hpc7a outperforms Hpc6a by up 1.3x on a per core basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;GROMACS&lt;/h2&gt; 
&lt;p&gt;Next, we looked at Max Planck Institute provided test case for &lt;a href="https://www.mpinat.mpg.de/grubmueller/bench"&gt;2M atoms ribosome in water (benchRIB)&lt;/a&gt; using GROMACS version 2021.5. We used the Intel compiler version 2022.1.2 and Intel MPI 2021.5.1 to compile and run GROMACS. In this case we used the best matching Intel Compiler flags for Hpc7a (“-march=skylake-avx512 -mtune=skylake-avx512”) and Hpc6a (“-march=core-avx2”) and Intel MKL for the Fast Fourier Transform.&lt;/p&gt; 
&lt;p&gt;For each scaling data point we used the optimal MPI rank versus OpenMP thread distribution, which means we start with 1 OpenMP thread and an MPI rank on each core at 1 instances and steadily increase the number of OpenMP threads when scaling further to better balance the workload. The maximum number of threads used is 2 threads at 8 instances and 4 at 16 instances for Hpc7a and Hpc6a, respectively. The graphs show simulated time per day (ns/day), higher is better.&lt;/p&gt; 
&lt;p&gt;We saw up to 2.05x speed-up on 2 instances and similar scaling when comparing Hpc7a to Hpc6a. We also see an up to 1.2x speed-up at 768 cores when comparing on a per core level.&lt;/p&gt; 
&lt;div id="attachment_2774" style="width: 974px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2774" loading="lazy" class="size-full wp-image-2774" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.39.25.png" alt="Figure 7 – A graph of performance of GROMACS on the benchRIB dataset. Hpc7a outperforms Hpc6a up to 2.05x on a per instance basis." width="964" height="536"&gt;
 &lt;p id="caption-attachment-2774" class="wp-caption-text"&gt;Figure 7 – A graph of performance of GROMACS on the benchRIB dataset. Hpc7a outperforms Hpc6a up to 2.05x on a per instance basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2775" style="width: 976px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2775" loading="lazy" class="size-full wp-image-2775" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.39.50.png" alt="Figure 8 – A graph of performance of GROMACS on the benchRIB dataset. Hpc7a outperforms Hpc6a up to 1.2x on a per core basis." width="966" height="535"&gt;
 &lt;p id="caption-attachment-2775" class="wp-caption-text"&gt;Figure 8 – A graph of performance of GROMACS on the benchRIB dataset. Hpc7a outperforms Hpc6a up to 1.2x on a per core basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;WRF&lt;/h2&gt; 
&lt;p&gt;We looked at &lt;a href="https://www2.mmm.ucar.edu/wrf/src/conus2.5km.tar.gz"&gt;CONUS 2.5km benchmark&lt;/a&gt; performance using WRF v4.2.2. We used the Intel compiler version 2022.1.2 and Intel MPI 2021.9.0 to compile and run WRF using the same compiler flags as for GROMACS. We used 48 MPI ranks per instances filling up the remaining cores with 4 OpenMP threads to use all 192 cores per instance.&lt;/p&gt; 
&lt;p&gt;We ran the scaling test up to 128 instances (24,576 cores) and used the total elapsed time to calculate the simulation speed as runs per day (higher is better). Comparing the instances types, we saw an up to 2.6x speed up at 8 instances, and better scalability for Hpc7a compared to previous generation Hpc6a due to the increase of on-node traffic.&lt;/p&gt; 
&lt;div id="attachment_2776" style="width: 975px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2776" loading="lazy" class="size-full wp-image-2776" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.40.19.png" alt="Figure 9 – A graph of performance of WRF on the Conus 2.5km dataset. Hpc7a outperforms Hpc6a by up 2.6x on a per instance basis." width="965" height="535"&gt;
 &lt;p id="caption-attachment-2776" class="wp-caption-text"&gt;Figure 9 – A graph of performance of WRF on the Conus 2.5km dataset. Hpc7a outperforms Hpc6a by up 2.6x on a per instance basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2777" style="width: 976px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2777" loading="lazy" class="size-full wp-image-2777" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.40.40.png" alt="Figure 10 – A graph of performance of WRF on the Conus 2.5km dataset. Hpc7a retains better scalability at &gt;10k cores due to increased on-node traffic." width="966" height="533"&gt;
 &lt;p id="caption-attachment-2777" class="wp-caption-text"&gt;Figure 10 – A graph of performance of WRF on the Conus 2.5km dataset. Hpc7a retains better scalability at &amp;gt;10k cores due to increased on-node traffic.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Performance is more than just higher core counts&lt;/h2&gt; 
&lt;p&gt;Hpc7a shows a great performance increase over the previous generation and not only due to doubling the numbers of cores. Figure 11 shows the performance gain for each workload on a per-core basis, so this is additional to that doubling. We ran all instance comparisons using either real production workloads or a close substitute.&lt;/p&gt; 
&lt;p&gt;Hpc7a instances show, on average, a performance improvement of 29% compared to the previous generation on a &lt;em&gt;per-core&lt;/em&gt; basis.&lt;/p&gt; 
&lt;div id="attachment_2778" style="width: 977px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2778" loading="lazy" class="size-full wp-image-2778" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.41.13.png" alt="Figure 11 - Relative performance of Hpc6a and Hpc7a for various applications. The performance results shown here use the same number of cores on HPC6a.48xlarge and Hpc7a.96xlarge to highlight the expected iso-core improvement. This translates to using only half the number Hpc7a instances per test case." width="967" height="539"&gt;
 &lt;p id="caption-attachment-2778" class="wp-caption-text"&gt;Figure 11 – Relative performance of Hpc6a and Hpc7a for various applications. The performance results shown here use the same number of cores on HPC6a.48xlarge and Hpc7a.96xlarge to highlight the expected iso-core improvement. This translates to using only half the number Hpc7a instances per test case.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog post we introduced the Amazon EC2 Hpc7a instance, which offers up to 2.5x better compute performance compared to previous generation AMD HPC instance. It has twice the cores per instance and yet still has increased per-core compute performance, better memory bandwidth, and greater network performance per core.&lt;/p&gt; 
&lt;p&gt;We hope you’ll try them for your workloads, too. Reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt; and let us know how you fare.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>How computer vision is enabling a circular economy</title>
		<link>https://aws.amazon.com/blogs/hpc/how-computer-vision-is-enabling-a-circular-economy/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Tue, 15 Aug 2023 15:04:30 +0000</pubDate>
				<category><![CDATA[Amazon Machine Learning]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">593227d1a134845db1c20527dfb0d67ff5c347eb</guid>

					<description>In this post, we show how Reezocar uses computer vision to change the way they detect damage and price used vehicles for re-sale in secondary markets. This reduces landfill and helps achieve the goals of the circular economy.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright wp-image-2675 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/boofla88_computer_vision_as_part_of_the_circular_economy_62b0baed-7a6b-49a3-9fc0-848575717090-1.png" alt="How computer vision is enabling a circular economy" width="380" height="212"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;This post was contributed by Laurent Fabre, Chief Technology Officer and his team from Reezocar, and &lt;/em&gt;&lt;em&gt;Ilan Gleiser, Pr. Specialist, Global Impact Computing, AWS and our team.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Computer vision has enormous potential to revolutionize the way we think about sustainability and the circular economy. One of the key ways in which computer vision is already being applied to these areas is using powerful graphics processing units (GPUs) and AWS cloud computing services.&lt;/p&gt; 
&lt;p&gt;One of the primary applications of computer vision in the circular economy is in product lifecycles. By using advanced computer vision algorithms and machine learning models, manufacturers and supply chain managers can more accurately track and analyze the lifecycle of products, from raw materials sourcing all the way through to end-of-life disposal. This allows companies to better understand the environmental impacts of their products and make more informed decisions about how to design and produce products that are more sustainable and can be easily reused or recycled.&lt;/p&gt; 
&lt;p&gt;Another area in which computer vision is being applied to the circular economy is in waste reduction. By using advanced image recognition algorithms and computer vision technologies, waste management companies can better sort and categorize recyclable materials, making it easier and more cost-effective to recover and reuse valuable resources.&lt;/p&gt; 
&lt;p&gt;In addition to these applications, computer vision is also being used to identify opportunities for energy and resource savings, and to monitor and analyze environmental impacts at both a local and global scale. For example, satellite imagery and other forms of remote sensing data can be used to track deforestation, monitor ocean pollution levels, and even create early warning systems to predict the impact of natural disasters on ecosystems.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll explore how Reezocar is using computer vision to change the way they detect car damage and price used vehicles for re-sale in secondary markets. This technology can be used to determine the useful life of a car and potentially reduce the need for landfill waste, therefore aligning with the goals of the circular economy: designing-out waste and pollution from the environment.&lt;/p&gt; 
&lt;p&gt;Secondary markets play a critical role in achieving a circular economy by extending the lifespan of products and reducing waste. The benefits of secondary markets go beyond environmental impact, as they also generate economic opportunities, job creation, and community empowerment.&lt;/p&gt; 
&lt;p&gt;In this context, computer vision is revolutionizing the way we detect car damage. By leveraging the power of machine learning, computer vision can detect even the smallest dents and scratches on a car’s body.&lt;/p&gt; 
&lt;p&gt;Overall, the applications of computer vision in the circular economy are wide-ranging and growing rapidly. By enabling more efficient and sustainable resource use, computer vision has the power to drive real change in our economy and help create a more sustainable future for us all.&lt;/p&gt; 
&lt;h2&gt;Who is Reezocar?&lt;/h2&gt; 
&lt;p&gt;Reezocar is an online platform for buying and selling used cars in France. It offers customers a safe purchase guarantee and a bespoke shopping help service. With access to over six-million car ads, Reezocar makes it easy to select makes and models that match a buyer’s search criteria. Customers also have the option of purchasing a certified car, with a 15-day money back guarantee included.&lt;/p&gt; 
&lt;p&gt;Reezocar uses GPU-accelerated machine learning algorithms, convolutional neural networks, and a damage estimation model to calculate car prices. This system helps customers get the best deal on their purchase, as well as an accurate assessment of the vehicle’s condition. The result is a fair price estimate of the car and reassurance that they’re getting a good value for their money.&lt;/p&gt; 
&lt;p&gt;More than 10,000 customers have already been won over by the Reezocar experience, making it one of the top on-line marketplaces for buying and selling used cars. With its reliable customer service and advanced technology, Reezocar is quickly becoming the go-to destination for those looking to purchase a used car.&lt;/p&gt; 
&lt;p&gt;Reezocar is committed to its environmental mission and employs various strategies to uphold its climate-conscious values. Here are four key approaches they employ:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Extending Vehicle Lifespan&lt;/strong&gt;: By refurbishing selected vehicles, they have successfully prolonged their lifespan by up to five years, using approximately 5% of their manufacturer’s suggested retail price (MSRP) as refurbishing budget. This initiative has prevented over 30,000 tons of waste from ending up in landfills.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Carbon Offset Initiatives&lt;/strong&gt;: To counterbalance the CO2 emissions produced by these vehicles, they consistently engage in carbon offset practices through their partnership with ReforestAction. Through this collaboration, they have planted 33,000 trees, effectively trapping an &lt;a href="https://www.reforestaction.com/reezocar"&gt;estimated 5,000 tons of CO2.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discouraging New Car Production&lt;/strong&gt;: Recognizing that the most environmentally friendly car is the one that doesn’t need to be built, they actively discourage the production of new cars by prioritizing the refurbishment of used vehicles. This approach has resulted in an estimated avoidance of 40,000 tons of material, considering the increasing weight of modern cars.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Supply Chain&lt;/strong&gt;: By advocating for a shorter supply chain, they significantly reduce energy consumption during the delivery of vehicles. Additionally, this approach helps them avoid the costs associated with waste management while minimizing the pollution caused by car-related materials such as plastics, thus safeguarding the surrounding ecosystem.&lt;br&gt; With a decade of experience and numerous satisfied customers, Reezocar remains dedicated to leading the way in climate action.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;The rise of computer vision&lt;/h2&gt; 
&lt;p&gt;For decades, car buyers and dealers have relied on manual inspection to detect body damage on used cars. Until recently, physical inspections were the only way to get a full picture of the car’s condition. Even then, most damage could only be detected with close examination from a trained eye.&lt;/p&gt; 
&lt;p&gt;The technology leverages a combination of machine learning algorithms and convolutional neural networks to detect dents in vehicles, resulting in a faster, more streamlined, and accurate process.&lt;/p&gt; 
&lt;p&gt;Initially, the technology employs machine learning algorithms to estimate an initial value of the car based on features such as its model and gearbox type (automatic vs. standard).&lt;/p&gt; 
&lt;p&gt;Next, Computer vision is used to process the car’s image and identify any form of damage, like scratches, dents, or severe degradation. Any detected damage will be factored into the initial value estimate by subtracting the repair costs. The proposed computer vision system determines the specific parts of the car that are damaged, which helps identify the necessary repairs and their costs.&lt;/p&gt; 
&lt;p&gt;After computing the repair cost, Reezocar generates a post-repair estimate of the car’s value. This estimate considers factors like age, condition, and make to ensure maximum accuracy. With these tools at their disposal, dealerships can offer more competitive pricing and decide whether a car should be refurbished or sent to the landfill.&lt;/p&gt; 
&lt;h2&gt;Reezocar reference architecture&lt;/h2&gt; 
&lt;div id="attachment_2656" style="width: 1079px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2656" loading="lazy" class="size-full wp-image-2656" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.31.43.png" alt="Figure 1: Reezocar’s architecture schema " width="1069" height="509"&gt;
 &lt;p id="caption-attachment-2656" class="wp-caption-text"&gt;Figure 1: Reezocar’s architecture schema&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Reezocar’s Reference architecture aims to detect car dents using computer vision, as illustrated in Figure 1. Their approach consists of three steps: acquiring a dataset, fine tuning a convolutional neural network, and measuring the success of the model.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, they acquired and combined a synthetic and a real-world dataset to ensure a diverse and robust sample dataset. Using Amazon EC2 instances, they generated a synthetic dataset by algorithmically deforming 3D CAD models of cars. This helped to create a labeled synthetic dataset of images. Afterwards, they used Amazon SageMaker Ground Truth to annotate a set of real-world images. To achieve this, Reezocar uses Amazon SageMaker Ground Truth to acquire and annotate data, and then employs machine learning models running on GPUs to train a damage estimation model.&lt;/p&gt; 
&lt;p&gt;By using Amazon SageMaker Ground Truth, Reezocar was able to avoid the need to manage their own data labeling workforce, which would have slowed down innovation and increased costs. This approach allowed Reezocar to focus on their core competencies and optimize their resources for maximum efficiency.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Second&lt;/strong&gt;, they leveraged GPUs to fine tune a convolutional neural network (CNN) such as Mask R-CNN or Detectron to do object segmentation and damage detection. This CNN model is used to segment car parts and detect car dents.&lt;/p&gt; 
&lt;p&gt;Reezocar adopted a gradual approach to tackle the dent detection problem. They aimed to ensure that their system could effectively differentiate between damaged and undamaged cars. Once they were satisfied with the performance of the initial model, they submitted images with low confidence for undamaged cars and images of damaged cars to Amazon SageMaker Ground Truth to obtain around 4000 more detailed and accurate annotations, which indicate which parts are damaged. This annotated dataset was then used to train an object detection model that could accurately identify the damaged parts of a car.&lt;/p&gt; 
&lt;p&gt;Reezocar uses AWS Batch to run inference and augment their data with damage information. Reezocar triggers AWS Batch conditionally by leveraging AWS Step Functions and an Amazon SQS queue. The Step Functions workflow showcased in Reference 1 checks for the arrival of a certain number of events related to the car damage data in the SQS queue. Once the required number of events is present, AWS Step Functions triggers AWS Batch to process the data using the proposed computer vision algorithm on P4d instances. AWS Batch dynamically provisions the optimal quantity and type of compute resources based on the volume and specific resource requirements of the batch jobs submitted.&lt;/p&gt; 
&lt;p&gt;P4d instances are a specific type of Amazon EC2 instance family that is optimized for high-performance computing and includes powerful NVIDIA GPUs. These instances provide the necessary computational power for running machine learning models and computer vision algorithms efficiently, making them suitable for Reezocar’s car damage detection system.&lt;/p&gt; 
&lt;p&gt;This combination of Step Functions, SQS, and AWS Batch allows Reezocar to efficiently process the data.&lt;/p&gt; 
&lt;h2&gt;Evaluating the model is in the eye of the beholder&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Finally&lt;/strong&gt;, to evaluate the performance of their dent detection models, Reezocar used a combination of accuracy and mean Average Precision (mAP) metrics. The accuracy metric was used to assess the classifier’s performance, while the mAP metric was used to evaluate the object detection model’s performance. They set a target accuracy of 0.85 and a target mAP of 0.9 to ensure high levels of accuracy and reliability in their results. These metrics allowed Reezocar to measure the success of their object detection model and assess the quality of the system’s output. The process is illustrated in Figure 2.&lt;/p&gt; 
&lt;div id="attachment_2657" style="width: 1015px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2657" loading="lazy" class="size-full wp-image-2657" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.32.56.png" alt="Figure 2: The damage detection system developed by Reezocar combines the generation of a synthetic dataset with real-world data labeling to ensure a diverse and robust dataset. The system is designed to detect relevant images that require labeling, which contributes to the overall efficiency and effectiveness of the damage detection system." width="1005" height="716"&gt;
 &lt;p id="caption-attachment-2657" class="wp-caption-text"&gt;Figure 2: The damage detection system developed by Reezocar combines the generation of a synthetic dataset with real-world data labeling to ensure a diverse and robust dataset. The system is designed to detect relevant images that require labeling, which contributes to the overall efficiency and effectiveness of the damage detection system.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Reezocar’s objective is to determine the refurbishing cost of cars, which involves an additional step to detect the severity of the damage in parts. The severity assessment process is illustrated in Figure 3, and the severity estimation is used to determine the repair cost of the damaged part. Table 1 shows that the repair cost estimation method involves applying a percentage of the initial price of the part, which varies according to the severity of the damage. In other words, different percentages are used based on the severity of the damage to estimate the repair cost accurately.&lt;/p&gt; 
&lt;div id="attachment_2658" style="width: 852px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2658" loading="lazy" class="size-full wp-image-2658" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.33.30.png" alt="Figure 3: Damage severity assessment pipeline. The severity assessment is based on the normal consistency of the damaged part." width="842" height="604"&gt;
 &lt;p id="caption-attachment-2658" class="wp-caption-text"&gt;Figure 3: Damage severity assessment pipeline. The severity assessment is based on the normal consistency of the damaged part.&lt;/p&gt;
&lt;/div&gt; 
&lt;table width="624"&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td width="156"&gt;&lt;strong&gt;Severity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="156"&gt;Scratched&lt;/td&gt; 
   &lt;td width="156"&gt;Damaged&lt;/td&gt; 
   &lt;td width="156"&gt;Wrecked&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="156"&gt;&lt;strong&gt;Repair cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="156"&gt;12%&lt;/td&gt; 
   &lt;td width="156"&gt;60%&lt;/td&gt; 
   &lt;td width="156"&gt;100%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;Table 1: Repair cost. Reezocar uses a percentage of the initial price of the damaged part to estimate the repair cost. The repair cost estimation process involves applying different percentages based on the severity of the damage.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Through the implementation of their damage detection pipeline, Reezocar has achieved a high degree of automation with a precision rate of 86.7%. However, due to challenging light conditions and reflective materials, some cars are being mislabeled as damaged. Reezocar has identified a mislabeling rate of 15.7%, and are currently labeling data in order to retrain the damage detection model.&lt;/p&gt; 
&lt;p&gt;The team was able to achieve a notable performance improvement by using Amazon Sagemaker Ground Truth to obtain 4000 labeled images, resulting in an increase in their mean Average Precision (MAP) from 0.22 to 0.3. This success has validated the effectiveness of their data-labeling pipeline and has motivated the company to pursue further data acquisition for labeling to enhance the performance of their system. As part of this effort, Reezocar is actively collecting images of damaged cars and labeling them to reach their goal of a MAP of 0.9.&lt;/p&gt; 
&lt;p&gt;In the figures below, we see some qualitative results of the proposed damage detection model. For example, in Figure 4, an image of a car with damaged parts is displayed, and the proposed model accurately detects and segments the damaged parts, as shown in the second row, first column. Additionally, they used a car-part segmentation model (See Row 1; Column 3 of Figure 4) to identify which specific part is damaged.&lt;/p&gt; 
&lt;div id="attachment_2659" style="width: 1009px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2659" loading="lazy" class="size-full wp-image-2659" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.34.17.png" alt="Figure 4: example of a wrecked car." width="999" height="458"&gt;
 &lt;p id="caption-attachment-2659" class="wp-caption-text"&gt;Figure 4: example of a wrecked car.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 5 shows an image of an undamaged car, while Figure 6 showcases the performance of the proposed method on scratched parts of a car, providing a closer look at the results.&lt;/p&gt; 
&lt;div id="attachment_2660" style="width: 1003px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2660" loading="lazy" class="size-full wp-image-2660" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.34.51.png" alt="Figure 5: Example of an undamaged car." width="993" height="392"&gt;
 &lt;p id="caption-attachment-2660" class="wp-caption-text"&gt;Figure 5: Example of an undamaged car.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2661" style="width: 998px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2661" loading="lazy" class="size-full wp-image-2661" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.35.19.png" alt="Figure 6: Example of a car with a scratched part." width="988" height="519"&gt;
 &lt;p id="caption-attachment-2661" class="wp-caption-text"&gt;Figure 6: Example of a car with a scratched part.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;The use of computer vision to detect car dents is quickly becoming a game-changer in the automotive industry. Machine learning and convolutional neural networks are allowing for a more accurate detection of car dents than ever before, leading to improved repair and maintenance processes.&lt;/p&gt; 
&lt;p&gt;Companies like Reezocar are leveraging computer vision-based damage estimation models to accurately and efficiently calculate car prices. Thanks to these models, Reezocar is now able to calculate the car price with a Mean Absolute Percentage (MAP) error of just 6%, improving the accuracy and efficiency of their pricing process.&lt;/p&gt; 
&lt;p&gt;This technology is not only helping to extend the life of cars but also to reduce landfill waste. It’s also helping to make the car buying process simpler and more transparent. As computer vision technology continues to improve, the way we detect and repair car dents will likely change as well.&lt;/p&gt; 
&lt;p&gt;By combining the power of Amazon EC2 instances, NVIDIA GPUs, AWS Batch linked by Elastic Fabric Adapter (EFA), and machine learning powered computer vision models, Reezocar can accurately detect car dents, ultimately extending the lifespan of selected cars by up to 5 years. By refurbishing these cars and reselling them in the secondary market, Reezocar helps reduce landfill waste and promote circularity.&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.41.23.png" alt="Laurent Fabre" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Laurent Fabre&lt;/h3&gt; 
  &lt;p&gt;Laurent Fabre is the CTO of Reezocar, a leading online platform for buying and selling used cars. Laurent brings over two decades of experience in the IT industry. He is a highly skilled Cybersecurity expert, having trained at Airbus, where he developed expertise in cryptography and secure programming techniques. He also studied mathematics, with a focus on operations research, which has enabled him to leverage data analytics and metrics to solve complex problems and achieve optimal solutions. Throughout his career, Laurent has taken on numerous mission-critical tasks for both civilian and military-grade projects, often on short notice. Outside of work, Laurent indulges his passion for interpretive dancing of quantum physics, which allows him to combine his love of science and art.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.41.26.png" alt="Atef Shaar" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Atef Shaar&lt;/h3&gt; 
  &lt;p&gt;Atef Shaar is currently a Lead Data at Reezocar. He is developing new machine-learning-based applications for the automobile industry. Additionally, he is managing a team of data engineers and data scientists. Prior to this work, Atef Shaar worked as a Research Engineer at Télécom Paris. He graduated with a Ph.D. degree from Télécom Paris in 2018. He was a visiting research student at the National University of Singapore in 2015. He completed a master’s study in International Business at Grenoble Graduate School of Business, after earning an engineering degree. His research work is related to machine learning and its application in multiple domains including marketing, distributed storage systems, and the automotive industry.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.46.26.png" alt="Julien Maksoud" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Julien Maksoud&lt;/h3&gt; 
  &lt;p&gt;Julien Maksoud is a highly skilled engineer with over a decade of experience in the Oil &amp;amp; Gas industry. As a consultant, he supported various companies in Europe and Africa in analyzing their data using machine learning-based methods, providing valuable insights to decision-makers. Julien’s passion for solving complex problems using machine learning led him to pursue an advanced Master’s degree from Télécom Paris, which equipped him with a deeper understanding of data science, including machine learning and deep learning frameworks. Currently, Julien is working as a Data Engineer at Reezocar, where he leverages his expertise to create data pipelines on AWS for ETL and data analysis. His experience in both data engineering and data science enables him to assist with model deployment and maintenance as well. Julien is committed to delivering results that enable organizations to make informed decisions based on data-driven insights.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.41.30.png" alt="Tarek " width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Tarek Ben Charrada&lt;/h3&gt; 
  &lt;p&gt;Tarek Ben Charrada graduated with distinction, ranking 28th out of over 2800 candidates in the highly competitive “concours d’entrée aux grandes écoles” to go and earn his engineering degree from Ecole polytechnique de Tunisie. He then pursued his PhD in 3D reconstruction from a single image, demonstrating his commitment to advancing the field of computer vision and computer graphics. Today, Tarek’s expertise is sought-after as a data scientist, where he leverages his skills in computer vision, speech processing and differentiable privacy to push the boundaries of what’s possible in these exciting fields.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
	</channel>
</rss>