<?xml version="1.0" encoding="UTF-8" standalone="no"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" version="2.0">

<channel>
	<title>AWS HPC Blog</title>
	<atom:link href="https://aws.amazon.com/blogs/hpc/feed/" rel="self" type="application/rss+xml"/>
	<link>https://aws.amazon.com/blogs/hpc/</link>
	<description>Just another Amazon Web Services site</description>
	<lastBuildDate>Wed, 20 Mar 2024 12:18:27 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>Choosing the right compute orchestration tool for your research workload</title>
		<link>https://aws.amazon.com/blogs/hpc/choosing-the-right-compute-orchestration-tool-for-your-research-workload/</link>
		
		<dc:creator><![CDATA[Patrick Guha]]></dc:creator>
		<pubDate>Tue, 19 Mar 2024 13:52:27 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">dacdd14c4bfac6c9245765550bc892676dd27683</guid>

					<description>Running big research jobs on AWS but not sure where to start? We break down options like Batch, ECS, EKS, and others to pick the right tool for your needs. Lots of examples for genomics, ML, engineering, and more!</description>
										<content:encoded>&lt;p&gt;&lt;img class="alignright wp-image-3375 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/boofla88_a_scientist_choosing_between_several_options_manga_sty_6b82dff1-510f-404c-97aa-360c46741720.png" alt="Choosing the right compute orchestration tool for your research workload" width="380" height="212"&gt;&lt;/p&gt; 
&lt;p&gt;Research organizations around the world run large-scale simulations, analyses, models, and other distributed, compute-intensive workloads on AWS every day. These jobs depend on an orchestration layer to coordinate tasks across the compute fleet.&lt;/p&gt; 
&lt;p&gt;As a researcher or systems administrator providing services for researchers, it can be difficult to choose which AWS service or solution to use because there are various options for different kinds of workloads.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll describe some typical research use cases and explain which AWS tool we think best fits that workload.&lt;/p&gt; 
&lt;h2&gt;Understanding your workload&lt;/h2&gt; 
&lt;p&gt;Before diving into the specifics of each tool, it’s important to understand the nature of your workload.&lt;/p&gt; 
&lt;p&gt;Factors like the requirement for tightly coupled processes, the use of containers, the need for machine learning capabilities, or the necessity for a cloud desktop are pivotal in your decision-making process.&lt;/p&gt; 
&lt;p&gt;Research is not a monolith, so AWS supports a diverse range of HPC-based research, from engineering simulations and drug discovery to genomics, machine learning (ML), financial risk analysis, and social sciences.&lt;/p&gt; 
&lt;p&gt;Also: the tool you choose is not exclusive. Customers can have a mix of solutions to meet their needs all in the same account.&lt;/p&gt; 
&lt;h2&gt;A deep dive into AWS compute orchestration tools&lt;/h2&gt; 
&lt;h3&gt;AWS ParallelCluster for classic HPC clusters&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; is a flexible tool for building and managing HPC clusters on AWS. It’s ideal for tightly coupled workloads, like running simulations or analytics that require a traditional HPC cluster. It supports &lt;a href="https://aws.amazon.com/hpc/efa/"&gt;Elastic Fabric Adapter&lt;/a&gt; (EFA) networking out-of-the-box for low latency and high throughput inter-instance communication, and a high-performance file system (Lustre – available through the &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt; managed service).&lt;/p&gt; 
&lt;p&gt;ParallelCluster provides a familiar interface with a job scheduler &amp;nbsp;– Slurm – making it easy to migrate or burst workloads from an on-premises cluster environment you’re possibly already using.&lt;/p&gt; 
&lt;div id="attachment_3366" style="width: 640px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3366" loading="lazy" class="size-full wp-image-3366" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.13.20.png" alt="Figure 1 – Overview of AWS ParallelCluster's architecture and its components for HPC workload. Integration with Amazon EC2 Auto Scaling right sizes compute nodes based on the job queue. Amazon FSx for Lustre integration allows for access to a high-performance file system while also taking advantage of Amazon S3 object storage." width="630" height="336"&gt;
 &lt;p id="caption-attachment-3366" class="wp-caption-text"&gt;Figure 1 – Overview of AWS ParallelCluster’s architecture and its components for HPC workload. Integration with &lt;a href="https://aws.amazon.com/ec2/autoscaling/"&gt;Amazon EC2 Auto Scaling&lt;/a&gt; right sizes compute nodes based on the job queue. &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html"&gt;Amazon FSx for Lustre&lt;/a&gt; integration allows for access to a high-performance file system while also taking advantage of &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon S3&lt;/a&gt; object storage.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;AWS Batch for container-based jobs&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; is suited for highly parallel, container-based jobs, including tightly-coupled workloads. It provides a fully-managed scheduler with seamless integration into container orchestrators, like &lt;a href="https://aws.amazon.com/eks/"&gt;Amazon Elastic Kubernetes Service&lt;/a&gt; (EKS) and &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon Elastic Container Service&lt;/a&gt; (ECS), allowing researchers to leverage existing containerized applications. A typical workload might involve independently running jobs on generic/non-specific compute, leveraging native AWS integrations, or requiring horizontal scalability through MPI or NCCL.&lt;/p&gt; 
&lt;div id="attachment_3367" style="width: 733px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3367" loading="lazy" class="size-full wp-image-3367" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.13.58.png" alt="Figure 2 – AWS Batch workflow illustrating container-based job processing and integration with AWS services. Compatibility with Amazon EKS and ECS allows for flexibility at the Compute Environment layer. " width="723" height="361"&gt;
 &lt;p id="caption-attachment-3367" class="wp-caption-text"&gt;Figure 2 – AWS Batch workflow illustrating container-based job processing and integration with AWS services. Compatibility with Amazon EKS and ECS allows for flexibility at the Compute Environment layer.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Amazon SageMaker for machine learning projects&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/sagemaker/"&gt;Amazon SageMaker&lt;/a&gt; is ideal for machine learning workloads, especially those developed in Jupyter Notebooks. While not focused on foundational building blocks for research computing, it instead provides a managed ecosystem of ML and data science tools, covering the entire spectrum from data discovery and exploration to model training and deployment.&lt;/p&gt; 
&lt;p&gt;SageMaker notebooks provide an interactive development environment, allowing researchers to develop and test models easily. SageMaker also contains pre-trained models, allowing researchers to jump-start their ML projects. It also provides managed inference endpoints, making it easier to deploy models and serve predictions.&lt;/p&gt; 
&lt;div id="attachment_3368" style="width: 587px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3368" loading="lazy" class="wp-image-3368 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.14.53.png" alt="Figure 3 – Amazon SageMaker ecosystem showcasing high-level end-to-end process from data preparation to model deployment. Integrates with services like Amazon EFS for a local file system in notebooks and also with highly-optimized AWS Deep Learning Containers for training models." width="577" height="309"&gt;
 &lt;p id="caption-attachment-3368" class="wp-caption-text"&gt;Figure 3 – &lt;a href="https://aws.amazon.com/pm/sagemaker"&gt;Amazon SageMaker&lt;/a&gt; ecosystem showcasing high-level end-to-end process from data preparation to model deployment. Integrates with services like &lt;a href="https://aws.amazon.com/efs/"&gt;Amazon EFS&lt;/a&gt; for a local file system in notebooks and also with highly-optimized AWS Deep Learning Containers for training models.&lt;/p&gt;
&lt;/div&gt; 
&lt;h4&gt;Let’s talk about the underlying compute resources&lt;/h4&gt; 
&lt;p&gt;The three services we just mentioned can take advantage of Amazon Elastic Compute Cloud (Amazon EC2) &lt;a href="https://aws.amazon.com/ec2/spot/"&gt;Spot Instances&lt;/a&gt; and also &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt;. Spot Instances are spare EC2 capacity, offered at a discounted rate but they can be reclaimed with a 2-minute warning.&lt;/p&gt; 
&lt;p&gt;SageMaker, Batch, and ParallelCluster can all use Spot Instances to take advantage of their favorable economics. In the case of Spot Instances, you’ll need to verify that your workload can tolerate interruptions from reclaimed capacity, or that the service can shift load on your behalf to avoid interrupting your processes. There are AWS technology partners, like &lt;a href="https://aws.amazon.com/marketplace/seller-profile?id=3b7c724c-fae7-4187-ae45-de1625e51395"&gt;MemVerge&lt;/a&gt;, that can handle this for you using &lt;a href="https://aws.amazon.com/blogs/hpc/save-up-to-90-using-ec2-spot-even-for-long-running-hpc-jobs/"&gt;OS-level memory checkpointing&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Fargate is a serverless compute engine that can be used for workloads running on Batch with ECS, and in native ECS and EKS clusters. It abstracts away the need for additional servers or infrastructure-related parameters (like instance type) to run containers. Fargate also has a few caveats regarding hardware specifications which you can find &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/fargate.html#when-to-use-fargate"&gt;in our documentation&lt;/a&gt;. But – generally speaking – it’s worthwhile to see if you can use Spot or Fargate with AWS orchestration tools for your research.&lt;/p&gt; 
&lt;h3&gt;Amazon Lightsail for Research for individual cloud desktops&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/lightsail/research/"&gt;Amazon Lightsail for Research&lt;/a&gt; is a simple (but powerful) solution for researchers looking for a predictably-priced, all-in-one cloud desktop. It’s tailored specifically for researchers, providing hardware specifications that are optimized for efficient research and a seamless user experience. Lightsail offers a range of pre-configured virtual private servers that can be customized to meet researchers’ needs and comes with research applications, like Scilab and RStudio. With its easy-to-use interface and affordable pricing, Lightsail for Research provides a reliable and efficient way for researchers to get started with AWS.&lt;/p&gt; 
&lt;div id="attachment_3369" style="width: 662px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3369" loading="lazy" class="size-full wp-image-3369" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.15.11.png" alt="Figure 4 – Researchers can use Amazon Lightsail for Research’s simplified management interface and options to deploy their favorite applications like Jupyter, RStudio, and Scilab. " width="652" height="319"&gt;
 &lt;p id="caption-attachment-3369" class="wp-caption-text"&gt;Figure 4 – Researchers can use Amazon Lightsail for Research’s simplified management interface and options to deploy their favorite applications like Jupyter, RStudio, and Scilab.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Research and Engineering Studio on AWS for managing cloud desktops at scale&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/hpc/res/"&gt;Research and Engineering Studio on AWS&lt;/a&gt; (RES) is an open-source web-based portal for administrators to create and manage secure, cloud-based research and engineering environments. It is ideal for research organizations that want a central IT team to easily manage the underlying infrastructure for multiple research environments. It provides one-click deployment for getting started quickly but can be customized to meet an organization’s specific needs.&lt;/p&gt; 
&lt;p&gt;Administrators can create virtual collaboration spaces for specific sets of users to access shared resources and collaborate. Users get a single pane of glass for launching and accessing virtual desktops to conduct scientific research, product design, engineering simulations, or data analysis workloads.&lt;/p&gt; 
&lt;div id="attachment_3371" style="width: 707px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3371" loading="lazy" class="size-full wp-image-3371" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.15.57.png" alt="Figure 5 – Researchers and admins alike can leverage RES to create Engineering Virtual Desktops (eVDI) backed by Amazon EC2. The RES Virtual Desktop screen shown here lists all the eVDI sessions a user created with controls to spin up, shut down, or schedule uptime. " width="697" height="404"&gt;
 &lt;p id="caption-attachment-3371" class="wp-caption-text"&gt;Figure 5 – Researchers and admins alike can leverage RES to create Engineering Virtual Desktops (eVDI) backed by Amazon EC2. The RES Virtual Desktop screen shown here lists all the eVDI sessions a user created with controls to spin up, shut down, or schedule uptime.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;AWS HealthOmics for bioinformatics&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/healthomics/"&gt;AWS HealthOmics&lt;/a&gt; is a comprehensive solution for bioinformatics work. It facilitates raw genomic storage and processing, allowing researchers to store and analyze genomic data. It supports popular bioinformatics workflow definition languages like WDL and Nextflow, enabling researchers to process and analyze genomic data efficiently.&lt;/p&gt; 
&lt;p&gt;HealthOmics even offers researchers the choice to bring their own workflow or use pre-built &lt;a href="https://docs.aws.amazon.com/omics/latest/dev/service-workflows.html"&gt;Ready2Run workflows&lt;/a&gt;. Ready2Run workflows are designed by industry leading third-party software companies like Sentieon, Inc. and NVIDIA, and includes common open-source pipelines like AlphaFold for protein structure prediction. Ready2Run workflows don’t require you to manage software tools or workflow scripts – this can save researchers significant amounts of time.&lt;/p&gt; 
&lt;div id="attachment_3372" style="width: 1017px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3372" loading="lazy" class="size-full wp-image-3372" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.17.22.png" alt="Figure 6 – AWS HealthOmics platform structure highlighting genomic data processing and analysis capabilities. Raw sequence and reference data can be processed through Nextflow or WDL workflows and then analyzed via AWS analytics services such as Amazon Athena." width="1007" height="286"&gt;
 &lt;p id="caption-attachment-3372" class="wp-caption-text"&gt;Figure 6 – AWS HealthOmics platform structure highlighting genomic data processing and analysis capabilities. Raw sequence and reference data can be processed through Nextflow or WDL workflows and then analyzed via AWS analytics services such as &lt;a href="https://aws.amazon.com/athena/"&gt;Amazon Athena&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Leveraging next-generation serverless technologies&lt;/h3&gt; 
&lt;p&gt;In the past decade, AWS has pioneered the field of serverless computing. Serverless computing is a model where you can build and deploy applications without managing server infrastructure. Instead of spinning up a full virtual machine that comes with overhead like patching and monitoring, you can abstract it away and focus just on the code or process you intend to run.&lt;/p&gt; 
&lt;p&gt;This is great for use cases like event handling or asynchronous tasks, but researchers have been using serverless computing to speed up embarrassingly parallel workloads, too – including ML hyperparameter optimization, genome search, and even MapReduce.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; is a serverless interface for running code without managing servers. It’s designed for loosely coupled workloads, allowing researchers to run code in response to events like changes in data (or the arrival of data). Lambda scales automatically, enabling you to run thousands of concurrent executions.&lt;/p&gt; 
&lt;p&gt;Even better: Lambda integrates with more than 200 other AWS services, making it easier for you to build quite complex workflows. It provides integration with &lt;a href="https://aws.amazon.com/step-functions/"&gt;AWS Step Functions&lt;/a&gt;, too, which means you can create visual workflows and construct multi-step, distributed applications. Lambda, combined with Step Functions, is useful for workloads that involve different compute steps, data needs, and may even require decision gates or human input.&lt;/p&gt; 
&lt;div id="attachment_3373" style="width: 860px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3373" loading="lazy" class="size-full wp-image-3373" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/27/CleanShot-2024-02-27-at-14.18.06.png" alt="Figure 7 – Illustration of a sample serverless computing architecture: a data stream of jobs to be processed is picked up by AWS Lambda and put into a downstream Amazon SQS queue. AWS Step Functions then reads from this queue and handles the heavy lifting of distributed compute orchestration via Lambda worker functions. Step Functions also leverages Amazon DynamoDB for workload state management, event handling with Amazon EventBridge and storing processed results in Amazon S3. " width="850" height="263"&gt;
 &lt;p id="caption-attachment-3373" class="wp-caption-text"&gt;Figure 7 – Illustration of a sample serverless computing architecture: a data stream of jobs to be processed is picked up by AWS Lambda and put into a downstream &lt;a href="https://aws.amazon.com/sqs/"&gt;Amazon SQS&lt;/a&gt; queue. AWS Step Functions then reads from this queue and handles the heavy lifting of distributed compute orchestration via Lambda worker functions. Step Functions also leverages &lt;a href="https://aws.amazon.com/dynamodb/"&gt;Amazon DynamoDB&lt;/a&gt; for workload state management, event handling with &lt;a href="https://aws.amazon.com/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt; and storing processed results in Amazon S3.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion: making the right choice&lt;/h2&gt; 
&lt;p&gt;Choosing the right AWS compute orchestration tool is not about finding a one-size-fits-all solution but about aligning the tool’s capabilities with the specific requirements of your workload. The nuances of your project, the nature of your data, and your computational needs &lt;em&gt;should&lt;/em&gt; guide your decision.&lt;/p&gt; 
&lt;p&gt;Start with a small workload to gauge the tool’s compatibility and scalability with your project. AWS has a comprehensive suite of services and is ready to support you at every step of your journey, ensuring that you have the right resources and environment to push the boundaries of your research.&lt;/p&gt; 
&lt;p&gt;AWS Partners are also here to help you implement these tools. Partners like &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-7shbi5vvdvezu"&gt;Ronin&lt;/a&gt; and &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-ppiyshk2oin3i?sr=0-1&amp;amp;ref_=beagle&amp;amp;applicationId=AWSMPContessa"&gt;Ansys&lt;/a&gt; bring valuable expertise that can accelerate your time to research on AWS.&lt;/p&gt; 
&lt;p&gt;We recommend consulting with your research team and AWS account team to help you make the best decision for your project. As your research evolves, AWS’s scalable and diverse computing environment will continue to provide the necessary tools and support to meet your computational needs. To dive deeper into the nuances of a few of the tools we touched on in this post, we encourage you to check out some of our previous posts about &lt;a href="https://aws.amazon.com/blogs/hpc/choosing-between-batch-or-parallelcluster-for-hpc/"&gt;Choosing between AWS Batch or AWS ParallelCluster for HPC&lt;/a&gt;, &lt;a href="https://aws.amazon.com/blogs/hpc/why-use-fargate-with-aws-batch-for-serverless-batch-compute/"&gt;why you should use Fargate with AWS Batch&lt;/a&gt;, and how you can &lt;a href="https://aws.amazon.com/blogs/hpc/save-up-to-90-using-ec2-spot-even-for-long-running-hpc-jobs/"&gt;save up to 90% using EC2 Spot&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Protein language model training with NVIDIA BioNeMo framework on AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/protein-language-model-training-with-nvidia-bionemo-framework-on-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Marissa Powers]]></dc:creator>
		<pubDate>Mon, 18 Mar 2024 15:59:38 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AI]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Protein Folding]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">61fad9f084c647737166e7ff7d7a263d356d7d5e</guid>

					<description>In this new post, we discuss pre-training ESM-1nv for protein language modeling with NVIDIA BioNeMo on AWS. Learn how you can efficiently deploy and customize generative models like ESM-1nv on GPU clusters with ParallelCluster. Whether you're studying protein sequences, predicting properties, or discovering new therapeutics, this post has tips to accelerate your protein AI workloads on the cloud.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Marissa Powers and Ankur Srivastava from AWS, and Neel Patel from NVIDIA.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Proteins are large, complex biomolecules. They make up muscles, form into enzymes and antibodies, and perform signaling throughout the body. Today, &lt;a href="https://www.proteinatlas.org/humanproteome/tissue/druggable"&gt;proteins are the therapeutic target&lt;/a&gt; for the majority of pharmaceutical drugs. Increasingly, scientists are using language models to better understand protein function, generate new protein sequences, and predict protein properties [1].&lt;/p&gt; 
&lt;p&gt;With the recent proliferation of new models and tools in this field, researchers are looking for help to simplify the training, customization, and deployment of these generative AI models. And our high performance computing (HPC) customers are asking for how to easily perform distributed training with these models on AWS.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll demonstrate how to pre-train the &lt;code&gt;ESM-1nv&lt;/code&gt; model with the &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/containers/bionemo-framework"&gt;NVIDIA BioNeMo framework&lt;/a&gt; using NVIDIA GPUs on AWS ParallelCluster, an open-source cluster management tool that makes it easy for you to deploy and manage HPC clusters on AWS. &lt;a href="https://www.nvidia.com/en-us/clara/bionemo/"&gt;NVIDIA BioNeMo&lt;/a&gt; is a generative AI platform for drug discovery. It supports running commonly used models, including ESM-1, ESM-2, ProtT5nv, DNABert, MegaMolBART, DiffDock, EquiDock, and OpenFold. For the latest information on supported models, see the &lt;a href="https://docs.nvidia.com/bionemo-framework/latest/"&gt;BioNeMo framework documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_3420" style="width: 911px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3420" loading="lazy" class="size-full wp-image-3420" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/18/CleanShot-2024-03-18-at-14.35.22.png" alt="Figure 1: This image shows the workflow for developing models on NVIDIA BioNeMo. The process is divided into phases for model development and customization and then fine-tuning and deployment." width="901" height="392"&gt;
 &lt;p id="caption-attachment-3420" class="wp-caption-text"&gt;Figure 1: This image shows the workflow for developing models on NVIDIA BioNeMo. The process is divided into phases for model development and customization and then fine-tuning and deployment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;This example deployment also leverages Amazon FSx for Lustre and the Elastic Fabric Adapter (EFA), which can both be provisioned and configured by ParallelCluster. With ParallelCluster, users can scale out distributed training jobs across hundreds or thousands of vCPUs. Code examples, including cluster configuration files, are &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/14.bionemo"&gt;available on GitHub&lt;/a&gt;&lt;strong&gt;. &lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Walkthrough&lt;/h2&gt; 
&lt;p&gt;For this example, we will (1) create an HPC cluster using AWS ParallelCluster, (2) configure the cluster with BioNeMo framework and download datasets, and (3) execute a pre-training job on the cluster.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;We assume you have access to an AWS account and are authenticated in that account. The configuration file used here uses &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;p4de.24xlarge&lt;/a&gt; instance types powered by 8 x &lt;a href="https://www.nvidia.com/en-us/data-center/a100/"&gt;NVIDIA A100&lt;/a&gt; 80GB Tensor Core GPUs and was tested in the us-east-1 region. We have also tested it with &lt;a href="https://aws.amazon.com/ec2/instance-types/p5/"&gt;p5.48xlarge&lt;/a&gt; instances powered by 8 x &lt;a href="https://www.nvidia.com/en-us/data-center/h100/"&gt;NVIDIA H100&lt;/a&gt; Tensor Core GPUs, which delivered better performance. Check your AWS service quotas to ensure you have sufficient access to these instance types.&lt;/p&gt; 
&lt;p&gt;ParallelCluster must be installed on a local node or instance. &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-virtual-environment.html"&gt;Follow the instructions&lt;/a&gt; in our documentation to install ParallelCluster in a virtual environment.&lt;/p&gt; 
&lt;p&gt;Finally, to pull down the BioNeMo container, you will need an NVIDIA NGC API key. To set this up, follow the guidance under “NGC Setup” in the &lt;a href="https://docs.nvidia.com/bionemo-framework/latest/quickstart-fw.html#ngc-setup"&gt;&lt;strong&gt;BioNeMo documentation&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Create a cluster&lt;/h2&gt; 
&lt;p&gt;The AWS &lt;code&gt;awsome-distributed-training&lt;/code&gt; &lt;a href="https://github.com/aws-samples/awsome-distributed-training"&gt;GitHub repo&lt;/a&gt; provides configuration templates for running distributed training on multiple AWS services, including Amazon EKS, AWS Batch, and AWS ParallelCluster. In this example, we’ll create a cluster using one of the provided ParallelCluster configuration files. You can see an overview of the architecture in Figure 2.&lt;/p&gt; 
&lt;div id="attachment_3421" style="width: 841px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3421" loading="lazy" class="size-full wp-image-3421" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/03/18/CleanShot-2024-03-18-at-14.36.56.png" alt="Figure 2 – In this distributed training architecture, ParallelCluster deploys a cluster consisting of a head node in a public subnet, a single queue of p4de.24xlarge instances in a private subnet, an FSx for Lustre filesystem, and a shared Amazon Elastic Block Storage (EBS) volume. FSx is used for input and output datasets, and the EBS volume is used to store job-specific submission scripts and log files. All the resources are deployed within a user’s own VPC." width="831" height="570"&gt;
 &lt;p id="caption-attachment-3421" class="wp-caption-text"&gt;Figure 2 – In this distributed training architecture, ParallelCluster deploys a cluster consisting of a head node in a public subnet, a single queue of p4de.24xlarge instances in a private subnet, an FSx for Lustre filesystem, and a shared Amazon Elastic Block Storage (EBS) volume. FSx is used for input and output datasets, and the EBS volume is used to store job-specific submission scripts and log files. All the resources are deployed within a user’s own VPC.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To start, clone the awsome-distributed-training GitHub repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;git clone https://github.com/aws-samples/awsome-distributed-training.git&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Navigate to the &lt;em&gt;templates&lt;/em&gt; directory in the locally cloned repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd 1.architectures/2.aws-parallelcluster&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The configuration file we will use is distributed-training-p4de_postinstall_scripts.yaml. Open the file and update to use an ubuntu-based AMI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Image:
  Os: ubuntu2004
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ParallelCluster will deploy the head node and compute nodes in subnets you specify in the YAML file. You can choose to deploy all resources in the same subnet or use separate subnets for the head node and compute nodes. We recommend that you deploy all resources in the same Availability Zone (AZ). That’s because ParallelCluster will create a Lustre file system for the cluster – which resides in a single AZ, and deploying workloads across multiple Availability Zones will cause additional cost incursions due to the cross-AZ traffic.&lt;/p&gt; 
&lt;p&gt;Update the head node subnet in L13 in the YAML file. Update the compute nodes subnet in L50:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;HeadNode:
  InstanceType: m5.8xlarge
  Networking:
    SubnetId: PLACEHOLDER_PUBLIC_SUBNET
  Ssh:
    KeyName: PLACEHOLDER_SSH_KEY
...
  SlurmQueues:
    - Name: compute-gpu
      CapacityType: ONDEMAND
      Networking:
        SubnetIds:
          - PLACEHOLDER_PRIVATE_SUBNET
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using an On-Demand Capacity Reservation (ODCR), provide the resource id in the YAML file. If not, comment out these two lines:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;      CapacityReservationTarget:
        CapacityReservationId: PLACEHOLDER_CAPACITY_RESERVATION_ID
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the YAML file has been updated, use ParallelCluster to create the cluster:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster \
   --cluster-name bionemo-cluster \
   --cluster-configuration distributed-training-p4de_postinstall_scripts.yaml \
   --region us-east-1 \
   --dryrun true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Cluster creation will take 20-30 minutes. You can monitor the status of cluster creation with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;watch pcluster describe-cluster \
   --cluster-name bionemo-cluster \
   --region us-east-1 \
   --query clusterStatus
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The cluster will be deployed as an AWS CloudFormation stack. You can also monitor resource creation from the CloudFormation console or by querying CloudFormation in the CLI.&lt;/p&gt; 
&lt;h3&gt;Configure cluster and input datasets&lt;/h3&gt; 
&lt;p&gt;Once the cluster status is complete, follow the guidance in the &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/14.bionemo"&gt;GitHub repo&lt;/a&gt; to finish setup and model pre-training. At a high level, these steps walk through how to (1) pull down the BioNeMo framework container; (2) build an AWS-optimized image; (3) download and pre-process the &lt;code&gt;UniRef50&lt;/code&gt; dataset; and (4) run the &lt;code&gt;ESM-1nv&lt;/code&gt; pre-training job.&lt;/p&gt; 
&lt;p&gt;Once you submit the pre-training job, you can monitor progress by running tail on the output log file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;tail -f /apps/slurm-esm1nv-&amp;lt;job-id&amp;gt;.out&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more detailed monitoring of the cluster, including memory, networking, and storage usage on both the head node and compute nodes, consider creating a Grafana dashboard. A detailed guide for creating a dashboard for ParallelCluster clusters is &lt;a href="https://github.com/aws-samples/aws-parallelcluster-monitoring"&gt;available on Github&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we demonstrated how to pre-train ESM-1nv with the NVIDIA BioNeMo framework and NVIDIA GPUs on AWS ParallelCluster. For information about other models supported by the NVIDIA BioNeMo framework, see the BioNeMo framework documentation. For guides on deploying other distributed training jobs on AWS, check out the additional test case in the awsome-distributed-training repository in GitHub.&lt;/p&gt; 
&lt;p&gt;For alternative options for deploying BioNeMo framework on AWS, check out our guide for &lt;a href="https://aws.amazon.com/blogs/industries/find-the-next-blockbuster-with-nvidia-bionemo-framework-on-amazon-sagemaker/"&gt;Amazon SageMaker&lt;/a&gt;.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Reference&lt;/h3&gt; 
&lt;p&gt;[1] Ruffolo, J.A., Madani, A. Designing proteins with language models. Nat Biotechnol 42, 200–202 (2024). &lt;a href="https://doi.org/10.1038/s41587-024-02123-4"&gt;https://doi.org/10.1038/s41587-024-02123-4&lt;/a&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Introducing new alerts to help users detect and react to blocked job queues in AWS Batch</title>
		<link>https://aws.amazon.com/blogs/hpc/introducing-new-alerts-to-help-users-detect-and-react-to-blocked-job-queues-in-aws-batch/</link>
		
		<dc:creator><![CDATA[Naina Thangaraj]]></dc:creator>
		<pubDate>Thu, 14 Mar 2024 18:29:42 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">4dfd9662b13958ccd5756233dff2259adb1eb906</guid>

					<description>Heads up AWS Batch users! Learn how to get notifications when your job queue gets blocked so you can quickly troubleshoot and keep your workflows moving. Details in our blog.</description>
										<content:encoded>&lt;p&gt;As many readers will know, AWS Batch provides functionality that enables you to run batch workloads on the managed container orchestration services in AWS: Amazon ECS and Amazon EKS. One of the core concepts of Batch is that it provides a job queue you can submit your work to. Batch is designed to transition your jobs from &lt;code&gt;SUBMITTED&lt;/code&gt; to &lt;code&gt;RUNNABLE&lt;/code&gt; states if they pass preliminary checks, and from &lt;code&gt;RUNNING&lt;/code&gt; to either &lt;code&gt;FAILED&lt;/code&gt; or &lt;code&gt;SUCCEEDED&lt;/code&gt; after the job is placed on a compute resource and completes. Batch is also sends an event to Amazon CloudWatch Events for each corresponding job state update.&lt;/p&gt; 
&lt;p&gt;Sometimes, though, a &lt;code&gt;RUNNABLE&lt;/code&gt; job at the head of the queue can block all other jobs behind it from running. This could be caused by a misconfiguration in your AWS account, or it might be because your account doesn’t have access to the specific instances required for the job (like a GPU, for example). We’ve&amp;nbsp;&lt;a href="https://repost.aws/knowledge-center/batch-job-stuck-runnable-status"&gt;documented several common causes&lt;/a&gt;&amp;nbsp;– and their resolutions – but to fix the issue you&amp;nbsp;&lt;em&gt;first need to know that the blocked job queue condition exists&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Today we’re introducing&amp;nbsp;CloudWatch Events notifications for blocked job queues. We’ve designed this new feature to provide you with an event any time Batch detects that a job queue is blocked by a &lt;code&gt;RUNNABLE&lt;/code&gt; job at the head of the queue. Even better, the feature is designed to show the reason the job is stuck in&amp;nbsp;&lt;em&gt;both&lt;/em&gt;&amp;nbsp;the CloudWatch event and the&amp;nbsp;&lt;code&gt;statusReason&lt;/code&gt;&amp;nbsp;of the job&amp;nbsp;returned from &lt;code&gt;DescribeJobs&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;ListJobs&lt;/code&gt;&amp;nbsp;API calls.&lt;/p&gt; 
&lt;h2&gt;Common causes of blocked job queues&lt;/h2&gt; 
&lt;p&gt;There are many reasons why a job at the head of the queue can block other jobs behind it from running.&lt;/p&gt; 
&lt;p&gt;IAM roles, network, and security settings are often the culprits, but there are several more reasons for a blocked job queue&amp;nbsp;&lt;em&gt;that Batch can detect&lt;/em&gt;&amp;nbsp;in your environment:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;your queue’s attached compute environments (CEs) have all received insufficient capacity errors&lt;/li&gt; 
 &lt;li&gt;your CEs have a&amp;nbsp;&lt;code&gt;maxVcpu&lt;/code&gt;&amp;nbsp;that’s too small for the job requirements&lt;/li&gt; 
 &lt;li&gt;your CEs lack any instances that meet the job requirements&lt;/li&gt; 
 &lt;li&gt;your service role has a permission issue&lt;/li&gt; 
 &lt;li&gt;all your CEs are in an &lt;code&gt;INVALID&lt;/code&gt; state (which usually means networking or IAM roles are preventing instances joining the compute fleet)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;With the new CloudWatch Events notifications we’re announcing today, Batch can now send a notification when it detects a common reason for jobs being stuck in &lt;code&gt;RUNNABLE&lt;/code&gt;, but&amp;nbsp;&lt;em&gt;not&lt;/em&gt; for every &lt;code&gt;RUNNABLE&lt;/code&gt; job simply waiting for its turn. Finally, sometimes Batch will detect a blocked job queue but can’t determine the specific reason. In this case, Batch can still send a notification of a blocked job queue, but you’ll need to do a bit of detective work to figure out the root cause.&lt;/p&gt; 
&lt;h2&gt;Taking action&lt;/h2&gt; 
&lt;p&gt;There are two ways you can programmatically act on the blocked job queue events that Batch sends to CloudWatch Events.&lt;/p&gt; 
&lt;h3&gt;Acting on events with Amazon EventBridge&lt;/h3&gt; 
&lt;p&gt;The first way you can automate an action (based on a matching event pattern) is to define Amazon EventBridge &lt;a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rules.html"&gt;rules&lt;/a&gt; with different&amp;nbsp;EventBridge &lt;a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html"&gt;targets&lt;/a&gt;&amp;nbsp;for each event type.&lt;/p&gt; 
&lt;p&gt;For example, when you receive a message about a job that requires more memory than any instance can provide, you could use an AWS Lambda function to terminate the job request—to unblock the job queue—and then send the event information into an Amazon SQS queue so you can inspect the job details later.&lt;/p&gt; 
&lt;p&gt;When you review those SQS messages, you can decide if you want to adjust the CE to meet the needs of these types of jobs,&amp;nbsp;&lt;em&gt;or&lt;/em&gt;&amp;nbsp;create a new Batch environment to handle more resource-intensive jobs.&lt;/p&gt; 
&lt;p&gt;The Batch user guide has examples showing how to&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/batch_cwet.html"&gt;listen for&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/batch_sns_tutorial.html"&gt;react to&lt;/a&gt;&amp;nbsp;the CloudWatch events using EventBridge.&lt;/p&gt; 
&lt;h3&gt;Auto terminating stuck jobs&lt;/h3&gt; 
&lt;p&gt;The second way you can act to address blocked job queues is at the level of the job queue itself.&lt;/p&gt; 
&lt;p&gt;Batch now has a&amp;nbsp;&lt;code&gt;jobStateTimeLimitActions&lt;/code&gt;&amp;nbsp;parameter for job queues that lets you automatically cancel a stuck job after a defined period of time,&amp;nbsp;&lt;code&gt;maxTimeSeconds&lt;/code&gt;. If you opt to define this parameter, Batch is designed to start the timeout clock ticking when it detects that a job is blocking the queue. Batch will also update the&amp;nbsp;&lt;code&gt;statusReason&lt;/code&gt;&amp;nbsp;field at this time. Once the stuck job at the head of the queue is cancelled, a “&lt;em&gt;Batch Job State Change”&lt;/em&gt;&amp;nbsp;CloudWatch event is emitted with the underlying reason. If Batch detects that another job at the head of the queue is blocking the queue, a &lt;em&gt;“Batch Job Queue Blocked”&lt;/em&gt; CloudWatch event is emitted, and starts a new &lt;code&gt;maxTimeSeconds&lt;/code&gt; timer to take the action you defined once the limit is reached.&lt;/p&gt; 
&lt;p&gt;Here’s an example showing how to specify that the job queue will wait for a job that is blocked by compute environments having reached maximum capacity for 4 hours (maxTimeSeconds=14400) before cancelling the job.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;"jobStateTimeLimitActions": [
      {                              
         "reason" :  "MISCONFIGURATION:COMPUTE_ENVIRONMENT_MAX_RESOURCE", 
         "state": "RUNNABLE",            
         "maxTimeSeconds" : 14400,
         "action" : "CANCEL"            
      }
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Table 1 lists the scenarios we discussed, the output message in CloudWatch Events, and the&amp;nbsp;&lt;code&gt;jobStateTimeLimitAction.reason&lt;/code&gt;&amp;nbsp;that you can specify to cancel the stuck job in the job queue. The table also lists the “&lt;em&gt;Batch Job State Change”&lt;/em&gt;&amp;nbsp;CloudWatch event message if the job was automatically canceled.&lt;/p&gt; 
&lt;table class="alignleft" style="border-color: #000000" border="0"&gt; 
 &lt;caption&gt;
  &lt;em&gt;Table 1: AWS Batch CloudWatch Events messages for blocked job queue events. The &lt;strong&gt;Scenerio&lt;/strong&gt; column describes the context Batch determined that a queue was blocked. &lt;strong&gt;Status Parameter&lt;/strong&gt; refers to key fields in event messages that are provided for an event. &lt;strong&gt;Status Parameter Value&lt;/strong&gt; lists an example value for the corresponding parameter. Note that the CAPACITY:INSUFFICIENT_INSTANCE_CAPACITY message will provide a specific instance type name, not just the one in the example.&amp;nbsp;&lt;/em&gt;
 &lt;/caption&gt; 
 &lt;thead&gt; 
  &lt;tr style="color: white;font-weight: bold;background-color: #000000"&gt; 
   &lt;td width="95"&gt;Scenario&lt;/td&gt; 
   &lt;td width="248"&gt;Status Parameter&lt;/td&gt; 
   &lt;td width="288"&gt;Status Parameter value&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td style="font-weight: bold;background-color: #d2d2d2" rowspan="3" width="95"&gt;&lt;strong&gt;All your job queue’s connected compute environments have received insufficient capacity errors.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;CAPACITY:INSUFFICIENT_INSTANCE_CAPACITY – Service cannot fulfill the capacity requested for instance type [instanceTypeName].&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="248"&gt;&lt;code&gt;jobStateTimeLimitActions.reason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;CAPACITY:INSUFFICIENT_INSTANCE_CAPACITY&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt; after job cancellation&lt;/td&gt; 
   &lt;td width="288"&gt;Canceled by JobStateTimeLimit action due to reason: CAPACITY:INSUFFICIENT_INSTANCE_CAPACITY&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="font-weight: bold;background-color: #d2d2d2" rowspan="3" width="95"&gt;&lt;strong&gt;All compute environments have maxVcpu that is smaller than job requirements.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;MISCONFIGURATION:COMPUTE_ENVIRONMENT_MAX_RESOURCE – CE(s) associated with the job queue cannot meet the CPU requirement of the job.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td width="248"&gt;&lt;code&gt;jobStateTimeLimitActions.reason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;MISCONFIGURATION:COMPUTE_ENVIRONMENT_MAX_RESOURCE&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt; after job cancellation&lt;/td&gt; 
   &lt;td width="288"&gt;Canceled by JobStateTimeLimit action due to reason: MISCONFIGURATION:COMPUTE_ENVIRONMENT_MAX_RESOURCE&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td style="font-weight: bold;background-color: #d2d2d2" rowspan="3" width="95"&gt;&lt;strong&gt;All compute environments have no connected instances that meet job requirements.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;MISCONFIGURATION:JOB_RESOURCE_REQUIREMENT – The job resource requirement (vCPU/memory/GPU) is higher than that can be met by the CE(s) attached to the job queue.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="248"&gt;&lt;code&gt;jobStateTimeLimitActions.reason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;MISCONFIGURATION:JOB_RESOURCE_REQUIREMENT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt; after job cancellation&lt;/td&gt; 
   &lt;td width="288"&gt;Canceled by JobStateTimeLimit action due to reason: MISCONFIGURATION:JOB_RESOURCE_REQUIREMENT&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="font-weight: bold;background-color: #d2d2d2" rowspan="3" width="95"&gt;&lt;strong&gt;All compute environments have service role issues.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;MISCONFIGURATION:SERVICE_ROLE_PERMISSIONS – Batch service role has a permission issue.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td width="248"&gt;&lt;code&gt;jobStateTimeLimitActions.reason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;MISCONFIGURATION:SERVICE_ROLE_PERMISSIONS&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt; after job cancellation&lt;/td&gt; 
   &lt;td width="288"&gt;Canceled by JobStateTimeLimit action due to reason: MISCONFIGURATION:SERVICE_ROLE_PERMISSIONS&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td style="font-weight: bold;background-color: #d2d2d2" rowspan="3" width="95"&gt;&lt;strong&gt;All connected compute environments are invalid.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;ACTION_REQUIRED – CE(s) associated with the job queue are invalid.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="248"&gt;&lt;code&gt;jobStateTimeLimitActions.reason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;&lt;em&gt;Not applicable&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt; after job cancellation&lt;/td&gt; 
   &lt;td width="288"&gt;&lt;em&gt;Not applicable&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td style="font-weight: bold;background-color: #d2d2d2" rowspan="3" width="95"&gt;&lt;strong&gt;Batch has detected a blocked queue, but is unable to determine the reason.&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;UNDETERMINED – Batch job is blocked, root cause is undetermined.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr style="background-color: #f2f2f2"&gt; 
   &lt;td width="248"&gt;&lt;code&gt;jobStateTimeLimitActions.reason&lt;/code&gt;&lt;/td&gt; 
   &lt;td width="288"&gt;&lt;em&gt;Not applicable&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="248"&gt;CloudWatch Event &lt;code&gt;statusReason&lt;/code&gt; after job cancellation&lt;/td&gt; 
   &lt;td width="288"&gt;&lt;em&gt;Not applicable&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;You’ll note that some reasons are not able to be used in the &lt;code&gt;jobStateTimeLimitActions&lt;/code&gt; parameter. One example is when all your queue’s attached compute environments are INVALID, or when Batch is unable to determine the root cause of the blockage. In both of those cases, we recommend setting up EventBridge rules to notify you when they occur.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;AWS Batch has introduced new functionality designed to help you detect and act on blocked job queues, where a job that’s at the head of a queue can’t run for one reason or another and prevents all the others behind it from running, too. We shared how to use EventBridge to catch these new CloudWatch Events, and the Batch job queue API to automatically terminate the stuck job and unblock the queue of jobs behind it.&lt;/p&gt; 
&lt;p&gt;Finally, we covered the most common root causes along with the error messages to look for. In most cases Batch can automatically determine the root cause of the blockage, allowing you to define specific automated actions for each class of error to take action on and unblock the queue.&lt;/p&gt; 
&lt;p&gt;To get started using AWS Batch, log into the&amp;nbsp;&lt;a href="https://console.aws.amazon.com/batch/home"&gt;AWS Management Console&lt;/a&gt;, or read the&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/what-is-batch.html"&gt;AWS Batch User Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For specific guidance on using these new events, refer to the AWS Batch Troubleshooting guide for &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/troubleshooting.html#job_stuck_in_runnable"&gt;jobs stuck in RUNNABLE&lt;/a&gt;, the &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/batch_cwe_events.html#batch-job-queue-blocked-events"&gt;blocked job queue events&lt;/a&gt; documentation, and the &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/cloudwatch_event_stream.html"&gt;Batch EventBridge&lt;/a&gt; documentation on how react to these events.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Using large-language models for ESG sentiment analysis using Databricks on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/using-large-language-models-for-esg-sentiment-analysis-using-databricks-on-aws/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Tue, 05 Mar 2024 12:34:23 +0000</pubDate>
				<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AI]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">5ff183a8b5fd12c0deb708f5dd9760595e67a42c</guid>

					<description>ESG is now a boardroom issue. See how Databricks' AI solution helps understand emissions data and meet new regulations.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="size-full wp-image-3333 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/14/boofla88_environmental_social_and_governance_policies_as_a_conc_58e6417e-1244-4776-8eef-c48fb6d9cb32.png" alt="Using large-language models for ESG sentiment analysis using Databricks on AWS" width="380" height="212"&gt;This post was contributed by Ilan Gleiser, Principal ML Specialist, Global Impact Computing, AWS, &lt;/em&gt;&lt;em&gt;Antoine Amend, Sr. Technical Director, Databricks, &lt;/em&gt;&lt;em&gt;Venkat Viswanathan, Senior Solutions Architect, AWS&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Regulators worldwide recognize the threat of climate change to economies and financial systems, forcing public companies in the US and Europe to be aware of – and disclose – their greenhouse emissions. This has led to a surge in demand from various stakeholders, including asset managers, asset owners, investors, and regulators, for machine-learning models that can analyze the plethora of data on their environmental, social and governance (ESG) policies.&lt;/p&gt; 
&lt;p&gt;Alongside regulatory change, there is a financial incentive to this endeavor. Typically, ESG ratings have a positive correlation with both valuation and profitability, &lt;a href="https://corpgov.law.harvard.edu/2020/01/14/esg-matters/"&gt;while showing a negative correlation with volatility&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In this post, we’re going to look at the challenge posed by ESG, as a data and AI problem. In the &lt;a href="https://www.databricks.com/blog/2020/07/10/a-data-driven-approach-to-environmental-social-and-governance.html"&gt;Databricks ESG Solution Accelerator&lt;/a&gt;, we will use natural language processing (NLP) to sort through the vast amounts of structured, and unstructured, data.&lt;/p&gt; 
&lt;h2&gt;Why Databricks?&lt;/h2&gt; 
&lt;p&gt;The main benefits for using the Databricks ESG Solution Accelerator are:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Detect differences between sustainability reports and real-time news data in portfolios of securities. Differences between sustainability reports and real-time news data may lead consumers and stakeholders to overvalue a company’s ESG score.&lt;/li&gt; 
 &lt;li&gt;View how companies’ business relationships with one another within a global marketplace can impact their respective ESG scores.&lt;/li&gt; 
 &lt;li&gt;Show how companies on the top decile of the ESG rank have half the volatility (as measured by &lt;em&gt;value-at-risk&lt;/em&gt;) of companies on the bottom end of the ESG ranks and better returns, indicating superior Sharpe ratios.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Users of the Databricks ESG Solution Accelerator can turn ESG risk into a source of excess returns (or &lt;em&gt;alpha&lt;/em&gt;) if spotted early in the process. This is done by processing unstructured data, like PDFs and real time news data and using machine learning (ML) algorithms that analyses the sentiment of the news data and compares with the sentiment of the data coming from the companies’ sustainability report on the same ESG policy&lt;/p&gt; 
&lt;h2&gt;The critical problem of unstructured data&lt;/h2&gt; 
&lt;p&gt;The ESG world generates data that is inherently unstructured. Out of the 40 frequently-disclosed ESG policies by companies, only ten can be measured as tangible figures while the rest are policy initiatives expressed in textual form across various IT systems. This raises the question of how AI can be applied to quantify and compare organizations ESG policies in a more objective manner.&lt;/p&gt; 
&lt;p&gt;To help customers unlock the value of their sustainability data stored in the AWS cloud, we are highlighting Databricks ESG Solution Accelerator as described in “&lt;a href="https://www.databricks.com/blog/2020/07/10/a-data-driven-approach-to-environmental-social-and-governance.html"&gt;&lt;em&gt;A Data-driven Approach to Environmental, Social, and Governance using the Databricks Solutions Accelerator&lt;/em&gt;&lt;/a&gt;” and showing you step by step, how to run the accelerator on AWS.&lt;/p&gt; 
&lt;p&gt;The notebooks show you how machine learning can enable asset managers, regulators, and investors to assess the sustainability exposure of their investments and empower their businesses with a holistic and data-driven view of their environmental, social, and corporate governance strategies.&lt;/p&gt; 
&lt;p&gt;Specifically, the first notebook will extract key ESG initiatives communicated in yearly sustainability PDF reports and compare these with real time actual media coverage from news analytics data as in Figure 1. The idea behind this approach is to spot differences between sustainability reports and real-time news data to offer decision makers with the most accurate and promptly available information.&lt;/p&gt; 
&lt;div id="attachment_3323" style="width: 840px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3323" loading="lazy" class="size-full wp-image-3323" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/13/CleanShot-2024-02-13-at-11.02.26.png" alt="Figure 1. Extract the key ESG initiatives as communicated in yearly PDF reports and compare these with the actual media coverage from news analytics data" width="830" height="406"&gt;
 &lt;p id="caption-attachment-3323" class="wp-caption-text"&gt;Figure 1. Extract the key ESG initiatives as communicated in yearly PDF reports and compare these with the actual media coverage from news analytics data&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Databricks Solution Accelerators on AWS: speedways to innovation&lt;/h2&gt; 
&lt;p&gt;Databricks ESG Solution Accelerator for AWS provides an expedited path to production for AWS customers who store their data on Amazon S3. Businesses can simplify the migration of their data and AI workloads to Databricks on AWS and quickly start utilizing the accelerator notebooks. The Databricks ESG Solution Accelerator come in a pair of notebooks that are easy to set up and provide prompt feedback, enabling asset management, MLOPS governance, and risk teams to achieve their goals in a much shorter time frame. Customers can take advantage of AWS’s computing power and virtually limitless storage, resulting in increased flexibility, scalability, and dependability at a reduced cost compared to developing an in-house solution.&lt;/p&gt; 
&lt;p&gt;To support users from different lines of business, these accelerators are designed to meet the different goals of multiple teams – portfolio managers and asset owners can use them to maximize alpha or minimize risk; regulators can identify new potential hot spots; and corporations can gather public sentiment of their news in the media.&lt;/p&gt; 
&lt;h2&gt;ESG Accelerator Logic&lt;/h2&gt; 
&lt;p&gt;Although all these accelerators work together, businesses can apply them as stand-alone projects or layer them. The process goes as follows:&lt;/p&gt; 
&lt;p&gt;Extract ESG initiatives from ESG reports using PyPDF2 and categorize them based on different topics. Then, tokenize and lemmatize the content for algorithmic ingestion. The next step is to automatically classify sentences extracted from ESG reports using atopic modeling algorithm. This classification allows customer/users to compare different companies’ by clustering them side-by-side to identify key focus areas (or ESG policies) voluntarily disclosed by companies in their sustainability reports.&lt;/p&gt; 
&lt;p&gt;To create a data-driven ESG score, the Databricks ESG Solution Accelerator runs a sentiment analysis on financial news articles related to each company is conducted using the Global Database of Event Location and Tones (GDELT) files, updated every 15mins. The assumption is that overall tone captured from financial news articles is a good proxy for companies’ ESG scores. This approach generates scores for each company across all its ESG dimensions. A propagated weighted ESG (PW-ESG) metric is then calculated to provide a global view of risk by quantifying the links between companies and assessing their importance.&lt;/p&gt; 
&lt;p&gt;Finally, to validate the assumption that high-ranked ESG companies offer better risk-adjusted returns than low-ranked ESG companies, the portfolio is split into two books – best and worst 10% ESG scores, respectively – and their historical returns and corresponding 95% Value-at-Risk are computed. The results show that low-ranked ESG portfolio has 2 times more risk than high-ranked ESG portfolio for the same level of returns.&lt;/p&gt; 
&lt;h3&gt;Benefits of notebook 1 – spotting differences between sustainability reports and real-time news data&lt;/h3&gt; 
&lt;p&gt;The goal of notebook #1 is to understand what the statements are all about, learning a vocabulary that is ESG specific with themes like diversity and inclusion, code of conduct, supporting communities and renewable energy.&lt;/p&gt; 
&lt;p&gt;The ESG Solution Accelerator supports Databricks/AWS customers who host their data on AWS, offering a machine learning platform that educates itself on a wide array of subjects and themes that are significant in today’s corporate social responsibility environment. These themes range from ‘diversity and inclusion’, ‘code of conduct’, and ‘supporting communities’, to ‘renewable energy’, ‘impact investing’, and ‘valuing employees’. The automated learning feature enables the algorithm to provide valuable insights in these specific fields, allowing businesses the opportunity to better understand and incorporate these aspects into their investment and risk management strategies.&lt;/p&gt; 
&lt;div id="attachment_3324" style="width: 765px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3324" loading="lazy" class="size-full wp-image-3324" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/13/CleanShot-2024-02-13-at-11.03.22.png" alt="Fig 2: Cluster analysis of machine learned policies emerging from unstructured Sustainability Reports PDF files" width="755" height="433"&gt;
 &lt;p id="caption-attachment-3324" class="wp-caption-text"&gt;Fig 2: Cluster analysis of machine learned policies emerging from unstructured Sustainability Reports PDF files&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3325" style="width: 774px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3325" loading="lazy" class="size-full wp-image-3325" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/13/CleanShot-2024-02-13-at-11.03.29.png" alt="Fig 3: Comparing organizations side-by-side, based on how much they disclose in each of those categories." width="764" height="554"&gt;
 &lt;p id="caption-attachment-3325" class="wp-caption-text"&gt;Fig 3: Comparing organizations side-by-side, based on how much they disclose in each of those categories.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;In a separate blog post, we show you how you can fine-tune a large language model and accelerate hyperparameter grid search for sentiment analysis with BERT models using Weights &amp;amp; Biases, Amazon EKS, and TorchElastic.&lt;/p&gt; 
&lt;h3&gt;Benefit of Notebook 2 – Understand How ESG Scores correlate with market risk and returns&lt;/h3&gt; 
&lt;p&gt;The second notebook presents a method for constructing a synthetic portfolio and incorporating ESG insights into the market risk framework. This involves considering not only a company’s outward appearance, but also its potential for performance. Research and literature suggest that companies with strong ESG practices typically exhibit lower market volatility. Our examination of value at risk highlights that a synthetic portfolio lacking such practices can be twice as volatile. Ultimately, we aim to connect research and product by integrating our findings into a comprehensive BI to AI Dashboard.&lt;/p&gt; 
&lt;p&gt;Combining all those insights, to understand what a company says about ESG vs how much a company does across those 24 machines learned policies, informs us in real time about news events that may positively or negatively affect the ESG score of every company, and in turn, the impact it may have on its market performance.&lt;/p&gt; 
&lt;p&gt;Combining all those insights into one platform helps us understand what a company says about ESG versus how much it actually does across the 24 machine-learned policies. This platform informs us in real time about news events that may positively or negatively affect the ESG score of every company, and subsequently, the impact it may have on its market performance.&lt;/p&gt; 
&lt;p&gt;ESG factors are among major market factors, like value, momentum, and volatility. The taxonomy of ESG factors has proved adaptive, as the market empirically prices new indicators. In addition, the recent advances in quantifying the effect of ESG factors on performance, in developing a regulatory and legal framework for ESG, and in establishing new ESG ratings should continue to have a positive effect on asset flows into ESG-related strategies&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Running the Accelerator on AWS&lt;/h2&gt; 
&lt;p&gt;Databricks built the ESG Solution Accelerator using the Delta Lake &lt;a href="https://www.databricks.com/glossary/medallion-architecture"&gt;Medallion Architecture&lt;/a&gt; to ingest data from raw, enriched, and purpose-built tables categorized as Bronze, Silver, and Gold Delta Lake tables.&lt;/p&gt; 
&lt;p&gt;Delta Lake is an open-source project built for data ‘lakehouses’ with compute engines, including Apache Spark, Trino, PrestoDB, Flink, and Hive, and with APIs for Scala, Java, Rust, Ruby, and Python. Delta Lake is an &lt;em&gt;ACID table storage layer&lt;/em&gt; over cloud object stores like S3 that provides data reliability including, but not limited to, schema enforcement and evolution, time travel, scalable metadata handling, audit history, DML operations, and unifies stream and batch processing. You can store the data in Delta Lake tables in your encrypted S3 bucket.&lt;/p&gt; 
&lt;div id="attachment_3326" style="width: 876px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3326" loading="lazy" class="size-full wp-image-3326" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/13/CleanShot-2024-02-13-at-11.04.47.png" alt="Fig 4. This schematic depicts how the Solutions Accelerator leverages corporate disclosures, news feeds, and portfolio ticker symbols to collect, transform, enrich, and apply machine learning models to create ESG scores from corporate disclosures and from the news feeds. This depicts the gaps between the corporate disclosures and the news feeds." width="866" height="424"&gt;
 &lt;p id="caption-attachment-3326" class="wp-caption-text"&gt;Figure 4. This schematic depicts how the Solutions Accelerator leverages corporate disclosures, news feeds, and portfolio ticker symbols to collect, transform, enrich, and apply machine learning models to create ESG scores from corporate disclosures and from the news feeds. This depicts the gaps between the corporate disclosures and the news feeds.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;A machine learning model is created using open-source MLFlow to enrich the data with ML-based approach. Databricks workloads are run within the customer’s VPC account in either Amazon EC2 instances or in Amazon Elastic Container Registry (Amazon ECR) containers.&lt;/p&gt; 
&lt;p&gt;AWS native services and AWS partner services further consume the data from Delta Lake. Some examples include using Amazon QuickSight for BI and visualization, Amazon Athena for advanced analytics, AWS Glue for data catalog, and Amazon SageMaker for more models, inference, and serving.&lt;/p&gt; 
&lt;p&gt;Along with these are ancillary components like AWS Identity &amp;amp; Access Management (IAM), which control access and set guardrails to the applications and services, and Amazon CloudWatch for monitoring the applications and services to ensure proper functioning, can be implemented.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Steps to use the template for the ESG Sentiment use case&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If you are new to AWS, &lt;a href="https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/"&gt;create and activate a new account&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;If you are new to Databricks on AWS, &lt;a href="https://aws.amazon.com/quickstart/architecture/databricks/#:~:text=If%20you%20don't%20already,existing%20cross%2Daccount%20IAM%20role"&gt;create and setup the Databricks account&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Login to Databricks &lt;code&gt;&amp;lt;account_name&amp;gt;.cloud.databricks.com&lt;/code&gt; with your username and password.&lt;/li&gt; 
 &lt;li&gt;Clone your repos using &lt;strong&gt;Repos&lt;/strong&gt; → &lt;strong&gt;Add Repo&lt;/strong&gt; → &lt;strong&gt;Select Git Hub&lt;/strong&gt; from the &lt;strong&gt;Git provider&lt;/strong&gt; → Enter &lt;code&gt;https://github.com/databricks-industry-solutions/esg-scoring&lt;/code&gt; in &lt;strong&gt;Git repository URL field&lt;/strong&gt; → provide your repository name in &lt;strong&gt;Repository name&lt;/strong&gt; field. Then &lt;strong&gt;submit&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Go to &lt;strong&gt;Compute&lt;/strong&gt; → &lt;strong&gt;Create Cluster&lt;/strong&gt; and select &lt;strong&gt;ML and 10.4 LTS ML&lt;/strong&gt; (Scala 2.12, Spark 3.2.1 and above) → &lt;strong&gt;Worker Type Instance Type&lt;/strong&gt;, &lt;strong&gt;Min workers&lt;/strong&gt; as 2, &lt;strong&gt;Max workers&lt;/strong&gt; as 8), and &lt;strong&gt;Driver Type&lt;/strong&gt; as Worker Type, Check &lt;strong&gt;Enable Auto scaling&lt;/strong&gt;, Check &lt;strong&gt;Terminate&lt;/strong&gt; after 60 minutes of inactivity&lt;/li&gt; 
 &lt;li&gt;Create your cluster&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Repos&lt;/strong&gt; → Select your repo → Select the Notebook (there are 5 note books) → Open the Notebook sequentially → Attach Cluster from the &lt;strong&gt;Connect&lt;/strong&gt; drop down and select the one you created in the last step —&amp;gt; &lt;strong&gt;Run&lt;/strong&gt; → &lt;strong&gt;Run All&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You can read the &lt;a href="https://www.databricks.com/blog/2020/07/10/a-data-driven-approach-to-environmental-social-and-governance.html"&gt;Databricks ESG solution accelerator explainer&lt;/a&gt; to learn more, or watch the &lt;a href="https://www.youtube.com/watch?v=tEjacxTxS_4"&gt;video&lt;/a&gt;. And you can check out the Databricks page in AWS Marketplace to get started with &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-ubybkhtmjqhhc"&gt;Databricks on AWS&lt;/a&gt;, and visit the &lt;a href="https://aws.amazon.com/marketplace/solutions/sustainability/?ref_=mp_nav_solution_sus"&gt;AWS Sustainability Solutions&lt;/a&gt; page for ready-to-deploy ESG solutions.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Through this blog, we have illustrated a streamlined method for summarizing complex documents into key ESG initiatives that offer a deeper comprehension of the sustainability aspects of your investments. With the implementation of machine learning methods powered by large language models (LLMs), we have introduced a unique approach to ESG that more effectively identifies the impact of global markets on both organizational strategy and reputational risk. Additionally, we have highlighted the significant economic influence of ESG factors on market risk calculation.&lt;/p&gt; 
&lt;p&gt;As a starting point to a data-driven ESG journey, this approach can be further improved by bringing the internal data you hold about your various investments and the additional metrics you could bring from third-party data, propagating the risks through the propagated-weighted ESG framework to keep driving more sustainable finance and impactful investments.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Improve the speed and cost of HPC deployment with Mountpoint for Amazon S3</title>
		<link>https://aws.amazon.com/blogs/hpc/improve-the-speed-and-cost-of-hpc-deployment-with-mountpoint-for-amazon-s3/</link>
		
		<dc:creator><![CDATA[Scott Ma]]></dc:creator>
		<pubDate>Tue, 27 Feb 2024 16:13:55 +0000</pubDate>
				<category><![CDATA[Amazon Simple Storage Service (S3)]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">42afa6fa28622939a4baad7a9810532f4a323104</guid>

					<description>Don't sacrifice performance OR ease of use with your HPC storage. Learn how Mountpoint for Amazon S3 combines high throughput and low latency with the simplicity of S3.</description>
										<content:encoded>&lt;p&gt;HPC workloads like genome sequencing and protein folding involve processing huge amounts of input data. Genome sequencing aims to determine an organism’s complete DNA sequence by analyzing extensive genome databases containing gene and genome reference sequences from thousands of species. Protein folding uses molecular dynamics simulations to model the physical movements of atoms and molecules in a protein.&lt;/p&gt; 
&lt;p&gt;These workloads require analyzing massive input datasets.&amp;nbsp;To support these kinds of applications that need high bandwidth, low latency, and parallel access to lots of data, AWS offers managed storage services like &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt; and &lt;a href="https://aws.amazon.com/efs"&gt;Amazon EFS &lt;/a&gt;that provide file systems optimized for compute-intensive workloads, which eliminates the need for customers to manage underlying storage infrastructure.&lt;/p&gt; 
&lt;p&gt;When selecting storage for your machine learning training data, &lt;a href="https://aws.amazon.com/s3/features/mountpoint/"&gt;Mountpoint for Amazon S3&lt;/a&gt; can provide a good alternative to support these types of workloads. Mountpoint for Amazon S3 is an open-source file client that you can use to &lt;em&gt;mount&lt;/em&gt; &lt;em&gt;an S3 bucket&lt;/em&gt; on your compute instances, accessing it as a local file system. It translates local file system API calls to REST API calls on S3 objects, and is optimized for high-throughput performance. It builds on the &lt;a href="https://docs.aws.amazon.com/sdkref/latest/guide/common-runtime.html"&gt;AWS Common Runtime&lt;/a&gt; (CRT) library, which is purpose-built for high-performance and low-resource usage to make efficient use of your fleet.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll deploy Mountpoint for Amazon S3 in an AWS ParallelCluster using the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;Community Recipe Library for HPC Infrastructure on AWS&lt;/a&gt;. We’ll then run the &lt;a href="https://ior.readthedocs.io/en/latest/index.html"&gt;IOR parallel I/O benchmark tool&lt;/a&gt;&amp;nbsp;to compare the performance of Mountpoint for Amazon S3 across the cluster, testing access speeds for reading files of varying sizes stored in Amazon S3.&lt;/p&gt; 
&lt;h2&gt;Setting expectations&lt;/h2&gt; 
&lt;p&gt;As shown in Figure 1, We should expect to see the parallel performance scaling well as we increase the number of nodes in the cluster accessing the shared Amazon S3 storage at the same time. We hope this shows you how to achieve high throughput access to virtually limitless Amazon S3 data using a simple approach running ParallelCluster.&lt;/p&gt; 
&lt;div id="attachment_3338" style="width: 1810px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3338" loading="lazy" class="size-full wp-image-3338" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/20/CleanShot-2024-02-20-at-13.25.38@2x.png" alt="Figure 1 - Read and write performance scales well with the number of nodes." width="1800" height="1058"&gt;
 &lt;p id="caption-attachment-3338" class="wp-caption-text"&gt;Figure 1 – Read and write performance scales well with the number of nodes.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;What do you need to try this out?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;AWS ParallelCluster Command line interface (CLI) with Node installed. See &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-pip.html"&gt;Installing AWS ParallelCluster in a non-virtual environment using pip&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;An existing bucket for the performance test.&lt;/li&gt; 
 &lt;li&gt;An existing Security Group, Subnet, EC2 Key&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Steps to deployment with ParallelCluster&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;For our deployment, we’ll download this ParallelCluster config file &lt;strong&gt;&lt;a href="https://raw.githubusercontent.com/aws-samples/aws-hpc-s3mountpoint/main/pcluster-example-config.yml"&gt;here&lt;/a&gt;&lt;/strong&gt;. This config file uses the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/mountpoint_s3"&gt;HPC Github Sample Recipe for Mountpoint for Amazon S3&lt;/a&gt; to help to deploy the mountpoint to the cluster nodes.&lt;/li&gt; 
 &lt;li&gt;Once you’ve downloaded the full ParallelCluster config file, update your own values in the config file,&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p style="padding-left: 40px"&gt;&lt;strong&gt;DEMO-BUCKET-NAME&lt;/strong&gt; – an existing bucket for testing. Create one if needed.&lt;br&gt; &lt;strong&gt;HOST-FILESYSTEM-PATH&lt;/strong&gt; – the path that the bucket will mount to inside the nodes. (e.g. &lt;code&gt;/testpath&lt;/code&gt;)&lt;br&gt; &lt;strong&gt;Your Subnet ID&lt;/strong&gt; – a subnet ID for deployment (e.g. &lt;code&gt;subnet-xxxxxxxx&lt;/code&gt;). Find the subnet Id in the &lt;a href="https://us-east-1.console.aws.amazon.com/vpcconsole/home?region=us-east-1#subnets:"&gt;Subnets home&lt;/a&gt;.&lt;br&gt; &lt;strong&gt;Your Security Group&lt;/strong&gt; – the id of the security group (e.g. &lt;code&gt;sg-xxxxxxxx&lt;/code&gt;). Find the security group id in the &lt;a href="https://us-east-1.console.aws.amazon.com/vpcconsole/home?region=us-east-1#SecurityGroups:"&gt;Security Groups home.&lt;/a&gt; Make sure the security group is for the same VPC the subnet is under.&lt;br&gt; &lt;strong&gt;Your ed25519 key&lt;/strong&gt; – the name of the EC2 Key Pair. You can find the name of your key in the &lt;a href="https://us-east-1.console.aws.amazon.com/ec2/home?region=us-east-1#KeyPairs:v=3;"&gt;Ec2 Key Pair home&lt;/a&gt;. If you do not have one, create a new one.&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Now, you can use this command to deploy a cluster:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster -c &amp;lt;template file&amp;gt; -r &amp;lt;region&amp;gt; -n &amp;lt;cluster_name&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deep-dive into the configuration&lt;/h2&gt; 
&lt;p&gt;To examine the config file further, it refers to three post-install scripts designed to work with &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/custom-bootstrap-actions-v3.html"&gt;ParallelCluster custom bootstrap actions&lt;/a&gt;. The first two scripts are from the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes"&gt;HPC recipes for AWS&lt;/a&gt;, whereas the last script was created specifically for this blog. These scripts are being run on the cluster head node &lt;em&gt;and&lt;/em&gt; all the compute nodes where the S3 bucket will be mounted. The script &lt;code&gt;s3-mp-install-ior.sh&lt;/code&gt; installs the IOR I/O performance benchmark suite and its necessary dependencies on the designated nodes. IOR is a parallel input/output benchmark that can test the performance of parallel storage systems using different interfaces and access patterns. The installation log, located at &lt;code&gt;/var/log/ior-install.log,&lt;/code&gt; provides debugging information about the installation process.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;install.sh&lt;/strong&gt; – installs Mountpoint for Amazon S3 and prepares the mount point directory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;mount.sh&lt;/strong&gt; – configures a systemd service that uses Mountpoint for Amazon S3 to mount a bucket to a directory&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;s3-mp-install-ior.sh&lt;/strong&gt; – installs the IOR I/O performance benchmark suite. Since the performance testing is for the compute node to perform, this is optional for the head node.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Cluster head node&lt;/h2&gt; 
&lt;p&gt;Here’s an example of the Head Node section of the ParallelCluster configuration. The bucket &lt;code&gt;DEMO-BUCKET-NAME&lt;/code&gt;&amp;nbsp;is mounted to the location &lt;code&gt;HOST-FILESYSTEM-PATH&lt;/code&gt;&amp;nbsp;on the host&lt;strong&gt;.&amp;nbsp;&lt;/strong&gt;&lt;code&gt;HOST-FILESYSTEM-PATH&lt;/code&gt; is an absolute path like &lt;code&gt;/s3mountpoint&lt;/code&gt;&lt;strong&gt;.&lt;/strong&gt;&amp;nbsp;You can mount multiple buckets on a single host.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;HeadNode:
  CustomActions:
    OnNodeConfigured:
      Sequence:
        - Script: https://aws-hpc-recipes.s3.us-east-1.amazonaws.com/main/recipes/storage/mountpoint_s3/assets/install.sh
        - Script: https://aws-hpc-recipes.s3.us-east-1.amazonaws.com/main/recipes/storage/mountpoint_s3/assets/mount.sh
          Args:
            - &amp;lt;&amp;lt;DEMO-BUCKET-NAME&amp;gt;&amp;gt;
            - &amp;lt;&amp;lt;HOST-FILESYSTEM-PATH&amp;gt;&amp;gt;
            - '--allow-delete --allow-root'
  Iam:
    AdditionalIamPolicies:
      - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
    S3Access:
      - BucketName: &amp;lt;&amp;lt;DEMO-BUCKET-NAME&amp;gt;&amp;gt;
        EnableWriteAccess: true 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We’ll be using &lt;code&gt;--allow-delete --allow-root&lt;/code&gt; to enable read/write to the bucket because read/write access is required by the performance testing tool. If you’re using this mountpoint for a &lt;strong&gt;read only&lt;/strong&gt; scenario like storing training data, it’s recommended that you use &lt;code&gt;--read-only&lt;/code&gt; to prevent accidental overwriting to the training data set. For more options to configure the mount point, see the &lt;a href="https://github.com/awslabs/mountpoint-s3/blob/main/doc/CONFIGURATION.md"&gt;Mountpoint configuration documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To access the S3 bucket from the header node, you need to enable S3 access through either the S3Access configuration under &lt;strong&gt;HeadNode/Iam/S3Access&lt;/strong&gt; or by attaching an additional IAM policy under &lt;strong&gt;HeadNode/Iam/AdditionalIamPolicies&lt;/strong&gt;. You can use a managed policy such as &lt;code&gt;AmazonS3ReadOnlyAccess&lt;/code&gt; to provide generic read-only access, or you can create a custom policy with more specific permissions tailored to your use case. The key requirement is that the &lt;code&gt;EC2 InstanceRole&lt;/code&gt; for the header node must have permissions to access the S3 bucket in order for it to read data from or write data to that location. The same will apply to the &lt;code&gt;ComputeNode&lt;/code&gt; section.&lt;/p&gt; 
&lt;h3&gt;Compute nodes&lt;/h3&gt; 
&lt;p&gt;Next, let’s look at an example of mounting the same bucket &lt;code&gt;DEMO-BUCKET-NAME&lt;/code&gt;&amp;nbsp;to a local path of&amp;nbsp;&lt;code&gt;HOST-FILESYSTEM-PATH&lt;/code&gt; in the Compute Nodes section of the ParallelCluster config file.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Scheduling:
    SlurmQueues:
        - Name: demo
          CustomActions:
            OnNodeConfigured:
                Sequence:
                  - Script: https://aws-hpc-recipes.s3.us-east-1.amazonaws.com/main/recipes/storage/mountpoint_s3/assets/install.sh
                  - Script: https://aws-hpc-recipes.s3.us-east-1.amazonaws.com/main/recipes/storage/mountpoint_s3/assets/mount.sh
                    Args:
                      - &amp;lt;&amp;lt;DEMO-BUCKET-NAME&amp;gt;&amp;gt;
                      - &amp;lt;&amp;lt;HOST-FILESYSTEM-PATH&amp;gt;&amp;gt;
                      - '--allow-delete --allow-root'
                  - Script: https://raw.githubusercontent.com/aws-samples/aws-hpc-s3mountpoint/main/s3-mp-install-ior.sh
Iam:
        AdditionalIamPolicies:
          - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
        S3Access:
          - BucketName: &amp;lt;&amp;lt;DEMO-BUCKET-NAME&amp;gt;&amp;gt;
            EnableWriteAccess: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Steps to run Performance Testing&lt;/h2&gt; 
&lt;p&gt;To execute the IOR benchmark across compute nodes, we’ll use &lt;code&gt;sbatch&lt;/code&gt; to submit a job to the HPC cluster. We’ll &lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-working-with-sessions-start.html#start-ec2-console"&gt;SSH into the head node&lt;/a&gt; using Session Manager to author an &lt;code&gt;sbatch&lt;/code&gt; script that calls &lt;code&gt;mpirun&lt;/code&gt; to launch the IOR executable with our desired parameters as below. The &lt;code&gt;sbatch&lt;/code&gt; script will specify the number of tasks, and Slurm will manage the number of nodes required.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;## create the sbatch submission script
cd ~
cat &amp;gt; ior_submission.sbatch &amp;lt;&amp;lt; EOF
#!/bin/bash
#SBATCH --job-name=ior-perf-test
#SBATCH --output=%N_%x_%j_%t.out
#SBATCH --ntasks-per-node=8
#SBATCH --ntasks=8

module load intelmpi
mpirun bash -c " 
     cd &amp;lt;&amp;lt;HOST-FILESYSTEM-PATH&amp;gt;&amp;gt;
     ior -r -w -v -F -o=S@S@S -b=2000m -i=1 -t=50m -a=POSIX --posix.odirect"
EOF

## submit the script
cd ~
sbatch ior_submission.sbatch
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Slurm will spawn the MPI processes across the cluster worker nodes to perform the parallel I/O tests. The output file will be available on the first compute node.&lt;/p&gt; 
&lt;h2&gt;Performance results&lt;/h2&gt; 
&lt;p&gt;Using this setup, we can use IOR to do some load tests across all our cluster compute nodes. It’s worth noting that the actual mounting operation itself takes just a few seconds&lt;/p&gt; 
&lt;p&gt;In our test runs, we used a single Amazon S3 bucket and attained exceptional read and write speeds – 4.23 GB/s (4.038 GiB/s) write throughput and 4.85 GB/s (4.622 GiB/s) read throughput per node (using c6i.16xlarge). This high read performance makes Mountpoint extremely well-suited for supporting the intensive read operations inherent in HPC workloads. Here’s some sample output from our own tests running on c6i.16xlarge instances:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-bash"&gt;Loading intelmpi version 2021.9.0
IOR-4.1.0+dev: MPI Coordinated Test of Parallel I/O
Began               : Thu Dec 14 00:13:39 2023
Command line        : /shared/ior/bin/ior -r -w -v -F -o=S@S@S -b=2000m -i=1 -t=50m -a=POSIX --posix.odirect
Machine             : Linux queue1-st-pclustercr1-1
TestID              : 0
StartTime           : Thu Dec 14 00:13:39 2023
Path                : S.00000000
FS                  : 0.0 GiB   Used FS: -nan%   Inodes: 0.0 Mi   Used Inodes: -nan%
Participating tasks : 64

Options:
api                 : POSIX
apiVersion          :
test filename       : S@S@S
access              : file-per-process
type                : independent
segments            : 1
ordering in a file  : sequential
ordering inter file : no tasks offsets
nodes               : 1
tasks               : 64
clients per node    : 64
memoryBuffer        : CPU
dataAccess          : CPU
GPUDirect           : 0
repetitions         : 1
xfersize            : 50 MiB
blocksize           : 1.95 GiB
aggregate filesize  : 125 GiB
verbose             : 1

Results:

access    bw(MiB/s)  IOPS       Latency(s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ----       ----------  ---------- ---------  --------   --------   --------   --------   ----
Commencing write performance test: Thu Dec 14 00:13:55 2023
write     4038       83.06      0.039900    2048000    51200      28.34      30.82      30.06      31.70      0
Commencing read performance test: Thu Dec 14 00:14:18 2023

read      4622       92.53      0.682358    2048000    51200      0.086130   27.67      17.48      27.69      0
remove    -          -          -           -          -          -          -          -          7.57       0
Max Write: 4037.71 MiB/sec (4233.85 MB/sec)
Max Read:  4622.06 MiB/sec (4846.58 MB/sec)

Summary of all tests:
Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev   Max(OPs)   Min(OPs)  Mean(OPs)     StdDev    Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt   blksiz    xsize aggs(MiB)   API RefNum
write        4037.71    4037.71    4037.71       0.00      80.75      80.75      80.75       0.00   31.70114         NA            NA     0     64  64    1   1     0        1         0    0      1 2097152000 52428800  128000.0 POSIX      0
read         4622.06    4622.06    4622.06       0.00      92.44      92.44      92.44       0.00   27.69330         NA            NA     0     64  64    1   1     0        1         0    0      1 2097152000 52428800  128000.0 POSIX      0
Finished            : Thu Dec 14 00:14:53 2023&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;IOR’s test is designed to scale and maintain consistent throughput &lt;em&gt;per server&lt;/em&gt; as we add more compute nodes. You also have the option to modify the performance test settings like &lt;code&gt;blockSize&lt;/code&gt; (-b) and &lt;code&gt;transferSize&lt;/code&gt; (-t) to see how the throughput changes when you adjust those values.&lt;/p&gt; 
&lt;p&gt;For our performance runs, we used the options listed in table 1. As you prepare your dataset for testing, take time to optimize the IOR settings. Tailor them to your specific data characteristics for improved accuracy. For instance, adjust the blockSize downward if holding many smaller files. Or remove the -f option if processes commonly read the same files. Taking these steps allows IOR to better simulate your real-world environment. For additional options, refer to the&lt;a href="https://ior.readthedocs.io/en/latest/userDoc/options.html"&gt; IOR official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_3339" style="width: 1846px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3339" loading="lazy" class="size-full wp-image-3339" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/20/CleanShot-2024-02-20-at-13.37.27@2x.png" alt="Table 1 - IOR Options used for our performance test runs." width="1836" height="574"&gt;
 &lt;p id="caption-attachment-3339" class="wp-caption-text"&gt;Table 1 – IOR Options used for our performance test runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;With data stored in Amazon S3 behind the scenes, Mountpoint for Amazon S3 delivers the durability, scalability, and high throughput needed to support demanding machine learning training workloads. It also provides a cost-efficient storage solution.&lt;/p&gt; 
&lt;p&gt;Mountpoint for Amazon S3 is easy to integrate with other AWS ParallelCluster, and this extends to AWS Batch, and self-managed EC2 instances, too – all popular methods for distributed training workflows. The quick, scalable, and economical nature of Amazon S3 behind Mountpoint removes traditional data storage challenges customers often face when pursuing ML training.&lt;/p&gt; 
&lt;p&gt;You can get started with Mountpoint for S3 today by building on the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/mountpoint_s3"&gt;sample we’ve provided in the HPC Recipe Library&lt;/a&gt;. We highly encourage you to closely examine how the install/configure scripts operate and other recipes in the recipe library. These recipes are designed for cross-platform compatibility and robustness working with ParallelCluster, requiring little to no modification to use.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Accelerating agent-based simulation for autonomous driving</title>
		<link>https://aws.amazon.com/blogs/hpc/accelerating-agent-based-simulation-for-autonomous-driving-with-hpc/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Mon, 26 Feb 2024 11:02:29 +0000</pubDate>
				<category><![CDATA[Automotive]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Industries]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">b5625f70c17b5e2ef046c36732be097d809e0318</guid>

					<description>AWS is powering the future of self-driving cars. Check out this post to see how high performance computing is transforming agent-based models for the CARLA RAI Challenge.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3356" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/23/boofla88_a_robot_wearing_a_hat_and_driving_a_car_on_a_sunny_day_6f8f05d1-fe46-48fc-9828-a49a1cf22794.png" alt="" width="380" height="212"&gt;Driving innovation has always been at the core of AWS and one of the realms in which this innovation is most palpable is in the burgeoning industry of autonomous vehicles. By accelerating agent-based models (ABM) simulations with high performance computing, AWS is at the forefront of this technological revolution, and nowhere is this more evident than in their support of the CARLA RAI Challenge and their commitment to Responsible AI (RAI).&lt;/p&gt; 
&lt;p&gt;In this post, we’ll tell you about the challenge itself, the tools we’ve leveraged to set it up, and show you how you can participate.&lt;/p&gt; 
&lt;h2&gt;The CARLA RAI challenge and autonomous driving&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://carla-rai-challenge.github.io/"&gt;CARLA Responsible AI Challenge&lt;/a&gt; is an initiative led by the Oxford Robotics Institute, building upon the previous &lt;a href="https://leaderboard.carla.org/challenge/"&gt;CARLA Challenges&lt;/a&gt; by extending the Leaderboard with additional RAI related metrics while leveraging the CARLA open-source autonomous driving simulator.&lt;/p&gt; 
&lt;p&gt;We’re hoping to stimulate creative minds to build capable artificial intelligence (AI) agents with the ability to navigate diverse and complex driving scenarios. The challenge has a strong emphasis on the principles of Responsible AI to promote safety in AI systems through rigorous assessment of models under &lt;em&gt;robustness&lt;/em&gt;, &lt;em&gt;environmental sustainability&lt;/em&gt;, and &lt;em&gt;transparency&lt;/em&gt; lenses.&lt;/p&gt; 
&lt;p&gt;The role of AI in autonomous driving has never been more crucial, and the CARLA RAI Challenge serves to highlight this fact. Participants are encouraged to push the limits of technology and create AI agents that can navigate in all possible driving conditions – not just ideal ones. From bustling city traffic to desolate country roads, from bright sunny days to foggy nights – the AI agent should be prepared to handle it all.&lt;/p&gt; 
&lt;div id="attachment_3349" style="width: 909px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3349" loading="lazy" class="size-full wp-image-3349" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/23/CleanShot-2024-02-23-at-13.16.47.png" alt="Fig 1: The main goal of the challenge remains the assessment of the driving proficiency of autonomous agents in realistic traffic situations, as defined in the leaderboard mechanics. Teams will have to complete 10 routes in 2 weather conditions through 5 repetitions." width="899" height="382"&gt;
 &lt;p id="caption-attachment-3349" class="wp-caption-text"&gt;Figure 1: The main goal of the challenge remains the assessment of the driving proficiency of autonomous agents in realistic traffic situations, as defined in the &lt;a href="https://leaderboard.carla.org/#task"&gt;leaderboard mechanics&lt;/a&gt;. Teams will have to complete 10 routes in 2 weather conditions through 5 repetitions. Source: CARLA Challenge Simulator.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The competition goes beyond just AI innovation. By challenging participants to ensure their AI systems can adapt to varying traffic, weather, and hardware conditions, the competition ensures driving models’ robustness.&lt;/p&gt; 
&lt;p&gt;But &lt;em&gt;Responsible AI&lt;/em&gt; is important, too. Participants are encouraged to develop transparent or explainable models to facilitate accountability. This allows for better understanding, and therefore, improvements in AI behavior.&lt;/p&gt; 
&lt;p&gt;Finally, the challenge promotes environmental sustainability by encouraging model development activities that cut down on carbon dioxide (CO2) emissions.&lt;/p&gt; 
&lt;p&gt;The timeline for the challenge is as follows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The challenge will be opened on April 10th, 2024.&lt;/li&gt; 
 &lt;li&gt;The closing date for the challenge is July 9th, 2024.&lt;/li&gt; 
 &lt;li&gt;Teams are required to submit their video presentation by July 21st, 2024.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Role of AWS in agent-based simulations&lt;/h2&gt; 
&lt;p&gt;Using AWS for agent-based simulations helps to provide a robust and scalable environment for testing – and refining – the AI agents participating in the challenge. AWS offers expansive computational resources via its cloud-based infrastructure.&lt;/p&gt; 
&lt;p&gt;AWS operates at a huge scale, making it possible to run the resource-intensive simulations needed for testing the AI agents. Each simulation is a high-fidelity virtual representation of various driving scenarios, involving many variables and needing a lot of processing power. These simulations are not just one-off events, but need to be run repeatedly as the AI agents are continuously tweaked and improved.&lt;/p&gt; 
&lt;p&gt;But it’s not just about resources: collaboration is critical for innovation. The cloud provides an environment where researchers, developers, and AI enthusiasts can come together to share ideas, learn from each other, and drive forward the development of autonomous driving technology.&lt;/p&gt; 
&lt;p&gt;This means that AWS is a collaborator and enabler in the world of autonomous driving, making significant contributions to the advancement of agent-based simulations and, by extension, the entire field of autonomous vehicles.&lt;/p&gt; 
&lt;h2&gt;The power of agent-based models in autonomous driving&lt;/h2&gt; 
&lt;p&gt;Agent-based models (ABM) are changing the world of autonomous driving. Their power lies in their ability to replicate real-world driving conditions at a local level, with remarkable accuracy and detail – the perfect environment for testing AI agents.&lt;/p&gt; 
&lt;p&gt;An agent-based model is a complex system where each entity (agent), operates independently according to a set of rules. In the context of autonomous driving, these agents might represent other vehicles, pedestrians, or even elements of the environment like traffic lights and road markings. By simulating the actions and interactions of these agents, ABMs &amp;nbsp;can recreate a wide range of driving scenarios – from rush hour in a busy city to a quiet country road at night.&lt;/p&gt; 
&lt;p&gt;Agent-based models are also flexible. We can easily modify them to include new types of agents, or adjust the behavior of existing ones, allowing researchers to simulate virtually any driving scenario imaginable. This adaptability is key for the development of AI systems trying to replicate what they might encounter on real roads.&lt;/p&gt; 
&lt;p&gt;But the true power of ABMs is that they can learn and evolve. As the agents navigate the virtual environment, they learn. Each run provides data that can be used to fine-tune the agent and optimize its algorithm. This iterative process of learning and adaptation is what makes agent-based models such a powerful tool for autonomous driving research.&lt;/p&gt; 
&lt;h2&gt;How the CARLA RAI Challenge runs on AWS&lt;/h2&gt; 
&lt;p&gt;Running multiple simulations simultaneously drastically reduces the time spent testing and refining the agents, clearly a boon for developers, who can iterate faster. That means we need to have a scalable mechanism to grow the resources when we need to do a lot of work.&lt;/p&gt; 
&lt;p&gt;Once a developer creates and containerizes their agents, they can deploy them on AWS. In Figure 2, you can tell that the first step involves using the EvalAI website to submit Docker containers to the Amazon Elastic Container Registry (ECR), makes it easy to store, share, and deploy container images.&lt;/p&gt; 
&lt;p&gt;In step 2, the user submits the simulation configuration data, which will be forwarded by EvalAI into an Amazon Simple Queue Service (Amazon SQS) queue, which then triggers an Amazon EventBridge event (step 3) that connects applications with data from diverse sources.&lt;/p&gt; 
&lt;p&gt;The event initiated by SQS leads to the execution of an AWS Step Functions pipeline (step 4), which carries out a series of tasks based on the established deployment graph. The Step Functions then execute step 5, where a Lambda function is used to stash user information into Amazon DynamoDB, which is a NoSQL database that offers excellent scalability.&lt;/p&gt; 
&lt;p&gt;Step 6 involves the actual simulation itself, which is done using the Amazon Elastic Kubernetes Service (Amazon EKS). EKS facilitates the scaling of containerized applications across a cluster. This elasticity allows for the automatic shutdown of all unused resources, too. All the operational logs and simulation results are logged into Amazon CloudWatch for analysis and diagnostics later.&lt;/p&gt; 
&lt;p&gt;Finally – at step 7 – another Lambda function stores the simulation results in an Amazon Simple Storage Service bucket (Amazon S3). This is an excellent solution for storing a lot of files. The last step also returns the results using a REST API to the user through the EvalAI web interface.&lt;/p&gt; 
&lt;div id="attachment_3350" style="width: 781px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3350" loading="lazy" class="size-full wp-image-3350" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/23/CleanShot-2024-02-23-at-13.18.08.png" alt="Figure 1 – The elastic cloud architecture for the CARLA RAI Challenge. Each stage is designed to scale, which means multiple simulations can run simultaneously. This drastically reduces the time spent on testing and refining the agents." width="771" height="390"&gt;
 &lt;p id="caption-attachment-3350" class="wp-caption-text"&gt;Figure 2 – The elastic cloud architecture powering the CARLA RAI Challenge. This architecture was introduced in the previous CARLA Challenge. Each stage is designed to scale, which means multiple simulations can run simultaneously. This drastically reduces the time spent on testing and refining the agents.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Challenge breakdown&lt;/h2&gt; 
&lt;p&gt;Here’s a breakdown of the challenge.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Getting Started with CARLA&lt;/strong&gt;: Participants need to &lt;a href="https://carla-rai-challenge.github.io/create/"&gt;download&lt;/a&gt; and set up a specific version of CARLA (0.9.10.1) on their computers. This version is necessary because it matches the environment used in the online servers where the agents are assessed.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configuration and Setup&lt;/strong&gt;: After installing CARLA, participants must adjust some configurations to link additional components specific for the challenge. This include the &lt;em&gt;Leaderboard&lt;/em&gt; system the &lt;em&gt;Scenario Runner&lt;/em&gt;, and &lt;em&gt;Routes:&lt;/em&gt; 
  &lt;ol&gt; 
   &lt;li&gt;&lt;strong&gt;Leaderboard&lt;/strong&gt;: the control center for the challenge. It runs the autonomous agent created by participants through a series of tests across different routes and traffic conditions. It evaluates how well the agent performs in each scenario.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Scenarios&lt;/strong&gt;: These are predefined traffic situations that the autonomous agent must navigate through successfully. There are ten types of scenarios, each with different parameters. These simulate real-world traffic situations in the virtual towns available in CARLA.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Routes&lt;/strong&gt;: These are paths from one point to another that the agent must follow. Routes have start and end points and may include specific weather conditions they have to simulate. Participants can train their agents using the provided routes, but the routes used for final evaluation in the challenge are a secret.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Testing the Agent&lt;/strong&gt;: Before submitting, we encourage participants to test their agents using a basic setup. This test them manually control an agent through a simulation, to get clear understanding of what the agent will face during the challenge.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Final Submission and Evaluation&lt;/strong&gt;: Once they complete the program, we’ll ask participants to prepare their autonomous agent as a Docker image, which will undergo rigorous testing by the Leaderboard on undisclosed routes and scenarios. It’s crucial that the agent is able to successfully navigate these routes while following traffic laws and handling a variety of traffic situations. We’ll evaluate the performance of the agent in these tasks, focusing on how the agents cope in degraded driving and traffic conditions. Special metrics in the challenge include things like robustness against harsh environmental situations, data drift, sensor failures, and environmental impacts of running the agents.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;What’s next? AWS and autonomous driving&lt;/h2&gt; 
&lt;p&gt;As the landscape of autonomous driving continues to advance, we see AWS continuing to advance. Their ongoing dedication to delivering robust, high performance computing solutions, and their support for forward-thinking initiatives like the CARLA RAI Challenge is a promising combination.&lt;/p&gt; 
&lt;p&gt;That’s because rapid progress in autonomous driving demands powerful, scalable, and reliable computational resources – definitely the cloud’s wheelhouse.&lt;/p&gt; 
&lt;p&gt;Beyond this, the contribution that AWS makes to promoting a culture of collaboration and innovation within the AI and autonomous driving communities is an important driver. The cloud is important for researchers, developers, and AI enthusiasts to exchange ideas and learn from each other. And all of this collectively pushes the boundaries of what’s possible.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>How agent-based models powered by HPC are enabling large scale economic simulations</title>
		<link>https://aws.amazon.com/blogs/hpc/how-agent-based-models-powered-by-hpc-are-enabling-large-scale-economic-simulations/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Tue, 20 Feb 2024 12:02:54 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Thought Leadership]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">cc9fc285c87dc520b888f3312bbdbf1f55224dd6</guid>

					<description>See how agent-based models, driven to scale by HPC in the cloud, are shedding new light on macroprudential policies with this post from Oxford's Institute for New Economic Thinking.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright wp-image-3306 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/12/boofla88_conways_game_of_life_3d_photo_realistic_06f8a4e2-75eb-4a1e-adba-4be1f3ce34b3.png" alt="How agent-based models powered by HPC are enabling large scale economic simulations" width="380" height="212"&gt;This post was contributed by J. Doyne&amp;nbsp;Farmer, Institute of New Economic Thinking, Oxford University, Jagoda&amp;nbsp;Kaszowska-Mojsa, Oxford University &amp;amp; Institute of Economics, Polish Academy of Sciences, Sam&amp;nbsp;Bydlon, Senior Solutions Architect and Ilan&amp;nbsp;Gleiser, Principal Machine Learning Specialist, WWSO Emerging Technologies, AWS&lt;br&gt; &lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Economists and policy makers have maintained a sustained interest in understanding the effects of macroprudential economic policies. Recently, a novel approach using agent-based models has emerged, which provides insights into the complexity of these policies.&lt;/p&gt; 
&lt;p&gt;Agent-based models (ABM) are a type of computer simulation that has proven instrumental in shedding light on how different types of economic agents interact in a heterogeneous environment.&lt;/p&gt; 
&lt;p&gt;In this post, we will explore the use of agent-based models and high performance computing on AWS used by researchers at the University of Oxford’s Institute for New Economic Thinking (INET) to enhance our comprehension of macroprudential policies and their potential implications on economic systems.&lt;/p&gt; 
&lt;h2&gt;Defining macroprudential policy&lt;/h2&gt; 
&lt;p&gt;Public policies play a pivotal role as instruments used by governments and central banks to maintain financial stability and promote sustainable economic growth. In recent years, there has been a growing focus on a significant category of policies known as &lt;em&gt;macroprudential policies&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;At its core, macroprudential policy is centered around implementing measures that are designed to mitigate the risks and vulnerabilities that are inherent in a financial system. It focuses on the stability of a financial system as a whole, rather than addressing the concerns of individual institutions. Its main objective is to prevent the build-up of systemic risks, such as excessive credit growth, asset price bubbles, and excessive leverage. These risks have the potential to precipitate financial crises and macroprudential policy is oriented towards preventing such destabilizing events.&lt;/p&gt; 
&lt;p&gt;Traditionally, macroprudential policies have been put into practice through a range of instruments, such as capital adequacy requirements, loan-to-value ratio restrictions and countercyclical buffers. These policy tools are designed to mitigate risks by affecting the behavior of financial institutions, and to influence the decisions made by households and businesses.&lt;/p&gt; 
&lt;p&gt;The most used economic models have limitations when it comes to capturing the complexities of a financial system and the interactions among the various economic agents. This is where agent-based modeling steps in as a powerful tool. ABM is a simulation approach that enables economists to model the behavior of individual agents, such as banks, households, and firms and facilitates an examination of their interactions.&lt;/p&gt; 
&lt;p&gt;Using ABM, economists can simulate how different macroprudential policies impact the behavior of agents and the overall stability of a financial system. For example, they can investigate the effectiveness of different capital requirements on mitigating systemic risks or explore the consequences of changing loan-to-value ratios on housing markets.&lt;/p&gt; 
&lt;p&gt;An interesting case study that highlights the potential of ABM in understanding macroprudential policy is the work by Dr. Jagoda Kaszowska-Mojsa from the University of Oxford (INET) &amp;amp; the Institute of Economics, Polish Academy of Sciences. In her article titled “&lt;em&gt;Macroprudential Policy in a Heterogeneous Environment: An Application of Agent-Based Approach in Systemic Risk Modelling&lt;sup&gt;1&lt;/sup&gt;&lt;/em&gt;”, she used ABM to study the impact of different policy measures on the stability of a banking system. Through simulations, she showed how the heterogeneity of agents, and their interactions can affect the effectiveness of policy interventions.&lt;/p&gt; 
&lt;p&gt;Using ABM to analyze macroprudential policies offers several advantages. Firstly, it provides a more realistic representation of a financial system and economy by capturing the diversity of economic agents and their decision-making processes. Secondly, ABM enables us to analyze nonlinear and dynamic interactions, which are often difficult to capture in traditional models. Lastly, ABM can provide insights into the unintended consequences of policy interventions and help policy makers design more robust and effective measures.&lt;/p&gt; 
&lt;p&gt;Looking ahead, the use of ABM in understanding macroprudential policies holds significant potential.&lt;/p&gt; 
&lt;p style="text-align: left"&gt;&lt;em&gt;“Agent based models are about to be the next technology revolution. In economics, we have shown&lt;/em&gt;&lt;em&gt;&lt;sup&gt;2&lt;/sup&gt; how agent-based models can make better real-time (before the fact) predictions than standard models. This is just the tip of a large iceberg …”&lt;/em&gt;&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;em&gt;Prof. J. Doyne Farmer, INET (Institute of New Economic Thinking), Oxford University&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Limitations of economic models&lt;/h2&gt; 
&lt;p&gt;The widely embraced economic models such as the Dynamic Stochastic General Equilibrium (DSGE) framework have also been used to assess the effects of macroprudential policies. However, these models still rely on the &lt;em&gt;representative agent&lt;/em&gt; assumption, which assumes that all individuals in the economy behave in the same way. This assumption limits their ability to capture the diversity and complexity of economic agents in the real world.&amp;nbsp;Efforts to integrate the heterogeneity of agents are currently in the early stages in most of the DSGE models, which primarily rely on stylized solutions. A case in point is the DSGE-3D model, which was developed and is used by the European Systemic Risk Board and the European Central Bank and has three layers of default (3D) that are aimed at scrutinizing the impact of macroprudential policies on the stability of financial systems&lt;sup&gt;3&lt;/sup&gt;.&lt;/p&gt; 
&lt;p&gt;This is where Agent-Based Modelling (ABM) takes center stage. A large-scale, data-driven ABM presents a more realistic framework for comprehending the consequences of macroprudential policies. It empowers researchers to construct an environment that embraces the inherent diversity of economic agents by encompassing households, businesses, and financial institutions. By simulating the behaviors of these diverse agents, ABM can provide valuable insights into the intricate dynamics and interactions within a financial system and the broader economy.&lt;/p&gt; 
&lt;p&gt;Furthermore, agent-based modelling offers the ability to conduct counterfactual simulations, which is a valuable tool for assessing the impact of macroprudential policies on both a financial system and the broader economy. For instance, researchers can use ABM to simulate the consequences of various policy instruments, such as changes in capital requirements or loan-to-value ratios, on the stability of a financial system. This approach furnishes a more holistic comprehension of the potential impacts of these policies than is typically achievable within the conventional economic frameworks.&lt;/p&gt; 
&lt;p&gt;Furthermore, ABM can shed light on the stabilizing effects of macroprudential policies during economic or financial distress. For instance, the Basel III regulatory framework, which was adopted in 2010-2011, imposes stricter capital requirements for financial institutions that are designed to enhance their resilience. ABM helps to assess the effectiveness of such policies in mitigating systemic risks and preventing financial crises.&lt;/p&gt; 
&lt;p&gt;In summary, most of the economic models have limitations when it comes to capturing the diversity and complexity of economic agents. ABM offers a more realistic and comprehensive framework for understanding the impact of macroprudential policies, thereby enabling counterfactual simulations – and providing insights into the stability of a financial system.&lt;/p&gt; 
&lt;h2&gt;An overview of agent-based modelling for simulating economics&lt;/h2&gt; 
&lt;p&gt;Agent-based modelling (ABM) is an innovative approach that has gained increasing popularity in economics and policy research. Unlike most economic models that assume rational and homogeneous agents, ABM provides a more realistic representation of the complexity and diversity of economic agents in a system. In this section, we will provide an overview of ABM and its key features.&lt;/p&gt; 
&lt;p&gt;At its core, ABM is a simulation-based modelling approach that focuses on individual agents and their interactions within a system. These agents can represent a variety of economic actors, such as households, firms, and financial institutions. Each agent has its own set of characteristics, preferences, and decision-making processes, which are influenced by both internal and external factors.&lt;/p&gt; 
&lt;p&gt;The main advantage of ABM is its ability to capture emergent properties and complex interactions that are often overlooked by standard models. Rather than assuming linear relationships, ABM enables the modelling of nonlinear dynamics, feedback loops, and the amplification of shocks within a system. This enables economists to explore how small changes in individual behaviors can have significant implications for the overall stability and functioning of a system.&lt;/p&gt; 
&lt;p&gt;ABM also provides the versatility to accommodate varying degrees of agent heterogeneity. This implies that the agents within the model can possess diverse levels of knowledge, information, and decision-making rules. To illustrate, within a banking system, certain banks may exhibit a greater degree of risk aversion, while others may have a higher risk appetite. By capturing these nuanced distinctions, ABM enables researchers to investigate how different types of agents react to policy interventions and how their interactions shape the dynamics of a system.&lt;/p&gt; 
&lt;p&gt;To implement ABM, researchers typically use computer simulations. Simulating systems with the scale and complexity of the real world, however, requires a great deal of computing power. High performance computing services like those offered by Amazon Web Services (AWS) Advanced Computing Team offer the necessary power to run, calibrate, test, and validate complex ABM simulations in a cost-efficient manner.&lt;/p&gt; 
&lt;p&gt;Case study: the Polish economy: applying an ABM to macroprudential policy&lt;/p&gt; 
&lt;p&gt;Organizations like the Institute for New Economic Thinking (INET) have been at the forefront of promoting ABM and supporting research in this area. They provide resources, funding, and a platform for economists to share their findings and collaborate. This support has been instrumental in advancing the use of ABM for analyzing policies and has fostered innovation in the field.&lt;/p&gt; 
&lt;p&gt;One notable case study that showcases the application of ABM in this context is the study by Dr. Jagoda Kaszowska-Mojsa from the University of Oxford (INET) &amp;amp; the Institute of Economics, Polish Academy of Sciences. Recently, Dr. Kaszowska-Mojsa teamed up with the AWS Global Impact Compute team to scale ABMs that explore how the implementation of new macroprudential policies can impact financial stability while minimizing their contribution to societal inequality.&lt;/p&gt; 
&lt;p&gt;In this project (‘&lt;em&gt;MACROPRU&lt;/em&gt;’), Dr. Kaszowska-Mojsa used state-of-the-art, large-scale, data-driven agent-based simulation techniques to uncover the redistributive consequences of macroprudential policies and evaluate the most advantageous combination of these policies. The findings from this project complemented the insights from the European Central Bank’s (ECB) system-wide stress-testing exercises by providing valuable data on the rise of inequality in European Union (EU) countries because of the adoption of new financial regulations. (That project secured funding from the European Union’s Horizon 2020 Research and Innovation Programme through a Marie Skłodowska-Curie grant (no 101023445)).&lt;/p&gt; 
&lt;p&gt;The MACROPRU model&lt;sup&gt;5 &lt;/sup&gt;(outlined in Figure 1) is a distinctive economic framework that differentiates between heterogeneous economic agents: individuals that form households (consumers), versatile firms that operate across sectors, and banks within a financial sector.&lt;/p&gt; 
&lt;div id="attachment_3307" style="width: 843px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3307" loading="lazy" class="size-full wp-image-3307" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/12/CleanShot-2024-02-12-at-10.03.47.png" alt="Figure 1. The relationship between the agents in the MACROPRU model. " width="833" height="597"&gt;
 &lt;p id="caption-attachment-3307" class="wp-caption-text"&gt;Figure 1. The relationship between the agents in the MACROPRU model.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;This model stands apart from conventional economic approaches such as the Dynamic Stochastic General Equilibrium (DSGE) model due to its emphasis on the unique characteristics of the economic entities, which are derived from empirical data.&lt;/p&gt; 
&lt;p&gt;In the case of households, we calibrated their attributes and behaviors using data from the Eurosystem Household Finance and Consumption Survey (HFCS), from the European Central Bank (ECB). This data provides essential insights into how individuals and households function as consumers.&lt;/p&gt; 
&lt;p&gt;Although the model focuses on Poland as a small open economy, we could use it to simulate the economic scenarios in 22 different European countries where data is available (Eurozone, Poland, Hungary). A key aspect of the model is its ability to capture both the statistical equilibrium &lt;em&gt;and&lt;/em&gt; disequilibrium states, thus shedding light on the internal forces that drive the economic and financial cycles.&lt;/p&gt; 
&lt;p&gt;The model is made up from 58 modules, each governing the interactions and behaviors of the agents. These modules consider various factors, such as production, the procurement of goods, the sources of finance, demographic dynamics, investment decisions, and intergenerational wealth transfers.&lt;/p&gt; 
&lt;p&gt;Additionally, the model is capable of simulating changes in employment scenarios that result from macroeconomic shifts and transformations in different sectors. Such transitions occur in a probabilistic manner, effectively representing the intricate nature of occupational shifts in interconnected networks.&lt;/p&gt; 
&lt;p&gt;The model also analyses the impact of macroprudential policies on income and wealth inequalities. It departs from the conventional money multiplier approach by recognizing banks as creators of money.&lt;/p&gt; 
&lt;p&gt;The use of Agent-Based Modelling (ABM) in this study offers valuable insights into how different policy measures could affect the stability of a financial system and individual agents, including their income and wealth. Policy makers can use this approach to assess the effectiveness of macroprudential policies and to make informed decisions to promote financial stability and sustainable economic growth. ABM emphasizes the importance of considering the heterogeneity of agents and their interactions when designing and implementing policies. By incorporating individual behaviors and feedback loops into the model, policy makers gain a more accurate understanding of how different policy measures could affect both the stability of a financial system and the well-being of individual agents.&lt;/p&gt; 
&lt;p&gt;ABM provides flexibility, which enables the simulation of various scenarios and the exploration of different policy interventions. This helps policy makers to identify any potential risks and vulnerabilities within a system and to devise appropriate measures to mitigate them.&lt;/p&gt; 
&lt;h2&gt;AWS reference architecture&lt;/h2&gt; 
&lt;p&gt;Jagoda worked with the AWS Emerging Technologies team, to design an architecture with several key advantages. AWS services like Amazon Relational Database Service (RDS), Amazon Elastic Container Service (ECS) on Amazon Elastic Compute Cloud (EC2), AWS Batch, and Amazon Simple Storage Service (S3) played a pivotal role in calibrating and validating the model, optimizing the simulations, ensuring data integrity, and enabling scalability. The cloud-based infrastructure depicted in Figure 2 enhances the model’s analytical abilities and promotes transparency and accessibility in accordance with modern research practices.&lt;/p&gt; 
&lt;div id="attachment_3308" style="width: 719px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3308" loading="lazy" class="size-full wp-image-3308" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/12/CleanShot-2024-02-12-at-10.04.15.png" alt="Figure 2. Reference architecture for implementation of the ABMs in the AWS cloud." width="709" height="360"&gt;
 &lt;p id="caption-attachment-3308" class="wp-caption-text"&gt;Figure 2. Reference architecture for implementation of the ABMs in the AWS cloud.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We harnessed all these services to streamline the entire process and to optimize its efficiency. Amazon RDS for PostgreSQL stored the essential input data, ensuring data integrity and accessibility. Containerizing the applications let us leverage the cloud’s elasticity, so it became possible to dynamically scale the resources when we needed. Store the output data and logs in comma-delimited CSV files and other text formats helped us adhere to the principles of the open data policy advocated by the European Commission.&lt;/p&gt; 
&lt;p&gt;These choices simplified our analysis using various econometric packages but also maintained compatibility with other AWS services, including ECS on Amazon EC2, AWS Batch and Amazon S3. This approach not only improved the simulation’s analytical abilities but it also promoted transparency and accessibility in accordance with modern research practices.&lt;/p&gt; 
&lt;p&gt;Dr. Kaszowska-Mojsa also leveraged AWS Batch processes to facilitate the calibration and Monte Carlo analysis of the model, which ultimately improved the accuracy of the simulations. These AWS services provided an ideal environment for conducting complex, compute-intensive agent-based simulations to assess the impact of macroprudential policies on both an economy and society. This approach ensured scalability, reliability, security, and seamless integration, aligning perfectly with the project’s requirements.&lt;/p&gt; 
&lt;p&gt;Running the MACROPRU simulation on the AWS cloud infrastructure also proved to be a cost-effective and efficient choice. The cloud environment made it easier to store data securely, but also permitted the parallel execution of the same simulation with different calibrations – this significantly expedited the research process. Moreover, the scalability offered by AWS, combined with the use of services like Amazon RDS, ECS on Amazon EC2, AWS Batch and Amazon S3, ensured that the simulation ran smoothly and efficiently.&lt;/p&gt; 
&lt;p&gt;The ability to achieve these results in a secure, fast, and cost-efficient way underscores the advantages of leveraging cloud computing in research project like ours.&lt;/p&gt; 
&lt;h2&gt;Simulation results&lt;/h2&gt; 
&lt;p&gt;Our simulation comprehensively considered all the macroprudential instruments that were outlined in the CRR/CRD IV directive and subsequent European legislation and aligned with the principles of Basel III.&lt;/p&gt; 
&lt;p&gt;Practical testing within this framework involves diverse calibrations, including Capital Adequacy Ratios (CAR), Liquidity Coverage Ratio (LCR), Leverage Ratio (LR), as well as Sectorial Requirements and Large Exposures (LE). We also incorporated national requirements – specifically the Financial Stability Authority recommendations – which encompassed Debt Service to Income (DSTI), Loan to Value (LTV), Debt to Income (DTI) and Debt to Assets (DTA).&lt;/p&gt; 
&lt;p&gt;Creditworthiness evaluation for individual entities or companies remained contingent on individual banks in the simulation, which resulted in varying requirements based on a bank’s market strategy and risk assessment. In extending our model’s capabilities, we could also analyze additional elements such as the Capital Conservation Buffer, specific Countercyclical Capital Buffer (CCB), Systemic Risk Buffer (SRB), Global Systemically Important Banks (G-SIB) buffer and the buffers for other Systemically Important Banks (the “D-SIB” buffer).&lt;/p&gt; 
&lt;p&gt;Figure 3 illustrates the main message of the MACROPRU project: an inappropriate combination and poor calibration of macroprudential tools result in significant adverse redistributive outcomes, which can potentially undermine the positive effects of other public policy instruments – like fiscal, monetary, or social policies – on both an economy and wider society.&lt;/p&gt; 
&lt;div id="attachment_3309" style="width: 877px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3309" loading="lazy" class="size-full wp-image-3309" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/12/CleanShot-2024-02-12-at-10.04.49.png" alt="Figure 3 (a) &amp;amp; (b). Changes in (a) income and (b) indebtedness of households that result from adjustments to the calibration of macroprudential instruments." width="867" height="1035"&gt;
 &lt;p id="caption-attachment-3309" class="wp-caption-text"&gt;Figure 3 (a) &amp;amp; (b). Changes in (a) income and (b) indebtedness of households that result from adjustments to the calibration of macroprudential instruments.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;An aside:&lt;/em&gt; In Poland, the Debt Service to Income Ratio (DSTI) is calculated by dividing the sum of annual liabilities by the sum of annual income and then multiplying the result by 100%. DSTI serves as a crucial measure of household indebtedness in Poland. Each bank is required to calculate DSTI when evaluating the creditworthiness of households and ensuring compliance with regulatory requirements, specifically Recommendation S.&lt;/p&gt; 
&lt;p&gt;To demonstrate the effects of our analysis, we performed two scenarios where we adjusted the calibration parameters for DSTI and LTV while keeping the official calibrations for other macroprudential instruments. This allowed us to isolate the impacts of changes to DSTI and LTV, which affect the creditworthiness of businesses and their access to credit.&lt;/p&gt; 
&lt;p&gt;In the first counterfactual scenario, tightening DSTI and LTV led to a deterioration in the financial positions of populations in the lower percentiles, especially those with limited loans, compared to the base scenario. We validated this against empirical data. Populations in the higher percentiles who were accessing loans were minimally impacted – underscoring the differentiated effect of macroprudential policy changes across different income spectrums.&lt;/p&gt; 
&lt;p&gt;In each scenario, we performed a distinct calibration of macroprudential instruments which let us evaluate changes in inequality using the simulation’s output data, based on wellbeing economic KPIs such as the &lt;a href="https://ourworldindata.org/what-is-the-gini-coefficient"&gt;Gini Coefficient&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Naturally, more complex analyses are feasible, including calculating any uni- or multi- dimensional inequality measures. We could also conduct a time-based analysis to examine how policies influence individual agents, sectors and markets, along with a comprehensive assessment of risk transmission between economic sectors.&lt;/p&gt; 
&lt;p&gt;The quantification of the effects is of utmost significance to policy makers, central bankers, and regulators who shoulder the responsibility of maintaining a resilient economy &lt;em&gt;and&lt;/em&gt; for safeguarding the overall societal welfare. There’s more information about this on &lt;a href="https://monetaristinheels.com/project"&gt;the home page for the project&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Potential future applications and developments&lt;/h2&gt; 
&lt;p&gt;Agent-based modelling (ABM) has significant potential for improving our comprehension of macroprudential policies and their ramifications on the financial system, economy, and society. As researchers continue to explore and refine the approach, there are many future applications and developments that could further increase our understanding of public policies such as macroprudential policies, monetary policy, fiscal policy, environmental policy and other sectoral policies.&lt;/p&gt; 
&lt;p&gt;One potential application is the use of ABM to analyze the impact of macroprudential policies on the different sectors of an economy. Currently, most research is focused on the banking system, but there is also a need to understand how policies affect the housing market dynamics, consumer credit, Universal Basic Income, Circular Economy, climate adaptation and corporate lending. By expanding the scope of an analysis, policy makers can gain a more comprehensive understanding of the transmission channels and potential spillover effects of macroprudential policies.&lt;/p&gt; 
&lt;p&gt;Central bankers, regulators, and researchers may use DSGE and ABM models together in the future. This can address complex questions. It can also combine the strengths of each model type.&lt;/p&gt; 
&lt;p&gt;DSGE models are common for macroeconomics. But they have limits in showing diversity and change in finance. ABM models are better for this.&lt;/p&gt; 
&lt;p&gt;Using both models together gives more insights. Some central banks do this already. These include the Bank of England, Bank of Canada, Bank of Hungary, Bank of Spain, and Bank of Poland.&lt;/p&gt; 
&lt;p&gt;Furthermore, there is a pressing need for the increased real-world testing of ABM models. While ABM has shown its ability to capture the complexities of the financial system, we have to subject these models to rigorous testing using real-world data, validate them against stylized facts, and compare them to other economic models. This will help build confidence in ABMs and ensure their reliability and accuracy for policy analysis.&lt;/p&gt; 
&lt;p&gt;If you are a policy maker, business executive or financial services professional, please feel free to &lt;a href="mailto:ask-hpc@amazon.com"&gt;reach out to the AWS Emerging Technologies Computing team&lt;/a&gt; and we will help you get your ABM up and running on AWS.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;The potential future applications and developments of ABM for understanding macroprudential policies are vast and as the complexity and scale of these models increases, AWS services like AWS Batch are well positioned to serve the needs of ABM developers and researchers.&lt;/p&gt; 
&lt;p&gt;By expanding the scope of analysis, integrating ABM with other modelling approaches, validating models with real-world data, and receiving support from organizations like INET, policy makers can benefit from a more comprehensive and nuanced understanding of macroprudential policy and its implications for a financial system. As we continue to refine and advance ABM, we expect to uncover new insights and approaches that will contribute to the design and implementation of effective macroprudential, monetary and fiscal policies.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;References&lt;/h3&gt; 
&lt;p&gt;1 – Kaszowska-Mojsa, J., Pipień, M. 2020. Macroprudential Policy in a Heterogeneous Environment – An Application of Agent-Based Approach in Systemic Risk Modelling, Entropy, 22(2), p. 129.&lt;/p&gt; 
&lt;p&gt;2 – Pichler, A., Pangallo, M., del Rio-Chanona, R.M., Lafond, F. and Farmer, J.D., 2022. Forecasting the propagation of pandemic shocks with a dynamic input-output model. Journal of Economic Dynamics and Control, 144, p.104527.&lt;/p&gt; 
&lt;p&gt;3 – Clerc et al., 2015. Capital Regulation in a Macroeconomic Model with Three Layers of Default, International Journal of Central Banking, 11(3), p. 9-63.&lt;/p&gt; 
&lt;p&gt;4 – Kaszowska-Mojsa, J., Farmer, D., Bydlon, S., Gleiser, I., 2023. Cloud-Powered Insights: Unveiling the Effects of Macroprudential Policy in a Small Open Economy, available on: &lt;a href="https://monetaristinheels.com/project"&gt;https://monetaristinheels.com/project&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;5 – Kaszowska-Mojsa, J., Farmer, J.D., Włodarczyk, P., 2023. The details of the model, available on: &lt;a href="https://monetaristinheels.com/project"&gt;https://monetaristinheels.com/project&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This research uses data from the &lt;em&gt;Eurosystem Household Finance and Consumption Survey&lt;/em&gt;. The results published and the related observations and analysis may not correspond to results or analysis of the data producers.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Amazon’s renewable energy forecasting: continuous delivery with Jupyter Notebooks</title>
		<link>https://aws.amazon.com/blogs/hpc/amazons-renewable-energy-forecasting-continuous-delivery-with-jupyter-notebooks/</link>
		
		<dc:creator><![CDATA[Alec Hewitt]]></dc:creator>
		<pubDate>Tue, 13 Feb 2024 15:03:26 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">982561089e42b9831b7f993bd780876777cb7008</guid>

					<description>Interested in eliminating friction between data science and engineering teams? Read this post to learn how Amazon successfully transitioned Jupyter Notebooks from the lab to production.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3301" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/09/boofla88_a_wind_farms_of_turbines_across_a_rolling_green_hills__e8683f7d-f097-4513-af62-46cbd2b6ad88.png" alt="Amazon’s renewable energy forecasting- continuous delivery with Jupyter Notebooks copy" width="380" height="212"&gt;&lt;/em&gt;You might associate the phrase ‘Jupyter Notebooks in production’ with a scrappy startup short on engineers or a hobbyist tinkering in their free time. However, this story unfolds at Amazon, where a team transitioned from requiring software engineers to replicate scientists’ work for production, to enabling scientists to seamlessly deploy Jupyter Notebooks into production.&lt;/p&gt; 
&lt;p&gt;Amazon’s Renewable Energy Optimization team produces software to maximize the effectiveness of our portfolio of wind and solar farms. The team develops and runs machine learning models that forecast the state of the electricity grid in the next few days. As Amazon’s portfolio of wind and solar farms has expanded, the techniques that our scientists and software engineers use to create and run these machine learning models has evolved, too.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk you through a science-to-production workflow that’s probably familiar to you, explain the logic that supported our novel approach and – finally – let you benefit from what we learned implementing it all.&lt;/p&gt; 
&lt;h2&gt;The problem&lt;/h2&gt; 
&lt;p&gt;Initially, scientists and software engineers were separated. Scientists developed models and optimization techniques using Jupyter Notebooks. They’d pass these to a software engineer who would translate them into ‘production code’. Scientists &lt;em&gt;never&lt;/em&gt; wrote production code.&lt;/p&gt; 
&lt;p&gt;Although this worked for our initial deployments, the process had several limitations:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;It was slow.&lt;/strong&gt; Once a scientist settled on a model they wanted to use in production, it took two or three months for an engineer to translate that code into production. This created a bottleneck for the scientists and ran against the model of &lt;a href="https://aws.amazon.com/builders-library/going-faster-with-continuous-delivery/"&gt;continuous delivery&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Nobody understood the production code.&lt;/strong&gt; The engineer translating the notebook didn’t fully understand the science being used in the code. Once the code had been translated, it was difficult for the scientist to understand because the engineer changed the code structure. We ended up with code that no one understood. This made it difficult to debug errors and issues in the production code.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;There were inconsistencies between environments.&lt;/strong&gt; Scientists developed the notebooks using Amazon SageMaker or their local machine using dependencies they had installed manually. The engineer built the code using an Amazon internal build system. This frequently meant there were different versions of libraries in use. Subtle differences led to large differences between what scientists were expecting and what happened in production.&lt;/p&gt; 
&lt;p&gt;We needed engineers and scientists to use the same platform. This meant either getting scientists to write production-level code or we had to run Jupyter Notebooks in production (a little heretical).&lt;/p&gt; 
&lt;h2&gt;What’s a Jupyter Notebook and why do scientists like them?&lt;/h2&gt; 
&lt;p&gt;A &lt;a href="https://docs.jupyter.org/en/latest/projects/architecture/content-architecture.html#the-jupyter-notebook-interface"&gt;Jupyter Notebook&lt;/a&gt; is a JSON document that contains both the source code and output of the code execution. Users execute cells of the notebook and then the output of each cell run is saved into the JSON notebook itself. Every time a notebook is run, it gets mutated and overwritten with the latest run.&lt;/p&gt; 
&lt;div id="attachment_3292" style="width: 751px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3292" loading="lazy" class="size-full wp-image-3292" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/09/CleanShot-2024-02-09-at-14.21.13.png" alt="Figure 1 – Jupyter Server reads the JSON Notebook file on disk and displays it to the user. The user then requests commands to be executed which the Jupyter Server send to the Jupyter Kernel to perform. The JupyteServer receives the output of the commands from the Jupyter Kernal and updates the Notebook on disk." width="741" height="325"&gt;
 &lt;p id="caption-attachment-3292" class="wp-caption-text"&gt;Figure 1 – Jupyter Server reads the JSON Notebook file on disk and displays it to the user. The user then requests commands to be executed which the Jupyter Server send to the Jupyter Kernel to perform. The JupyteServer receives the output of the commands from the Jupyter Kernal and updates the Notebook on disk.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Jupyter Notebooks have a lot of advantages for scientists. They allow for quick iteration of ideas because a single operation can be run multiple times without having to run the entire program again. They’re also great for visualizing and analyzing the output from code. Tables, charts, or images can be displayed in the notebook, right next to the source code that generated them.&lt;/p&gt; 
&lt;div id="attachment_3293" style="width: 905px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3293" loading="lazy" class="size-full wp-image-3293" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/09/CleanShot-2024-02-09-at-14.21.42.png" alt="Figure 2 – Information displayed a graph in a Jupyter Notebook vs the same information in CloudWatch Logs." width="895" height="361"&gt;
 &lt;p id="caption-attachment-3293" class="wp-caption-text"&gt;Figure 2 – Information displayed a graph in a Jupyter Notebook vs the same information in CloudWatch Logs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;However, Jupyter Notebooks are not designed to run in production. There’s no built-in functionality to programmatically run a notebook with different parameters. They’re also not easy to test and they’re constantly evolving documents that change every time they are run which makes collaboration and reproducibility difficult. So despite Jupyter Notebooks offering desirable features, they weren’t compatible with our &lt;a href="https://docs.aws.amazon.com/whitepapers/latest/practicing-continuous-integration-continuous-delivery/summary-of-best-practices.html"&gt;software engineering best practices&lt;/a&gt; for production deployments.&lt;/p&gt; 
&lt;h2&gt;Why did we change our mind?&lt;/h2&gt; 
&lt;p&gt;Our views changed when we discovered the framework &lt;a href="https://github.com/nteract/papermill/tree/main"&gt;Papermill&lt;/a&gt; and its &lt;a href="https://netflixtechblog.com/scheduling-notebooks-348e6c14cfd6"&gt;usage at Netflix&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2209.09125"&gt;other industry leaders.&lt;/a&gt; Papermill is a library for executing notebooks programmatically. Specifically, it allowed us to turn a Jupyter Notebook into an immutable object, and then parametrize it.&lt;/p&gt; 
&lt;p&gt;Papermill replaces the user and UI. It acts as another client to the Jupyter Kernel using the same protocol as the Jupyter Server. However, rather than overriding the source notebook each time it runs, it will output a new notebook for each run. Therefore, each time Papermill runs a notebook, a new notebook is created. The notebook has gone from being a mutable document to immutable source code.&lt;/p&gt; 
&lt;p&gt;Additionally, Papermill allows each run to have different parameters. This allows a single notebook to perform a solar forecast for different locations based on a parameter that’s passed to the notebook.&lt;/p&gt; 
&lt;div id="attachment_3294" style="width: 851px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3294" loading="lazy" class="size-full wp-image-3294" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/09/CleanShot-2024-02-09-at-14.22.13.png" alt="Figure 3 – With Papermill, the Jupyter Server is replaced. Papermill is responsible for sending the commands to the Jupyter Kernel and reading and writing the output notebooks to disk. " width="841" height="396"&gt;
 &lt;p id="caption-attachment-3294" class="wp-caption-text"&gt;Figure 3 – With Papermill, the Jupyter Server is replaced. Papermill is responsible for sending the commands to the Jupyter Kernel and reading and writing the output notebooks to disk.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We now had a way to programmatically trigger a parametrized notebook. The next step was to ensure the scientists’ development environment matched the production environment.&lt;/p&gt; 
&lt;h2&gt;Keeping science environment consistent with production&lt;/h2&gt; 
&lt;p&gt;When we had to answer why one of our wind farms had been underperforming at one of Amazon’s infamous “&lt;a href="https://wa.aws.amazon.com/wat.concept.coe.en.html"&gt;Correction of Errors&lt;/a&gt;” meetings, it wasn’t the first time someone said, “&lt;em&gt;It worked on my machine!&lt;/em&gt;”. This was not a new problem and we knew existing technologies existed to solve it.&lt;/p&gt; 
&lt;p&gt;Initially, scientists used the pre-built &lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-images.html"&gt;SageMaker images&lt;/a&gt; to manage the dependencies for their notebooks. These images are useful for experimentation; however, &lt;em&gt;we didn’t have control of the exact version of each library that we were using&lt;/em&gt;. Specifically, some scientific Python libraries had the same version, but the systems used different &lt;a href="https://numpy.org/doc/stable/reference/routines.linalg.html"&gt;linear algebra libraries&lt;/a&gt; which affected the end result of our forecasts. We therefore decided to &lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-byoi.html"&gt;bring our own custom image into SageMaker&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-byoi.html"&gt;SageMaker custom images&lt;/a&gt; allows you to import an image from &lt;a href="https://aws.amazon.com/ecr/"&gt;Amazon Elastic Container Registry (Amazon ECR)&lt;/a&gt; that you have built yourself. This could be built in AWS CodeBuild or in another build system outside of AWS. We used this feature of SageMaker to build our own custom images with all the dependencies decided by our internal AWS Code Artifact Repository.&lt;/p&gt; 
&lt;p&gt;The same image being imported into SageMaker is exactly the same image that will be deployed to production and used on our development machines. This ensures when scientists are using a third party dependency, like &lt;a href="https://numpy.org/devdocs/index.html"&gt;Numpy&lt;/a&gt; or &lt;a href="https://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;, they have the confidence that it’s exactly the same version that will be used in production.&lt;/p&gt; 
&lt;p&gt;Once the scientist is happy with the notebook they’ve been experimenting with, they’ll submit it to our internal build system. The end result is a Docker image that contains the notebook as well as all of its dependencies.&lt;/p&gt; 
&lt;div id="attachment_3295" style="width: 909px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3295" loading="lazy" class="size-full wp-image-3295" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/09/CleanShot-2024-02-09-at-14.22.48.png" alt="Figure 4 – Scientists use Amazon SageMaker Studio for experimentation using the same docker image that is deployed into production." width="899" height="620"&gt;
 &lt;p id="caption-attachment-3295" class="wp-caption-text"&gt;Figure 4 – Scientists use Amazon SageMaker Studio for experimentation using the same docker image that is deployed into production.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;How do the notebooks run in production?&lt;/h2&gt; 
&lt;p&gt;Once a notebook has been built into the image, it can be run in production. The image has the notebook as well as a framework we built that runs the notebook using the Papermill library. An &lt;a href="https://aws.amazon.com/pm/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt; rule triggers the notebook to run at the appropriate time with the appropriate parameters.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; runs the notebook in &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon Elastic Container Service&lt;/a&gt; using the image that we pushed to the ECR repository. After the notebook finishes executing, we save the raw JSON notebook and the HTML representation of the notebook to &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Services (Amazon S3).&lt;/a&gt; These output notebooks can also be viewed in a web browser via an &lt;a href="https://aws.amazon.com/cloudfront/"&gt;Amazon CloudFront distribution&lt;/a&gt; in front of the Amazon S3 bucket.&lt;/p&gt; 
&lt;div id="attachment_3296" style="width: 834px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3296" loading="lazy" class="size-full wp-image-3296" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/09/CleanShot-2024-02-09-at-14.23.33.png" alt="Figure 5 – Notebooks are run on Amazon ECS using AWS Batch. The Batch jobs are triggered by schedules in Amazon EventBridge." width="824" height="813"&gt;
 &lt;p id="caption-attachment-3296" class="wp-caption-text"&gt;Figure 5 – Notebooks are run on Amazon ECS using AWS Batch. The Batch jobs are triggered by schedules in Amazon EventBridge.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We use &lt;a href="https://aws.amazon.com/dynamodb/"&gt;Amazon DynamoDB&lt;/a&gt; to store the status of each notebook run as well as additional metadata about the notebook run. Engineers and scientists can then use this information to debug the status of a notebook run. Even better, non-technical users can view that status of our forecasting and see key outputs in the notebook itself.&lt;/p&gt; 
&lt;div id="attachment_3315" style="width: 892px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3315" loading="lazy" class="size-full wp-image-3315" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/12/CleanShot-2024-02-12-at-10.57.02.png" alt="Figure 6 – Internal website that displays the list of notebook runs that have been saved into DynamoDB." width="882" height="406"&gt;
 &lt;p id="caption-attachment-3315" class="wp-caption-text"&gt;Figure 6 – Internal website that displays the list of notebook runs that have been saved into DynamoDB.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We set up the infrastructure and build pipeline using &lt;a href="https://aws.amazon.com/cdk/"&gt;AWS CDK&lt;/a&gt; which enables us to treat our &lt;a href="https://docs.aws.amazon.com/whitepapers/latest/introduction-devops-aws/infrastructure-as-code.html"&gt;infrastructure as code&lt;/a&gt;. This simplifies spinning up new environments, minimizes configuration errors, and leverages high-level constructs for faster, more reliable work.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;We started this project with the aim of allowing scientists to deploy more frequently without relying on engineers. We achieved that, but we &lt;em&gt;also&lt;/em&gt; improved how both scientists &lt;em&gt;and&lt;/em&gt; engineers interact with the production system. They can debug production notebooks through a web server that displays the output notebooks. This led to the use of these notebooks for reports and metrics presentations – accessible to others via shared links.&lt;/p&gt; 
&lt;p&gt;The infrastructure choices also increased our velocity. Previously our system was computationally intensive and it took two weeks to evaluate it sequentially on a very large machine. AWS Batch allows us to run more than a thousand &lt;em&gt;much smaller&lt;/em&gt; instances in parallel so our model evaluation went down to 3 hours. Solving these problems allowed our small &lt;a href="https://aws.amazon.com/executive-insights/content/amazon-two-pizza-team/"&gt;2-pizza team&lt;/a&gt; to dramatically increase our iteration speed and at the same time increased the reliability of our system.&lt;/p&gt; 
&lt;p&gt;At the outset of this work, scientists couldn’t tell their containers from their images – and the only thing engineers knew about Jupyter Notebooks was that they “&lt;em&gt;were not for production!&lt;/em&gt;”. But, by taking the time to understand each other’s tools and the problems that they were solving we built a system that let us to have consistent environments &lt;strong&gt;&lt;em&gt;and&lt;/em&gt;&lt;/strong&gt; continuous delivery of new science models.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Dynamic HPC budget control using a core-limit approach with AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/dynamic-hpc-budget-control-using-a-core-limit-approach-with-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Roberto Meda]]></dc:creator>
		<pubDate>Thu, 08 Feb 2024 16:10:16 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<guid isPermaLink="false">9bdb3110e846b9e53cd91e0e541ef049df9eb76f</guid>

					<description>Balancing fixed budgets with fluctuating HPC needs is challenging. Discover a customizable solution for automatically setting weekly resource limits based on previous spending.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3283" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/07/boofla88_budgets_as_a_concept_ea608982-89d1-447e-8d4d-71e53e623d15.png" alt="Dynamic HPC budget control using a core-limit approach with AWS ParallelCluster" width="380" height="212"&gt;Cloud computing provides an experience similar to uncapped or unlimited resources for HPC workloads, helping organizations to accelerate research and development. When using cloud, the business owner typically allocates a fixed annual budget for HPC resources. The budget then needs to be split by multiple groups, across departments, business units, or projects.&lt;/p&gt; 
&lt;p&gt;But while budgets are fixed, HPC workload needs fluctuate throughout the year for nearly everyone. That’s challenging, and can often make the shift to cloud too much of a puzzle for some.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll describe a solution for managing your budget using a&amp;nbsp;&lt;em&gt;dynamic&lt;/em&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Amazon Elastic Compute Cloud (Amazon EC2) &lt;em&gt;core allocation limit&lt;/em&gt; for each group asking for HPC resources. It automatically sets the resource limit for each group &lt;em&gt;every week&lt;/em&gt; based on the spending of that group in the &lt;em&gt;previous week&lt;/em&gt;. The solution can be easily customized and enhanced to meet the specific workload and environmental needs.&lt;/p&gt; 
&lt;h2&gt;HPC in the cloud is different, yet familiar&lt;/h2&gt; 
&lt;p&gt;Budget management for HPC in the cloud can be challenging because it presents a different set of problems than most customers are used to.&lt;/p&gt; 
&lt;p&gt;Due to project deadlines, HPC workloads must have &lt;a href="https://slurm.schedmd.com/qos.html"&gt;Quality of Service (QoS)&lt;/a&gt; configured in the workload scheduler. When HPC workloads are executed through the workload scheduler as jobs, each job has a run-time and a number of required CPU cores. The QoS is managed by both the cloud resource provider &lt;em&gt;and&lt;/em&gt; the HPC workload scheduler. To make this work requires integration between the workload scheduler and the cloud resource provisioning.&lt;/p&gt; 
&lt;p&gt;HPC workloads frequently spike, or have unexpected deadlines. Budget decisions might be subject to revisions and adaptations over time, as well as accommodating new projects that pop-up anytime during the budget year.&lt;/p&gt; 
&lt;p&gt;Generic cloud budget control services&amp;nbsp;like&amp;nbsp;&lt;a href="https://aws.amazon.com/aws-cost-management/aws-budgets/"&gt;AWS Budgets&lt;/a&gt; are often too coarse-grained for HPC workloads. Without some effort, they can’t provide the necessary insights to track usage and cost details for HPC jobs running on shared nodes, and it’s hard for them to adapt quickly to HPC workload changes that the business requires.&lt;/p&gt; 
&lt;p&gt;Finally, although &lt;a href="https://aws.amazon.com/ec2/pricing/reserved-instances/"&gt;Reserved Instances&lt;/a&gt; and &lt;a href="https://aws.amazon.com/savingsplans/"&gt;Savings Plans&lt;/a&gt; can provide up to 72% discount for stable workloads, &lt;em&gt;fluctuating &lt;/em&gt;HPC workloads may still need to use the &lt;strong&gt;on-demand &lt;/strong&gt;pay&lt;strong&gt;–&lt;/strong&gt;per&lt;strong&gt;–&lt;/strong&gt;use pricing model.&lt;/p&gt; 
&lt;h2&gt;Our solution&lt;/h2&gt; 
&lt;p&gt;To manage budgets in this environment, the EC2 core allocation limit can be enforced by the HPC workload scheduler. This has a key advantage: it’s a familiar mechanism you’ve probably used elsewhere, which is easy to measure.&lt;/p&gt; 
&lt;p&gt;It works like this: HPC&amp;nbsp;jobs from a group that would exceed their weekly quota limit will be kept in the queue in pending status until cores are released by finished jobs. An appropriate job pending reason can be provided, making business or job owners aware of their spending implications. Since the cloud provides a “virtually unlimited” capacity, urgent jobs can still be run by shifting a budget up.&lt;/p&gt; 
&lt;p&gt;For our purposes, we’ll configure&amp;nbsp;the core allocation limit based on physical CPU cores rather vCPUs, because many HPC applications perform better with the hyperthreading disabled, &lt;em&gt;and&lt;/em&gt; more customers are used to working this way.&lt;/p&gt; 
&lt;p&gt;We tested our solution with &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; 3.5+ with SLURM and &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/tutorials_07_slurm-accounting-v3.html"&gt;job accounting enabled&lt;/a&gt;. You can get a &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/slurm_accounting"&gt;complete ParallelCluster environment, with Slurm accounting enabled&lt;/a&gt; from the &lt;a href="https://hpc.news/recipes"&gt;HPC Recipes Library&lt;/a&gt;. If this works for you, you can adapt the recipes for your own purposes by customizing them to suit your local needs.&lt;/p&gt; 
&lt;h2&gt;Overview of the solution&lt;/h2&gt; 
&lt;p&gt;To calculate past spending, we query data from &lt;a href="https://aws.amazon.com/aws-cost-management/aws-cost-explorer/"&gt;AWS Cost Explorer&lt;/a&gt;, and calculate the spending of each subgroup in the past week. We then compare the result with the allocated budget of the subgroup, and set the EC2 core limit of the subgroup in the coming week. This is implemented by a weekly &lt;a href="https://en.wikipedia.org/wiki/Cron"&gt;&lt;em&gt;cron&lt;/em&gt;&lt;/a&gt; task running on the cluster head node.&lt;/p&gt; 
&lt;p&gt;The solution consists of a configuration files and a weekly script run from &lt;code&gt;cron&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The configuration file&lt;/strong&gt;, located on the HPC head node, lists the weekly budget for each subgroup. A subgroup could be a business unit, a project, or just an arbitrary group of users. In this example:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;&amp;lt;BU1&amp;gt;,&amp;lt;BU1 weekly budget&amp;gt;&lt;/code&gt;&lt;br&gt; &lt;code&gt;&amp;lt;BU2&amp;gt;,&amp;lt;BU2 weekly budget&amp;gt;&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Where &lt;code&gt;BU1&lt;/code&gt;, &lt;code&gt;BU2&lt;/code&gt; are subgroup identifiers or labels, followed by their respective weekly budgets (in dollars), separated by comma. This file can be modified anytime by administrators or budget owners if the workload or business priorities change.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The script, executed weekly&lt;/strong&gt;&amp;nbsp;(e.g. Sunday at midnight) on the HPC head node by the Linux &lt;code&gt;cron&lt;/code&gt; daemon, reads the configuration file, queries Cost Explorer and appends a new row to a CSV file (e.g. &lt;code&gt;&lt;em&gt;control.csv&lt;/em&gt;&lt;/code&gt;) with contents like this:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;date, BU1, BU1 cost, BU1 budget, BU1 core limit, BU2, BU2 cost, BU2 budget, BU2 core limit,...&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;In this file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;date&lt;/code&gt; is the day/timestamp of what the row represents&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;BU1,BU2,BUn&lt;/code&gt; are subgroup identifiers from the configuration file described in step 1&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cost&lt;/code&gt; is the accumulated EC2 cost of this subgroup during the past week, retrieved from the AWS Cost Explorer by this script&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;budget&lt;/code&gt; is the weekly budget of the subgroup specified in the configuration file described in the step 1&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;cores limit&lt;/code&gt; is calculated by this script according to cost and budget. The calculation is described in the next step.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;An example of core limit calculation&lt;/strong&gt; uses the formula like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-python"&gt;if Cost &amp;gt; Budget: 
    core limit = default core limit / (Cost / Budget)
if Cost &amp;lt;= Budget:
    core limit = default core limit&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here, &lt;code&gt;default core limit&lt;/code&gt; is calculated from the single-core weekly cost of a sample EC2 instance type for memory intensive workloads, e.g. 23.6880 USD, which is the weekly cost for a single core of an r5.large in the AWS region &lt;code&gt;eu-west-1&lt;/code&gt;. So:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-python"&gt;default core limit = budget / 1 core weekly cost&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Finally, for each subgroup &lt;/strong&gt;defined in the configuration file&lt;strong&gt;, our dynamic resource&lt;/strong&gt; &lt;strong&gt;scheduler &lt;/strong&gt;keeps track of the available number of EC2 cores as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-python"&gt;current&amp;nbsp;BU core limit - currently allocated cores&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that this solution requires jobs to specify the number of required cores. Using the information from the scheduler and Cost Explorer, the solution limits the total number of cores used by a subgroup to the defined allocation in the configuration file. Any job that would exceed the limit will be pending until cores are freed up by finished jobs of the same subgroup.&lt;/p&gt; 
&lt;h2&gt;An example&lt;/h2&gt; 
&lt;p&gt;We’ll use a simple configuration for this example based on two business units BU1 and BU2, with different weekly budgets.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;BU1 1000&lt;/code&gt;&lt;br&gt; &lt;code&gt;BU2 2000&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Our script will calculate default core limits of 42 and 84 respectively, according to an EC2 r5.large weekly core cost of 23.688 USD (Ireland-Dublin region).&lt;/p&gt; 
&lt;p&gt;The following table lists budgets, actual spending (costs) and calculated core limits of the following weeks for two subgroups: BU1 and BU2:&lt;/p&gt; 
&lt;div id="attachment_3271" style="width: 1001px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3271" loading="lazy" class="size-full wp-image-3271" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/07/CleanShot-2024-02-07-at-11.13.56.png" alt="Table 1: sample budgets, with actual spend and calculated core limits over a period of 4 weeks." width="991" height="328"&gt;
 &lt;p id="caption-attachment-3271" class="wp-caption-text"&gt;Table 1: sample budgets, with actual spend and calculated core limits over a period of 4 weeks.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In this example (under-limit values are marked in green, over-limit ones are in red):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In week 3, BU1 exceeds its budget by 1.5 (50%). Its core limit for the next week is calculated and set as: 42 / 1.5 = 28.&lt;/li&gt; 
 &lt;li&gt;In week 3, BU2 exceeds its budget by 1.1 (10%). Its core limit for the next week is calculated and set as: 84 / 1.1 = 76.&lt;/li&gt; 
 &lt;li&gt;In week 4, BU1 exceeds its budget by 1.05 (5%). Its core limit for the next week is calculated and set as: 84 / 1.05 = 80.&lt;/li&gt; 
 &lt;li&gt;All other limits are defaults because of the spending under the corresponding budget.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Figure 1 illustrates the solution high level logic. The script &lt;code&gt;budget.control.updater.sh&lt;/code&gt; is invoked regularly as a cron task, queries budget control configuration together with Cost Explorer, adds a new line into budget control table CSV, and updates scheduler configuration. The scheduler enforces each job to run only if its related subgroup budget is not over spent.&lt;/p&gt; 
&lt;div id="attachment_3272" style="width: 789px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3272" loading="lazy" class="size-full wp-image-3272" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/02/07/CleanShot-2024-02-07-at-11.15.44.png" alt="Figure 1: Dynamic budget control high level architecture. " width="779" height="664"&gt;
 &lt;p id="caption-attachment-3272" class="wp-caption-text"&gt;Figure 1: Dynamic budget control high level architecture.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Getting started and making this your own&lt;/h2&gt; 
&lt;p&gt;We’ve put our code, including sample scripts and some examples in a &lt;a href="https://github.com/aws-samples/dynamic-ec2-budget-control"&gt;specific AWS Samples repository on GitHub&lt;/a&gt;. You can try out this solution in your own AWS account by following the walk-through and code that we provided there.&lt;/p&gt; 
&lt;p&gt;You can also further customize and improve the solution in a couple of different ways, for example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Manage more instance types&lt;/strong&gt;. You can augment your scripts to filter the Cost Explorer query output according to the job’s instance-type requirement. Administrators could add multiple 1 core/hour instance-type costs in the budget configuration file.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Send email notifications when a daily cost exceeds the budget. &lt;/strong&gt;You can do that by integrating the &lt;em&gt;sendmail&lt;/em&gt; process on the head node with the &lt;a href="https://aws.amazon.com/ses/"&gt;AWS Simple Email Service&lt;/a&gt; as &lt;a href="https://docs.aws.amazon.com/ses/latest/dg/Welcome.html"&gt;described in our documentation&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enforce job submission cores resource requirement&lt;/strong&gt; – for example by using SLURM’s&amp;nbsp;&lt;code&gt;PrologSlurmctld&lt;/code&gt; (see &lt;a href="https://slurm.schedmd.com/prolog_epilog.html"&gt;SLURM prolog and epilog guide&lt;/a&gt;). Prologs and epilogs can be extremely useful if you need to implement input-validation checks, perform data movements, or do pre-execution environment setup.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Retrieve 1-core cost automatically&lt;/strong&gt; by querying &lt;a href="https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/using-pelong.html"&gt;AWS Price List Query APIs&lt;/a&gt;. This will ensure calculations always use up-to-date AWS pricing.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Modify the core logic or timings for a specific HPC environment&lt;/strong&gt;, for example, more frequent budget and limit controls, or smoothing limit changes.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Subgroup tags, together with others could be enforced and validated&lt;/strong&gt; as suggested in the &lt;a href="https://aws.amazon.com/blogs/aws-cloud-financial-management/cost-allocation-blog-series-3-enforce-and-validate-aws-resource-tags/"&gt;Enforce and validate AWS resource tags&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we described a straight-forward, but fine-grained and dynamic solution to keep work groups or business unit budgets under control. This has several advantages.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;It allows administrators to manually update budgets and core limits&lt;/strong&gt; according to the business needs. Admins can modify budgets limits in simple config files. The changes get picked up by automated cron tasks.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Historical budgets and calculated core-limits are recorded in a CSV file&lt;/strong&gt;, which makes further analysis possible using Amazon QuickSight or a spreadsheet. This is useful for annual budget reviews, justifying more investments, spotting trends, and making forecasts.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;HPC jobs breaching the limits are kept in pending status until more cores are freed up or job condition changes. &lt;/strong&gt;In addition, the scheduler provides the information about job pending reasons when core limits are reached, helping users better understand and manage their own budgets.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Accelerating molecule discovery with computational chemistry and Promethium on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/accelerating-molecule-discovery-with-computational-chemistry-and-promethium-on-aws/</link>
		
		<dc:creator><![CDATA[Christoph Siegert]]></dc:creator>
		<pubDate>Tue, 30 Jan 2024 13:06:40 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Life Sciences]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">c43bf42043ce54aab45c523d478b4877bab0956a</guid>

					<description>Interested in performing high-accuracy computational chemistry simulations faster? Check out this new post about Promethium, a solution from QC Ware that leverages AWS to accelerate simulations by up to 100x.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Perminder Singh Quantum Solution Architect, ISV, and Satish Gandhi, partner development manager ISV at AWS; and Christoph Siegert, SVP Product at QC Ware&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Computational chemistry for small molecule discovery remains one of the major opportunities across many high impact industries, such as pharmaceuticals and chemicals, as highlighted by &lt;a href="https://www.nature.com/articles/d41573-022-00025-1"&gt;Nature&lt;/a&gt;, &lt;a href="https://www.bcg.com/publications/2022/ai-in-drug-discovery-impact"&gt;BCG&lt;/a&gt;, and &lt;a href="https://www.mckinsey.com/industries/life-sciences/our-insights/pharmas-digital-rx-quantum-computing-in-drug-research-and-development"&gt;McKinsey&lt;/a&gt;. Many of the world’s largest problems are chemistry problems at their core.&lt;/p&gt; 
&lt;p&gt;For example, creating new catalysts is a high-impact chemistry problem that will benefit from accelerated molecular simulations. Discoveries in creating new catalysts could lower the gas requirements in the ammonia production process, which uses about 3% of &lt;a href="https://youtu.be/RfvL_423a-I?t=6553"&gt;global gas consumption&lt;/a&gt;. Another example is potentially shortening the drug development timeline, which currently can take 5 or more years. &lt;a href="https://www.bcg.com/publications/2023/enterprise-grade-quantum-computing-almost-ready"&gt;BCG estimates&lt;/a&gt; that molecular simulations could create a $60-130 billion value across pharma and chemistry alone.&lt;/p&gt; 
&lt;p&gt;However, in computational chemistry, researchers typically &lt;a href="https://onlinelibrary.wiley.com/doi/10.1002/qua.26381"&gt;have to make a tradeoff&lt;/a&gt; between speed and accuracy. QC Ware saw an opportunity to deliver both speed &lt;em&gt;and&lt;/em&gt; accuracy. They developed a new computational chemistry solution called Promethium that leverages &lt;a href="https://aws.amazon.com/ec2/"&gt;Amazon Elastic Compute Cloud (Amazon EC2)&lt;/a&gt; GPU instances. Promethium performs up to 100 times faster than traditional high accuracy solutions. In this post, we’ll dive into Promethium and explain how it achieves this.&lt;/p&gt; 
&lt;h2&gt;Computational chemistry capabilities&lt;/h2&gt; 
&lt;p&gt;Promethium is a density-functional theory (DFT) computational chemistry solution. Density-functional theory is a highly accurate computational quantum chemistry method that &lt;em&gt;doesn’t require training data,&lt;/em&gt; making it immediately applicable across industries:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pharma:&lt;/strong&gt; small molecule drugs, covalent inhibitors, protein-ligand binding, and PROTACs&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Agriculture:&lt;/strong&gt; pesticides, fertilizers, ammonia&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Energy:&lt;/strong&gt; oil &amp;amp; gas catalysts, reforming, battery materials&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Electronics:&lt;/strong&gt; organic light emitting diodes (OLEDs), organic electronics&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chemicals: &lt;/strong&gt;polymers, polyurethane, catalysts, and beyond&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automotive / Aerospace: &lt;/strong&gt;catalysts, new materials, polymers&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Environmental: &lt;/strong&gt;water treatment, air purification&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;However, the high accuracy provided by DFT comes at a price. It requires a high computational power, which limits the throughput, size, and complexity of simulated molecules. Promethium improves processing speed by using GPUs, specialized GPU software, and the scalability of AWS cloud. When using Promethium, computational chemists get highly accurate results, quickly.&lt;/p&gt; 
&lt;p&gt;Promethium delivers significant speedups over current best-in-class DFT software. The following chart shows how many single point energy calculations can be run on Amazon EC2 instances with various DFT solutions.&lt;/p&gt; 
&lt;div id="attachment_3246" style="width: 1135px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3246" loading="lazy" class="size-full wp-image-3246" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/19/CleanShot-2024-01-19-at-14.31.59.png" alt="Figure 1: Chart comparing single instance 24-hour simulation performance across solutions." width="1125" height="515"&gt;
 &lt;p id="caption-attachment-3246" class="wp-caption-text"&gt;Figure 1: Chart comparing single instance 24-hour simulation performance across solutions.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;This chart shows the number of molecules (Λ-FL172 organoruthenium complex) that can be simulated with DFT in 24hrs for each compute node across various commercial and open-source solutions.&lt;/p&gt; 
&lt;p&gt;The computational requirement also impacts the size and complexity of molecules that can be simulated. The new solutions allow treating a large space of protein-ligand interaction as an active area. For example, a 2,056 atom protein with ωB97/def2-SVP level of theory runs for about 14 hours on a single GPU from an &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;Amazon EC2 P4d&lt;/a&gt; instance.&lt;/p&gt; 
&lt;h2&gt;Scalable and secure architecture&lt;/h2&gt; 
&lt;p&gt;Promethium is a software as a service (SaaS) solution built on AWS and uses Amazon EC2 GPU instances to provide customers on-demand scalable computational resources. Promethium uses a microservices-based architecture deployed on Kubernetes. The solution dynamically reacts to customer demand and automatically scales Amazon EC2 &lt;a href="https://aws.amazon.com/ec2/instance-types/p3/"&gt;P3&lt;/a&gt; and &lt;a href="https://aws.amazon.com/ec2/instance-types/p4/"&gt;P4&lt;/a&gt; instances with Karpenter, an open-source compute resource manager for Kubernetes.&lt;/p&gt; 
&lt;p&gt;Molecule intellectual property is the most sensitive and important asset for QC Ware’s customers. Therefore, Promethium’s architecture follows AWS &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/welcome.html"&gt;best practices for cloud security&lt;/a&gt;. For example, the solution includes least-privilege role-based access control, encryption at rest and in transit, multi-factor authentication (MFA), and single sign-on. The following diagram outlines Promethium’s architecture.&lt;/p&gt; 
&lt;div id="attachment_3247" style="width: 1096px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3247" loading="lazy" class="size-full wp-image-3247" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/19/CleanShot-2024-01-19-at-14.32.43.png" alt="Figure 2: Promethium’s architecture on AWS cloud" width="1086" height="539"&gt;
 &lt;p id="caption-attachment-3247" class="wp-caption-text"&gt;Figure 2: Promethium’s architecture on AWS cloud&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;User Interface&lt;/h2&gt; 
&lt;p&gt;Promethium can be accessed through graphical and command-line interfaces. For ease of use, the graphical user interface (GUI) includes drag and drop, molecule visualizations, pre-set defaults, and automated error checking. As a SaaS solution hosted on AWS, customers don’t need to worry about managing the underlying AWS infrastructure, the dependency stack, or software updates. And thus, customers can focus on their chemistry work.&lt;/p&gt; 
&lt;p&gt;Users can customize all common settings within the GUI, and additionally, use a JSON customization form for advanced settings.&lt;/p&gt; 
&lt;p&gt;A REST API is also available for chaining workflows together and accessing advanced capabilities. Users can access the API through a Python SDK. The API can be integrated with other workflow tools and software products. For example, Electronic Lab Notebooks (ELBs) or data and workflow platforms, such as Schrodinger LiveDesign.&lt;/p&gt; 
&lt;h2&gt;Example use case: Conformer Search&lt;/h2&gt; 
&lt;p&gt;Promethium currently features nine different chemistry simulations focused on molecular properties, nonbonded interactions, and chemical reactions. Molecular property simulations include conformer searches (featured here), geometry optimizations, single point energies, and torsion scans. Non-bonded interactions include SAPT / F-SAPT; and reaction simulations include reaction path optimizations, transition states, and reactant-product transition states. Additional calculations, such as Vibrational Frequency and thermodynamics, are available for all workflows. This list is continuously growing as customers request new capabilities, such as spectroscopy, ligand ranking, and more.&lt;/p&gt; 
&lt;p&gt;One common problem that chemists want to solve when designing new molecules is to understand the different 3D shapes that a molecule can take and how often each shape appears. Chemists can use Promethium’s Conformer Search capability to identify all of the molecule’s stable shapes (lowest relative energy) and their prevalence (Boltzmann probability distribution).&lt;/p&gt; 
&lt;p&gt;Based on a given molecule input, Promethium goes through several stages of calculations and filtering. Each stage gets more precise until it has the final distribution of conformers with DFT-level accuracy. Promethium completes this process in three primary steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Launching Amazon EC2 instances for parallel processing&lt;/li&gt; 
 &lt;li&gt;Identifying and filtering potential conformers&lt;/li&gt; 
 &lt;li&gt;Analyzing conformers and visualizing the results&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;First, Promethium launches Amazon EC2 instances based on the number of molecules submitted. Customers can simultaneously submit as many molecules as they desire. Simultaneous submissions are parallelized across available EC2 instances. Within each molecule’s conformer search, Promethium parallelizes the DFT steps across multiple GPUs to ensure low wall clock time. Promethium uses a queuing system to manage submitted jobs and Amazon EC2 capacity.&lt;/p&gt; 
&lt;div id="attachment_3248" style="width: 1187px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3248" loading="lazy" class="size-full wp-image-3248" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/19/CleanShot-2024-01-19-at-14.33.31.png" alt="Figure 3: Promethium workflow diagram of processing parallel conformer searches in parallel. " width="1177" height="415"&gt;
 &lt;p id="caption-attachment-3248" class="wp-caption-text"&gt;Figure 3: Promethium workflow diagram of processing conformer searches in parallel.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The diagram represents the following workflow:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Customers submit multiple molecules for conformer searches&lt;/li&gt; 
 &lt;li&gt;Promethium scales Amazon EC2 instances to parallelize submitted jobs&lt;/li&gt; 
 &lt;li&gt;Promethium scales additional EC2 instances within each single job&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Second, Promethium identifies and filters potential conformers over four stages. In the first stage, Promethium generates a large set of potential conformers. These are subsequently filtered with a force field (FF), Artificial Narrow Intelligence (ANI) neural network potentials, or Geometry, Frequency, and Noncovalent Interaction (GFN) methods. This removes the conformers that are the least stable and leaves a smaller set to evaluate with DFT. Promethium evaluates the remaining conformers with two high accuracy DFT filters to determine the conformers with the lowest relative energy values. These conformers have the highest stability.&lt;/p&gt; 
&lt;p&gt;Third, Promethium analyzes the results. The solution visualizes the three-dimensional structures, the stage progression, probability distribution and relative energies of each conformer.&lt;/p&gt; 
&lt;div id="attachment_3249" style="width: 1111px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3249" loading="lazy" class="size-full wp-image-3249" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/19/CleanShot-2024-01-19-at-14.34.01.png" alt="Figure 4: One of Promethium’s output charts from a conformer search." width="1101" height="739"&gt;
 &lt;p id="caption-attachment-3249" class="wp-caption-text"&gt;Figure 4: One of Promethium’s output charts from a conformer search.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The graphic represents the relative energy of possible conformers as they are filtered by a force field (1&lt;sup&gt;st&lt;/sup&gt; row), an ANI filter (2&lt;sup&gt;nd&lt;/sup&gt; row), and two separate DFT filters (3&lt;sup&gt;rd&lt;/sup&gt; and 4&lt;sup&gt;th&lt;/sup&gt; rows).&lt;/p&gt; 
&lt;p&gt;The following graphic shows the visualization of the Boltzmann weight and probability distribution for the conformers that passed the final filter stage.&lt;/p&gt; 
&lt;div id="attachment_3250" style="width: 1129px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3250" loading="lazy" class="size-full wp-image-3250" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/19/CleanShot-2024-01-19-at-14.34.56.png" alt="Figure 5: Probability distribution of the computed conformers." width="1119" height="879"&gt;
 &lt;p id="caption-attachment-3250" class="wp-caption-text"&gt;Figure 5: Probability distribution of the computed conformers.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Pricing&lt;/h2&gt; 
&lt;p&gt;Promethium uses a consumption-based pricing model, which means that customers only pay based on the actual GPU-hours used (accrued by the second). There are no upfront costs, no annual costs, and no per user or per seat costs. The total cost of ownership is often &lt;a href="https://www.promethium.qcware.com/pricing-details"&gt;lower&lt;/a&gt; than using legacy open source DFT software.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we started with the crucial opportunities and challenges of computational chemistry and its role in advancing pharmaceuticals and chemicals. We outlined Promethium, a SaaS solution running on Amazon EC2 GPU instances. Promethium takes a step towards resolving the speed-accuracy tradeoff, a longstanding obstacle in computational chemistry.&lt;/p&gt; 
&lt;p&gt;We used a conformer search example to demonstrate Promethium tackling the challenges associated with larger and complex molecules through GPU-optimized algorithms. Promethium’s scalable architecture and solution approach make it a practical tool in molecule discovery. If you want to learn more, you can visit &lt;a href="https://www.promethium.qcware.com/"&gt;Promethium’s homepage&lt;/a&gt; or find additional details through &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-tyzdk5juffetu"&gt;Promethium on AWS Marketplace&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;References&lt;/h3&gt; 
&lt;p&gt;[1]: &lt;a href="https://www.nature.com/articles/d41573-022-00025-1"&gt;https://www.nature.com/articles/d41573-022-00025-1&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[2]: &lt;a href="https://www.bcg.com/publications/2022/ai-in-drug-discovery-impact"&gt;https://www.bcg.com/publications/2022/ai-in-drug-discovery-impact&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[3]: &lt;a href="https://www.mckinsey.com/industries/life-sciences/our-insights/pharmas-digital-rx-quantum-computing-in-drug-research-and-development"&gt;https://www.mckinsey.com/industries/life-sciences/our-insights/pharmas-digital-rx-quantum-computing-in-drug-research-and-development&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[4]: Werner Vogel at AWS re:Invent 2022 &lt;a href="https://youtu.be/RfvL_423a-I?t=6553"&gt;https://youtu.be/RfvL_423a-I?t=6553&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[5]: &lt;a href="https://www.bcg.com/publications/2023/enterprise-grade-quantum-computing-almost-ready"&gt;https://www.bcg.com/publications/2023/enterprise-grade-quantum-computing-almost-ready&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[6]: &lt;a href="https://onlinelibrary.wiley.com/doi/10.1002/qua.26381"&gt;https://onlinelibrary.wiley.com/doi/10.1002/qua.26381&lt;/a&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Leveraging Seqera Platform on AWS Batch for machine learning workflows – Part 2 of 2</title>
		<link>https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-2-of-2/</link>
		
		<dc:creator><![CDATA[Ben Sherman]]></dc:creator>
		<pubDate>Tue, 23 Jan 2024 16:01:54 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Genomics]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<guid isPermaLink="false">0082e40ba30d67f985dcb877d3cb64053a9b9f0a</guid>

					<description>In this second part of using Nextflow for machine learning for life science workloads, we provide a step-by-step guide, explaining how you can easily deploy a Seqera environment on AWS to run ML and other pipelines.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3236" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/18/Batch-and-Seqera.png" alt="Batch and Seqera" width="380" height="190"&gt;This post was contributed by Dr Ben Sherman (Seqera) and Dr Olivia Choudhury (AWS), Paolo Di Tomasso, and Gord Sissons from Seqera, and Aniket Deshpande and Abhijit Roy from AWS.&lt;br&gt; &lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;In &lt;a href="https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-1-of-2/"&gt;part one of this two-part series&lt;/a&gt;, we explained how Nextflow and &lt;a href="https://seqera.io/platform/"&gt;Seqera Platform&lt;/a&gt; work with AWS, and we provided a sample Nextflow pipeline that performs ML model training and inference for image analysis to illustrate how Nextflow can support custom ML-based workflows. We also discussed how AWS customers are using this today.&lt;/p&gt; 
&lt;p&gt;In this second post, we will provide a step-by-step guide explaining how users new to Seqera Platform can rapidly get started on AWS, maximizing the use of &lt;a href="https://aws.amazon.com/batch"&gt;AWS Batch&lt;/a&gt;, &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service (Amazon S3&lt;/a&gt;), and other AWS services.&lt;/p&gt; 
&lt;p&gt;Depending on your familiarity with AWS, these steps should take around an hour to complete.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before following the step-by-step guide here, readers should:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Have an AWS account and be familiar with using the &lt;a href="https://aws.amazon.com/console/"&gt;AWS Management Console&lt;/a&gt; or &lt;a href="https://aws.amazon.com/cli/"&gt;A&lt;/a&gt;&lt;a href="https://aws.amazon.com/cli/"&gt;WS&lt;/a&gt;&lt;a href="https://aws.amazon.com/cli/"&gt; Command Line Interface&lt;/a&gt; (CLI).&lt;/li&gt; 
 &lt;li&gt;Have a high-level understanding of &lt;a href="https://aws.amazon.com/iam"&gt;AWS Identity and Access Management&lt;/a&gt; (IAM) users and roles.&lt;/li&gt; 
 &lt;li&gt;Obtain a free &lt;a href="https://tower.nf"&gt;Seqera Cloud&lt;/a&gt; account for testing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Step-by-step guide&lt;/h2&gt; 
&lt;p&gt;This section explains how to set up an AWS environment to support pipeline execution, obtain a free Seqera Cloud account, and configure Seqera to launch and manage the ML pipeline introduced in part 1 of this series.&lt;/p&gt; 
&lt;h3&gt;Create an Amazon S3 bucket&lt;/h3&gt; 
&lt;p&gt;The first step is to create an S3 bucket in your preferred AWS region (we used &lt;em&gt;us-east&lt;/em&gt;).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Once logged into the AWS console, navigate to the &lt;strong&gt;Amazon S3&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Click on the &lt;strong&gt;Create bucket&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Give the bucket a globally unique name (for example, we used &lt;code&gt;seqera-work-bucket&lt;/code&gt;, but you will need to choose something else) and select the AWS region where the bucket will reside.&lt;/li&gt; 
 &lt;li&gt;You can leave ACLs disabled and block all public access to the bucket since Seqera Platform will use this bucket internally. Others do not need to see the contents.&lt;/li&gt; 
 &lt;li&gt;Accept the default for the remaining settings and select &lt;strong&gt;Create bucket&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once the bucket is created, select it and record its &lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html"&gt;Amazon Resource Name&lt;/a&gt; (ARN) by clicking &lt;strong&gt;Copy ARN&lt;/strong&gt;. In our example, the ARN is &lt;code&gt;arn:aws:s3:::seqera-work-bucket&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You will need the name of the ARN to set up an AWS policy in the following steps.&lt;/p&gt; 
&lt;h3&gt;Setup your IAM credentials&lt;/h3&gt; 
&lt;p&gt;To create an AWS Batch environment and marshal other AWS cloud resources, Seqera will need AWS credentials. We’ll use the &lt;a href="https://docs.aws.amazon.com/iam/"&gt;AWS Identity and Access Management&lt;/a&gt; (IAM) service to create appropriate IAM policies and attach these policies to an AWS IAM user or role.&lt;/p&gt; 
&lt;p&gt;First, create a policy with sufficient privileges to manage the AWS Batch environment and support pipeline execution. Follow the steps below to create a custom “&lt;em&gt;seqera_forge_policy&lt;/em&gt;.”&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Navigate to the IAM console by searching for the &lt;strong&gt;IAM&lt;/strong&gt;&lt;strong&gt; service&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Access Management&lt;/strong&gt;, select &lt;strong&gt;Policies&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Click on &lt;strong&gt;Create policy&lt;/strong&gt; to create a new IAM policy&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Specify permissions&lt;/strong&gt;, select the &lt;strong&gt;JSON&lt;/strong&gt;&lt;strong&gt; policy editor&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Copy the contents of the default &lt;code&gt;forge-policy.json&lt;/code&gt; file &lt;a href="https://github.com/seqeralabs/nf-tower-aws/blob/master/forge/forge-policy.json"&gt;located on GitHub&lt;/a&gt; into the policy editor&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Next&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Give the policy a name (like &lt;code&gt;Seqera_Forge_Policy&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Select &lt;strong&gt;Create policy&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that this is a sample policy and you should review the policy based on your security requirements, particularly if you want to use it for production.&lt;/p&gt; 
&lt;p&gt;With these steps, you have created a policy to give Seqera sufficient permissions to deploy an AWS Batch environment on your behalf.&lt;/p&gt; 
&lt;p&gt;Next, you’ll need to create a similar policy that gives your IAM user or role permission to access the Amazon S3 bucket you created earlier.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Repeat the steps in IAM as before to create a new policy for accessing the S3 bucket by clicking &lt;strong&gt;Create policy&lt;/strong&gt; from the &lt;strong&gt;Policies&lt;/strong&gt; screen&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Specify permissions&lt;/strong&gt;, select the &lt;strong&gt;JSON&lt;/strong&gt;&lt;strong&gt; policy editor&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Copy the contents of the default &lt;code&gt;s3-bucket-write.json&lt;/code&gt; file &lt;a href="https://github.com/seqeralabs/nf-tower-aws/blob/master/launch/s3-bucket-write.json"&gt;located on GitHub&lt;/a&gt; into the policy editor&lt;/li&gt; 
 &lt;li&gt;Update the ARN in the &lt;code&gt;S3-bucket-write&lt;/code&gt; policy to reflect the ARN for the S3 bucket you created&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Next&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Give the policy a name (like &lt;code&gt;Seqera_S3_Bucket_Write&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Select &lt;strong&gt;Create policy&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You should now have two new customer-managed policies, as shown in Figure 1.&lt;/p&gt; 
&lt;div id="attachment_3221" style="width: 1089px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3221" loading="lazy" class="size-full wp-image-3221" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.31.29.png" alt="Figure 1: Create IAM policies to manage AWS Batch environment and execute pipelines." width="1079" height="367"&gt;
 &lt;p id="caption-attachment-3221" class="wp-caption-text"&gt;Figure 1: Create IAM policies to manage AWS Batch environment and execute pipelines.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Next, attach these policies to an IAM user or IAM role. You may already have your own IAM users or roles defined, in which case you can bind the policies to existing users or roles.&lt;/p&gt; 
&lt;p&gt;If you are doing this for the first time, follow these steps to create a new IAM user called &lt;code&gt;seqera-user,&lt;/code&gt; and bind the policies created above to that user like this:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;From the AWS console, navigate to the &lt;strong&gt;IAM&lt;/strong&gt;&lt;strong&gt; console&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Users&lt;/strong&gt;, select &lt;strong&gt;Create user&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Give the user a name like &lt;code&gt;seqera-user&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;There is no need to provide this IAM user with access to the AWS Management Console&lt;/li&gt; 
 &lt;li&gt;Select &lt;strong&gt;Attach policies directly&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Search for the two customer-managed policies you created earlier by searching on the string Seqera and then select these two policies to apply them to the new IAM user&lt;/li&gt; 
 &lt;li&gt;After attaching the policies, select &lt;strong&gt;Create user&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The final step is to create an access key based on the new IAM user that will be used by Seqera:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Under &lt;strong&gt;Access management&lt;/strong&gt;&lt;strong&gt; / &lt;/strong&gt;&lt;strong&gt;Users&lt;/strong&gt;, select &lt;strong&gt;seqera-user&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Record the user’s ARN for future reference&lt;/li&gt; 
 &lt;li&gt;Click on &lt;strong&gt;Create Access key&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Record the &lt;strong&gt;Access key&lt;/strong&gt; and &lt;strong&gt;Secret access key&lt;/strong&gt; and &lt;em&gt;store them in a safe place&lt;/em&gt;. You’ll need these credentials to create a compute environment in the Seqera Platform.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a free Seqera Cloud account&lt;/h3&gt; 
&lt;p&gt;The next step is to create a free Seqera Cloud account. Navigate to &lt;a href="https://seqera.io"&gt;https://seqera.io&lt;/a&gt; and click on &lt;strong&gt;Login / Sign up&lt;/strong&gt;. You can create a free account and sign into the Seqera cloud environment hosted on AWS infrastructure using GitHub or Google credentials – or by providing an email address.&lt;/p&gt; 
&lt;p&gt;After logging in, you’ll be directed to the Seqera Platform console.&lt;/p&gt; 
&lt;h3&gt;Setup an AWS Batch Compute Environment in Seqera&lt;/h3&gt; 
&lt;p&gt;On initial login to the Seqera Platform, you’ll be taken to a community showcase workspace with ready-to-run pipelines and preconfigured AWS compute environments. New users can experiment with community showcase pipelines and Datasets and enjoy up to 100 hours of free AWS cloud credits.&lt;/p&gt; 
&lt;div id="attachment_3222" style="width: 1095px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3222" loading="lazy" class="size-full wp-image-3222" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.32.39.png" alt="Figure 2: Log in to Seqera Platform to use community showcase pipelines or add your own pipelines." width="1085" height="680"&gt;
 &lt;p id="caption-attachment-3222" class="wp-caption-text"&gt;Figure 2: Log in to Seqera Platform to use community showcase pipelines or add your own pipelines.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;You can also watch a &lt;a href="https://www.youtube.com/watch?v=bZUHuCSKFpA"&gt;short introductory video&lt;/a&gt; that explains how to get started with the Seqera interface.&lt;/p&gt; 
&lt;p&gt;To add your own compute environments and pipelines, you’ll need to navigate to your personal workspace, as shown in Figure 2. Click on the selector next to &lt;strong&gt;community | showcase&lt;/strong&gt; and select your personal workspace by selecting your name where it appears under &lt;strong&gt;user&lt;/strong&gt;. You can optionally create a new workspace and store your compute environments and pipelines there.&lt;/p&gt; 
&lt;p&gt;The first time you login, your personal workspace will have no compute environments, datasets, or pipelines. You can create a new AWS Batch Compute Environment (CE) by following these steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Select the &lt;strong&gt;Compute Environments&lt;/strong&gt; tab from within your personal workspace&lt;/li&gt; 
 &lt;li&gt;Click on &lt;strong&gt;Add Compute Environment&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Define a new AWS Batch compute environment, by completing the form, as shown in Figure 3:&lt;/p&gt; 
&lt;div id="attachment_3223" style="width: 1102px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3223" loading="lazy" class="size-full wp-image-3223" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.33.12.png" alt="Figure 3: Create a new compute environment." width="1092" height="309"&gt;
 &lt;p id="caption-attachment-3223" class="wp-caption-text"&gt;Figure 3: Create a new compute environment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The first time you add user credentials, you’ll be asked to supply the IAM Access key and Secret access key you generated earlier.&lt;/p&gt; 
&lt;p&gt;Seqera Platform will store these credentials securely and ask you to assign a name to the credentials for future reference. Your AWS credentials will &lt;u&gt;not&lt;/u&gt; be visible to you, or other users.&lt;/p&gt; 
&lt;p&gt;Continue filling in the form, as illustrated in Figure 4:&lt;/p&gt; 
&lt;div id="attachment_3224" style="width: 1036px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3224" loading="lazy" class="size-full wp-image-3224" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.33.49.png" alt="Figure 4: Add credentials to store access keys and tokens for your compute environment." width="1026" height="541"&gt;
 &lt;p id="caption-attachment-3224" class="wp-caption-text"&gt;Figure 4: Add credentials to store access keys and tokens for your compute environment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Nextflow pipelines running on AWS Batch typically use Amazon S3 storage. Nextflow supports S3 natively, automatically copying data in and out of S3 and making it accessible to containerized bioinformatics tools.&lt;/p&gt; 
&lt;p&gt;To provide more efficient data handling, we use Seqera’s &lt;a href="https://seqera.io/fusion/"&gt;Fusion file system&lt;/a&gt; (Fusion v2) when setting up the pipeline. Fusion is a virtual, lightweight, distributed filesystem that allows any existing application to access object storage using the standard POSIX interface. Using the Fusion file system is optional but it’s recommended because it reduces the pipeline execution time and your cost by avoiding overhead related to data movement. If you use the Fusion file system, you must also enable &lt;a href="https://seqera.io/wave/"&gt;Wave containers&lt;/a&gt; in the compute environment since the Fusion client is deployed in a Wave container.&lt;/p&gt; 
&lt;p&gt;You can learn more about Fusion file system in the whitepaper &lt;a href="https://seqera.io/whitepapers/breakthrough-performance-and-cost-efficiency-with-the-new-fusion-file-system/"&gt;Breakthrough performance and cost-efficiency with the new Fusion file system&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Under &lt;strong&gt;Config mode&lt;/strong&gt;, select &lt;strong&gt;Batch Forge&lt;/strong&gt; to have Seqera automatically configure the AWS Batch account on your behalf.&lt;/p&gt; 
&lt;p&gt;For the provisioning model, select &lt;strong&gt;Spot&lt;/strong&gt;, as shown in Figure 5. Since Nextflow pipeline tasks can tolerate Amazon Elastic Compute Cloud (Amazon EC2) Spot Instances being interrupted, we recommend you use Spot Instances in most cases.&lt;/p&gt; 
&lt;div id="attachment_3225" style="width: 1082px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3225" loading="lazy" class="size-full wp-image-3225" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.34.17.png" alt="Figure 5: Select Batch Forge for Config mode and Spot for Provisioning model." width="1072" height="256"&gt;
 &lt;p id="caption-attachment-3225" class="wp-caption-text"&gt;Figure 5: Select Batch Forge for Config mode and Spot for Provisioning model.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3226" style="width: 1068px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3226" loading="lazy" class="size-full wp-image-3226" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.34.49.png" alt="Figure 6: Select default values for the rest of the configuration options." width="1058" height="566"&gt;
 &lt;p id="caption-attachment-3226" class="wp-caption-text"&gt;Figure 6: Select default values for the rest of the configuration options.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Accept the defaults for the remainder of the configuration options, as shown in Figure 6.&lt;/p&gt; 
&lt;p&gt;Once you’ve provided details of your Batch Compute Environment, click &lt;strong&gt;Add&lt;/strong&gt; to add the CE.&lt;/p&gt; 
&lt;p&gt;Assuming you have valid AWS credentials and have filled in the form correctly, you should see a new Compute Environment appear in your Seqera workspace.&lt;/p&gt; 
&lt;p&gt;After creating the Compute Environment in Seqera, you can login into the AWS Console and navigate to AWS Batch. Assuming you selected Spot provisioning, two new AWS Batch CEs – and queues – will have been created on your behalf.&lt;/p&gt; 
&lt;p&gt;Seqera Forge configures a &lt;em&gt;Head queue&lt;/em&gt; that dispatches Nextflow head jobs to a compute environment configured to use EC2 On-Demand instances. This prevents the Nextflow head job from being interrupted during execution.&lt;/p&gt; 
&lt;p&gt;A separate &lt;em&gt;Work queue&lt;/em&gt; and Spot-based CE are also automatically configured to support Nextflow compute tasks during pipeline execution.&lt;/p&gt; 
&lt;p&gt;The names of the queues and compute environments are assigned by Seqera Forge, like in Figure 7:&lt;/p&gt; 
&lt;div id="attachment_3227" style="width: 1098px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3227" loading="lazy" class="size-full wp-image-3227" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.35.34.png" alt="Figure 7: Assign queues and compute environments with Seqera Forge." width="1088" height="373"&gt;
 &lt;p id="caption-attachment-3227" class="wp-caption-text"&gt;Figure 7: Assign queues and compute environments with Seqera Forge.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Add the ML pipeline to the Launchpad&lt;/h3&gt; 
&lt;p&gt;Now that we have an AWS Batch CE set up in our Seqera workspace, we can add the ML pipeline to the Seqera Launchpad.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;From your workspace in the Seqera UI, select the &lt;strong&gt;Launchpad&lt;/strong&gt; tab&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Add pipeline&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Give the pipeline a name like &lt;code&gt;ML-hyperopt-pipeline&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select the Compute environment created above&lt;/li&gt; 
 &lt;li&gt;Supply the pipeline’s repository URI – for our machine learning example, use &lt;code&gt;https://github.com/nextflow-io/hyperopt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Click on the &lt;strong&gt;Revision number&lt;/strong&gt; field, and select the tag or branch name retrieved from the source code manager to select a particular pipeline branch or version. Use &lt;strong&gt;master&lt;/strong&gt; in this example.&lt;/li&gt; 
 &lt;li&gt;Specify the S3 bucket you created earlier as the work directory (e.g. &lt;code&gt;s3://seqera-work-bucket&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Config profiles&lt;/strong&gt;, select &lt;strong&gt;wave&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Advanced Options&lt;/strong&gt; in the Nextflow config file field, optionally set &lt;code&gt;dag.enabled=true&lt;/code&gt; if you would like Nextflow to generate a DAG (directed acyclic graph) file&lt;/li&gt; 
 &lt;li&gt;Accept the defaults for the rest of the pipeline configuration options&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Add&lt;/strong&gt; to add the pipeline to the Launchpad&lt;/li&gt; 
 &lt;li&gt;You should see the new &lt;code&gt;ML-hyperopt-pipeline&lt;/code&gt; appear in the Seqera Launchpad in your workspace&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div id="attachment_3228" style="width: 1112px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3228" loading="lazy" class="size-full wp-image-3228" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.37.06.png" alt="Figure 8: Add ML pipeline to Seqera Launchpad." width="1102" height="441"&gt;
 &lt;p id="caption-attachment-3228" class="wp-caption-text"&gt;Figure 8: Add ML pipeline to Seqera Launchpad.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Run the pipeline&lt;/h2&gt; 
&lt;p&gt;To run the &lt;code&gt;ML-hyperopt-pipeline&lt;/code&gt;, click on &lt;strong&gt;Launch&lt;/strong&gt; beside the &lt;strong&gt;ML-hyperopt-pipeline&lt;/strong&gt; on the Launchpad.&lt;/p&gt; 
&lt;p&gt;After launching the pipeline, you can monitor execution under the &lt;strong&gt;Runs&lt;/strong&gt; tab. The pipeline will take a few minutes to start the Amazon EC2 instances supporting the AWS Batch CEs and begin dispatching tasks to Batch.&lt;/p&gt; 
&lt;p&gt;Assuming the pipeline runs successfully, you should see the following output in the &lt;strong&gt;Execution log&lt;/strong&gt; accessible through the Seqera interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;N E X T F L O W  ~  version 23.10.0
Pulling nextflow-io/hyperopt ...
 downloaded from https://github.com/nextflow-io/hyperopt.git
Launching `https://github.com/nextflow-io/hyperopt` [mighty_koch] DSL2 - revision: eb2b84a91a [master]

    M L - H Y P E R O P T   P I P E L I N E
    =======================================
    fetch_dataset   : true
    dataset_name    : wdbc

    visualize       : true

    train           : true
    train_data      : data/*.train.txt
    train_meta      : data/*.meta.json
    train_models    : [dummy, gb, lr, mlp, rf]

    predict         : true
    predict_models  : data/*.pkl
    predict_data    : data/*.predict.txt
    predict_meta    : data/*.meta.json

    outdir          : results
    
Monitor the execution with Nextflow Tower using this URL: https://tower.nf/user/gordon-sissons/watch/5MUVbQjbhEFjRu
[da/4b6a7f] Submitted process &amp;gt; fetch_dataset (wdbc)
[c5/78cd5e] Submitted process &amp;gt; split_train_test (wdbc)
[68/29cb37] Submitted process &amp;gt; train (wdbc/dummy)
[01/0b89d1] Submitted process &amp;gt; train (wdbc/mlp)
[37/7e1cc8] Submitted process &amp;gt; train (wdbc/lr)
[22/8452c6] Submitted process &amp;gt; train (wdbc/rf)
[6e/763ce3] Submitted process &amp;gt; visualize (1)
[2a/3f3d84] Submitted process &amp;gt; train (wdbc/gb)
[5a/1c71a0] Submitted process &amp;gt; visualize (2)
[cc/7dd9ad] Submitted process &amp;gt; predict (wdbc/dummy)
[b5/f30153] Submitted process &amp;gt; predict (wdbc/mlp)
[de/340f09] Submitted process &amp;gt; predict (wdbc/lr)
[e5/d205ef] Submitted process &amp;gt; predict (wdbc/rf)
[38/6a6eaf] Submitted process &amp;gt; predict (wdbc/gb)
The best model for ‘wdbc’ was ‘mlp’, with accuracy = 0.991

Done!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the pipeline trains each model, runs each model against the dataset, and determines the model with the best predictive accuracy.&lt;/p&gt; 
&lt;p&gt;If you’ve gotten this far: Congratulations! You’ve successfully set up your first compute environment in Seqera and used AWS Batch to execute a machine learning pipeline.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;For organizations collaborating on large-scale data analysis and ML workloads, Seqera Platform running on AWS can be an excellent solution. You can more easily deploy powerful AWS compute and storage resources at scale, while also reducing your costs through optimized resource usage, and managing spend across projects and teams. You can also leverage best-in-class curated bioinformatics pipelines and modules from &lt;a href="https://nf-co.re"&gt;&lt;em&gt;nf-core&lt;/em&gt;&lt;/a&gt; and other sources.&lt;/p&gt; 
&lt;p&gt;This can help improve research productivity with a unified view of data, pipelines, and results, while collaborating more effectively with local and remote teams.&lt;/p&gt; 
&lt;p&gt;We’re excited to know how this works out for you, or if you have other challenges that AWS or Seqera can help you meet. Contact us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt; if you want to discuss these with us.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Leveraging Seqera Platform on AWS Batch for machine learning workflows – Part 1 of 2</title>
		<link>https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-1-of-2/</link>
		
		<dc:creator><![CDATA[Ben Sherman]]></dc:creator>
		<pubDate>Tue, 23 Jan 2024 15:59:51 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Genomics]]></category>
		<guid isPermaLink="false">4153d168ace17c15a8bf48f252882386c3f15251</guid>

					<description>Nextflow is popular workflow framework for genomics pipelines, but did you know you can also use it for machine-learning? ML is already being used for medical imaging, protein folding, drug discovery, and gene editing. In this post, we explain how to build an example Nextflow pipeline that performs ML model-training and inference for image analysis.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3236" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/18/Batch-and-Seqera.png" alt="Batch and Seqera" width="380" height="190"&gt;This post was contributed by Dr Ben Sherman (Seqera) and Dr Olivia Choudhury (AWS), Paolo Di Tomasso, and Gord Sissons from Seqera, and Aniket Deshpande and Abhijit Roy from AWS.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Machine learning (ML) is used for multiple healthcare and life sciences (HCLS) applications, including medical imaging, protein folding, drug discovery, and gene editing. While &lt;a href="https://nextflow.io/"&gt;Nextflow&lt;/a&gt; pipelines (like those in &lt;a href="https://nf-co.re"&gt;nf-core&lt;/a&gt;) are commonly used for genomics, they are also being adopted for machine learning workloads.&lt;/p&gt; 
&lt;p&gt;Nextflow is an excellent solution for many ML-based scenarios. Sometimes you need to continuously (and automatically) retrain your models based on rapidly-changing datasets from external sources such as sequencers. Sometimes you need training and inference resources sporadically but you face constraints getting GPUs or FPGAs – even for short periods. And often pipelines like &lt;a href="https://nf-co.re/proteinfold/1.0.0"&gt;nf-core/proteinfold&lt;/a&gt; have compute and data-intensive inference steps where many samples need to be processed in parallel.&lt;/p&gt; 
&lt;p&gt;In the next two posts, we’ll show you how these kinds of challenges can be addressed using Nextflow and the &lt;a href="https://seqera.io/platform/"&gt;Seqera Platform&lt;/a&gt; integrated with AWS.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;In part one&lt;/strong&gt; of this two-part blog series, we explain how to build an example Nextflow pipeline that performs ML model-training and inference for image analysis, illustrating how Nextflow supports custom ML-based workflows. We also discuss how health care and life science customers are using this today.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;In &lt;a href="https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-2-of-2/"&gt;part two&lt;/a&gt;&lt;/strong&gt;, we’ll provide a step-by-step guide explaining how users new to the Seqera Platform can rapidly get started with AWS, maximizing the use of &lt;a href="https://aws.amazon.com/batch"&gt;AWS Batch&lt;/a&gt;, &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service (Amazon S3&lt;/a&gt;), and other AWS services.&lt;/p&gt; 
&lt;h2&gt;Seqera on AWS&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://seqera.io/platform/"&gt;Seqera Platform&lt;/a&gt; (previously &lt;em&gt;Nextflow Tower&lt;/em&gt;) is a comprehensive bioinformatics data analysis platform deeply integrated with AWS. Seqera is used by leading biotechnology and pharmaceutical companies globally, including 10 of the top 20 global BioPharmas and roughly 10,000 bioinformaticians across hundreds of organizations.&lt;/p&gt; 
&lt;p&gt;Seqera Platform has several key features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It is explicitly designed for Nextflow pipelines.&lt;/li&gt; 
 &lt;li&gt;It is cross platform – Seqera works with AWS and other cloud and HPC providers, including on-premises systems. It supports customer’s preferred container runtimes, registries, source code managers, and it uses multiple AWS services, including AWS Batch, Amazon S3, Amazon FSx for Lustre, Amazon Elastic File System (EFS), Amazon Elastic Kubernetes Service (EKS), AWS Secrets Manager and others.&lt;/li&gt; 
 &lt;li&gt;It has a large, active, user and developer community which provide high-quality curated &lt;a href="https://nf-co.re"&gt;nf-core&lt;/a&gt; pipelines and modules.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://seqera.io/platform/"&gt;Seqera Platform&lt;/a&gt; can be deployed in two different ways.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Seqera Cloud&lt;/strong&gt; is a fully managed service hosted exclusively on AWS infrastructure. Presently, there are 8,000+ corporate and research Seqera Cloud users. Researchers can use Seqera Cloud for free and progress to paid plans as their needs evolve.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Seqera Enterprise&lt;/strong&gt; is a customer-managed version of the Seqera platform that is deployable on-premises or on a customer’s preferred cloud. Some customers install Seqera on-premises, while others deploy Seqera on AWS using Docker Compose or the &lt;a href="https://aws.amazon.com/eks/"&gt;Amazon Elastic Kubernetes Service&lt;/a&gt; (EKS).&lt;/p&gt; 
&lt;p&gt;Seqera employs a “&lt;em&gt;bring your own credentials&lt;/em&gt;” model. As illustrated in the architecture diagram in Figure 1, users log into Seqera and add &lt;a href="https://help.tower.nf/23.2/compute-envs/overview/"&gt;compute environments&lt;/a&gt; by supplying credentials for their preferred cloud or HPC workload manager.&lt;/p&gt; 
&lt;div id="attachment_3210" style="width: 1102px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3210" loading="lazy" class="size-full wp-image-3210" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.10.09.png" alt="Figure 1: High-level architecture of Seqera on the AWS Cloud." width="1092" height="591"&gt;
 &lt;p id="caption-attachment-3210" class="wp-caption-text"&gt;Figure 1: High-level architecture of Seqera on the AWS Cloud.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Seqera Platform users have a private workspace and can be assigned to various shared workspaces, each with its own pipelines, datasets, and compute environments. Seqera sidesteps the complexity of running in different cloud or HPC environments by providing a consistent user experience regardless of the underlying infrastructure.&lt;/p&gt; 
&lt;p&gt;While most workloads run on-premises or on AWS infrastructure, this ability to deploy to different compute environments is useful for several reasons:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Customers can leverage on-premises HPC clusters and tap cloud capacity when their own resources are fully utilized.&lt;/li&gt; 
 &lt;li&gt;Research frequently involves datasets hosted on third-party clouds, making it more cost-effective to &lt;em&gt;bring the compute to the data&lt;/em&gt; rather than transferring large datasets to a local execution environment.&lt;/li&gt; 
 &lt;li&gt;Academic and research efforts frequently involve collaboration among institutions using different infrastructure. Seqera allows these users to seamlessly share pipelines, datasets, computing infrastructure, and research results without exposing private cloud credentials.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Seqera Forge&lt;/h2&gt; 
&lt;p&gt;While users can choose to run pipelines on pre-existing AWS Batch environments, Seqera Forge fully automates creating and configuring AWS Batch compute environments and queues for Nextflow pipelines, following best practices. Seqera can also dispose of cloud resources when they’re not in use, helping reduce costs.&lt;/p&gt; 
&lt;p&gt;By leveraging AWS APIs, Forge dramatically simplifies the deployment, configuration, and teardown of AWS infrastructure, making it possible for researchers with minimal knowledge of “CloudOps” to deploy cloud infrastructure themselves.&lt;/p&gt; 
&lt;h2&gt;A sample training dataset&lt;/h2&gt; 
&lt;p&gt;To illustrate how machine learning and inference workloads can be run on AWS using Seqera Platform, we used the &lt;a href="https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic"&gt;Wisconsin Diagnostic Breast Cancer&lt;/a&gt; (WDBC) dataset. This is a well-known dataset, often used as an example for learning or comparing different ML techniques, specifically for image classification. It consists of 589 samples, each with a set of 30 features taken from an image of a breast tissue. The diagnosis column in the data indicates whether the sample was benign (B) or malignant (M), as illustrated in Figure 2.&lt;/p&gt; 
&lt;div id="attachment_3211" style="width: 1131px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3211" loading="lazy" class="size-full wp-image-3211" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.10.50.png" alt="Figure 2: Images from Breast Cancer Wisconsin (Diagnostic) Dataset aligning with tabular data showing that samples are either malignant or benign." width="1121" height="328"&gt;
 &lt;p id="caption-attachment-3211" class="wp-caption-text"&gt;Figure 2: Images from Breast Cancer Wisconsin (Diagnostic) Dataset aligning with tabular data showing that samples are either malignant or benign.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In the sample pipeline, we will train and evaluate multiple models to classify these breast samples as benign or malignant. In a real-world scenario, we can also use k-fold cross-validation to evaluate each model on several randomized partitions of the dataset and use multiple performance metrics with minimum requirements to determine whether a model is “good enough” to be used in production.&lt;/p&gt; 
&lt;p&gt;For our purposes here, we will simply evaluate each model on a single 80/20 train/test split and select the model with the highest test accuracy.&lt;/p&gt; 
&lt;h2&gt;A sample pipeline&lt;/h2&gt; 
&lt;p&gt;For illustration purposes, we use a simple proof-of-concept pipeline called &lt;a href="https://github.com/nextflow-io/hyperopt"&gt;Hyperopt&lt;/a&gt; developed by Seqera Labs. The pipeline takes any tabular dataset as input (or the name of a dataset on &lt;a href="https://www.openml.org/"&gt;OpenML&lt;/a&gt;). It then trains and evaluates a set of ML models on the dataset, reporting the model that achieved the highest test accuracy. You can learn more about this pipeline in the article &lt;a href="https://seqera.io/blog/nextflow-and-tower-for-machine-learning/"&gt;Nextflow and Tower for Machine Learning&lt;/a&gt;. The pipeline code is available on &lt;a href="https://github.com/nextflow-io/hyperopt"&gt;GitHub&lt;/a&gt;. Figure 3 shows a &lt;a href="https://mermaid.js.org/"&gt;Mermaid diagram&lt;/a&gt;&lt;u&gt;, automatically generated by Nextflow,&lt;/u&gt; of the overall pipeline.&lt;/p&gt; 
&lt;div id="attachment_3212" style="width: 643px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3212" loading="lazy" class="size-full wp-image-3212" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.11.37.png" alt="Figure 3: The pipeline steps are implemented as Python scripts that use several common packages for ML, including numpy, pandas, scikit-learn, and matplotlib. These dependencies are defined in a Conda environment file called conda.yml." width="633" height="784"&gt;
 &lt;p id="caption-attachment-3212" class="wp-caption-text"&gt;Figure 3: The pipeline steps are implemented as Python scripts that use several common packages for ML, including numpy, pandas, scikit-learn, and matplotlib. These dependencies are defined in a Conda environment file called conda.yml.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;By default, the pipeline uses the &lt;a href="https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic"&gt;WDBC&lt;/a&gt; dataset described above and evaluates five different classification models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"&gt;baseline model&lt;/a&gt; (i.e. “dummy” model) that simply predicts the most common label&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier"&gt;Gradient Boosting&lt;/a&gt; (gb)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"&gt;Logistic Regression&lt;/a&gt; (lr)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier"&gt;Multi-layer Perceptron&lt;/a&gt; (mlp) (i.e. neural network)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn-ensemble-randomforestclassifier"&gt;Random Forest&lt;/a&gt; (rf)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When you run the pipeline, you should see something like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ nextflow run hyperopt -profile wave
[...]
The best model for ‘wdbc’ was ‘mlp’, with accuracy = 0.991
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This shows that Nextflow ran a pipeline that trained different ML models on the WDBC dataset and evaluated their performance during model inference. Multi-layer perceptron was most accurate in classifying breast tumor images as benign or malignant. For further details of the pipeline and its deployment, refer to part two of this blog series.&lt;/p&gt; 
&lt;p&gt;While the hyperopt pipeline implements a simple classification model, it provides all the building blocks you need to create your own ML pipelines with Nextflow.&lt;/p&gt; 
&lt;p&gt;Seqera is also an excellent solution for deploying GPU-based workloads in the AWS cloud. For a hands-on tutorial, see the article &lt;a href="https://seqera.io/blog/running-ai-workloads-in-the-cloud-with-nextflow-tower-a-step-by-step-guide/"&gt;Running AI workloads in the cloud with Nextflow Tower — a step-by-step guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;It’s the customers that matter most&lt;/h2&gt; 
&lt;p&gt;Seqera is used by hundreds of pharmaceutical, healthcare, and biotech companies to run data analysis pipelines in the AWS Cloud. According to the latest &lt;a href="https://seqera.io/blog/the-state-of-the-workflow-2023-community-survey-results/"&gt;2023 State of the Workflow Survey&lt;/a&gt;, AWS is the most popular cloud environment among Nextflow users, with 49% of all Nextflow users surveyed already using or planning to use AWS within the next two years and 35.1% of survey respondents using AWS Batch [1,2]. The survey results showed strong cloud adoption, with the percentage of Nextflow users running in the cloud up 20% over 2022 [2].&lt;/p&gt; 
&lt;p&gt;Among the customers running Seqera and Nextflow on AWS are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arcusbio.com/"&gt;Arcus Biosciences&lt;/a&gt;—Arcus Biosciences is at the forefront of designing combination therapies, with best-in-class potential, in the pursuit of cures for cancer. By using Seqera Platform on AWS, Arcus was able to improve productivity, ensure pipeline traceability, and use cloud resources more efficiently. They were also able to prepare for future growth by scaling capacity for research and clinical trials while providing an intuitive, collaborative user experience to researchers and clinicians. &lt;a href="https://seqera.io/case-studies/arcus-biosciences/"&gt;Read the case study here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gritstonebio.com/"&gt;Gritstone Bio&lt;/a&gt;—Gritstone Bio is developing targeted immunotherapies for cancer and infectious disease. Gritstone’s approach seeks to generate a therapeutic immune response by leveraging insights into the immune system’s ability to recognize and destroy diseased cells by targeting select antigens. Their workloads involve massive compute requirements for analysis of individual biopsies and makes extensive use of machine learning for tumor classification models. Gritstone use Seqera and multiple AWS cloud services to manage their bioinformatics pipelines. &lt;a href="https://seqera.io/case-studies/gritstone/"&gt;Read the case study here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tesseratherapeutics.com/"&gt;Tessera Therapeutics&lt;/a&gt;—Tessera Therapeutics are pioneers in a new category of genetic medicine and rely heavily on genomic analysis pipelines to identify promising new treatments. By using Seqera Platform to manage analysis pipelines on AWS, Tessera increased its analysis throughput and research productivity while simultaneously containing cloud spending by using resources more efficiently. You can &lt;a href="https://seqera.io/case-studies/tessera-therapeutics/"&gt;read the case study here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;For organizations collaborating on large-scale data analysis and ML workloads, Seqera on AWS is an excellent solution. You can easily deploy powerful AWS compute and storage resources at scale, reduce costs through optimized resource usage, and manage spending across projects and teams.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;In &lt;a href="https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-2-of-2/"&gt;part two&lt;/a&gt;&lt;/strong&gt; of this blog series, we will provide a step-by-step guide, explaining how you can easily deploy a Seqera environment on AWS to run ML pipelines like the one above, and other Nextflow pipelines.&lt;/p&gt; 
&lt;h3&gt;References&lt;/h3&gt; 
&lt;p&gt;[1] &lt;a href="https://seqera.io/blog/the-state-of-the-workflow-2023-community-survey-results/"&gt;The State of the Workflow 2023: Community Survey Results&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;[2] &lt;a href="https://seqera.io/blog/state-of-the-workflow-2022-results/"&gt;The State of the Workflow 2022: Community Survey Results&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Save up to 90% using EC2 Spot, even for long-running HPC jobs</title>
		<link>https://aws.amazon.com/blogs/hpc/save-up-to-90-using-ec2-spot-even-for-long-running-hpc-jobs/</link>
		
		<dc:creator><![CDATA[Eran Brown]]></dc:creator>
		<pubDate>Tue, 16 Jan 2024 16:08:38 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[EC2]]></category>
		<category><![CDATA[EC2 Spot]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">17797086c47ab3a6385b921c1983c198c3da1c2c</guid>

					<description>New OS-level checkpointing tools can let you run existing HPC codes on EC2 Spot instances with minimal impact from interruptions. Read on for the details.</description>
										<content:encoded>&lt;p&gt;Amazon Elastic Compute Cloud (Amazon EC2) &lt;a href="https://aws.amazon.com/ec2/spot/"&gt;Spot Instances&lt;/a&gt; enable AWS customers to save up to 90% of their compute cost by using spare EC2 capacity. They are really popular with HPC customers who use a lot of compute every day. But long running HPC jobs often can’t survive an &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html"&gt;EC2 Spot interruption&lt;/a&gt;. This makes it hard for them to benefit from all this low cost and large volume of compute.&lt;/p&gt; 
&lt;p&gt;Amazon EC2 Spot interruptions do provide a 2-minute warning, though, that you can detect from &lt;a href="///Users/eranbr/Downloads/Conclusion"&gt;within your running instance&lt;/a&gt;, or externally using &lt;a href="https://aws.amazon.com/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt;. Some HPC tools are starting to handle these interruptions by saving their state and allowing the next instance to resume work from that point.&lt;/p&gt; 
&lt;p&gt;But what about the much larger number of HPC tools that don’t have this capability? Can we add them into the mix – to make them fault-tolerant without changing the applications? AWS technology partners such as&amp;nbsp;&lt;a href="https://aws.amazon.com/marketplace/seller-profile?id=3b7c724c-fae7-4187-ae45-de1625e51395"&gt;MemVerge&lt;/a&gt;&amp;nbsp;offer solutions for checkpointing at the VM level. They capture the process tree and the working directory (like local temp files) and write it to shared storage. By handling this at the VM level, they allow your existing HPC tools to run to completion unaware of Spot interruptions. This opens savings opportunities for AWS customers using EC2 Spot instances.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll show you how these new technologies can help you optimize HPC compute costs by using EC2 Spot Instances. Depending on your environment, you may not need to modify your applications to do it. We’ll cover the underlying technologies, their operational implications, and their limitations.&lt;/p&gt; 
&lt;h2&gt;Outline of the checkpoint/restore process&lt;/h2&gt; 
&lt;p&gt;Figure 1 describes the process: When a Spot interruption happens, EC2 sends an event to Amazon CloudWatch. A monitoring script inside the instance identifies this from the instance metadata (1). This creates a checkpoint, stored in a shared file system (2). The HPC scheduler will detect the execution node was terminated, however the HPC tools will still be unaware of Spot interruptions. (3). It’ll call the EC2 API to launch a new Spot instance (5). The new instance will detect a checkpoint exists (6), and will restore it (instead of starting the job from scratch). The job now continues, and communicates with the license server (7) for the duration of the job.&lt;/p&gt; 
&lt;div id="attachment_3199" style="width: 1094px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3199" loading="lazy" class="size-full wp-image-3199" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/12/CleanShot-2024-01-12-at-14.28.33.png" alt="Figure 1 – The checkpoint / restore process, which uses the Spot interruption notification to force a checkpoint to take place and recycles the job onto a new instance when one is available." width="1084" height="585"&gt;
 &lt;p id="caption-attachment-3199" class="wp-caption-text"&gt;Figure 1 – The checkpoint / restore process, which uses the Spot interruption notification to force a checkpoint to take place and recycles the job onto a new instance when one is available.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;How we tested this&lt;/h2&gt; 
&lt;p&gt;This blog post is the result of running MemVerge with an HPC code over a few weeks, slowly incrementing the complexity of the solution. Our goal was to minimize changes to the existing HPC workflow.&lt;/p&gt; 
&lt;p&gt;Where changes are required we’ll mention them as we go along.&lt;/p&gt; 
&lt;p&gt;Our test introduced one new element at each step to be able to identify which steps impacted the runtime the most. We repeated each step at least 15 times to identify variability in the results:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Baseline: test a “clean” copy of the environment (no additional checkpointing tools installed)&lt;/li&gt; 
 &lt;li&gt;Test the checkpointing tool binaries installed, but not running&lt;/li&gt; 
 &lt;li&gt;Test with the tool running, but without creating checkpoints&lt;/li&gt; 
 &lt;li&gt;Test with a checkpoint, restore on the &lt;em&gt;same&lt;/em&gt; node&lt;/li&gt; 
 &lt;li&gt;Test with a checkpoint restore on a &lt;em&gt;different&lt;/em&gt; node&lt;/li&gt; 
 &lt;li&gt;Create multiple checkpoints in the same run, resume each one on a different node&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With two exceptions (which we’ll discuss in minute), we didn’t experience longer runtimes from checkpointing. We chose to limit testing to a single worker node. See “&lt;em&gt;Workloads spanning multiple nodes&lt;/em&gt;” near the end of this post for more on that.&lt;/p&gt; 
&lt;h2&gt;Operational considerations&lt;/h2&gt; 
&lt;p&gt;We learned a lot from this lab work – mainly things that you can work around with adequate planning.&lt;/p&gt; 
&lt;p&gt;For the sake of what follows, we’ll refer to the first EC2 Spot Instance that gets interrupted as the &lt;em&gt;old node&lt;/em&gt;. The new instance launched to continue the job is the &lt;em&gt;new node&lt;/em&gt;.&lt;/p&gt; 
&lt;h3&gt;Compute&lt;/h3&gt; 
&lt;p&gt;You must restart your job on a new host with similar configuration as the old host. This includes the CPU manufacturer (AMD, Intel, Arm), memory size, and the CPU generation. This can be incorporated easily by diversifying the instances before launching the job by selecting the conforming instances using the&amp;nbsp;&lt;a href="https://github.com/aws/amazon-ec2-instance-selector"&gt;ec2-instance-selector tool&lt;/a&gt;&amp;nbsp;. Your HPC tool may check for specific instruction-set support and rely on it. If this instruction set is not available in the next new node, it may fail to complete the job. It’s possible that you can move up to newer generations with backwards compatible instruction sets, but we didn’t test that.&lt;/p&gt; 
&lt;p&gt;Since instances can be interrupted, it’s best to run each worker in its own instance. This limits the number of impacted workers. It also prevents them from competing for network bandwidth when they need to write the checkpoint.&lt;/p&gt; 
&lt;h3&gt;Scheduler integration&lt;/h3&gt; 
&lt;p&gt;Since the scheduler is the one restarting the interrupted job, it’s important to understand its role in this.&lt;/p&gt; 
&lt;p&gt;Most schedulers offer a mechanism for pre-execution scripts (to setup the environment for execution) and post-execution scripts (to clean up or capture logs). Pre-execution scripts are repeated on the new node too, so they’ll need to detect the existing of a checkpoint and restore it. You’ll need to check with your technology provider if they’re able to capture environment variables. If not, those may need to move to your pre-execution script.&lt;/p&gt; 
&lt;p&gt;Another aspect of the scheduler integration is the locality of the workload. While relevant to all HPC jobs, it’s worth highlighting it in this context. If the old node ran in one Availability Zone (AZ), the new node &lt;em&gt;really&lt;/em&gt; should be launched in the same AZ. This allows low-latency communication with the shared storage or other workers, and it’ll save you from paying for cross-AZ network traffic charges.&lt;/p&gt; 
&lt;p&gt;And, of course, the scheduler should not attempt to resume the job if a license isn’t available to run it (more on licensing later).&lt;/p&gt; 
&lt;h3&gt;Shared storage – performance&lt;/h3&gt; 
&lt;p&gt;The checkpoint has to be stored in some shared storage so it can outlive the old node and be restored on the new node. The Spot interruptions will generate temporary high-throughput writes to the shared storage. This checkpointing process needs to complete within 2 minutes and hence incremental checkpoints are preferred. The size of the checkpoint depends on the process memory size and the size of the working directory that needs to be backed.&lt;/p&gt; 
&lt;p&gt;In a distributed HPC cluster environment running Spot instances,&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-placement-score.html"&gt;Spot Placement Score&lt;/a&gt;&amp;nbsp;(SPS) can help you find the optimal Spot configuration to take informed Spot availability decisions, more specifically, which Availability Zones AZ(s) to use as highlighted in the blog about&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/compute/optimizing-amazon-ec2-spot-instances-with-spot-placement-scores/"&gt;SPS&lt;/a&gt;. However, storage performance sizing should still be accounted for when running a multi-machine workload. You’ll need a shared volume to host these checkpoints and it has to be sized to handle higher rate of interruptions.&lt;/p&gt; 
&lt;p&gt;If storage is a performance bottleneck, some checkpoints might fail to complete within 2 minutes. Your job will then have to be restarted or failback to the previous checkpoint.&lt;/p&gt; 
&lt;h3&gt;Shared storage – permissions&lt;/h3&gt; 
&lt;p&gt;To capture the entire process-tree of the job, the checkpoint software will need to have root permissions. This means checkpoint storage writes will come from the root user. Since most HPC environments use &lt;code&gt;root_squash&lt;/code&gt; for security reasons, you’ll need to decide how to allow these writes. You can setup a dedicated volume for checkpoints with &lt;code&gt;no_root_squash&lt;/code&gt;, or map the root user to another user. We recommend consulting your security team because any root access should be thoroughly reviewed.&lt;/p&gt; 
&lt;h3&gt;Shared storage – capacity&lt;/h3&gt; 
&lt;p&gt;Beyond the obvious capacity requirements for the checkpoints, you’ll need to consider how you clean up old checkpoints from successfully completed jobs. This minimizes storage capacity over time. You can use the post-execution script to handle that.&lt;/p&gt; 
&lt;p&gt;This script does &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; run on the old node, where the job never completed, but &lt;strong&gt;&lt;em&gt;does&lt;/em&gt;&lt;/strong&gt; run on the new node when it &lt;em&gt;is&lt;/em&gt; completed. However, you might want to keep them available for a short period of time for troubleshooting. If so, consider a queue of these jobs that gets cleaned up periodically.&lt;/p&gt; 
&lt;h3&gt;Licensing impact&lt;/h3&gt; 
&lt;p&gt;If your HPC job relies on a license checked out from a license server, you’ll need to consider how this impacts the license count. Licenses bound to MAC addresses may cause you to double up on checked-out licenses – until the first one is released.&lt;/p&gt; 
&lt;p&gt;Some licensed tools allow the client to set a keep-alive interval forcing it to communicate with the license servers to keep the license assignment. Setting the keep-alive be below the time it takes to restart the job can resolve this. However, setting it too low can overwhelm the license server with requests.&lt;/p&gt; 
&lt;p&gt;Another possible approach is to have the MAC address move with the job between nodes. You can achieve this using a second &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html"&gt;Elastic Network Interface&lt;/a&gt; (ENI) on the old node. ENIs can be migrated to another node. This avoids the MAC address change. You can achieve this in your pre-execution script, for example. Another way is to use a &lt;a href="https://aws.amazon.com/lambda/"&gt;Lambda function&lt;/a&gt; called by the scheduler. This minimizes access to this permission (“&lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege"&gt;least privilege&lt;/a&gt;” – one of our guiding principles for designing anything in AWS).&lt;/p&gt; 
&lt;h3&gt;Incremental checkpoints&lt;/h3&gt; 
&lt;p&gt;To minimize network traffic and storage throughput bottlenecks, you can use incremental checkpoints.&lt;/p&gt; 
&lt;p&gt;By periodically checkpointing your process you reduce the amount of data that needs to be written during the 2-minute Spot interruption. The tradeoff is that you’ll write &lt;strong&gt;more&lt;/strong&gt; data in total, because your memory changes throughout the job’s execution. Writing more frequently means you’ll write data in checkpoint 1, 2 and 3 that’s not relevant to the last checkpoint (the one that’s restored).&lt;/p&gt; 
&lt;p&gt;It’s hard to recommend a good “rule of thumb” for this, but we recommend measuring this tradeoff for the specific application, and considering these questions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;What is the cost of a longer recovery point (less frequent checkpoints)? &lt;/strong&gt;A high-cost license may make you prefer to checkpoint more frequently to minimize amount of work that can be lost in a worst-case scenario.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;How long does a checkpoint take? &lt;/strong&gt;The checkpoint might need to freeze the process to take a consistent checkpoint, which means your job takes longer as you add checkpoints. Here too you’ll need to balance the protection against lost work with the impact on the time to results. In our lab, a 32GB machine was able to complete the checkpoint in 3-4 seconds (negligible).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;What’s the memory size used? &lt;/strong&gt;The larger the memory model, the more data you’ll need to write. At some point the data volume becomes too great to write in two minutes, and incremental checkpoints become mandatory.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;How long is your job?&lt;/strong&gt; This defines your worst-case-scenario for losing work. For a job that takes 5 days to complete, checkpointing daily means you may lose a full day of work. If that’s not acceptable – consider more frequent checkpoints.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Handling Spot interruptions&lt;/h3&gt; 
&lt;p&gt;The impact of Spot Instances interruptions can be minimized by aligning to Spot best practices of Instance Diversification. Alternatively, use one of the Capacity allocation strategies such as Price Capacity Optimized, and Spot placement scores. As highlighted in this&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/compute/best-practices-to-optimize-your-amazon-ec2-spot-instances-usage/"&gt;Spot best practice blog&lt;/a&gt;, this also helps to optimize your Spot Instances usage and is applicable to HPC workloads.&lt;/p&gt; 
&lt;p&gt;Many customers need to achieve a Service Level Agreement (SLA) for jobs to complete; this calls for a trade-off between cost and SLA. An interrupted job that is at risk of missing the SLA can be requeued and restarted on On-Demand instances in order to meet that SLA. When asking for the On-Demand instances, make sure to diversify your fleet request, including multiple host types. You can also explore low cost &lt;a href="https://aws.amazon.com/blogs/hpc/deep-dive-into-hpc7a-the-newest-amd-powered-member-of-the-hpc-instance-family/"&gt;HPC-specific instances&lt;/a&gt; using On-Demand pricing if jobs have to be done intermittently or &lt;a href="https://aws.amazon.com/savingsplans/"&gt;Savings Plans&lt;/a&gt;&amp;nbsp; (to further reduce the cost if the jobs need to run regularly.&lt;/p&gt; 
&lt;h3&gt;Checkpointing tool specific constraints&lt;/h3&gt; 
&lt;p&gt;The checkpointing tool may include additional requirements (like the need to run as root to capture the process tree). However, it may also introduce new requirements as well on &lt;em&gt;how&lt;/em&gt; you run your job.&lt;/p&gt; 
&lt;p&gt;These may include running them under a dedicated cgroup / container / namespace, which will introduce additional time to start / teardown the job. In our test we found a 20-second average setup time to create a new namespace, which for a long running job was acceptable.&lt;/p&gt; 
&lt;p&gt;Other requirements might include specific user permissions for running your tools, which will need to be evaluated against your security guidelines.&lt;/p&gt; 
&lt;p&gt;Some tools offer the ability to preempt the Spot interruption and move the workload in advance of this happening. This is an alternative to incremental checkpoints, but you can also use these two techniques together.&lt;/p&gt; 
&lt;h3&gt;Workloads spanning multiple nodes&lt;/h3&gt; 
&lt;p&gt;While we didn’t test workloads spanning multiple hosts in this lab, we &lt;em&gt;will&lt;/em&gt; offer two insights:&lt;/p&gt; 
&lt;p&gt;In most cases long &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/tightly-coupled-scenarios.html"&gt;tightly-coupled&amp;nbsp;&lt;/a&gt;workloads are not suitable to run on Spot instances. In tightly-coupled workloads workers rely on one another. The interruption of just one instance may leave the rest of the cluster stuck wasting time and resources. HPC instances (such as&amp;nbsp;&lt;a href="https://aws.amazon.com/ec2/instance-types/hpc7a/"&gt;Hpc7a&lt;/a&gt;,&amp;nbsp;&lt;a href="https://aws.amazon.com/ec2/instance-types/hpc7g/"&gt;Hpc7g&lt;/a&gt;, or&amp;nbsp;&lt;a href="https://aws.amazon.com/ec2/instance-types/hpc6i/"&gt;Hpc6id&lt;/a&gt;),&amp;nbsp;offer lower cost and offer more cores in a single host, reducing this challenge. They were designed exactly for this kind of workloads. Saving plans are a great option to get a discount in scenarios where the workload can benefit from a 1 year or 3 year commitment.&lt;/p&gt; 
&lt;p&gt;If your workers are &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/loosely-coupled-scenarios.html"&gt;loosely-coupled&lt;/a&gt;, you should still look at the head-node that manages them. (1) This node should &lt;strong&gt;not&lt;/strong&gt; be using Spot so it’s not interrupted. (2) You should also check how this node will react to an instance interruption and returning with a new hostname. This is tool-specific, and you might need to do some testing to validate this.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Checkpoint/restore solutions offer a new way of reducing the cost of long running HPC jobs, but introduces some new requirements. For competitive markets like semiconductor and healthcare, this can mean faster time to market without a similar increase in the R&amp;amp;D budget.&lt;/p&gt; 
&lt;p&gt;It requires some planning and implementation to meet your specific software needs. For more information, visit the &lt;a href="https://aws.amazon.com/marketplace/search/results?searchTerms=spot+instances"&gt;AWS marketplace&lt;/a&gt; to explore partner solutions for using EC2 Spot with your HPC workload, or reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Enhancing ML workflows with AWS ParallelCluster and Amazon EC2 Capacity Blocks for ML</title>
		<link>https://aws.amazon.com/blogs/hpc/enhancing-ml-workflows-with-aws-parallelcluster-and-amazon-ec2-capacity-blocks-for-ml/</link>
		
		<dc:creator><![CDATA[Austin Cherian]]></dc:creator>
		<pubDate>Thu, 11 Jan 2024 14:50:51 +0000</pubDate>
				<category><![CDATA[Amazon Machine Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<guid isPermaLink="false">f588e38a5ba88c676ee43776193f2153800b334e</guid>

					<description>No more guessing if GPU capacity will be available when you launch ML jobs! EC2 Capacity Blocks for ML let you lock in GPU reservations so you can start tasks on time. Learn how to integrate Caacity Blocks into AWS ParallelCluster to optimize your workflow in our latest technical blog post.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3192" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/09/boofla88_capacity_blocks_like_a_tetris_game_in_3d_f07df4ca-4b16-4d57-8d7a-2a8ab7030ad6.png" alt="" width="380" height="212"&gt;Have you ever geared up to run machine learning (ML) tasks with GPU instances, only to hit a roadblock? It’s a common scenario: you spend hours meticulously preparing your training sessions or simulations, tackling the intricate setup from fine-tuning application parameters to organizing job output directories, and managing the detailed process of data set cleaning and pre-processing.&lt;/p&gt; 
&lt;p&gt;But just when you’re ready to launch, you hit an unexpected snag – the GPU capacity you need isn’t available. This leaves you with two less-than-ideal options: wait for the GPU instances to become available &lt;strong&gt;or&lt;/strong&gt; reshape and resize your job to fit the limited resources at hand.&lt;/p&gt; 
&lt;p&gt;Recently Amazon EC2 announced the availability of Capacity Blocks for ML which allows you to reserve blocks of GPU time with specific start and end dates enabling you to launch your jobs when those reservations start. This approach effectively eliminates uncertainties around job start times.&lt;/p&gt; 
&lt;p&gt;AWS ParallelCluster now supports integrating these reservation blocks with your cluster and in this post, we show you how to setup a cluster to use them.&lt;/p&gt; 
&lt;h2&gt;A word or two about Capacity Blocks&lt;/h2&gt; 
&lt;p&gt;Amazon EC2 Capacity Blocks are carved out of Amazon EC2 UltraClusters. These Capacity Blocks enable users to reserve GPU instances for one to 14 days and can support a maximum of 64 instances or 512 GPUs, making them ideal for ML workloads ranging from small-scale experiments to extensive, distributed training sessions.&lt;/p&gt; 
&lt;p&gt;Their integrated, low-latency, and high-throughput network connectivity simplifies the setup by removing the need for managing cluster placement groups – streamlining the architecture, and improving the performance of distributed applications. EC2 Capacity Blocks currently support Amazon EC2 P5 instances, which are powered by NVIDIA H100 Tensor Core GPUs integrated into UltraClusters with advanced second-generation Elastic Fabric Adapter (EFA) networking. This enables remarkable connectivity and scalability up to hundreds of GPUs, making them well-suited for the most resource-intensive ML workloads. For more information, refer to the AWS official documentation on &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-blocks.html"&gt;Capacity Blocks for ML&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;AWS ParallelCluster integration with EC2 Capacity Blocks&lt;/h2&gt; 
&lt;p&gt;With &lt;strong&gt;AWS&lt;/strong&gt; &lt;strong&gt;ParallelCluster 3.8&lt;/strong&gt; you can specify and assign EC2 Capacity Blocks to be used by queues when launching jobs. ParallelCluster spins up the instances when the reserved capacity becomes active – enabling Slurm to launch jobs as soon as those instances are ready. In this way, you can setup your complex workflows ahead of time, submit them into the queue, and be assured that they’ll run as soon as the reserved capacity you purchased becomes available.&lt;/p&gt; 
&lt;h3&gt;Setting up clusters to use EC2 Capacity Blocks&lt;/h3&gt; 
&lt;p&gt;Setting up clusters to use Capacity Blocks and using them is largely a three-step process that involves, &lt;strong&gt;reserving the Capacity Block, integrating the reservations with the cluster and, and aligning and launching&lt;/strong&gt; &lt;strong&gt;your workloads&lt;/strong&gt; to use the capacity when it becomes available. Let’s have a closer look at these steps.&lt;/p&gt; 
&lt;h3&gt;Reserving a Capacity Block&lt;/h3&gt; 
&lt;p&gt;To reserve a Capacity Block, you start by defining the number of instances you need, the amount of time you need them for, and the date window when you want the capacity to run your jobs. You can use the AWS EC2 console to search for a Capacity Block based on your requirements. Once you find the ideal match, you can reserve the Capacity Block with just a couple clicks. You can also find and reserve a Capacity Block using the &lt;a href="https://aws.amazon.com/cli/"&gt;AWS Command Line Interface (AWS CLI)&lt;/a&gt; and &lt;a href="http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/EC2.html"&gt;AWS SDKs&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_3185" style="width: 1102px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3185" loading="lazy" class="size-full wp-image-3185" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/09/CleanShot-2024-01-09-at-16.06.48.png" alt="Figure 1 – To get started with Capacity Blocks in the AWS EC2 console, start by navigating to the US East 1 (Ohio) Region, select Capacity Reservations in the navigation pane, and then click Purchase Capacity Blocks for ML to see available capacity." width="1092" height="509"&gt;
 &lt;p id="caption-attachment-3185" class="wp-caption-text"&gt;Figure 1 – To get started with Capacity Blocks in the AWS EC2 console, start by navigating to the US East 1 (Ohio) Region, select Capacity Reservations in the navigation pane, and then click Purchase Capacity Blocks for ML to see available capacity.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3186" style="width: 896px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3186" loading="lazy" class="size-full wp-image-3186" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/09/CleanShot-2024-01-09-at-16.07.34.png" alt="Figure 2 – After you specify your capacity requirements, click Find Capacity Block to see an available Capacity Block offering that matches your inputs, including the exact start and end times, Availability Zone, and total price of the reservation. Once you find a Capacity Block offering that you want to reserve, click Next to proceed with purchasing the reservation." width="886" height="725"&gt;
 &lt;p id="caption-attachment-3186" class="wp-caption-text"&gt;Figure 2 – After you specify your capacity requirements, click Find Capacity Block to see an available Capacity Block offering that matches your inputs, including the exact start and end times, Availability Zone, and total price of the reservation. Once you find a Capacity Block offering that you want to reserve, click Next to proceed with purchasing the reservation.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Capacity Blocks are delivered as a type of Capacity Reservation, and when you reserve a Capacity Block you get a reservation ID which is required for the next step of using the Capacity Block with your cluster. You can find your &lt;strong&gt;Capacity Block reservation ID&lt;/strong&gt; in the &lt;strong&gt;Capacity Reservations&lt;/strong&gt; resource table in the AWS EC2 console. For more information about finding and reserving Capacity Blocks, refer to the &lt;a href="https://docs.aws.amazon.com/en_us/AWSEC2/latest/UserGuide/capacity-blocks-using.html#capacity-blocks-purchase"&gt;Find and Purchase Capacity Blocks&lt;/a&gt; section of the EC2 Capacity Blocks documentation.&lt;/p&gt; 
&lt;h3&gt;Configuring a cluster to use a Capacity Block&lt;/h3&gt; 
&lt;p&gt;With the Capacity Block reservation ID in hand, you can now configure an existing cluster or create a &lt;em&gt;new&lt;/em&gt; cluster to target instance launches from the Capacity Block you purchased when it becomes active.&lt;/p&gt; 
&lt;p&gt;ParallelCluster previously already supported On-Demand Capacity Reservations (ODCRs). The same set of configuration parameters can now support Capacity Reservations based on EC2 Capacity Blocks with the addition of a newly supported capacity type: CAPACITY_BLOCK.&lt;/p&gt; 
&lt;p&gt;The cluster configuration file snippet that follows shows the highlighted parameters that need to be set in the ParallelCluster configuration file to configure a cluster to use a Capacity Block.&lt;/p&gt; 
&lt;div id="attachment_3187" style="width: 672px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3187" loading="lazy" class="size-full wp-image-3187" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/09/CleanShot-2024-01-09-at-16.08.11.png" alt="Figure 3 - The cluster configuration that is needed to configure the use of a Capacity Block." width="662" height="326"&gt;
 &lt;p id="caption-attachment-3187" class="wp-caption-text"&gt;Figure 3 – The cluster configuration that is needed to configure the use of a Capacity Block.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Let’s assume we already have an existing cluster and we want to configure one of its queues to use the Capacity Block we just reserved – for all jobs submitted to this queue. With the Capacity Block reservation ID, we obtained in step 1, we can now proceed as follows:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;You can skip this step if you’ve set &lt;em&gt;QueueUpdateStrategy&lt;/em&gt; property to &lt;code&gt;DRAIN&lt;/code&gt; or &lt;code&gt;TERMINATE&lt;/code&gt; in your ParallelCluster configuration file. However, if you’ve set the &lt;em&gt;QueueUpdateStrategy&lt;/em&gt; property to &lt;code&gt;COMPUTE_FLEET_STOP&lt;/code&gt; or do not use the property at all, you must stop the cluster’s compute fleet using pcluster update-compute-fleet command as follows:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-compute-fleet --cluster-name &amp;lt;cluster-name&amp;gt; --status STOP_REQUSTED&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Using the &lt;em&gt;QueueUpdateStrategy&lt;/em&gt; property set to &lt;code&gt;DRAIN&lt;/code&gt; or &lt;code&gt;TERMINATE&lt;/code&gt; avoids the need to stop your compute fleet when updating queue or storage configurations. Refer to the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/Scheduling-v3.html#yaml-Scheduling-SlurmSettings-QueueUpdateStrategy"&gt;official ParallelCluster documentation&lt;/a&gt; for more information about the &lt;em&gt;QueueUpdateStrategy&lt;/em&gt; parameter.&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;In the cluster’s configuration file, for the selected queue, update its parameters to point to the Capacity Block by setting its &lt;em&gt;CapacityType&lt;/em&gt; parameter to &lt;code&gt;CAPACITY_BLOCK&lt;/code&gt; and update the &lt;em&gt;CapacityReservationId&lt;/em&gt; property at the queue level to the reservation ID of the Capacity Block.&lt;/li&gt; 
 &lt;li&gt;Configuring Capacity Blocks also requires setting the &lt;em&gt;MinCount&lt;/em&gt; and &lt;em&gt;MaxCount&lt;/em&gt; properties to be the same value because these counts specify the number of static nodes that would be launched from the available capacity of the Capacity Block for jobs submitted to the queue. Make sure the counts don’t exceed the size of the Capacity Block.&lt;/li&gt; 
 &lt;li&gt;When specifying Capacity Blocks for a queue, you need to specify the &lt;em&gt;InstanceType&lt;/em&gt; property, setting it to the instance type of the Capacity Block. As of this writing, EC2 Capacity Blocks only support P5 instances. The &lt;em&gt;InstanceType &lt;/em&gt;set needs to match the instance type of the Capacity Block or you’ll get a validation error during the configuration of the cluster.&lt;/li&gt; 
 &lt;li&gt;With these configurations, we’ll now update the cluster configuration using &lt;code&gt;pcluster update-cluster&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-cluster --cluster-configuration &amp;lt;cluster-config-file&amp;gt; --cluster-name &amp;lt;cluster-name&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you’ve stopped the compute fleet as directed in step 1 then after the re-configuration of the cluster completes you’ll need to enable the compute fleet again, to allow for job submission. This can be done by the running the &lt;code&gt;pcluster update-compute-fleet&lt;/code&gt; command again, but this time with the &lt;code&gt;START_REQUESTED&lt;/code&gt; status as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-compute-fleet --cluster-name &amp;lt;cluster-name&amp;gt; --status START_REQUESTED&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However, if you’ve set the &lt;em&gt;QueueUpdateStrategy&lt;/em&gt; property you won’t need to enable the compute fleet and the Capacity Block will be configured after the existing instances have been Drained or Terminated depending on the setting of the property.&lt;/p&gt; 
&lt;p&gt;And that’s it! Your cluster is now configured to use an EC2 Capacity Block. When the Capacity Block becomes active, Parallelcluster will launch the EC2 instances as static compute nodes for the configured queue. If you were starting from scratch and creating a new cluster, you could modify the configuration file as illustrated and use it when creating the cluster. &amp;nbsp;If you’d like to learn more about creating and updating clusters to use Capacity Blocks, you can refer to the ParallelCluster &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html"&gt;official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Running jobs to use the configured Capacity Block&lt;/h4&gt; 
&lt;p&gt;With the cluster configured to use a Capacity Block reservation, submitting jobs is straight forward. You prepare your jobs and submit them to run in the configured queue. You don’t need to wait for the Capacity Block to become active. All the instances associated with the Capacity Block will be associated with a Slurm reservation. Instances in Slurm reservations continue to accept jobs and keep them pending. As soon as your Capacity Block becomes active, the instances will become available as static nodes, and the job will begin.&lt;/p&gt; 
&lt;p&gt;Jobs that exceed the Capacity Blocks duration will fail, because EC2 will terminate compute instances when the &lt;em&gt;end time&lt;/em&gt; of the Capacity Block is reached. To avoid job termination, you’ll need to estimagfe your job runtime and ensure it doesn’t exceed the Capacity Block’s duration &lt;em&gt;before&lt;/em&gt; launching the job.&lt;/p&gt; 
&lt;p&gt;You can check a Capacity Block’s current validity using the AWS console, CLI, or programmatically using the SDK before launching the job. Capacity Blocks also support &lt;a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-events.html"&gt;EventBridge&lt;/a&gt; notifications sent out as the Capacity Blocks nears expiry – you can use this to save jobs states before they are preempted. For more information on how to monitor your Capacity Blocks refer to &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/capacity-blocks-monitor.html"&gt;Monitor Capacity Blocks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We recommended enabling checkpointing for your job (if that’s available) so you can restart it from a more proximate point to when it was terminated, rather than starting it from the very beginning.&lt;/p&gt; 
&lt;p&gt;When you’re done using the Capacity Block, we recommend deactivating the queue or updating the queue to use another Capacity Block or a different Capacity Type. To deactivate the queue, you can run the Slurm Scontrol command as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;scontrol update partitionname=&amp;lt;queuename&amp;gt; state=INACTIVE&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To update the queue with the configuration of another Capacity Block, you just need to update the &lt;em&gt;CapacityReservationId&lt;/em&gt; parameter with the ID of the new Capacity Block and update the cluster as shown earlier. Alternatively, to setup the queue with a different capacity type you can delete the &lt;em&gt;CapacityReservationTarget&lt;/em&gt; and &lt;em&gt;CapacityReservationId&lt;/em&gt; parameters and set the &lt;em&gt;CapacityType&lt;/em&gt; to &lt;code&gt;ONDEMAND&lt;/code&gt; or &lt;code&gt;SPOT&lt;/code&gt; as you prefer. When changing the capacity type, you may want to set the &lt;em&gt;InstanceType&lt;/em&gt; parameter of the associated compute resource to another instance type (other than P5) depending on the configuration you prefer.&lt;/p&gt; 
&lt;h3&gt;Maximizing on your Capacity Blocks utilization&lt;/h3&gt; 
&lt;p&gt;Given that Capacity Blocks are intentionally ephemeral with a finite lifetime, maximizing their usage is crucial. Here are two key considerations for achieving that.&lt;/p&gt; 
&lt;h4&gt;Considerations for GPU failures&lt;/h4&gt; 
&lt;p&gt;If you’ve been running long-running ML training jobs, there’s a good chance you’ve faced GPU failures midway through the job, resulting in failed jobs and requiring job restarts. You may have faced this problem enough to have introduced checkpointing. To lower the probability of being affected by GPU failures, ParallelCluster already supports doing GPU health checks on the compute nodes assigned for a job &lt;em&gt;before the job starts&lt;/em&gt;. For more on using this feature, check out our post “&lt;a href="https://aws.amazon.com/blogs/hpc/introducing-gpu-health-checks-in-aws-parallelcluster-3-6/"&gt;Introducing GPU health check in ParallelCluster 3.6&lt;/a&gt;”.&lt;/p&gt; 
&lt;p&gt;These GPU health checks can be used with Capacity Block reservations, too. In the case of GPU failures when using Capacity Blocks, unhealthy instances are terminated and new instances are provisioned within a few minutes. Jobs are launched when the health checks on all instances pass.&lt;/p&gt; 
&lt;h4&gt;Multi-queue and multi-compute-resource configurations&lt;/h4&gt; 
&lt;p&gt;You may be using Slurm partitions to organize your cluster’s usage. For example, you may assign users or groups to a queue that they need to use for their jobs, or you might have multiple projects in flight using the same cluster, where each project gets its own queue. The permutations and combinations can work well for your organization, but when it comes to capacity utilization – especially with reserved capacity that you book and pay for – your cluster’s organization must have considerations to maximize utilization.&lt;/p&gt; 
&lt;p&gt;To help match your cluster organization while using Capacity Blocks, ParallelCluster supports sharing the same Capacity Block across multiple queues and compute resources. To share the Capacity Block across multiple queues, you simply need to configure the queues to use the same Capacity Block just like we showed earlier in the single queue case. However, take care to split the capacity between the queues where the Min/Max counts of &lt;em&gt;all the queues combined&lt;/em&gt; is &lt;em&gt;less than or equal to&lt;/em&gt; the size of the Capacity Block.&lt;/p&gt; 
&lt;p&gt;The reason this is required is that Capacity Blocks are spun up as &lt;strong&gt;static capacity&lt;/strong&gt; and assigned to a queue, and the static capacity assigned to a queue is not available in another queue.&lt;/p&gt; 
&lt;p&gt;In addition, the flexibility of multiple queues allows you to reserve and specify multiple Capacity Blocks on the same queue. You’d do this if you wanted to use the same queue for all your jobs but keep it replenished with Capacity Blocks that you keep purchasing as and when you’re able to reserve them.&lt;/p&gt; 
&lt;h3&gt;Conclusion&lt;/h3&gt; 
&lt;p&gt;To wrap up, combining AWS ParallelCluster with EC2 Capacity Blocks is an effective solution to solving the GPU availability constraints we are all facing in this AI enabled era. This combination helps with aligning your workload to the required GPU power it requires, getting you the crucial determinism you need to effectively plan your workflows.&lt;/p&gt; 
&lt;p&gt;You can reserve the GPU power you need ahead of time, submit your job, and it’ll run with the required capacity when the capacity arrives, which means less waiting and fewer project delays.&lt;/p&gt; 
&lt;p&gt;In this post, we briefly introduced EC2 Capacity Blocks and how they integrate with AWS ParallelCluster. We also illustrated a step-by-step walkthrough of how you can configure a cluster to use a Capacity Block. We talked about some smart ways to get the most out of these Capacity Blocks, like how to handle GPU issues and how to set up your projects in a smart way.&lt;/p&gt; 
&lt;p&gt;These tips aren’t just helpful – they’re crucial for anyone who wants to use Capacity Blocks with ParallelCluster to their full potential.&lt;/p&gt; 
&lt;p&gt;Lastly, the integration EC2 Capacity Blocks and AWS Parallelcluster is a glimpse into the future of how AWS services integrate to produce solutions to specific problems like the GPU capacity crunch we all face today. As more compute power is needed to solve tomorrow’s problems, these kinds of smart solutions are important. If you’d like to learn more about creating and updating clusters to use Capacity Blocks, you can refer to the ParallelCluster &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html"&gt;official documentation&lt;/a&gt;. If you have ideas or feedback that can help us make these features better , reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Create a Slurm cluster for semiconductor design with AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/create-a-slurm-cluster-for-semiconductor-design-with-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Allan Carter]]></dc:creator>
		<pubDate>Wed, 10 Jan 2024 15:46:22 +0000</pubDate>
				<category><![CDATA[Best Practices]]></category>
		<category><![CDATA[Hi-Tech and Electronics]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Industries]]></category>
		<category><![CDATA[Semiconductor]]></category>
		<category><![CDATA[Technical How-to]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">cdc660c35037e33dd83c140e30c100fc2cd1cd3a</guid>

					<description>If you work in the semiconductor industry with electronic design automation tools and workflows, this guide will help you build an HPC cluster on AWS specifically configured for your needs. It covers AWS ParallelCluster and customizations specifically to cater to EDA.</description>
										<content:encoded>&lt;p&gt;Chips are fingernail-sized pieces of glass that power all your electronic devices like cell phones, computers, and TVs. Chip designers use electronic design automation (EDA) tools to draw billions of microscopic switches and connect them with wires that are smaller than the finest hair.&lt;/p&gt; 
&lt;p&gt;EDA engineers measure device sizes in nanometers (1×10&lt;sup&gt;-9 &lt;/sup&gt;m or 1 billionth of a meter) and time in picoseconds (1×10&lt;sup&gt;-12&lt;/sup&gt; s, or &lt;em&gt;one-trillionth&lt;/em&gt; of a second). They analyze their designs to ensure that they meet power, performance, and area (PPA) goals and that they meet rigorous design rules so that silicon foundries can manufacture them. The designers send the completed drawings to a foundry that manufactures them in automated factories that reliably create structures at atomic scale. The entire design and manufacturing process requires vast amounts of high-performance storage and high-performance computing (HPC) to run millions of EDA jobs for large teams of engineers.&lt;/p&gt; 
&lt;p&gt;Today, we’re going to show you how to create a cluster on AWS, using AWS ParallelCluster, that’s purpose-built for this environment, and ready for the kinds of workloads that EDA users work with every day.&lt;/p&gt; 
&lt;h2&gt;A world of complex constraints&lt;/h2&gt; 
&lt;p&gt;EDA workflows impose complex requirements on the compute cluster. A workflow may consist of hundreds of different licensed EDA tools with very different requirements. The tool licenses are typically much more expensive than the infrastructure they run on so the scheduler must ensure that jobs only run when a license and the required compute resources are available.&lt;/p&gt; 
&lt;p&gt;Large teams must share these critical resources so it’s also essential that the scheduler can enforce a fair-share allocation policy to prevent some users from monopolizing resources at the expense of others. The EDA jobs themselves have widely differing requirements, so the compute cluster must support a very diverse set of Amazon EC2 instance sizes and families from compute-optimized to high-memory variations.&lt;/p&gt; 
&lt;h2&gt;A day in the life&lt;/h2&gt; 
&lt;p&gt;Figure 1 shows the basic workflow for designing a chip. The front-end and back-end tasks have quite distinct requirements for compute and storage. An advantage of AWS is the diversity of compute and storage solutions available so design teams can tailor their infrastructure for the needs of each step in the workflow.&lt;/p&gt; 
&lt;p&gt;The process is also extremely iterative. If engineers find a problem in the late stages of the project, the fix may require architectural changes that require the team to rerun all the steps of the workflow. This is where design teams can get into capacity crunches – and risk catastrophic schedule delays if they can’t access enough compute and storage capacity.&lt;/p&gt; 
&lt;div id="attachment_3159" style="width: 567px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3159" loading="lazy" class="size-full wp-image-3159" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/14/CleanShot-2023-12-14-at-12.33.30.png" alt="Figure 1 - Chip design workflow diagram. Architects create and edit the architecture. Designers create and edit the design and create RTL and circuit diagrams. Design verification engineers run simulations to verify that the design works and gather coverage information to verify completeness of tests. Back end design engineers synthesize RTL to create netlists. Then they do physical layout that converts the netlists to layout and GDSII. Then they run physical verification and power and signal analysis to make sure the layout meets power, performance, and area requirements and that the GDSII meets all design rules. After verification is complete, the developers tape out the GDSII to a silicon foundry for manufacturing." width="557" height="1055"&gt;
 &lt;p id="caption-attachment-3159" class="wp-caption-text"&gt;Figure 1 – Chip design workflow diagram. Architects create and edit the architecture. Designers create and edit the design and create RTL and circuit diagrams. Design verification engineers run simulations to verify that the design works and gather coverage information to verify completeness of tests. Back end design engineers synthesize RTL to create netlists. Then they do physical layout that converts the netlists to layout and GDSII. Then they run physical verification and power and signal analysis to make sure the layout meets power, performance, and area requirements and that the GDSII meets all design rules. After verification is complete, the developers tape out the GDSII to a silicon foundry for manufacturing.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;At the beginning of a project, compute usage is typically low and sporadic. Usage peaks around project milestones and at the end of a project when it runs hot for several months – and is usually in the critical path for project completion. Figure 2 shows the number of jobs running on different instance types in an EDA cluster over a typical 24-hour window. Utilization would likely be higher and more sustained near the end of a project.&lt;/p&gt; 
&lt;p&gt;Notice the variability and the diverse mix of instance types that the cluster uses ranging from high-frequency general purpose m5zn to memory-optimized and high-memory r6i and x2iezn instances. The scalability of AWS can ensure that jobs are able to run on instance types that are ideal for the jobs without infrastructure capacity constraints.&lt;/p&gt; 
&lt;div id="attachment_3160" style="width: 887px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3160" loading="lazy" class="size-full wp-image-3160" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/14/CleanShot-2023-12-14-at-12.34.21.png" alt="Figure 2 - The EDA job profile is highly variable. This graph shows peaks and troughs of usage throughout a 24-hour window. Engineers run jobs that use different instance types as required by the job requirements." width="877" height="790"&gt;
 &lt;p id="caption-attachment-3160" class="wp-caption-text"&gt;Figure 2 – The EDA job profile is highly variable. This graph shows peaks and troughs of usage throughout a 24-hour window. Engineers run jobs that use different instance types as required by the job requirements.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Enter Slurm and AWS ParallelCluster&lt;/h2&gt; 
&lt;p&gt;Design teams have traditionally used commercial schedulers, but are increasingly showing interest in Slurm. Slurm is open-source, free to use, and meets all the requirements for running EDA workloads. &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; is an AWS service that allows Slurm to automatically scale up and down fleets of compute instances on AWS. It supports high job throughput and job capacity for even the most demanding workloads. It also supports the high security demands of the semiconductor industry by not requiring any internet access for its functionality.&lt;/p&gt; 
&lt;p&gt;Starting with version 3.7.0, AWS ParallelCluster has all the features required to easily configure and deploy an EDA Slurm cluster that takes full advantage of the scalability of AWS to design the most advanced chips. It offers Slurm accounting, which allows administrators to configure license-sharing and enables fair-share allocation for cluster users. It can schedule jobs based on the number of cores and the amount of memory that the job requires, too. ParallelCluster also increased the number of instance types that it can support in a cluster. It added support for Redhat Enterprise version 8 which all EDA tools will require, starting in 2024. It added support for custom &lt;em&gt;instance type weighting&lt;/em&gt; so Slurm can schedule the lowest cost instance type that meets a job’s specific requirements. And finally, ParallelCluster added a Python management API that you can use in a Lambda Layer to &lt;a href="https://aws.amazon.com/blogs/hpc/automate-your-clusters-by-creating-self-documenting-hpc-with-aws-parallelcluster/"&gt;completely automate the deployment and updating of your cluster&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Putting this all together&lt;/h3&gt; 
&lt;p&gt;The &lt;a href="https://github.com/aws-samples/aws-eda-slurm-cluster"&gt;aws-eda-slurm-cluster&lt;/a&gt; repository on GitHub uses all these new ParallelCluster features to quickly and easily create an EDA-specific ParallelCluster in the VPC of your choosing. The cluster supports CentOS 7, RHEL 7 and 8, x86_64 and arm64 architectures, On Demand and Spot Instances, heterogeneous queues, and up to 50 instance types. You can easily configure EDA license counts and fair share allocations using simple configuration files. By default, it selects the instance types that are best for EDA workloads. The cluster sets compute node weights based on the cost of instance types so that Slurm chooses the lowest cost node that meets job requirements.&lt;/p&gt; 
&lt;p&gt;In addition to the normal ParallelCluster partitions, it defines a batch and an interactive partition that each contain all the compute nodes. The batch partition is the default and the interactive partition is identical except that it has a much higher weight. If you need a job to run quickly, for example to debug a simulation failure, and the batch partition is full, &lt;strong&gt;you can jump to the head of the line&lt;/strong&gt; by submitting your job to the interactive queue and Slurm will schedule it using the next available license and compute node.&lt;/p&gt; 
&lt;p&gt;ParallelCluster and Slurm typically expect you to &lt;code&gt;ssh&lt;/code&gt; to a login node or the head node to use the cluster, but semiconductor engineers expect to be able to use the cluster from a shell on their virtual desktops. With &lt;em&gt;aws-eda-slurm-cluster&lt;/em&gt;, you can configure submitter hosts so that they can directly access one or more clusters. The cluster configures submitter hosts as Slurm login nodes and creates modulefiles you load to set up the shell environment to use the cluster. It also supports submitters that have a different OS or CPU architecture than the cluster.&lt;/p&gt; 
&lt;h2&gt;Deployment&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://aws-samples.github.io/aws-eda-slurm-cluster/"&gt;aws-eda-slurm-cluster GitHub page&lt;/a&gt; documents the simple deployment process. The cluster uses the &lt;a href="https://aws.amazon.com/cdk/"&gt;AWS Cloud Development Kit&lt;/a&gt; (CDK) to create a &lt;a href="https://github.com/aws-samples/aws-eda-slurm-cluster/blob/main/source/cdk/cdk_slurm_stack.py"&gt;custom application&lt;/a&gt; that reads a configuration file and creates an AWS CloudFormation stack that creates and configures the cluster in less than 30 minutes.&lt;/p&gt; 
&lt;p&gt;The CloudFormation stack creates and configures your customized ParallelCluster. When you no longer need the cluster, you simply delete the CloudFormation &lt;em&gt;stack&lt;/em&gt; and CloudFormation deletes the cluster for you. If you need to update the cluster, then update the configuration file and rerun the CDK application.&lt;/p&gt; 
&lt;p&gt;The cluster uses a YAML configuration file with a different &lt;a href="https://github.com/aws-samples/aws-eda-slurm-cluster/blob/main/source/cdk/config_schema.py#L241-L454"&gt;schema&lt;/a&gt; than ParallelCluster’s configuration file. The following basic configuration will use ParallelCluster to create a RHEL 8 Slurm cluster configured for EDA workloads. Note the prerequisite items highlighted in red. It requires an AWS VPC, subnet, and EC2 key pair. If you don’t already have these, &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/tutorials_07_slurm-accounting-v3.html#slurm-accounting-vpc-v3"&gt;this tutorial&lt;/a&gt; shows how to create them. &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/tutorials_07_slurm-accounting-v3.html#slurm-accounting-db-stack-v3"&gt;This tutorial&lt;/a&gt; shows how to create a Slurm accounting database stack. Include the stack name in the configuration. The licenses section allows the configuration of 1 or more software licenses that Slurm will track to make sure that jobs don’t use more than the number of configured licenses.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;StackName: eda-pc-3-7-2-rhel8-x86-config
Region: &amp;lt;region&amp;gt;
SshKeyPair: &amp;lt;ec2-key-pair&amp;gt;
VpcId: vpc-xxxx
SubnetId: subnet-xxxx
slurm:
  ClusterName: eda-pc-3-7-2-rhel8-x86
  MungeKeySecret: /slurm/munge_key
  ParallelClusterConfig:
    Version: '3.7.2'
    Image:
      Os: 'rhel8'
    Architecture: 'x86_64'
    Database:
      DatabaseStackName: &amp;lt;parallel-cluster-database-stack&amp;gt;
  SlurmCtl: {}
  InstanceConfig:
    NodeCounts:
      DefaultMaxCount: 10
Licenses:
  &amp;lt;license-name&amp;gt;:
    Count: 10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Deployment is as simple as executing the following commands from the root of the git repository.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ source setup.sh
$ ./install.sh --config-file &amp;lt;config-filename&amp;gt; --cdk-cmd create&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will create a CloudFormation stack named &lt;strong&gt;eda-pc-3-7-2-rhel8-x86-config&lt;/strong&gt; that will create and configure a ParallelCluster called &lt;strong&gt;eda-pc-3-7-2-rhel8-x86&lt;/strong&gt;. If you need to update the configuration, for example to add a custom compute node AMI, then you just edit the configuration file and run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ source setup.sh
$ ./install.sh --config-file &amp;lt;config-filename&amp;gt; --cdk-cmd update&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use they cluster by connecting to the head node or a login node. If you would like the convenience of accessing the cluster directly from a submitter host, &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-view-stack-data-resources.html"&gt;the output&lt;/a&gt; of the &lt;strong&gt;eda-pc-3-7-2-rhel8-x86-config&lt;/strong&gt; stack has commands for mounting the cluster’s NFS export on submitter hosts and configuring them to use the stack. After you configure the submitter host, you can easily use the cluster. Simply load the provided modulefile and run Slurm commands like in the following example which will open an interactive bash shell on a compute node with 1 GB of memory and 1 CPU core for, at most, an hour.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ module load eda-pc-3-7-2-rhel8-x86
$ srun -p interactive --mem 1G -c 1 --time 1:00:00 --pty /bin/bash&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The module file sets up the environment for the Slurm cluster and configures Slurm defaults for the path, number of cores, default amount of memory, job timeout, and more. This makes it so that users must override the defaults to get more than minimal cluster resources. By default, their jobs will only get 1 core, 100 MB of memory, and a time limit of 1 hour.&lt;/p&gt; 
&lt;p&gt;Another nice feature of the cluster is that it creates custom ParallelCluster AMI build configuration files. You can use them to create AMIs with all the packages typically required by EDA tools. You can find the build configuration files in the repo at &lt;em&gt;source/resources/parallel-cluster/config/&amp;lt;parallel-cluster-version&amp;gt;/&amp;lt;ClusterName&amp;gt;&lt;/em&gt; or on the head node or submitter host at &lt;em&gt;/opt/slurm/&amp;lt;ClusterName&amp;gt;/config/build-files&lt;/em&gt;. The GitHub page documents the &lt;a href="https://aws-samples.github.io/aws-eda-slurm-cluster/custom-amis/"&gt;process for building custom EDA AMIs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;You can easily configure and deploy a Slurm cluster on AWS using AWS ParallelCluster and &lt;a href="https://github.com/aws-samples/aws-eda-slurm-cluster"&gt;aws-eda-slurm-cluster&lt;/a&gt; that can run your most demanding EDA workloads and take advantage of the performance and scalability of AWS to meet your project needs.&lt;/p&gt; 
&lt;p&gt;Contact your AWS account team and schedule a meeting with our semiconductor industry specialists for more information and help getting your EDA workloads running on AWS.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Using a Level 4 Digital Twin for scenario analysis and risk assessment of manufacturing production on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/using-a-level-4-digital-twin-for-scenario-analysis-and-risk-assessment-of-manufacturing-production-on-aws/</link>
		
		<dc:creator><![CDATA[Ross Pivovar]]></dc:creator>
		<pubDate>Tue, 12 Dec 2023 15:09:36 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">f0833eb293524843a9cbd91da39f3e12e62cf6bb</guid>

					<description>This post was contributed by Orang Vahid (Dir of Engineering Services) and Kayla Rossi (Application Engineer) at Maplesoft, and Ross Pivovar (Solution Architect) and Adam Rasheed (Snr Manager) from Autonomous Computing at AWS One of the most common objectives for our Digital Twin (DT) customers is to use DTs for scenario analysis to assess risk […]</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by &lt;/em&gt;&lt;em&gt;Orang Vahid (Dir of Engineering Services) &lt;/em&gt;&lt;em&gt;and Kayla Rossi (Application Engineer) &lt;/em&gt;&lt;em&gt;at Maplesoft, and &lt;/em&gt;&lt;em&gt;Ross Pivovar (Solution Architect) and &lt;/em&gt;&lt;em&gt;Adam Rasheed (Snr Manager) from Autonomous Computing at AWS&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;One of the most common objectives for our Digital Twin (DT) customers is to use DTs for scenario analysis to assess risk and drive operational decisions. Customers want Digital Twins of their industrial facilities, production processes, and equipment to simulate different scenarios to optimize their operations and predictive maintenance strategies.&lt;/p&gt; 
&lt;p&gt;In a prior post, we described &lt;a href="https://aws.amazon.com/blogs/iot/digital-twins-on-aws-unlocking-business-value-and-outcomes/"&gt;a four-level digital twin framework&lt;/a&gt; to help you understand these use cases and the technologies required to build them.&lt;/p&gt; 
&lt;p&gt;In this post today, we’re going to show you how to use an L4 &lt;em&gt;Living Digital Twin&lt;/em&gt; for scenario analysis and operational decision support for a manufacturing line. We’ll use &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow&lt;/a&gt; to combine a physics-based model created using &lt;a href="https://www.maplesoft.com/products/maplesim/"&gt;MapleSim&lt;/a&gt; with probabilistic Bayesian methods to calibrate the L4 DT so it can adapt to the changing real-world conditions as the equipment degrades over time.&lt;/p&gt; 
&lt;p&gt;Then we’ll show you how to use the calibrated L4 DT to perform scenario analysis and risk assessment to simulate possible outcomes and make informed decisions. MapleSim provides the tools to create engineering simulation models of machine equipment and TwinFlow is an AWS open-source framework for building and deploying predictive models at scale.&lt;/p&gt; 
&lt;h2&gt;L4 Living Digital Twin of roll-to-roll manufacturing&lt;/h2&gt; 
&lt;p&gt;For our use case, we considered the web-handling process in roll-to-roll manufacturing for continuous materials like paper, film, and textiles. The web-handling process involves unwinding the material from a spool, guiding it through various treatments like printing or coating, and then winding it onto individual rolls. It’s essential that we precisely control the tension, alignment, and speed to ensure smooth processing and maintain product quality.&lt;/p&gt; 
&lt;p&gt;Figure 1 shows a schematic diagram of the web-handling equipment consisting of 9 rollers (labeled with “R”) and 12 material spans (labeled with “S”).&lt;/p&gt; 
&lt;div id="attachment_3136" style="width: 635px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3136" loading="lazy" class="size-full wp-image-3136" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.03.46.png" alt="Figure 1 Schematic diagram of the web-handling equipment in which each span is labeled as S1 through S12 and rollers are label R1 through R10." width="625" height="199"&gt;
 &lt;p id="caption-attachment-3136" class="wp-caption-text"&gt;Figure 1 Schematic diagram of the web-handling equipment in which each span is labeled as S1 through S12 and rollers are label R1 through R10.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In our &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/"&gt;previous post&lt;/a&gt;, we showed you how to build and deploy L4 Digital Twin &lt;em&gt;self-calibrating&lt;/em&gt; virtual sensors for predicting the tension in each of the spans and the slip velocity at each of the rollers throughout the web-handling process.&lt;/p&gt; 
&lt;p&gt;An L4 Living Digital Twin focuses on modeling the behavior of the physical system by updating the model parameters using real world observations. The capability to update the model is what makes it a “living” digital twin that’s synchronized with the physical system and continuously adapting as the physical system evolves. These situations are common in industrial facilities or manufacturing plants as equipment degrades over time. Examples of real-world observations include continuous data (time-series data from physical sensors), discrete measurements (discrete sensor measurements), or discrete observations (visual inspection data).&lt;/p&gt; 
&lt;p&gt;In this post, we’re extending our earlier example of L4 Digital Twin self-calibrating virtual sensors to conduct &lt;em&gt;what-if&lt;/em&gt; scenario analysis predictions. Conceptually, an L4 Digital Twin forecast looks like Figure 2 which plots the predicted value over time. For our example, we focused on predicting the span tension within the material being manufactured. This is operationally important because tension failures occur when the tension within the material exceeds a threshold (175 Newtons in this example) resulting in product quality issues (e.g., scratches, breakage, wrinkling, or troughing).&lt;/p&gt; 
&lt;div id="attachment_3137" style="width: 645px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3137" loading="lazy" class="size-full wp-image-3137" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.04.28.png" alt="Figure 2 Conceptual plot showing historical measured data (observations) and L4 Digital Twin future forecast with uncertainty bounds showing when the predicted value will cross the failure threshold." width="635" height="380"&gt;
 &lt;p id="caption-attachment-3137" class="wp-caption-text"&gt;Figure 2 Conceptual plot showing historical measured data (observations) and L4 Digital Twin future forecast with uncertainty bounds showing when the predicted value will cross the failure threshold.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The vertical line (marked “&lt;em&gt;Today&lt;/em&gt;”) shows the delineation between historical data collected in the past (the blue dots) and the future prediction with uncertainty bounds. In our case, the historical observations are values determined from the past inferred viscous damping coefficients because physical sensor measurements weren’t available. The L4 Digital Twin makes a prediction of the tension value today, and at all points in the future, including when the tension value will cross the damage threshold. The delta between &lt;em&gt;Today&lt;/em&gt; and the time at which the prediction crosses the threshold represents the amount of time the operator has available to take some corrective action.&lt;/p&gt; 
&lt;p&gt;Of course, we can’t predict the future with perfect certainty so it’s important that all predictions be quantified with uncertainty bounds. This allows the operator to plan their corrective actions based on their risk tolerance. If the operator is risk averse, then they’d take corrective actions before the earliest uncertainty band crosses the threshold. In this way, they have a high probability of applying corrective action before failure happens. If they’re risk neutral, they would use the mean value. If they are willing to accept high risk, they could delay scheduling corrective actions to the later uncertainty band, recognizing that there’s a high probability of failure before they apply corrective actions. This last option may be logical when part replacement is more costly than lost production downtime.&lt;/p&gt; 
&lt;p&gt;In our example, we’re focusing on tension failure that results in a product quality issue, but the exact same approach can be used to predict equipment failure and remaining useful life (RUL) to proactively develop preventive maintenance plans.&lt;/p&gt; 
&lt;h2&gt;Example scenario analysis&lt;/h2&gt; 
&lt;p&gt;A potential scenario requiring risk assessment involves the detection of gradual dirt build-up on the roller bearings. Using the time series of the component degradation, we can predict when a failure could occur in the future. This knowledge lets us estimate when maintenance should be done.&lt;/p&gt; 
&lt;p&gt;The end result is that we can 1) maximize throughput of the manufacturing line to increase revenue and; 2) reduce costs by reducing defects and unnecessary downtime. A &lt;em&gt;third&lt;/em&gt; benefit that’s rather application specific is a more targeted maintenance scheme. If we regularly schedule maintenance on all components, we run the risk of performing maintenance on components that don’t need servicing, increasing waste. Using a calibrated digital twin, we can specifically target the components that are degrading instead of everything at once.&lt;/p&gt; 
&lt;p&gt;To solidify these ideas, let’s think about the web-handling roll-to-roll manufacturing line again. Using the L4 Digital Twin we described in our last post lets us simulate a synthetic degradation scenario. In that scenario, we simulated bearing degradation by manually increasing the viscous damping coefficient for roller 9 from 0 on Day 15 to 0.2 on Day 19.&lt;/p&gt; 
&lt;p&gt;In Figure 3, we see the L4 Digital Twin is capturing that roller 9’s viscous damping coefficient is increasing from Day 15 to Day 19. We can tell that viscous damping is increasing, causing a change in tension in multiple spans, and we need to decide our course of action. The immediate risk to assess is whether we need to shut down the line to perform maintenance – or not.&lt;/p&gt; 
&lt;p&gt;Imagine that the dirt build up is detected on a Friday morning and the maintenance crew will be out on Monday because it’s a 3-day holiday weekend. Can we continue to generate product over the weekend and run the risk of increasing defects, or is it more cost effective to shut down the line for repairs on Friday before the maintenance crew heads home? Making this decision requires a &lt;em&gt;forecast&lt;/em&gt; of the viscous damping coefficient and the span tension, &lt;em&gt;and&lt;/em&gt; the uncertainty around these predictions. For example, predicting the damage threshold will be exceeded in 4 days ± 1 day means the maintenance can be deferred to Tuesday. If the prediction is 4 days ± 3 days, then it could very well be an issue tomorrow when the maintenance crew is out for the weekend.&lt;/p&gt; 
&lt;div id="attachment_3138" style="width: 878px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3138" loading="lazy" class="size-full wp-image-3138" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.05.06.png" alt="Figure 3 The inferred viscous damping coefficients calculated via TwinFlow UKF with MapleSim digital twin. Viscous damping coefficient of roller 9 is predicted to be increasing over the last several days" width="868" height="784"&gt;
 &lt;p id="caption-attachment-3138" class="wp-caption-text"&gt;Figure 3 The inferred viscous damping coefficients calculated via TwinFlow UKF with MapleSim digital twin. Viscous damping coefficient of roller 9 is predicted to be increasing over the last several days&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Using TwinFlow to perform scenario analysis&lt;/h2&gt; 
&lt;p&gt;Like in our previous post, we first used MapleSim to create the physics-based model of the web-handling process and exported the model as a Modelica Functional Mockup Unit (FMU) which is an industry standard file format for simulation models.&lt;/p&gt; 
&lt;p&gt;We then used &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow&lt;/a&gt; to combine the MapleSim model with an &lt;a href="https://en.wikipedia.org/wiki/Kalman_filter"&gt;Unscented Kalman Filter (UKF)&lt;/a&gt; to infer the viscous damping coefficients of the rollers in the manufacturing line. This calibration process tunes the model based on the available physical sensor data (rotation speeds of the rollers). In that previous post, we showed how the self-calibrated virtual sensors then used the incoming sensor measurements (roller angular velocity and rotation speeds) to predict tension and slip velocity at any moment in time. Here, we’ll show how to use the same L4 Digital Twin to probabilistically forecast when the span tension will exceed the threshold. Readers can find this example described in an &lt;a href="https://aws.amazon.com/solutions/guidance/self-calibrating-level-4-digital-twins-on-aws/"&gt;AWS Solution&lt;/a&gt;, and a full CDK deployment of the code used for this example up on &lt;a href="https://github.com/aws-solutions-library-samples/guidance-for-self-calibrating-level-4-digital-twins-on-aws"&gt;Github&lt;/a&gt; with instructions about how to customize for your application and deploy it.&lt;/p&gt; 
&lt;p&gt;Using TwinFlow we combine the calibrated L4 Digital Twin with forecasting models to both predict when we’ll exceed the failure threshold – and the uncertainty associated with that prediction. There are a large variety of forecasting models that range from parametric models that work well on small data sets, to non-parametric deep-learning models that often require larger data sets and hyperparameter tuning.&lt;/p&gt; 
&lt;p&gt;For our application, we specifically want an estimate of the uncertainty in the forecast and a non-parametric method so that we don’t need to manually develop the model. A &lt;a href="https://en.wikipedia.org/wiki/Gaussian_process"&gt;Gaussian Process&lt;/a&gt; (GP) is a natural fit for these requirements and a scalable low-code representation is available in TwinFlow. Using the historical time series data of the viscous damping coefficient, we can fit a GP to the data and forecast 4 days into the future along with the 95% uncertainty – as we’ve done in Figure 4.&lt;/p&gt; 
&lt;div id="attachment_3139" style="width: 838px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3139" loading="lazy" class="size-full wp-image-3139" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.05.35.png" alt="Figure 4 Forecast of viscous damping coefficient over next 4 days." width="828" height="372"&gt;
 &lt;p id="caption-attachment-3139" class="wp-caption-text"&gt;Figure 4 Forecast of viscous damping coefficient over next 4 days.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The uncertainty represents the prediction of possible future states for the viscous damping coefficients which we can then use with our L4 Digital Twin to forecast the span tension like in Figure 5. This figure corresponds to the conceptual diagram we drew in Figure 2 and you can tell that the tension crosses the threshold at 21.7 days with a lower uncertainty bound of 20.9 days. Given that today is Day 19, we can restate this as &lt;em&gt;failure is expected in 2.7 days with an uncertainty lower bound of 1.9 days from today&lt;/em&gt;. We now have enough information to decide whether to shut down the manufacturing line for maintenance or not.&lt;/p&gt; 
&lt;p&gt;In our example, the operator notices a potential issue on Friday morning before the Monday long weekend. Our prediction is that failure will most likely occur after 2.7 days (Mon), but it could be as soon as 1.9 days (Sun) and the operator should preemptively perform the repair to avoid risking losing the weekend production. This type of analysis is very challenging for an operator to perform and the decision is often based on best judgement and operator experience. An additional benefit is that the L4 Digital Twin is modeling each component within the web-handling line – enabling the operator to identify &lt;em&gt;which specific rollers&lt;/em&gt; are likely to require maintenance instead of spending time and money on all the rollers in the line.&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt;
&lt;div id="attachment_3140" style="width: 874px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3140" loading="lazy" class="size-full wp-image-3140" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.06.07.png" alt="Figure 5 Plot showing historical measured data (observations) and L4 Digital Twin future forecast with uncertainty bounds showing when the tension will cross the failure threshold [175 N]." width="864" height="387"&gt;
 &lt;p id="caption-attachment-3140" class="wp-caption-text"&gt;Figure 5 Plot showing historical measured data (observations) and L4 Digital Twin future forecast with uncertainty bounds showing when the tension will cross the failure threshold [175 N].&lt;/p&gt;
&lt;/div&gt;The value of using an L4 Digital Twin is further understood by examining the probability distributions of the forecasts. While the GP uncertainty of the viscous damping coefficient is (by definition) a Gaussian distribution, the resulting uncertainty of the tension (the shaded region) in Figure 5 isn’t symmetric around the mean value – indicating that the probability distribution at each time slice is 
&lt;em&gt;not&lt;/em&gt; Gaussian (without this information, it would likely be assumed to be Gaussian).
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;Figure 6 shows the forecast result at different time slices after being inserted into the digital twin. We can see that behavior is very non-linear with bimodal probability distributions, unlike the presumed normal (Gaussian) distributions shown in the conceptual diagram in Figure 2. We see the value of the L4 Digital Twin here because it’s impossible to know before-hand what the probability distributions would have been – and using the common assumption of Gaussian distributions would have resulted in inaccurate predictions.&lt;/p&gt; 
&lt;p&gt;The physical cause of this non-Gaussian distribution is the nature of the physics. The changing viscous damping coefficients can exhibit discontinuous changes in material slip, resulting in abrupt tension changes. This wouldn’t have been detected if we just used a data-only approach.&lt;/p&gt; 
&lt;div id="attachment_3141" style="width: 631px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3141" loading="lazy" class="size-full wp-image-3141" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.06.40.png" alt="Figure 6 Probability density distributions for max span tension at different forecasted time slices. " width="621" height="440"&gt;
 &lt;p id="caption-attachment-3141" class="wp-caption-text"&gt;Figure 6 Probability density distributions for max span tension at different forecasted time slices.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;AWS Architecture&lt;/h2&gt; 
&lt;p&gt;The AWS architecture used for the L4 Digital Twin risk assessment is depicted in Figure 7. This architecture assumes the user has already enabled an &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/"&gt;L4 self-calibrating digital twin&lt;/a&gt; where the output of the previous architecture is being pushed into an &lt;a href="https://aws.amazon.com/iot-sitewise/"&gt;AWS IoT SiteWise&lt;/a&gt; database. You can find a full CDK deployment of the code used for the example in this post on &lt;a href="https://github.com/aws-solutions-library-samples/guidance-for-self-calibrating-level-4-digital-twins-on-aws"&gt;Github&lt;/a&gt; with instructions on how to customize for your application and deployment. In Figure 7, steps 1 – 3 are the same as the Level 4 digital twin.&lt;/p&gt; 
&lt;p&gt;With a small amount of code, we can use TwinFlow to pull down the IoT SiteWise data to an &lt;a href="https://aws.amazon.com/ec2/"&gt;Amazon EC2&lt;/a&gt; instance. We then fit a Gaussian Process (from the TwinStat module in TwinFlow) to the AWS IoT SiteWise data, forecast potential future outcomes, and then sampled the potential outcomes to obtain a data set to simulate with our digital twin.&lt;/p&gt; 
&lt;p&gt;Once we have a dataset, we can submit the different inputs to the cloud (step 4) like an on-premises HPC cluster. The main point of divergence from a standard on-premises HPC cluster is the requirement to containerize the application-specific code. &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; is our cloud-native HPC scheduler that includes backend options for &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon ECS&lt;/a&gt;, which is an AWS-specific container orchestrations service, &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt;, which is a serverless execution option, and &lt;a href="https://aws.amazon.com/eks/"&gt;Amazon EKS&lt;/a&gt; which is our Kubernetes option. We used Amazon ECS because we wanted EC2 instances with large numbers of CPUs than those available for Fargate. Also, ECS enables fully-automated deployment unlike EKS.&lt;/p&gt; 
&lt;p&gt;TwinFlow reads a task list, or loads from memory, the various scenarios to be simulated. A container that includes the MapleSim digital twin and any application-specific automation is stored in a container within ECR for cloud access. The specific EC2 instance type is automatically selected by AWS Batch auto-scaling based on the user-defined CPU/GPU and memory requirements.&lt;/p&gt; 
&lt;p&gt;At step 5, the output predictions of the L4 DT simulations are generated with textual explanations and stored in an &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service&lt;/a&gt; (Amazon S3) bucket which can then be made available to users via an interface. We can also store the prediction results in a database such as &lt;a href="https://aws.amazon.com/rds/"&gt;Amazon RDS&lt;/a&gt; (step 6), which can be pulled back into &lt;a href="https://aws.amazon.com/iot-twinmaker/"&gt;AWS IoT TwinMaker&lt;/a&gt; to compare with other data streams or review past forecasts for accuracy assessment.&lt;/p&gt; 
&lt;div id="attachment_3142" style="width: 865px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3142" loading="lazy" class="size-full wp-image-3142" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.07.13.png" alt="Figure 7 AWS cloud architecture needed to achieve digital twin periodic-calibration" width="855" height="412"&gt;
 &lt;p id="caption-attachment-3142" class="wp-caption-text"&gt;Figure 7 AWS cloud architecture needed to achieve digital twin periodic-calibration&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;In this post, we showed how to use a L4 Digital Twin to perform risk assessment and scenario analysis using an FMU model created by MapleSim. MapleSim provides the physics-based model in the form of an FMU and TwinFlow allows us to run scalable number of scenarios on AWS, providing efficient and elastic computing resources. In this L4 Digital Twin example the versatility of the hybrid modeling-based approach is demonstrated as the means of predicting fault scenarios that may have been missed if solely relying on physics-only or data-only based methods. Using the information gained from the scenario analysis enables you to do risk assessment and informed decision making.&lt;/p&gt; 
&lt;p&gt;If you want to request a proof of concept or if you have feedback on the AWS tools, please reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-21.24.55.png" alt="Kayla Rossi" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Kayla Rossi&lt;/h3&gt; 
  &lt;p&gt;Kayla is an Application Engineer at Maplesoft providing customers with advanced engineering support for the digital modeling of their complex production machines. Her project experience includes the simulation and analysis of web handling and converting systems across various industrial applications, including printing and battery production.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-21.25.00.png" alt="Orang Vahid" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Orang Vahid&lt;/h3&gt; 
  &lt;p&gt;Orang has over 20 years of experience in system-level modeling, advanced dynamic systems, frictional vibration and control, automotive noise and vibration, and mechanical engineering design. He is a frequent invited speaker and has published numerous papers on various topics in engineering design. At Maplesoft he is Director of Engineering Services, with a focus on the use and development of MapleSim solutions.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Slurm REST API in AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/slurm-rest-api-in-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Sean Smith]]></dc:creator>
		<pubDate>Wed, 06 Dec 2023 15:30:55 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">c6ddf535719513e79c43687d50d07ddd004a82a6</guid>

					<description>Looking to integrate AWS ParallelCluster into an automated workflow? This post shows how to submit and monitor jobs programmatically with Slurm REST API (code examples included).</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Sean Smith, Sr HPC Solution Architect, and Ryan Kilpadi, SDE Intern, HPC&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;AWS ParallelCluster offers powerful compute capabilities for problems ranging from discovering new drugs, to designing F1 race cars, to predicting the weather. In all these cases there’s a need for a human to sit in the loop – maybe an engineer running a simulation or perhaps a scientist submitting their lab results for analysis.&lt;/p&gt; 
&lt;p&gt;In this post we’ll show you how to programmatically submit and monitor jobs using the open-source Slurm REST API. This allows ParallelCluster to be integrated into an automated system via API calls. For example, this could mean that whenever a genome sample is read from a sequencer, it’s automatically fed through a secondary analysis pipeline to align the individual reads, or when new satellite data lands in an Amazon S3 bucket, it triggers a job to create the latest weather forecast.&lt;/p&gt; 
&lt;p&gt;Today, we’ll show how to set this up with AWS ParallelCluster. We’ll also link to a GitHub repository with code you can use and show examples of how to call the API using both curl and Python.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;This diagram shows an example cluster architecture with the Slurm REST API. The REST API runs on the HeadNode and submits jobs to the compute queues. The credentials used to authenticate with the API are stored in AWS Secrets Manager. The compute queues shown are examples only: the cluster can be configured with any instance configuration you desire.&lt;/p&gt; 
&lt;div id="attachment_3120" style="width: 849px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3120" loading="lazy" class="size-full wp-image-3120" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/27/CleanShot-2023-11-27-at-13.37.39.png" alt="Figure 1 – The REST API runs on the HeadNode and submits jobs to the compute queues. The credentials used to authenticate with the API are stored in AWS Secrets Manager. The compute queues shown are examples only, the cluster can be configured with any instance configuration you desire." width="839" height="385"&gt;
 &lt;p id="caption-attachment-3120" class="wp-caption-text"&gt;Figure 1 – The REST API runs on the HeadNode and submits jobs to the compute queues. The credentials used to authenticate with the API are stored in AWS Secrets Manager. The compute queues shown are examples only, the cluster can be configured with any instance configuration you desire.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For this tutorial, we’ll be using ParallelCluster UI to set up our cluster with the Slurm REST API enabled. To set up ParallelCluster UI, &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-pcui-v3.html"&gt;refer to our online documentation&lt;/a&gt;. If you’d rather use the ParallelCluster CLI, see the example YAML configuration step 5.&lt;/p&gt; 
&lt;h2&gt;Step 1 – Create a Security Group to allow inbound API requests&lt;/h2&gt; 
&lt;p&gt;By default, your cluster will not be able to accept incoming HTTPS requests to the REST API. You will need to create a security group to allow traffic from outside the cluster to call the API.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to the &lt;a href="http://console.aws.amazon.com/ec2/#SecurityGroups"&gt;EC2 Security Group console&lt;/a&gt; and choose &lt;strong&gt;Create security group&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Under Security group name, enter &lt;code&gt;Slurm REST API&lt;/code&gt;&amp;nbsp;(or another name of your choosing)&lt;/li&gt; 
 &lt;li&gt;Ensure the VPC matches your cluster’s VPC&lt;/li&gt; 
 &lt;li&gt;Add an Inbound rule and select &lt;code&gt;HTTPS&lt;/code&gt; under Type, then change the Source to only the CIDR range that you want to have access. For example, you can use the CIDR associated with your VPC to restrict access to within your VPC.&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;Create security group&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_3121" style="width: 931px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3121" loading="lazy" class="size-full wp-image-3121" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/27/CleanShot-2023-11-27-at-13.39.07.png" alt="Figure 2 – Create your security group, adding a VPC and an inbound rule to allow only HTTPS connections from a specific CID" width="921" height="453"&gt;
 &lt;p id="caption-attachment-3121" class="wp-caption-text"&gt;Figure 2 – Create your security group, adding a VPC and an inbound rule to allow only HTTPS connections from a specific CID&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Step 2 – Add Additional IAM Permissions&lt;/h2&gt; 
&lt;p&gt;If you’re using the AWS ParallelCluster UI, please follow the instructions under the ParallelCluster UI Tutorial &lt;strong&gt;section ‘G’&lt;/strong&gt; – &lt;a href="https://pcluster.cloud/02-tutorials/07-setup-iam.html"&gt;Setup IAM Permissions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Step 3 – Configure your Cluster&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;In your cluster configuration, return to the &lt;strong&gt;HeadNode section &amp;gt; Advanced options &amp;gt; Additional Security Groups &amp;gt;&lt;/strong&gt; add the &lt;em&gt;Slurm REST API&lt;/em&gt; Security Group you created in Step 1. Under &lt;strong&gt;Scripts &amp;gt; on node configured &amp;gt;&lt;/strong&gt; add the following script:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/rest-api/postinstall.sh&lt;/code&gt;&lt;/pre&gt; 
&lt;div id="attachment_3122" style="width: 639px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3122" loading="lazy" class="size-full wp-image-3122" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/27/CleanShot-2023-11-27-at-13.41.18.png" alt="Figure 4 – Add the script to be run after node configuration" width="629" height="288"&gt;
 &lt;p id="caption-attachment-3122" class="wp-caption-text"&gt;Figure 4 – Add the script to be run after node configuration&lt;/p&gt;
&lt;/div&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Under &lt;strong&gt;Additional IAM permissions&lt;/strong&gt;, add the policy:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;arn:aws:iam::aws:policy/SecretsManagerReadWrite&lt;/code&gt;&lt;/pre&gt; 
&lt;div id="attachment_3123" style="width: 838px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3123" loading="lazy" class="size-full wp-image-3123" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/27/CleanShot-2023-11-27-at-13.42.08.png" alt="Figure 5 – Add IAM policy to allow updates to AWS SecretsManager. This is needed to automatically refresh the JSON Web Token (JWT)." width="828" height="231"&gt;
 &lt;p id="caption-attachment-3123" class="wp-caption-text"&gt;Figure 5 – Add IAM policy to allow updates to AWS SecretsManager. This is needed to automatically refresh the JSON Web Token (JWT).&lt;/p&gt;
&lt;/div&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Create your cluster.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Step 4 – Validate the configuration&lt;/h2&gt; 
&lt;p&gt;Your configuration file should look something like the text that follows. If you opted to use the CLI instead of the UI, you will need to replace:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;AdditionalSecurityGroups&lt;/code&gt; – this should contain an additional security group that allows connections to the Slurm REST API (Step 1).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;OnNodeConfigured&lt;/code&gt;: thish should reference the post-install script: &lt;code&gt;&lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/rest-api/postinstall.sh"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/rest-api/postinstall.sh&lt;/a&gt;&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Imds:
  ImdsSupport: v1.0
HeadNode:
  InstanceType: c5.xlarge
  Imds:
    Secured: true
  Ssh:
    KeyName: amzn2
  LocalStorage:
    RootVolume:
      VolumeType: gp3
  Networking:
    SubnetId: subnet-xxxxxxxxxxxxxx
    AdditionalSecurityGroups:
      - sg-slurmrestapixxxxxxxxxx
  Iam:
    AdditionalIamPolicies:
      - Policy: arn:aws:iam::aws:policy/SecretsManagerReadWrite
  CustomActions:
    OnNodeConfigured:
      Script: &amp;gt;-
        &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/rest-api/postinstall.sh" rel="noopener noreferrer"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/rest-api/postinstall.sh&lt;/a&gt;
Scheduling:
  Scheduler: slurm
  SlurmQueues:
    - Name: queue-1
      ComputeResources:
        - Name: queue-1-cr-1
          Instances:
            - InstanceType: c5.xlarge
          MinCount: 0
          MaxCount: 4
      ComputeSettings:
        LocalStorage:
          RootVolume:
            VolumeType: gp3
      Networking:
        SubnetIds:
          - subnet-xxxxxxxxxxxxxxxxxx
Region: us-east-2
Image:
	Os: alinux2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Step 5 – Call the API&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Log in to a machine on the same network that you allowed via the Security Group in Step 1. Make sure this machine is able to talk to the HeadNode.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;ssh username@ip&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Set the following environment variable:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;export CLUSTER_NAME=[name of cluster]&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Find the information needed to call the API and construct an API request. To do this, we’ll need a few pieces of information. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;JWT token&lt;/strong&gt;: The post install script will have created a secret in &lt;a href="http://console.aws.amazon.com/secretsmanager"&gt;AWS SecretsManager&lt;/a&gt; under the name &lt;code&gt;slurm_token_$CLUSTER_NAME&lt;/code&gt; . Either use the AWS console or the AWS CLI to find your secret based on the cluster name:&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;export JWT=$(aws secretsmanager get-secret-value --secret-id slurm_token_$CLUSTER_NAME | jq -r '.SecretString')&lt;/code&gt;NOTE: Since the Slurm REST API script is not integrated into ParallelCluster, this secret will not be automatically deleted along with the cluster. You may want to remove it manually on cluster deletion.&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li style="list-style-type: none"&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Head node public IP:&lt;/strong&gt; This can be found in your Amazon EC2 dashboard or by using the ParallelCluster CLI:&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;export HEADNODE_IP=$(pcluster describe-cluster-instances -n $CLUSTER_NAME | jq -r '.instances[0].publicIpAddress')&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li style="list-style-type: none"&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Cluster user&lt;/strong&gt;: This depends on your AMI, but it will usually be either ec2-user , ubuntu , or centos.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;export CLUSTER_USER=ec2-user&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Call the API using curl:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;curl -H "X-SLURM-USER-NAME: $CLUSTER_USER" -H "X-SLURM-USER-TOKEN: $JWT" https://$HEADNODE_IP/slurm/v0.0.39/ping -k&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You’ll get a response back like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;{
    "meta": {
        "plugin": {
            "type": "openapi\/v0.0.39",
            "name": "REST v0.0.39"
        },
        "Slurm": {
            "version": {
            "major": 23,
            "micro": 2,
            "minor": 2
        },
         "release": "23.02.2"
    }...
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Submit a job using the API. Specify the job parameters using JSON. You may need to modify the standard directories depending on the cluster user.&lt;/li&gt; 
 &lt;li&gt;Post a job to the API:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;curl -H "X-SLURM-USER-TOKEN: $CLUSTER_USER" -H "X-SLURM-USER-TOKEN: $JWT" -X POST https://$IP/slurm/v0.0.39/job/submit -H "Content-Type: application/json" -d @testjob.json -k&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;c. Verify that the job is running:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;curl -H "X-SLURM-USER-NAME: $CLUSTER_USER" -H "X-SLURM-USER-TOKEN: $JWT" https://$IP/slurm/v0.0.39/jobs -k&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Calling the API using the Python requests library&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a script called &lt;code&gt;slurmapi.py&lt;/code&gt; with the following contents:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;#!/usr/bin/env python3
import argparse
import boto3
import requests
import json

# Create argument parser
parser = argparse.ArgumentParser()
parser.add_argument('-n', '--cluster-name', type=str, required=True)
parser.add_argument('-u', '--cluster-user', type=str, required=False)
subparsers = parser.add_subparsers(dest='command', required=True)

diag_parser = subparsers.add_parser('diag', help="Get diagnostics")
ping_parser = subparsers.add_parser('ping', help="Ping test")

submit_job_parser = subparsers.add_parser('submit-job', help="Submit a job")
submit_job_parser.add_argument('-j', '--job', type=str, required=True)

list_jobs_parser = subparsers.add_parser('list-jobs', help="List active jobs")

describe_job_parser = subparsers.add_parser('describe-job', help="Describe a job by id")
describe_job_parser.add_argument('-j', '--job-id', type=int, required=True)

cancel_parser = subparsers.add_parser('cancel-job', help="Cancel a job")
cancel_parser.add_argument('-j', '--job-id', type=int, required=True)

args = parser.parse_args()

# Get JWT token
client = boto3.client('secretsmanager')
boto_response = client.get_secret_value(SecretId=f'slurm_token_{args.cluster_name}')
jwt_token = boto_response['SecretString']

# Get cluster headnode IP
client = boto3.client('ec2')
filters = [{'Name': 'tag:parallelcluster:cluster-name', 'Values': [args.cluster_name]}]
boto_response = client.describe_instances(Filters=filters)
headnode_ip = boto_response['Reservations'][0]['Instances'][0]['PublicIpAddress']

url = f'https://{headnode_ip}/slurm/v0.0.39'
headers = {'X-SLURM-USER-TOKEN': jwt_token}
if args.cluster_user:
    headers['X-SLURM-USER-NAME'] = args.cluster_user

# Make request
if args.command == 'ping':
    r = requests.get(f'{url}/ping', headers=headers, verify=False)
elif args.command == 'diag':
    r = requests.get(f'{url}/diag', headers=headers, verify=False)
elif args.command == 'submit-job':
    with open(args.job) as job_file:
        job_json = json.load(job_file)
    r = requests.post(f'{url}/job/submit', headers=headers, json=job_json, verify=False)
elif args.command == 'list-jobs':
    r = requests.get(f'{url}/jobs', headers=headers, verify=False)
elif args.command == 'describe-job':
    r = requests.get(f'{url}/job/{args.job_id}', headers=headers, verify=False)
elif args.command == 'cancel-job':
    r = requests.delete(f'{url}/job/{args.job_id}', headers=headers, verify=False)

print(r.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Submitt a job&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;./slurmapi.py -n [cluster_name] submit-job -u [cluster_user] -j testjob.json&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Getting more information&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;./slurmapi.py -h&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Setting up the Slurm REST API allows you to programmatically control the cluster, this makes it possible to build the cluster into an automated workflow. This enables new use cases such as automated secondary analysis of genomics data, risk analysis in financial markets, weather prediction, among a myriad of other use cases. We’re excited to see what you build, drop us a line on twitter to showcase what you come up with.&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/27/CleanShot-2023-11-27-at-14.07.01.png" alt="Ryan Kilpadi " width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Ryan Kilpadi&lt;/h3&gt; 
  &lt;p&gt;Ryan Kilpadi is a returning SDE intern on the HPC team working on AWS ParallelCluster. He worked on implementing the Slurm REST API on ParallelCluster as a summer internship project in 2022.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>New: Research and Engineering Studio on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/new-research-and-engineering-studio-on-aws/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Mon, 13 Nov 2023 22:14:01 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[End User Computing]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[DCV]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[FEA]]></category>
		<category><![CDATA[Finite Element Analysis]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[VDI]]></category>
		<category><![CDATA[visualization]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">e085244ba2853fcab2e7687e1f7ed8d1e80e089c</guid>

					<description>Today we’re announcing Research and Engineering Studio on AWS, a self-service portal to help scientists and engineers access and manage virtual desktops to see their data and run their interactive applications in the cloud.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="size-full wp-image-3095 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/08/Ingenio-icon_2pt-line-version-illustration.png" alt="" width="380" height="235"&gt;Your brain is extravagantly equipped for visualization. More than 50% of your cerebral cortex is dedicated to processing sensory input from your eyes, making it the information super highway that helps complex ideas form in your head incredibly quickly. Our output devices – our limbs and our voices – had to develop unique articulation skills, just to match the speed at which we can &lt;em&gt;learn from seeing&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;So, it’s fitting that today we’re announcing Research and Engineering Studio on AWS (RES), a self-service portal to help scientists and engineers access and manage their research and design workspaces, including virtual desktops to &lt;em&gt;see&lt;/em&gt; their data and run their interactive applications in the cloud.&lt;/p&gt; 
&lt;p&gt;Setting up – and then managing – virtual desktops manually takes a lot of work, taking time away from the core mission of research and development groups: solving hard scientific problems. We’ve also noticed that when processes to access resources become complicated, users often take shortcuts (like sharing passwords or setting file permissions to be globally read/write).&lt;/p&gt; 
&lt;p&gt;With RES, we’re putting all the launch and monitoring tools behind a single pane of glass, so administrators can deploy new desktop environments quickly, from their own pre-prepared library of software images and trusted applications. This gives their users the freedom to come and go when needed – incredibly important when chasing new lines of inquiry – without impacting their budgets or the security posture of their organization.&lt;/p&gt; 
&lt;p&gt;In today’s post, we’ll explain what RES is and how it works, and we’ll explain how to deploy it in your own AWS account, and get it ready for your users.&lt;/p&gt; 
&lt;h2&gt;Visualization without the headaches&lt;/h2&gt; 
&lt;p&gt;Research and Engineering Studio on AWS has been designed to make it easier for site admins to provide a suite of applications, data, and large-scale compute resources through familiar interfaces – starting today with interactive desktops.&lt;/p&gt; 
&lt;p&gt;When you login to RES through your browser, you’ll have the ability to see the virtual desktops that are available to you, based on permissions that draw from your membership of user &lt;strong&gt;Groups&lt;/strong&gt; that are associated with &lt;strong&gt;Projects&lt;/strong&gt;. We’ll explain this a little further on.&lt;/p&gt; 
&lt;p&gt;On the main Virtual Desktops screen, you’ll have the ability to see thumbnails of sessions that are live and running, and other sessions that are suspended. All have sets of controls to spin them up, or shut them down. You can even schedule sessions start and stop during specific time windows (like 9-5 Mon-Fri, if that’s your thing). This is also where you can create new sessions, based on software stacks your admin has provided for you – this only take a couple of minutes, typically.&lt;/p&gt; 
&lt;p&gt;By clicking &lt;strong&gt;Connect&lt;/strong&gt; in the RES interface for a running session, you can open a new browser tab containing an encrypted shared desktop session with the remote system using NICE DCV – our own &lt;a href="https://aws.amazon.com/blogs/hpc/pushing-pixels-with-nice-dcv/"&gt;high performance streaming protocol&lt;/a&gt;. DCV uses a lot of tricks with network protocols designed to make remote desktops feel like they’re local. The techniques it uses to do this are remarkable enough that Netflix uses it to provide their editors and FX teams access to powerful Amazon EC2 systems so they can produce the great content you’re probably watching this week on TV. This is great for scientists and engineers, since our interactive applications often need precision screen controls and only work well with access to truly large memory models.&lt;/p&gt; 
&lt;div id="attachment_3096" style="width: 853px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3096" loading="lazy" class="size-full wp-image-3096" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/08/CleanShot-2023-11-08-at-10.57.51.png" alt="Figure 1 - the RES Virtual Desktops screen lists all the sessions a user has previously created, with controls to spin up, shutdown, or schedule uptime." width="843" height="481"&gt;
 &lt;p id="caption-attachment-3096" class="wp-caption-text"&gt;Figure 1 – the RES Virtual Desktops screen lists all the sessions a user has previously created, with controls to spin up, shutdown, or schedule uptime.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;RES allows users to share their desktops, too. While the sessions they start in RES will always belong to them, they can share a session too, like the way you can do that in a conferencing system like Zoom. This can be incredibly useful for working sessions where a group of engineers in a team need draw design insights by poring over visualizations of massive datasets – without needing to move the datasets themselves. A desktop share can have an expiration date attached to it, too.&lt;/p&gt; 
&lt;div id="attachment_3104" style="width: 1767px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3104" loading="lazy" class="wp-image-3104 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/09/share-session.png" alt="Figure 2 - RES permits users to share their virtual desktop sessions with others in a controlled fashion." width="1757" height="988"&gt;
 &lt;p id="caption-attachment-3104" class="wp-caption-text"&gt;Figure 2 – RES permits users to share their virtual desktop sessions with others in a controlled fashion.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Projects, users, and groups&lt;/h2&gt; 
&lt;p&gt;To manage all of this, RES works from a few important concepts, all of which are likely familiar to you already.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;First is the &lt;strong&gt;Project&lt;/strong&gt;. In most organizations – public and private – everything revolves around &lt;em&gt;project teams&lt;/em&gt;. Resources are typically created for – and consumed by – project teams, with access, billing, and even software stacks being driven by project needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Users and Groups: &lt;/strong&gt;Users can be members of multiple Groups, and in RES a Group can be associated with one or more Projects.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Like most institutions, you probably have an investment in a user identity and authentication system, complete with existing project groups that you’ve defined to define access for shared files and other resources. We wanted to align with that, so RES – out of the box – is built to connect with your existing directory service, starting with Active Directory (AD), helping you avoid doing this work twice.&lt;/p&gt; 
&lt;p&gt;At launch, RES can connect to your existing AD server &lt;em&gt;on-premises&lt;/em&gt; or &lt;em&gt;in the cloud&lt;/em&gt;. If you want to separate your RES environment from your regular user space – to create an engineering sandbox, for instance – you can spin up a managed AD using AWS Directory Service to privately manage a set of users.&lt;/p&gt; 
&lt;div id="attachment_3133" style="width: 3526px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3133" loading="lazy" class="wp-image-3133 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/01/users-shot.png" alt="Figure 3 - Identity functions, like defining users and groups, are done centrally in an Active Directory environment, which you specify when installing RES in your AWS environment. The AD can be on-premises or in the cloud and can - if you wish - be separate from your corporate directory service." width="3516" height="1976"&gt;
 &lt;p id="caption-attachment-3133" class="wp-caption-text"&gt;Figure 3 – Identity functions, like defining users and groups, are done centrally in an Active Directory environment, which you specify when installing RES in your AWS environment. The AD can be on-premises or in the cloud and can – if you wish – be separate from your corporate directory service.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;RES enables access to &lt;strong&gt;Project&lt;/strong&gt; resources, by specifying which &lt;strong&gt;Groups&lt;/strong&gt; are assigned to the &lt;strong&gt;Project&lt;/strong&gt;. You define this at Project creation, but of course you can change it afterwards, too (see Figure 4).&lt;/p&gt; 
&lt;div id="attachment_3107" style="width: 1768px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3107" loading="lazy" class="wp-image-3107 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/09/create-project.png" alt="Figure 4 - when you create a Project in RES, you can specify which user groups are permitted to access its resources. The Groups and Users are inherited from your Active Directory service." width="1758" height="988"&gt;
 &lt;p id="caption-attachment-3107" class="wp-caption-text"&gt;Figure 4 – when you create a Project in RES, you can specify which user groups are permitted to access its resources. The Groups and Users are inherited from your Active Directory service.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;RES is designed to bind to your AD and draw its knowledge of your teams from there, which allows RES to reflect changes you make to Group memberships right away.&lt;/p&gt; 
&lt;h2&gt;Storage&lt;/h2&gt; 
&lt;p&gt;At launch, RES supports two file systems which customers have told us are most useful for managing users’ files. Amazon Elastic File System (EFS) and Amazon FSx for NetApp ONTAP. While EFS is great for customers with large Linux deployments using NFSv4, FSx for NetApp ONTAP can offer the same file system using either NFS or SMB – which is helpful if you’re running a hybrid Windows and Linux environment and your users regularly flip back and forth, depending on the applications they’re using.&lt;/p&gt; 
&lt;div id="attachment_3100" style="width: 851px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3100" loading="lazy" class="size-full wp-image-3100" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/08/CleanShot-2023-11-08-at-10.59.48.png" alt="Figure 5 - You can manage file systems in RES, and associate them with Projects. At launch, RES supports Amazon EFS, and Amazon FSx for NetApp ONTAP." width="841" height="229"&gt;
 &lt;p id="caption-attachment-3100" class="wp-caption-text"&gt;Figure 5 – You can manage file systems in RES, and associate them with Projects. At launch, RES supports Amazon EFS, and Amazon FSx for NetApp ONTAP.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;RES allows an administrator to spin up new filesystems or to &lt;em&gt;onboard existing filesystems&lt;/em&gt; which you might have previously created for your project. You need to associate a file system resource with a &lt;strong&gt;Project&lt;/strong&gt; to make it visible to the users who are member of the Groups associated with that project. Individual file and folder access is still controlled in the usual way at an operating system level with permissions, which again will usually map back to your Users and Groups definitions in the AD you’re using for your directory service.&lt;/p&gt; 
&lt;h2&gt;Software stacks and machine images&lt;/h2&gt; 
&lt;p&gt;RES uses Amazon Machine Images (AMIs) as the basis for customizing the environments you deploy for your users. If this is new to you, an AMI is an image that provides the information required to launch an instance. You need to specify an AMI whenever you launch an instance. You can launch multiple instances from a single AMI when you need lots of machines with the same configuration. And you can use different AMIs to launch instances when you need add some local flavor to your users’ environment.&lt;/p&gt; 
&lt;p&gt;RES extends this concept a little further by narrowing the set of instances and GPUs it will offer based on an AMI. And – as you’d expect – the resulting stack needs to be associated with a Project before a user will know about it. This is helpful not just for customizing what your users can &lt;em&gt;do&lt;/em&gt;, but it’s great when you have commercially licensed software, too. For example – all members of Project &lt;em&gt;Beeblebrox&lt;/em&gt; can access an AMI with the &lt;em&gt;Marvin&lt;/em&gt; license embedded in it.&lt;/p&gt; 
&lt;p&gt;Creating an AMI is almost as easy as launching an existing one, making your changes, and then &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html#creating-an-ami"&gt;asking the Amazon EC2 console&lt;/a&gt; to create a new AMI from it. After that, you just need to take the ID of your AMI to RES admin screen and register it.&lt;/p&gt; 
&lt;p&gt;Starting today, RES is designed to support Windows and Linux operating systems, specifically: Windows Server 2019 (Datacenter Version 1809), Amazon Linux 2, CentOS 7, Red Hat Enterprise Linux 7, Red Hat Enterprise Linux 8, and Red Hat Enterprise Linux 9. We’ll keep the list of supported operating systems up to date in the online documentation for RES.&lt;/p&gt; 
&lt;h2&gt;One click deployment – or two&lt;/h2&gt; 
&lt;p&gt;Research and Engineering Studio is a self-managed, open-source product which you install in your own AWS account.&lt;/p&gt; 
&lt;p&gt;There are several ways of getting up and running with RES. Which one you choose depends on your experience with AWS, and how heavily you want to customize it to suit your local needs. This includes a “&lt;strong&gt;batteries included”&lt;/strong&gt; method which is a full, one-click launchable AWS CloudFormation stack, that can be used to build an entire environment, including a managed AD server.&lt;/p&gt; 
&lt;p&gt;RES is a supported open-source project, available via its public &lt;a href="https://github.com/aws/res"&gt;GitHub repository&lt;/a&gt;, which is where you can find the RES installation recipes (including several components, supplied by the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;HPC Recipe Library&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;RES can be used in the following Regions: US East (Ohio), US East (N. Virginia), US West (N. California), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Sydney), Europe (Ireland), Europe (Frankfurt), Europe (London), Europe (Paris), Asia Pacific (Mumbai) and AWS GovCloud (US-West).&lt;/p&gt; 
&lt;p&gt;Links to documentation, and more are available on the &lt;a href="https://aws.amazon.com/hpc/res"&gt;Research and Engineering Studio homepage&lt;/a&gt;. Try it out and let us know what you think at&amp;nbsp;&lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Deep-dive into Ansys Fluent performance on Ansys Gateway powered by AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/deep-dive-into-ansys-fluent-performance-on-ansys-gateway-powered-by-aws/</link>
		
		<dc:creator><![CDATA[Dnyanesh Digraskar]]></dc:creator>
		<pubDate>Thu, 09 Nov 2023 15:38:16 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">62218975828b3a44d88617555b4bff2d5132a214</guid>

					<description>In this post, we’ll show you the performance and price curves when Ansys Gateway, powered by AWS runs on different HPC instances - this should help you make the right hardware choices for running Fluent simulations in the cloud.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Dnyanesh Digraskar, Principal HPC Partner Solutions Architect, AWS, Ashwini Kumar, Senior Principal Engineer, Ansys, Nicole Diana, Director, Fluids Business Unit, Ansys.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Today, we’re going to deep-dive into the performance and associated cost of running computational fluid dynamics (CFD) simulations on AWS using Ansys Fluent through the &lt;a href="https://www.ansys.com/products/cloud/ansys-gateway"&gt;Ansys Gateway powered by AWS&lt;/a&gt; (or just “Ansys Gateway” for the rest of this post).&lt;/p&gt; 
&lt;p&gt;Ansys Gateway is an &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-ppiyshk2oin3i"&gt;AWS Marketplace&lt;/a&gt; hosted solution for users to manage their complete Ansys virtual desktop and HPC simulation workflows with more than fifty Ansys products in their AWS cloud environment. &lt;a href="https://www.ansys.com/resource-center/case-study/ansys-emirates-team-new-zealand"&gt;Emirates Team New Zealand&lt;/a&gt; and &lt;a href="https://www.ansys.com/resource-center/case-study/ansys-turntide-technologies"&gt;Turntide Technologies&lt;/a&gt; use Ansys Gateway to accelerate their design and engineering simulation cycles.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.ansys.com/products/fluids/ansys-fluent"&gt;Ansys Fluent&lt;/a&gt;, an advanced physics modeling simulation software is used by engineers and scientists across industries like automotive, aerospace, manufacturing, and energy to innovate and optimize product development.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll evaluate the performance and price characteristics for three test cases on various configurations of Amazon Elastic Compute Cloud (Amazon EC2) instance types. Using the results in this post, you’ll be able to make appropriate hardware choices for running Ansys Fluent simulations.&lt;/p&gt; 
&lt;h2&gt;Ansys Gateway recap&lt;/h2&gt; 
&lt;p&gt;Ansys Gateway is a secure, online platform that enables users to create, manage, and execute complete computer-aided engineering (CAE) workflows in their own AWS accounts. Earlier this year, we published a &lt;a href="https://aws.amazon.com/blogs/apn/accelerate-your-ansys-simulations-with-ansys-gateway-powered-by-aws/"&gt;blog post&lt;/a&gt; that described solution architecture components, security implementation, and typical end-user workflows for using Ansys Gateway.&lt;/p&gt; 
&lt;p&gt;The Ansys applications (solvers) available through Ansys Gateway are pre-configured, validated, and extensively benchmarked on various Amazon EC2 hardware for performance and price. Ansys users can refer to the &lt;a href="https://ansyshelp.ansys.com/account/secured?returnurl=/Views/Secured/gateway/v000/en/gateway_ru/bk_gateway_ru.html"&gt;recommended usage&lt;/a&gt; guidelines on the Ansys Gateway documentation in &lt;a href="https://www.ansys.com/support"&gt;Ansys Help&lt;/a&gt; to setup their virtual desktop infrastructure (VDI) or HPC environment with the recommended Amazon EC2 instance types. Users can then carry out simulations with solvers of their choice straight out-of-the-box without the need to manually setup and tune the solvers, simulation environment, and hardware parameters.&lt;/p&gt; 
&lt;h2&gt;Benchmark information and simulation environment&lt;/h2&gt; 
&lt;h3&gt;Benchmarks&lt;/h3&gt; 
&lt;p&gt;For the benchmarking purposes of this post, we’ve used the test cases from the standard &lt;a href="https://www.ansys.com/it-solutions/benchmarks-overview"&gt;Ansys Fluent Benchmarks&lt;/a&gt; suite. The model description of each test case, including the mesh size represented in terms of number of cells, turbulence model used, and the fluid-flow condition are shown in Table 1. These benchmarks, shown in Figures 1a – 1c for visual reference, represent the typical size and physics modeled by users. We used Ansys Fluent version 2023 R1 to run the simulations.&lt;/p&gt; 
&lt;div id="attachment_3037" style="width: 899px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3037" loading="lazy" class="size-full wp-image-3037" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.06.26.png" alt="Table 1: Description of Ansys Fluent benchmarking test cases used for this post." width="889" height="198"&gt;
 &lt;p id="caption-attachment-3037" class="wp-caption-text"&gt;Table 1: Description of Ansys Fluent benchmarking test cases used for this post.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3038" style="width: 1107px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3038" loading="lazy" class="size-full wp-image-3038" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.10.32.png" alt="Figure 1a: Visual representation of Ansys Fluent benchmarking test cases used for this post – Steady-state simulation of flow through a vehicle exhaust system with 33 million cells. Figure 1b: Visual representation of Ansys Fluent benchmarking test cases used for this post – Transient simulation of flow through a combustor with 71 million cells. Figure 1c: Visual representation of Ansys Fluent benchmarking test cases used for this post – Steady-state simulation of external aerodynamics of Formula 1 race car with 140 million cells." width="1097" height="241"&gt;
 &lt;p id="caption-attachment-3038" class="wp-caption-text"&gt;Figure 1 – &lt;strong&gt;a&lt;/strong&gt;: Visual representation of Ansys Fluent benchmarking test cases used for this post – Steady-state simulation of flow through a vehicle exhaust system with 33 million cells. &lt;strong&gt;b&lt;/strong&gt;: Visual representation of Ansys Fluent benchmarking test cases used for this post – Transient simulation of flow through a combustor with 71 million cells. &lt;strong&gt;c&lt;/strong&gt;: Visual representation of Ansys Fluent benchmarking test cases used for this post – Steady-state simulation of external aerodynamics of Formula 1 race car with 140 million cells.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Simulation environment&lt;/h3&gt; 
&lt;p&gt;AWS recently announced the &lt;a href="https://aws.amazon.com/ec2/instance-types/hpc7a/"&gt;Amazon EC2 Hpc7a&lt;/a&gt; instance type, powered by 4&lt;sup&gt;th&lt;/sup&gt; generation AMD EPYC&lt;sup&gt;TM&lt;/sup&gt; (Genoa) processors with up to 192 physical cores and 300 Gbps Elastic Fabric Adapter (EFA) network bandwidth. We compared the performance of these benchmarks on Hpc7a and the previous generation &lt;a href="https://aws.amazon.com/ec2/instance-types/hpc6a/"&gt;Hpc6a&lt;/a&gt; instances.&lt;/p&gt; 
&lt;p&gt;The following table (Table 2) summarizes Amazon EC2 instance configurations used:&lt;/p&gt; 
&lt;div id="attachment_3039" style="width: 936px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3039" loading="lazy" class="size-full wp-image-3039" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.12.16.png" alt="Table 2: Amazon EC2 instances used for running the benchmarking test cases" width="926" height="362"&gt;
 &lt;p id="caption-attachment-3039" class="wp-caption-text"&gt;Table 2: Amazon EC2 instances used for running the benchmarking test cases&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Analysis methodology&lt;/h2&gt; 
&lt;p&gt;Our objective for running these benchmarks was to quantify Ansys Fluent performance, associated hardware, platform, and license costs, and thus be able to recommend the appropriate Amazon EC2 instance type to use on Ansys Gateway. With that in mind, we performed the analysis by measuring the following:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Solver Rating to represent the solver performance&lt;/li&gt; 
 &lt;li&gt;Ratio of performance to hardware configuration cost&lt;/li&gt; 
 &lt;li&gt;Ratio of performance to total job cost&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Solver Rating:&lt;/strong&gt; the &lt;em&gt;Solver Rating&lt;/em&gt; is defined as the number of times the benchmark can be run on a given machine in 24 hours. It’s computed by dividing the number of seconds in a day by the number of seconds required to run the benchmark. A higher &lt;em&gt;Solver Rating&lt;/em&gt; indicates better performance. &lt;em&gt;Solver Rating&lt;/em&gt; is the primary metric we used in this post to report the performance. We ran our simulations for 1000 iterations for steady-state flow or 1000 timesteps for transient flow.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Job cost:&lt;/strong&gt; The total job (or simulation) cost is comprised of three main components: the Amazon EC2 cost, Ansys Gateway charge of $0.25 per running Amazon EC2 instance per hour, and the Ansys Licensing&amp;nbsp;cost.&lt;/p&gt; 
&lt;p&gt;These cost representations can guide you to select the right HPC configuration to&amp;nbsp;meet any one of these three simulation goals:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;minimize job cost&lt;/li&gt; 
 &lt;li&gt;maximize performance&lt;/li&gt; 
 &lt;li&gt;achieve the best performance/cost ratio&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Note that for the purpose of this post, we used the Ansys Elastic Licensing to account for the licensing costs which might not be fully representative if you’re using leased or perpetual licenses. Also, we haven’t accounted for storage and networking charges in our calculations, simply because compute constitutes most (or nearly all) of the infrastructure costs for these simulations. We used Amazon EC2 on-demand costs from the us-east-2 (Ohio) region. You can take advantage of flexible Amazon EC2 pricing options like &lt;a href="https://aws.amazon.com/savingsplans/"&gt;Compute Savings Plan&lt;/a&gt; or &lt;a href="https://aws.amazon.com/ec2/pricing/reserved-instances/"&gt;Reserved Instances&lt;/a&gt; (RI) which provide a significant discount (up to 72%) compared to On-Demand pricing.&lt;/p&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;p&gt;To understand Ansys Fluent performance, we plotted the variation of the Solver Rating against the number of instance cores for each of our three test cases. These plots are in Figures 2 (a, b, c). Higher Solver Rating signifies better performance. The &lt;em&gt;Vehicle Exhaust&lt;/em&gt; model with 33 million cells was scaled to 1536 cores, while the &lt;em&gt;Combustor&lt;/em&gt; and &lt;em&gt;F1 Race Car&lt;/em&gt; models were scaled to over 6000 cores, respectively.&lt;/p&gt; 
&lt;div id="attachment_3040" style="width: 708px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3040" loading="lazy" class="size-full wp-image-3040" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.13.17.png" alt="Figure 2a: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Vehicle exhaust benchmark." width="698" height="451"&gt;
 &lt;p id="caption-attachment-3040" class="wp-caption-text"&gt;Figure 2a: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Vehicle exhaust benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3041" style="width: 697px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3041" loading="lazy" class="size-full wp-image-3041" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.13.53.png" alt="Figure 2b: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Combustor benchmark." width="687" height="436"&gt;
 &lt;p id="caption-attachment-3041" class="wp-caption-text"&gt;Figure 2b: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Combustor benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3042" style="width: 703px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3042" loading="lazy" class="size-full wp-image-3042" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.14.32.png" alt="Figure 2c: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Formula 1 car benchmark." width="693" height="442"&gt;
 &lt;p id="caption-attachment-3042" class="wp-caption-text"&gt;Figure 2c: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Formula 1 car benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Since the number of physical cores on Hpc6a and Hpc7a instance types are different, the relative performance between the two instance types on cores and node level differ, too. In Figures 3 (a, b, c) we plotted the variation of the Solver Rating with the number of instances (nodes).&lt;/p&gt; 
&lt;div id="attachment_3043" style="width: 706px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3043" loading="lazy" class="size-full wp-image-3043" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.15.25.png" alt="Figure 3a: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Vehicle Exhaust benchmark." width="696" height="398"&gt;
 &lt;p id="caption-attachment-3043" class="wp-caption-text"&gt;Figure 3a: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Vehicle Exhaust benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3044" style="width: 708px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3044" loading="lazy" class="size-full wp-image-3044" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.15.53.png" alt="Figure 3b: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Combustor benchmark." width="698" height="444"&gt;
 &lt;p id="caption-attachment-3044" class="wp-caption-text"&gt;Figure 3b: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Combustor benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3045" style="width: 748px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3045" loading="lazy" class="size-full wp-image-3045" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.16.19.png" alt="Figure 3c: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Formula 1 race car benchmark." width="738" height="470"&gt;
 &lt;p id="caption-attachment-3045" class="wp-caption-text"&gt;Figure 3c: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Formula 1 race car benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For the Vehicle Exhaust model with 33 million cells, the performance improvement with Hpc7a instances was 1.2x at 1536 cores on a &lt;em&gt;per-core &lt;/em&gt;basis, and 1.78x at 32 instances on a &lt;em&gt;per-instance&lt;/em&gt; basis. For the Combustor benchmark model with 71 million cells, the performance improvement with Hpc7a instances is 1.3x at 3072 cores on a &lt;em&gt;per-core&lt;/em&gt; basis, and was 1.83x at 32 instances on &lt;em&gt;per-instance&lt;/em&gt; basis.&lt;/p&gt; 
&lt;p&gt;As the benchmark model size increased, the instance topology and higher EFA networking bandwidth of Hpc7a instances helps to achieve better scaling. For the Formula 1 Race Car model with 140 million cells, the performance improvement &lt;em&gt;per-core&lt;/em&gt; with Hpc7a instances was 1.3x at 6144 cores, and was 2.1x at 32 instances on a &lt;em&gt;per-instance&lt;/em&gt; basis.&lt;/p&gt; 
&lt;h3&gt;Full cores vs partial cores&lt;/h3&gt; 
&lt;p&gt;These plots show performance results when running Ansys Fluent on the full available set of physical cores: 192 for hpc7a.96xlarge, and 96 for hpc6a.48xlarge. But it’s possible to manually disable certain numbers of physical cores – or use process pinning on each instance – to achieve better &lt;em&gt;per-core&lt;/em&gt; performance because of increased memory bandwidth per core. We call this under-subscribing.&lt;/p&gt; 
&lt;p&gt;When running simulations on the Hpc6a instance type, which is available in only one size, under-subscribing is implemented in Ansys Gateway via job setup scripts. Under-subscribing on Hpc7a isn’t required because it’s a feature of the different instance sizes.&lt;/p&gt; 
&lt;p&gt;In this section we’ll take a detailed look at the impact this under-subscribing technique has on Ansys Fluent performance. Figures 4a, 4b, 4c show variations of Solver Rating with the number of instance cores for 100%, 50%, and 25% cores enabled per-instance for Hpc6a and corresponding sizes of Hpc7a instance type.&lt;/p&gt; 
&lt;div id="attachment_3046" style="width: 694px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3046" loading="lazy" class="size-full wp-image-3046" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.17.08.png" alt="Figure 4a: Vehicle exhaust benchmark - comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance." width="684" height="413"&gt;
 &lt;p id="caption-attachment-3046" class="wp-caption-text"&gt;Figure 4a: Vehicle exhaust benchmark – comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3047" style="width: 689px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3047" loading="lazy" class="size-full wp-image-3047" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.17.34.png" alt="Figure 4a: Vehicle exhaust benchmark - comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance." width="679" height="463"&gt;
 &lt;p id="caption-attachment-3047" class="wp-caption-text"&gt;Figure 4a: Vehicle exhaust benchmark – comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3048" style="width: 718px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3048" loading="lazy" class="size-full wp-image-3048" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.17.57.png" alt="Figure 4c: F1 Race Car benchmark - comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. " width="708" height="454"&gt;
 &lt;p id="caption-attachment-3048" class="wp-caption-text"&gt;Figure 4c: F1 Race Car benchmark – comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;From Figures 4a – 4c, it’s clear that as we undersubscribe an instance (thus increasing the available memory bandwidth per core), the simulation performance improves. For the vehicle exhaust benchmark, the performance improvement with 50% and 25% cores enabled compared to 100% cores for both Hpc6a and Hpc7a instances was between 1.3x and 1.8x.&lt;/p&gt; 
&lt;p&gt;As the size of the simulation model increased, the performance improvement due to the under-subscribing became even more pronounced. For the Combustor model, the performance improvement with 50% and 25% cores enabled compared to 100% cores on both Hpc6a and Hpc7a was between 1.7x and 2.6x.&lt;/p&gt; 
&lt;p&gt;Finally, for the F1 Race Car model, the performance improvement with 50% and 25% cores enabled compared to 100% cores for both Hpc6a and Hpc7a instances was between 1.8x and 2.9x.&lt;/p&gt; 
&lt;h2&gt;Simulation costs&lt;/h2&gt; 
&lt;p&gt;Now let’s look at the cost impacts (based on cloud infrastructure and licensing) for running these benchmarks with our various instance configurations.&lt;/p&gt; 
&lt;p&gt;Figures 5a – 5b show variations of &lt;em&gt;performance to cloud-infrastructure-cost&lt;/em&gt; with across a range of &lt;em&gt;cores-enabled per-instance&lt;/em&gt; for Hpc6a and Hpc7a, for the smallest and the largest benchmark case.&lt;/p&gt; 
&lt;p&gt;The cloud infrastructure costs plotted here represent the total hardware cost based on Amazon EC2, and the Ansys Gateway hardware flat charge of $0.25 per-instance per-hour. This is a good metric to follow when you want to evaluate the instances for best performance while considering the cloud infrastructure costs only.&lt;/p&gt; 
&lt;div id="attachment_3049" style="width: 711px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3049" loading="lazy" class="size-full wp-image-3049" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.18.51.png" alt="Figure 5a: Vehicle exhaust benchmark – performance to cloud infrastructure cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Higher is better. Fully subscribed Hpc6a and Hpc7a instances provide the best performance to cost ratio. " width="701" height="416"&gt;
 &lt;p id="caption-attachment-3049" class="wp-caption-text"&gt;Figure 5a: Vehicle exhaust benchmark – performance to cloud infrastructure cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Higher is better. Fully subscribed Hpc6a and Hpc7a instances provide the best performance to cost ratio.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3051" style="width: 706px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3051" loading="lazy" class="size-full wp-image-3051" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.19.56.png" alt="Figure 5b: F1 race car benchmark – performance to cloud infrastructure cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Higher is better. Fully subscribed Hpc6a and Hpc7a instances provide the best performance to cost ratio. " width="696" height="433"&gt;
 &lt;p id="caption-attachment-3051" class="wp-caption-text"&gt;Figure 5b: F1 race car benchmark – performance to cloud infrastructure cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Higher is better. Fully subscribed Hpc6a and Hpc7a instances provide the best performance to cost ratio.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;From Figures 5a – 5b, you can tell that simulations run on the fully-subscribed Hpc6a and Hpc7a instances have the best performance &lt;em&gt;per hardware-configuration&lt;/em&gt; cost. At higher core counts, the cloud infrastructure costs start to increase, resulting in a drop of the &lt;em&gt;performance-to-cost&lt;/em&gt; ratio.&lt;/p&gt; 
&lt;p&gt;In Figure 6, we plotted the &lt;em&gt;performance to total-job-cost&lt;/em&gt; ratio, where the total job cost is the sum of cloud infrastructure &lt;em&gt;and&lt;/em&gt; the Ansys licensing cost. As the Ansys licensing costs can vary for each customer depending on their licensing agreement, for the purpose of this post we used the Ansys Elastic Currency (AEC) to represent the licensing cost, to keep it simple.&lt;/p&gt; 
&lt;div id="attachment_3052" style="width: 752px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3052" loading="lazy" class="size-full wp-image-3052" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.20.40.png" alt="Figure 6: F1 race car benchmark – performance to total cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Total cost includes Ansys licensing. Higher is better. Hpc6a with 25% subscribed and hpc7a.24xlarge instances provide the best performance to total cost ratio. " width="742" height="459"&gt;
 &lt;p id="caption-attachment-3052" class="wp-caption-text"&gt;Figure 6: F1 race car benchmark – performance to total cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Total cost includes Ansys licensing. Higher is better. Hpc6a with 25% subscribed and hpc7a.24xlarge instances provide the best performance to total cost ratio.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;From Figure 6, you can see that simulations run on Hpc6a instances with 25% cores enabled, and hpc7a.24xlarge show the best &lt;em&gt;performance to total-cost&lt;/em&gt; ratio. Running the simulations with 50% cores enabled on Hpc6a or on Amazon EC2 hpc7a.48xlarge is a good idea for customers with pre-existing Ansys licensing who are looking to balance performance and cloud infrastructure costs. All instance types provide better value at higher core counts because per-core license costs decrease when you use more cores.&lt;/p&gt; 
&lt;p&gt;In our plots, we focused on performance and cost comparisons, but customers are also interested in making the right Amazon EC2 choices based on &lt;strong&gt;simulation runtime and costs&lt;/strong&gt;. In Figures 7 and 8, we plotted the variation of cloud infrastructure and total costs with simulation runtime. By referring to these plots, you can choose the right instance for your preferred simulation runtime.&lt;/p&gt; 
&lt;div id="attachment_3053" style="width: 685px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3053" loading="lazy" class="size-full wp-image-3053" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.21.18.png" alt="Figure 7: F1 race car benchmark – variation of cloud infrastructure cost with simulation runtime on Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Lower cost and runtime are better. Fully subscribed Hpc6a and Hpc7a instances offer the lowest costs for a given simulation runtime. " width="675" height="441"&gt;
 &lt;p id="caption-attachment-3053" class="wp-caption-text"&gt;Figure 7: F1 race car benchmark – variation of cloud infrastructure cost with simulation runtime on Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Lower cost and runtime are better. Fully subscribed Hpc6a and Hpc7a instances offer the lowest costs for a given simulation runtime.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3054" style="width: 678px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3054" loading="lazy" class="size-full wp-image-3054" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.21.43.png" alt="Figure 8: F1 race car benchmark – variation of total cost i.e., cloud infrastructure + licensing cost with simulation runtime on Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Lower cost and runtime are better. Hpc6a with 25% subscribed and hpc7a.24xlarge instances offer the best total cost for a given simulation runtime. " width="668" height="435"&gt;
 &lt;p id="caption-attachment-3054" class="wp-caption-text"&gt;Figure 8: F1 race car benchmark – variation of total cost i.e., cloud infrastructure + licensing cost with simulation runtime on Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Lower cost and runtime are better. Hpc6a with 25% subscribed and hpc7a.24xlarge instances offer the best total cost for a given simulation runtime.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;From Figure 7, you can see that when prioritizing cloud infrastructure cost for a desired simulation runtime, the fully-subscribed instances offer the lowest cost because of utilizing maximum available cores on the instances.&lt;/p&gt; 
&lt;p&gt;When we add the software licensing costs – which can be high for larger simulations – the benefit of running the simulations on under-subscribed instances for performance gain helps to drive the total cost down. It’s clear from Figure 8 that under-subscribed instances offer the best total cost for a given simulation runtime.&lt;/p&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;Today we described the price-performance characteristics of Ansys Fluent CFD simulations on Ansys Gateway powered by AWS. We showed you some of our best practices for running simulations on different &lt;a href="https://aws.amazon.com/ec2/"&gt;Amazon EC2&lt;/a&gt; compute-optimized instances. And we described how total job costs comprise of infrastructure costs and Ansys licenses for the Ansys Fluent simulations. With these results you should be able to select the right instance types for running your simulations, whether your goal is to maximize performance or minimize costs.&lt;/p&gt; 
&lt;p&gt;You can get started with Ansys Gateway powered by AWS by subscribing through the &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-ppiyshk2oin3i"&gt;AWS Marketplace&lt;/a&gt;. Follow the &lt;a href="https://www.youtube.com/playlist?list=PL0lZXwHtV6Omw8rjdXmr4UiDj2WZksygA"&gt;Ansys Gateway YouTube channel&lt;/a&gt; for in-depth step-by-step tutorials and video guidelines to get started with setup and running Ansys applications. Finally, you can refer to the &lt;a href="https://forum.ansys.com/forums/forum/installation-and-licensing/ansys-gateway/"&gt;Ansys innovation Space learning forums&lt;/a&gt; for Ansys Gateway specific help.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.28.29.png" alt="Ashwini Kumar " width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Ashwini Kumar&lt;/h3&gt; 
  &lt;p&gt;Ashwini Kumar is a senior principal application engineer at Ansys, specializing in Ansys Cloud solutions, where he plays a pivotal role in assisting Ansys clients with optimizing their cloud investments. With over three decades of expertise in customer relationship management, cloud computing, technical support, and services related to Fluid Flow and Heat Transfer, he brings a wealth of knowledge to his role. His educational background includes a Ph.D. from the University of Minnesota in Minneapolis-St. Paul, Minnesota, a master’s degree from the University of Manitoba in Winnipeg, Canada, and a B.Tech from G.B. Pant University of Agriculture and Technology in Pantnagar, India.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.28.34.png" alt="Nicole Diana " width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Nicole Diana&lt;/h3&gt; 
  &lt;p&gt;Nicole Diana is Director of research and development Verification for the computational fluids dynamics (CFD) product line at Ansys. She has over twenty-five years of experience in the development, verification, and application of Ansys CFD products.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Lattice Boltzmann simulation with Palabos on AWS using Graviton-based Amazon EC2 Hpc7g instances</title>
		<link>https://aws.amazon.com/blogs/hpc/lattice-boltzmann-simulation-with-palabos-on-aws-using-graviton-based-amazon-ec2-hpc7g-instances/</link>
		
		<dc:creator><![CDATA[Jun Tang]]></dc:creator>
		<pubDate>Wed, 08 Nov 2023 13:47:14 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<guid isPermaLink="false">ecb740a48f70c44fb346183b25ed64dbe70baea3</guid>

					<description>In this post we’ll show you the performance when running the Parallel Lattice Boltzmann Solver (Palabos) on the latest generation of AWS Graviton CPUs in Hpc7g instances on AWS.</description>
										<content:encoded>&lt;p&gt;Computational fluid dynamics (CFD) has grown over several decades to become a widely used tool to study a broad range of important industrial and academic problems, ranging from automotive and aircraft design to the study of blood flow inside the human body.&lt;/p&gt; 
&lt;p&gt;Whilst traditional Navier-Stokes (NS) based codes using the finite-volume approach are still the most widely used, an alternative approach – the Lattice Boltzmann method (LBM) – has emerged in the last two decades. LBM methods are particularly attractive to both industry and academia.&lt;/p&gt; 
&lt;p&gt;In this post we’ll show you the performance and cost benefits of running the Parallel Lattice Boltzmann Solver (&lt;em&gt;Palabos&lt;/em&gt;) [1] – a specific LBM solver – on the latest generation of Amazon Elastic Compute Cloud instances based on the AWS Graviton.&lt;/p&gt; 
&lt;p&gt;Palabos is an open-source, C++ solver developed by the University of Geneva. It’s designed to run and scale efficiently on HPC clusters. Palabos is a highly versatile computational tool and has been widely used in the LBM community and is often a reference implementation for many Lattice Boltzmann models.&lt;/p&gt; 
&lt;h2&gt;Lattice Boltzmann models (LBMs)&lt;/h2&gt; 
&lt;p&gt;We mentioned LBMs are popular with both industry and academia. There are several reasons for this.&lt;/p&gt; 
&lt;p&gt;First, they’re flexible. You can model complex geometries whether they’re stationary or moving. Second, they suffer from minimal numerical dissipation, which makes them ideal for high-fidelity scale-resolving methods and aeroacoustics.&lt;/p&gt; 
&lt;p&gt;They also have superior computational performance, which stems from two key characteristics: data locality, and the fact that they’re easier to vectorize and set up for multithreading (on both CPUs and GPUs) compared to NS-based methods.&lt;/p&gt; 
&lt;p&gt;To illustrate these points, we’re going to show you how Palabos simulates blood flows [2][3] that can help researchers better understand vascular diseases and cancer cell movement through blood vessels. This can also provide a controlled environment to evaluate different treatment options.&lt;/p&gt; 
&lt;p&gt;There’s another reason LBMs are interesting. Given they’re highly scalable, researchers can run them on thousands or tens of thousands of cores to either get answers very quickly, or drive up the fidelity of the simulations themselves – or often both. But the availability of HPC resources can potentially become a bottleneck for many. As you can imagine, that causes a lot of researchers to look for large scale resources beyond their usual environments: enter AWS which can offer all the additional capacity they’re asking for. It’s also robust, secure by design, low cost and incredibly flexible.&lt;/p&gt; 
&lt;h2&gt;Introducing AWS Graviton3E&lt;/h2&gt; 
&lt;p&gt;Amazon EC2 offers hundreds of instance types optimized to fit different use cases. Instances vary based on CPU, memory, storage, and networking bandwidth giving customers the flexibility to choose the right mix of resources for their applications.&lt;/p&gt; 
&lt;p&gt;Amazon EC2 Hpc7g instances are the &lt;a href="https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/"&gt;latest addition&lt;/a&gt; to the extended family of Graviton-based EC2 instances. They carry DDR5 memory with an AWS Graviton3E processor, and 200Gbps low-latency networking delivered by Elastic Fabric Adapter – especially important when you’re scaling to a large number of instances.&lt;a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc7g-instances-powered-by-aws-graviton3e-processors-optimized-for-high-performance-computing-workloads/"&gt; They also consume up to 60% less energy&lt;/a&gt; for the same workloads than other comparable x86-based instances tailored for HPC applications – this is great for the planet.&lt;/p&gt; 
&lt;p&gt;In &lt;a href="https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/"&gt;an earlier&amp;nbsp;post&lt;/a&gt;, we published some performance results for real world workloads from CFD, finite-element analysis (FEA), molecular dynamics, and numerical weather prediction (NWP).&lt;/p&gt; 
&lt;p&gt;For this post, we compared Palabos performance across two HPC instance types offered by AWS and&amp;nbsp;we found that Hpc7g has competitive performance. In our tests, Hpc7g delivered up to&amp;nbsp;75% better performance and up to 3x better &lt;em&gt;price-performance&lt;/em&gt; compared to the previous generation Graviton instances (C6gn).&lt;/p&gt; 
&lt;h2&gt;Benchmark simulation result and performance&lt;/h2&gt; 
&lt;p&gt;For our tests, we used two instance types: Hpc7g.16xlarge (the latest processor in Graviton family customized for HPC applications) and C6gn.16xlarge (the previous generation processor in Graviton family).&amp;nbsp;We used AWS ParallelCluster to launch our environment, manage these fleets, and provide an Amazon FSx for Lustre (a popular parallel file system). ParallelCluster uses Slurm for its workload manager, making it familiar and easier to use. There’s an example ParallelCluster configuration file in &lt;a href="https://github.com/aws/aws-graviton-getting-started/blob/main/HPC/scripts-setup/hpc7g-ubuntu2004-useast1.yaml"&gt;the Graviton HPC best practices guide&lt;/a&gt;. You can also find a one-click launchable recipe in the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;HPC Recipes Library&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Cavity 3D benchmark&lt;/h3&gt; 
&lt;p&gt;In our first test, we used the “&lt;a href="https://gitlab.unige.ch/hpc/benchmark-aimp-2022/-/blob/main/Palabos/README.md"&gt;lid-driven cavity problem in a cuboid&lt;/a&gt;”, a three-dimension benchmark with 1 billion cells (1001x1001x1001). The top wall moves to the right with a constant velocity, while the other walls are stationary. Figure 1 is a snapshot of the simulated velocity field at 10.1 sec. We used Palabos v2.3.0, compiled with GNU Compiler v11.1.0 and Open MPI v4.1.4.&lt;/p&gt; 
&lt;div id="attachment_3081" style="width: 767px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3081" loading="lazy" class="size-full wp-image-3081" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.24.40.png" alt="Figure 1. A display of Palabos output velocity field at 10.1 sec for the three-dimension lid driven cavity problem. Arrow shows flow direction and speed." width="757" height="495"&gt;
 &lt;p id="caption-attachment-3081" class="wp-caption-text"&gt;Figure 1. A display of Palabos output velocity field at 10.1 sec for the three-dimension lid driven cavity problem. Arrow shows flow direction and speed.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We ran the same benchmark on both Hpc7g and C6gn scaling from 2 to 128 instances (8192cores) in both cases. We calculated throughput in &lt;em&gt;million site updates per second&lt;/em&gt; (MLUPS)&amp;nbsp;which we’ve shown in Table 1.&lt;/p&gt; 
&lt;p&gt;At 8192 cores, the solver maintains a strong scaling efficiency of 60% on Hpc7g (Figure 2), which also had 75% higher throughput than the previous generation Graviton 2 (C6gn) instance. The simulation cost on Hpc7g is close to 1/3 of the cost of doing it on C6gn (Figure 3).&lt;/p&gt; 
&lt;div id="attachment_3082" style="width: 564px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3082" loading="lazy" class="size-full wp-image-3082" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.25.27.png" alt="Table 1. Cavity 3D benchmark performance (higher is better)" width="554" height="306"&gt;
 &lt;p id="caption-attachment-3082" class="wp-caption-text"&gt;Table 1. Cavity 3D benchmark performance (higher is better)&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3083" style="width: 758px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3083" loading="lazy" class="size-full wp-image-3083" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.26.07.png" alt="Figure 2. Cavity 3D benchmark throughput (higher is better) on our two instance types; Hpc7g shows good efficiency at 8192 cores for this strong scaling test." width="748" height="561"&gt;
 &lt;p id="caption-attachment-3083" class="wp-caption-text"&gt;Figure 2. Cavity 3D benchmark throughput (higher is better) on our two instance types; Hpc7g shows good efficiency at 8192 cores for this strong scaling test.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3084" style="width: 760px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3084" loading="lazy" class="size-full wp-image-3084" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.26.33.png" alt="Figure 3. Cavity 3D benchmark cost for 10000 iterations on our two instance types, lower is better." width="750" height="564"&gt;
 &lt;p id="caption-attachment-3084" class="wp-caption-text"&gt;Figure 3. Cavity 3D benchmark cost for 10000 iterations on our two instance types, lower is better.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Cellular blood flow simulation benchmark&lt;/h3&gt; 
&lt;p&gt;Next, we studied the &lt;em&gt;cellular blood computation&lt;/em&gt;, which has three components: the &lt;em&gt;fluid solver&lt;/em&gt;, the &lt;em&gt;solid solver,&lt;/em&gt; and the &lt;em&gt;fluid-solid interaction&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;The fluid solver solves the weakly-compressible Navier-Stokes equations. The solid solver, based on nodal projective finite elements method (npFEM) resolves the trajectories and deformations of the blood cells. And the fluid-solid interaction is loosely coupled through an immersed boundary condition.&lt;/p&gt; 
&lt;p&gt;The result of this hybrid simulation is shown in Figure 4.&amp;nbsp;For &lt;a href="https://gitlab.com/unigespc/palabos/-/blob/master/examples/showCases/bloodFlowDefoBodies/README.md?ref_type=heads"&gt;this second benchmark&lt;/a&gt;, we simulated the blood flow in a 50x50x50um cube with 476 red blood cells (RBCs) and 95 platelets (PLTs).&lt;/p&gt; 
&lt;div id="attachment_3085" style="width: 767px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3085" loading="lazy" class="size-full wp-image-3085" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.27.17.png" alt="Figure 4. Hybrid simulation of blood plasma (Palabos) and deformable RBCs/PLTs (npFEM)." width="757" height="533"&gt;
 &lt;p id="caption-attachment-3085" class="wp-caption-text"&gt;Figure 4. Hybrid simulation of blood plasma (Palabos) and deformable RBCs/PLTs (npFEM).&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Table 2 and Figure 5 show the performance in the number of iterations we can simulate &lt;em&gt;per minute&lt;/em&gt; (so higher is better).&lt;/p&gt; 
&lt;p&gt;Hpc7g delivered up to 50% better performance and 2.45x better price-performance over C6gn for these tests.&lt;/p&gt; 
&lt;div id="attachment_3086" style="width: 678px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3086" loading="lazy" class="size-full wp-image-3086" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.27.55.png" alt="Table 2. Cellular bloodFlow benchmark throughput (higher is better)" width="668" height="211"&gt;
 &lt;p id="caption-attachment-3086" class="wp-caption-text"&gt;Table 2. Cellular bloodFlow benchmark throughput (higher is better)&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3087" style="width: 717px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3087" loading="lazy" class="size-full wp-image-3087" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.30.23.png" alt="Figure 5. Cellular blood flow benchmark throughput (higher is better) on our two instance types." width="707" height="552"&gt;
 &lt;p id="caption-attachment-3087" class="wp-caption-text"&gt;Figure 5. Cellular blood flow benchmark throughput (higher is better) on our two instance types.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusions&lt;/h2&gt; 
&lt;p&gt;In this post we showed you that a popular LBM code can be easily run on Graviton-based Amazon EC2 instances with up to 8192 cores. The Amazon EC2 Hpc7g instances showed up to 70% better performance and up to 3x better price-performance over the previous generation Graviton instances for Palabos.&lt;/p&gt; 
&lt;p&gt;The 3D Cavity benchmark scaled efficiently on Hpc7g instances up to 8192&amp;nbsp;cores and we think this illustrates how the combination of a computationally efficiency code, and large-scale cloud HPC facility can enable you to run larger cases than you could likely complete using limited on-prem HPC resources.&lt;/p&gt; 
&lt;p&gt;You can find the instructions to set up and run HPC applications on Graviton in our &lt;a href="https://github.com/aws/aws-graviton-getting-started/tree/main/HPC"&gt;best practice guide&lt;/a&gt;. And you can find easy to consume recipes for building clusters to suite your taste in the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;HPC Recipe Library&lt;/a&gt;. These are open source projects on GitHub so you can &lt;a href="https://github.com/aws/aws-graviton-getting-started/issues"&gt;let us know&lt;/a&gt; directly if you run into any technical issues. Let us know how you get on.&lt;/p&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;p&gt;[1] Latt, J., Malaspinas, O., Kontaxakis, D., Parmigiani, A., Lagrava, D., Brogi, F., Belgacem, M. B., Thorimbert, Y., Leclaire, S., Li, S., Marson, F., Lemus, J., Kotsalos, C., Conradin, R., Coreixas, C., Petkantchin, R., Raynaud, F., Beny, J., &amp;amp; Chopard, B. (2021). &lt;a href="https://www.sciencedirect.com/science/article/pii/S0898122120301267?via%3Dihub"&gt;Palabos: Parallel lattice boltzmann solver&lt;/a&gt;. Computers and Mathematics with Applications, 81, 334–350. https://doi.org/10.1016/j.camwa.2020.03.022&lt;/p&gt; 
&lt;p&gt;[2] Kotsalos, C., Latt, J., Beny, J., &amp;amp; Chopard, B. (2020). &lt;a href="https://arxiv.org/abs/1911.03062"&gt;Digital Blood in massively parallel CPU/GPU systems for the study of platelet transport.&lt;/a&gt; Interface Focus, 11(1), 20190116. https://doi.org/10.1098/rsfs.2019.0116&lt;/p&gt; 
&lt;p&gt;[3] Zavodszky, G., van Rooij, B., Azizi, V., Alowayyed, S., &amp;amp; Hoekstra, A. (2017). &lt;a href="https://www.sciencedirect.com/science/article/pii/S1877050917306245"&gt;Hemocell: A high-performance microscopic cellular library&lt;/a&gt;. &lt;em&gt;Procedia Computer Science&lt;/em&gt;, &lt;em&gt;108&lt;/em&gt;, 159–165. https://doi.org/10.1016/j.procs.2017.05.084&lt;/p&gt;</content:encoded>
					
		
		
			</item>
	</channel>
</rss>