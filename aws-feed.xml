<?xml version="1.0" encoding="UTF-8" standalone="no"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" version="2.0">

<channel>
	<title>AWS HPC Blog</title>
	<atom:link href="https://aws.amazon.com/blogs/hpc/feed/" rel="self" type="application/rss+xml"/>
	<link>https://aws.amazon.com/blogs/hpc/</link>
	<description>Just another Amazon Web Services site</description>
	<lastBuildDate>Tue, 16 Jul 2024 15:02:38 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>Strategies for distributing executable binaries across grids in financial services</title>
		<link>https://aws.amazon.com/blogs/hpc/strategies-for-distributing-executable-binaries-grids-financial-services/</link>
		
		<dc:creator><![CDATA[Kirill Bogdanov]]></dc:creator>
		<pubDate>Tue, 16 Jul 2024 15:02:38 +0000</pubDate>
				<category><![CDATA[Amazon Elastic Block Store (Amazon EBS)]]></category>
		<category><![CDATA[Amazon Elastic File System (EFS)]]></category>
		<category><![CDATA[Amazon File Cache]]></category>
		<category><![CDATA[Amazon FSx]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[Amazon FSx for NetApp ONTAP]]></category>
		<category><![CDATA[Amazon FSx for OpenZFS]]></category>
		<category><![CDATA[Amazon Simple Storage Service (S3)]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[FSI]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Lustre]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">ca9e22473e1200ab94d24f287d767e37cfaf1f73</guid>

					<description>You can boost the performance of your compute grids by strategically distributing your binaries. Our experts looked at lots of strategies for fast &amp;amp; efficient compute grid operations - to save you some work.</description>
										<content:encoded>&lt;p&gt;&lt;img class="alignright size-full wp-image-3906" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/24/AdobeStock_852512916.png" alt="Strategies for distributing executable binaries across grids in financial services" width="380" height="212"&gt;Financial Services Institutions (FSIs) rely heavily on compute grids for their daily operations. One key challenge of operating such grids is the distribution of data, including the distribution of binaries required for computation.&lt;/p&gt; 
&lt;p&gt;In this blog post, we’ll focus on classifying different binary distribution techniques and providing intuition and guidance for customers about when to use each technique in connection with their business objectives and requirements. We’ll also offer insights and recommendations that others can apply outside of the FSI industry.&lt;/p&gt; 
&lt;h2&gt;The problem&lt;/h2&gt; 
&lt;p&gt;Most customer concerns regarding data distribution revolve around business objectives, and cost. Some of these concerns include cost-effectively distributing large volumes of data regularly, and doing it quickly, to ensure the rapid start of compute instances. This helps to avoid subsequent delays throughout the execution, and often translates into cost savings, too.&lt;/p&gt; 
&lt;p&gt;Consider a grid configured to scale to 5,000 x Amazon Elastic Compute Cloud (Amazon EC2) instances. With fluctuating demand causing frequent scaling events, each new instance has to download the binary files it needs. Assuming up to 5 scaling events daily, with a 1-minute download time, this results in over 416 hours or 17 days of wasted EC2 time – &lt;em&gt;highlighting the value of efficient binary distribution&lt;/em&gt;.&lt;/p&gt; 
&lt;h2&gt;How to assess your options&lt;/h2&gt; 
&lt;p&gt;Choosing the suitable data layer is not just a technical decision but a strategic one that impacts the cost efficiency of the entire computing grid. The wrong choice can lead to significant inefficiencies and increased costs. Therefore, it’s crucial to consider the options and their implications carefully.&lt;/p&gt; 
&lt;p&gt;What factors should customers consider when evaluating their options for binary distribution based on workload requirements and characteristics?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Size – &lt;/strong&gt;What is the total size of the &lt;em&gt;super-set of all binaries&lt;/em&gt; that must be present on the Amazon EC2 instance? What is the size of the &lt;em&gt;most commonly used&lt;/em&gt; binary files sufficient to start the workload in a lazy loading situation (i.e., waiting for the rest of the files to be delivered later)?&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Size distribution: &lt;/strong&gt;Does the binary set consist of a large number of small files, or a small number of large files?&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scaling: &lt;/strong&gt;What’s the maximum number of instances you need to have running simultaneously? What’s the maximum scaling speed (the higher the speed, the more bandwidth necessary)?&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantics: &lt;/strong&gt;Does the workload require POSIX file access? Will it be necessary to have excess capacity, to allow adding to, or altering, binary sets during the production stage?&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Options&lt;/h2&gt; 
&lt;h3&gt;Amazon Simple Storage Service (Amazon S3)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="mailto:https://aws.amazon.com/s3/"&gt;Amazon S3&lt;/a&gt; is the AWS object storage service, allowing users to store and retrieve any data from anywhere. Amazon S3 can act as a common destination for all built and versioned binary files, and distribute these files among Amazon EC2 instances.&lt;/p&gt; 
&lt;p&gt;To illustrate typical tradeoffs, we begin with the most intuitive approach of copying the required files from Amazon S3. A common destination could be EC2 Instance Store or an Elastic Block Service (EBS) volume attached to the EC2 instance. A grid administrator can script the copy operation at instance boot time (e.g., using &lt;code&gt;&lt;a href="https://docs.aws.amazon.com/cli/latest/reference/s3/sync.html"&gt;aws s3 sync&lt;/a&gt;&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;The primary advantage is that once files are copied, the workload has the lowest latency and highest bandwidth for accessing these files. Performance isn’t impacted by the total number of EC2 instances as each instance has its own volume.&lt;/p&gt; 
&lt;p&gt;Furthermore, Amazon S3 provides very high throughput, sufficient even for the most demanding use cases. Combined with the pay-as-you-go model, grid administrators don’t need to scale S3 to match fluctuating bandwidth demands, simplifying administration.&lt;/p&gt; 
&lt;p&gt;Amazon S3 is a regional service with no charges for data transfers across Availability Zones (AZs) and has very low storage costs. In the context of binary distribution, the bulk of the cost will be associated with the number of files transferred, which comes from API calls. This is important to consider for binaries consisting of many small files.&lt;/p&gt; 
&lt;p&gt;The key tradeoff is the initial delay associated with the time it takes to copy the files from S3. During this time, EC2 will have to idle until all the data is copied. You can partially mitigate this by splitting your binaries and downloading them in parts – at the expense of added complexity to manage this.&lt;/p&gt; 
&lt;h3&gt;Amazon S3 Express One Zone (EOZ)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/s3/storage-classes/express-one-zone/"&gt;Amazon S3 Express One Zone&lt;/a&gt; (EOZ) stores data in a single Availability Zone, reducing data access latency and increasing bandwidth from within the AZ. Because FSI grids often require significant computing capacity our recommendation is to diversify EC2 instance types and use as many AZs as possible. However, this conflicts with the benefits of single AZ solutions.&lt;/p&gt; 
&lt;p&gt;Grid administrators faced with this have two main options. They can have a single instance of the storage service running in a single AZ, which will partially alleviate latency and bandwidth advantages when instances from other AZs access data. Alternatively, grid administrators can have a separate instance of the storage in each AZ – this will increase storage costs and add complexity to the synchronization of data between S3 EOZ buckets before the start of the workload.&lt;/p&gt; 
&lt;h3&gt;Mountpoint for Amazon S3&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/s3/features/mountpoint/"&gt;Mountpoint for Amazon S3&lt;/a&gt; – an open-source file client that enables easy access to objects in an S3 bucket through standard file system APIs on your EC2 instance.&lt;/p&gt; 
&lt;p&gt;With Mountpoint, files stay in S3, and only get locally cached when accessed, unlike the previously described technique. This makes it possible to start the workload instantaneously and load files on-demand throughout the execution process. This approach particularly useful when you don’t know beforehand which binary files you require for the workload execution, or if only a small fraction of files are likely to be needed.&lt;/p&gt; 
&lt;p&gt;However, the first execution of the workload will still experience higher latency than subsequent executions due to the need to cache files locally.&lt;/p&gt; 
&lt;h3&gt;Amazon Elastic File System (EFS)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/efs/"&gt;EFS&lt;/a&gt; is a fully managed, network-attached storage service that supports NFS protocols. It provides a POSIX file access API and supports up to 20 GB/sec of read throughput, up to 250,000 read IOPS, and up to 25,000 connections. The actual performance will depend on the throughput mode selected for EFS: &lt;em&gt;Elastic&lt;/em&gt;, &lt;em&gt;Provisioned&lt;/em&gt;, or &lt;em&gt;Bursting &lt;/em&gt;which you should choose after assessing your workload requirements (see more details in the &lt;a href="https://docs.aws.amazon.com/efs/latest/ug/performance.html"&gt;EFS documentation&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Unlike S3-based solutions there are no charges associated with accessing individual files stored on EFS. Instead, there is a cost associated with the amount of data transferred.&lt;/p&gt; 
&lt;p&gt;Similarly to S3 Express One Zone, is EFS One Zone (OZ). However, in addition to tradeoffs associated with one AZ solutions (which we described earlier) EFS OZ has cross-AZ data transfer charges, thus we don’t recommend using EFS OZ for this task.&lt;/p&gt; 
&lt;h3&gt;Amazon FSx&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/fsx/"&gt;&lt;strong&gt;Amazon FSx&lt;/strong&gt;&lt;/a&gt; lets you choose between several widely-used, distributed, network-attached file systems: &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Lustre&lt;/a&gt;, &lt;a href="https://aws.amazon.com/fsx/netapp-ontap/"&gt;NetApp ONTAP&lt;/a&gt;, &lt;a href="https://aws.amazon.com/fsx/openzfs/"&gt;OpenZFS&lt;/a&gt;, and &lt;a href="https://aws.amazon.com/fsx/windows/"&gt;Windows File Server&lt;/a&gt;. Each of these file systems provides even greater throughput and lower latency than EFS. You can explore a more detailed comparison of these services &lt;a href="https://aws.amazon.com/fsx/when-to-choose-fsx/"&gt;using our published advice&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This post focuses on FSx for Lustre (FSxL), which can scale to &lt;a href="https://aws.amazon.com/blogs/hpc/build-and-deploy-a-1-tb-s-file-system-in-under-an-hour/"&gt;provide 1TByte/sec throughput&lt;/a&gt; (or more), &lt;em&gt;millions&lt;/em&gt; of IOPs, and sub-millisecond access latency, making it the best choice for many large-scale workloads.&lt;/p&gt; 
&lt;p&gt;FSxL also integrates with Amazon S3, allowing users POSIX-level file access to files stored on S3. When a client attempts to access a file, FSx first retrieves it from S3 and caches it at the FSxL server, making it readily available for subsequent requests by other users.&lt;/p&gt; 
&lt;p&gt;FSxL can be configured to match target capacity and/or throughput. This is a task the admin must invoke manually. FSxL deploys into a single AZ, meaning file access across AZs will incur additional data charges.&lt;/p&gt; 
&lt;p&gt;Both FSxL and EFS offer full POSIX file-access support, allowing writes by any Amazon EC2 instance to be available to other instances. While binary files are generally immutable and don’t need this feature, it’s beneficial for the same data layer to be used for other needs like providing computational input, storing results, or maintaining shared state.&lt;/p&gt; 
&lt;h3&gt;Amazon File Cache&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/filecache/"&gt;&lt;strong&gt;Amazon File Cache&lt;/strong&gt;&lt;/a&gt; is a high-speed caching service similar to FSxL but with a number of distinct differences.&lt;/p&gt; 
&lt;p&gt;First, in addition to supporting Amazon S3, File Cache can support data repositories using NFSv3, which means that the source files can be located virtually anywhere, including on-premises. This can simplify the process of delivering quant libraries – especially if they are built on-premises.&lt;/p&gt; 
&lt;p&gt;File Cache automatically loads files and metadata from the origin and releases the least recently-used cached files to ensure the most active files are available in the cache for your applications. File Cache is built using Lustre, which means effective I/O throughput can be very high.&lt;/p&gt; 
&lt;h3&gt;Amazon EBS volumes&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/ebs/"&gt;&lt;strong&gt;Amazon EBS volumes&lt;/strong&gt;&lt;/a&gt; are persistent block storage devices that can be attached to an EC2 instance. They function like HDD/SSD drives, with varying performance characteristics and costs. For example, &lt;em&gt;io2 Block Express&lt;/em&gt; volumes offer a maximum throughput of 4000 MB/s and 256,000 IOPS, per volume.&lt;/p&gt; 
&lt;p&gt;To use EBS volumes for binary distribution, you can follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a source EBS volume for the binary distributions, you can do this as part of CI/CD pipeline.&lt;/li&gt; 
 &lt;li&gt;Create a persistent EBS snapshot, which is a point-in-time copy of an EBS volume saved in S3. You can use EBS snapshots to restore and create multiple copies of the original EBS volume.&lt;/li&gt; 
 &lt;li&gt;Configure an EC2 Launch Template to include an EBS volume based on the newly created snapshot.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;When an EBS volume is created from a snapshot and attached to an EC2 instance, only the metadata of the volume is loaded initially. The volume’s data blocks are not loaded from S3 into the EBS block storage until they’re accessed or needed by the instance. These blocks will be lazy-loaded as they’re being accessed. As a result, the very first access to these blocks (and consequently the first workload invocation) will incur additional latency (we explain this in more detail in &lt;a href="https://docs.aws.amazon.com/ebs/latest/userguide/ebs-initialize.html"&gt;our documentation&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;When creating EBS volumes from a snapshot within the same region, there are no additional charges for data transfer, i.e., you only pay for the GBs of storage capacity.&lt;/p&gt; 
&lt;p&gt;For completeness, we should mention that &lt;code&gt;io1&lt;/code&gt; and &lt;code&gt;io2&lt;/code&gt; volume types enable you to attach a single EBS volume to up to 16 x EC2 instances with the same AZ, reducing the total number of EBS volumes needed. However, this comes at the tradeoff of additional management efforts – mapping volumes to EC2 instances. Also, the total IOPS performance across all the attached instances cannot exceed the volume’s maximum provisioned IOPS. Finally, you should also consider EBS &lt;em&gt;performance&lt;/em&gt; when sharing a volume among multiple concurrent workers running on a large EC2 instance.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"&gt;&lt;strong&gt;AMIs&lt;/strong&gt;&lt;/a&gt; are a template that contains the software configuration required to launch an Amazon EC2 instance. An AMI often includes a reference to one or more EBS Snapshots. In the context of binary distribution, AMIs can be thought of as an abstraction layer on top of EBS volumes, bypassing the need to manually attach an EBS volume to an instance. There are no performance or cost benefits in prebaking binary files into AMIs over using EBS volumes. Therefore, the choice is based on the managerial overhead and the grid’s architecture. We explore this topic in an article about &lt;a href="https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/prebaking-vs.-bootstrapping-amis.html"&gt;Prebaking vs. bootstrapping AMIs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html"&gt;Warm Pools&lt;/a&gt; in EC2 Auto Scaling let you pre-initialize EC2 instances and then stop them, &lt;em&gt;until you need to scale out&lt;/em&gt;. The EBS volume continues to hydrate in the background while the instance is stopped, saving you time and resources when you’re ready to scale, since your instances will spin up quickly and EBS volumes will be hydrated.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/ebs/latest/userguide/ebs-fast-snapshot-restore.html"&gt;&lt;strong&gt;Fast Snapshot Restore&lt;/strong&gt; (FSR)&lt;/a&gt; is a feature of Amazon EBS that optimizes EBS snapshots for faster volume restoration. When an EBS snapshot is marked for FSR, the EBS volumes restored from this snapshot can benefit from full performance immediately as the volume is created, without the need for a lazy loading mechanism. There is a &lt;a href="https://docs.aws.amazon.com/ebs/latest/userguide/ebs-fast-snapshot-restore.html#volume-creation-credits"&gt;credit-based system&lt;/a&gt; associated with the rate at which you can create EBS volumes from an FSR-enabled snapshot. These limits might render this method unsuitable for large-scale grids.&lt;/p&gt; 
&lt;h2&gt;Summary of options discussed&lt;/h2&gt; 
&lt;p&gt;Let’s now summarize the relevant characteristics of these services in a table (Figure 1). The &lt;em&gt;Source&lt;/em&gt; column shows where the binaries come from, while the &lt;em&gt;Destination&lt;/em&gt; column indicates if files are relocated or accessed remotely.&lt;/p&gt; 
&lt;p&gt;The next column indicates if &lt;em&gt;all files are accessible as soon as EC2 boots up&lt;/em&gt;. For example, if the files are stored on Amazon S3 and need to be copied to EBS before the workload can start, this step requires manual implementation. Next, some services &lt;em&gt;deliver full performance upon the first invocation&lt;/em&gt;, while other are not. For example, EFS provides full performance right away, but restoring an EBS volume from a snapshot may result in longer access times initially, due to hydration.&lt;/p&gt; 
&lt;p&gt;The cost column indicates workload dimensions that are most likely to account for the bulk cost as your service scales horizontally. We’ve shown this column only for an indicative purpose – review the costs of each service yourself to get an accurate estimation. For example, for a workload with many small files stored on Amazon S3 and accessed via S3 Mount Point, there is a cost associated with API calls per file. In contrast, the cost of FSxL varies based on provisioned throughput and total data transferred.&lt;/p&gt; 
&lt;p&gt;Finally, the last three &lt;em&gt;general&lt;/em&gt; columns indicate if the same service can be reused to deliver more files at runtime. For example, you can place a new version of a binary distribution on S3 and retrieve it via S3 Mount Point. But if you distribute the files using EBS or an AMI, then a new EBS needs to be mounted – or a different method needs to be used – to get the updates. The &lt;em&gt;global file system capabilities&lt;/em&gt; indicates if writes from one instance are available for reading by other instances in the grid.&lt;/p&gt; 
&lt;div id="attachment_3897" style="width: 1238px" class="wp-caption aligncenter"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/27/IMG-2024-06-27-12.57.52.png" target="_new" rel="noopener"&gt;&lt;img aria-describedby="caption-attachment-3897" loading="lazy" class="size-full wp-image-3897" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/27/IMG-2024-06-27-12.57.52.png" alt="Figure 1: Comparison of the discussed binary distribution techniques in the relationship to different workload requirements. Click on the image to get a larger (more readable) version." width="1228" height="605"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-3897" class="wp-caption-text"&gt;Figure 1: Comparison of the discussed binary distribution techniques in the relationship to different workload requirements. Click on the image to get a larger (more readable) version.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Some single AZ services have costs associated with cross-AZ network traffic, so refer to the documentation of each service for details.&lt;/p&gt; 
&lt;h2&gt;Conclusion and recommendations&lt;/h2&gt; 
&lt;p&gt;The purpose of this post is to provide a general guide on how to reason about the challenge of binary distribution in large FSI grids.&lt;/p&gt; 
&lt;p&gt;Our suggestion is to begin by evaluating the problem dimensions outlined in this post, in relation to your workload. Then, consider whether binaries need to be relocated closer to the computing resources or if they can be accessed over the network. We recommend starting with the simplest solution that fits the requirements, testing it, and then determining if further improvements are necessary.&lt;/p&gt; 
&lt;p&gt;When moving files to a compute instance, there may be a high initial overhead, but it can ensure consistent, isolated high performance. When it comes to network file systems, it’s important to keep in mind that they are shared among all compute instances that access them. Thus, NFS should be pre-provisioned for the load and scale you anticipate. Finally, you should always consider cross-AZ traffic costs and latency when making your choice.&lt;/p&gt; 
&lt;p&gt;Complexity often requires balancing performance and costs. You can incorporate your knowledge of your workloads and some simple heuristics in how you select the distribution methods presented in this post to mitigate many trade-offs, and feel good about your decisions.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Running FSI workloads on AWS with YellowDog</title>
		<link>https://aws.amazon.com/blogs/hpc/running-fsi-workloads-on-aws-with-yellowdog/</link>
		
		<dc:creator><![CDATA[Kirill Bogdanov]]></dc:creator>
		<pubDate>Wed, 10 Jul 2024 14:51:21 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[Financial Services]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Amazon FSx]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[EC2 Spot]]></category>
		<category><![CDATA[FSI]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">d573e34c0811266707460763324d754054fc016c</guid>

					<description>Financial services firms: we stress-tested YellowDog's HPC environment to see if it could handle a 10m task batch at 3,000 tasks per second. Check out the results.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt; &lt;img loading="lazy" class="alignright size-full wp-image-3913" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/24/Running-FSI-workloads-on-AWS-with-YellowDog.png" alt="Running FSI workloads on AWS with YellowDog 2" width="380" height="212"&gt;This post was contributed by Kirill Bogdanov and Alan Parry, CTO at YellowDog&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Large compute grids are critical components of any financial services industry (FSI) organisation today. They’re used in daily operations from pricing products, to risk management and monitoring, and even regulatory reporting. Historically, though, compute grids have been complex to operate and expensive to run. Even worse, some organizations’ demand for compute regularly exceeds their limited on-premises capacity. All of this has led to FSI firms exploring cloud for modernisation and as a cost effective alternative.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://yellowdog.co/"&gt;YellowDog&lt;/a&gt; is a high performance computing (HPC) environment that specializes in large-scale workloads and hybrid-cloud deployments, allowing customers to use their on-premises capacity while simultaneously taking advantage of the scale and capabilities of AWS.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll look at YellowDog’s performance and its operational characteristics that are important for FSI customers including: scheduling latency, throughput, and integration with AWS services.&amp;nbsp;This is the second post in a series – in our &lt;a href="https://aws.amazon.com/blogs/hpc/running-a-3-2m-vcpu-hpc-workload-on-aws-with-yellowdog/"&gt;previous post&lt;/a&gt;, we showed you how YellowDog can scale up to 3.2 million vCPUs on AWS in just under 33 minutes.&lt;/p&gt; 
&lt;h2&gt;The challenge familiar to FSIs&lt;/h2&gt; 
&lt;p&gt;Today, new financial regulatory requirements like FRTB (the &lt;em&gt;Fundamental Review of the Trading Book&lt;/em&gt;), RWA (&lt;em&gt;Risk Weighted Assets&lt;/em&gt;), and ESG (&lt;em&gt;Environmental, Social, and Governance&lt;/em&gt;) have resulted in a significant increase (up to 10x) in computational demand, compelling FSI firms to find additional compute capacity.&lt;/p&gt; 
&lt;p&gt;Furthermore, FSI workloads often involve large numbers of short-running tasks, requiring a variation of HPC known as high throughput computing (HTC). It’s not uncommon to process more than 100 million tasks per day, with a substantial portion of these tasks taking less than two seconds to run. These characteristics can make many ‘normal’ job schedulers ill-suited for the task.&amp;nbsp;But the ability to perform more computations, sooner, translates directly into improved quality of service and market competitiveness for an institution.&lt;/p&gt; 
&lt;p&gt;These factors are driving FSI firms to reimagine their computing grids and to explore cloud-based modernization pathways in pursuit of improved functionality and cost efficiency.&lt;/p&gt; 
&lt;p&gt;YellowDog is a leading cloud provisioning and job scheduling stack that enables customers across FSI to optimise their compute infrastructure and run complex workloads across multiple AWS regions, simultaneously. Today we’re evaluating the suitability of running YellowDog on AWS for FSI HTC workloads, specifically.&lt;/p&gt; 
&lt;h2&gt;Definitions&lt;/h2&gt; 
&lt;p&gt;For the rest of this post, let’s standardize on some terms so we don’t get tangled up:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Task&lt;/strong&gt; – A single unit of work submitted for processing.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Job&lt;/strong&gt; – A logical group of tasks that are submitted and executed together, often with interdependencies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Workload – &lt;/strong&gt;A larger group of jobs that are executed to deliver a business outcome, for example, a complete nightly batch.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Worker&lt;/strong&gt; – A process of some kind running on EC2 that can execute a task. An EC2 instance can facilitate support multiple workers, consuming different amounts of resource, each launching processes or containers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Scheduling overhead&lt;/h2&gt; 
&lt;p&gt;Scheduling overhead (or scheduling &lt;em&gt;latency&lt;/em&gt;) is the time it takes for a client application to submit a job to the grid and to receive a processing result, excluding the time for the actual computational processing itself.&lt;/p&gt; 
&lt;p&gt;Low scheduling overhead can let traders price instruments and assess risk exposure in close to real-time, which can be critical in many situations.&amp;nbsp;Furthermore, when processing short-running jobs, low scheduling overhead ensures high grid utilisation, with most of the compute time spent on job processing rather than job scheduling and book-keeping.&lt;/p&gt; 
&lt;p&gt;Let’s look at how YellowDog serves these near real-time intraday workloads. Figure 1 illustrates the concepts of scheduling overhead.&lt;/p&gt; 
&lt;div id="attachment_3910" style="width: 1003px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3910" loading="lazy" class="size-full wp-image-3910" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/24/IMG-2024-06-24-15.01.14.png" alt="Figure 1: The timeline depicting events from the moment a task is submitted to a grid until results are received by the client. Scheduling overhead is the difference between the completion time perceived by the client and the actual task execution time. &amp;nbsp;" width="993" height="339"&gt;
 &lt;p id="caption-attachment-3910" class="wp-caption-text"&gt;Figure 1: The timeline depicting events from the moment a task is submitted to a grid until results are received by the client. Scheduling overhead is the difference between the completion time perceived by the client and the actual task execution time.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;This figure presents a timeline depicting the sequence of events from the moment a task is submitted to a grid until results are received by the client. For the sake of simplicity, we’ll consider a job consisting of a single task.&lt;/p&gt; 
&lt;p&gt;At time &lt;em&gt;T1&lt;/em&gt;, a client application prepares a job and submits it to the grid at time &lt;em&gt;T2&lt;/em&gt;.&amp;nbsp;At this stage, the task enters the scheduling queue of the grid and waits to be assigned to a worker process.&amp;nbsp;At time &lt;em&gt;T3&lt;/em&gt;, a worker picks up the task and executes it until completion at &lt;em&gt;T4&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Finally, at time T5, using push or pull mechanisms, the client application becomes aware that the job has completed, allowing the collection of results.&amp;nbsp;At time T6, the client application collects the results (though sometimes, notification and result retrieval are combined).&lt;/p&gt; 
&lt;p&gt;It’s important to note that while the actual execution time of the task spans from T3 to T4, the perceived time from the client’s perspective is T2 to T6. For this post, we’ll refer to the time difference between execution time &lt;em&gt;perceived&lt;/em&gt; by the client application and &lt;em&gt;actual&lt;/em&gt; job execution time as the &lt;strong&gt;scheduling overhead&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Based on these definitions, a client can submit &lt;em&gt;at most&lt;/em&gt; (1 / T2 – T6) tasks per second into the grid.&lt;/p&gt; 
&lt;h2&gt;Testing YellowDog’s scheduling&lt;/h2&gt; 
&lt;p&gt;We wanted to stress YellowDog’s scheduling components, so we used &lt;em&gt;zero-work tasks &lt;/em&gt;(tasks that did literally nothing), so the processing step would simply return a hard coded output. That way we minimised the T3 to T4 time interval, and focused on the scheduling overhead itself.&lt;/p&gt; 
&lt;p&gt;Figure 2 shows the cumulative distribution functions (CDFs) of the measured scheduling overhead. In this figure, the &lt;em&gt;x&lt;/em&gt;-axis shows the end-to-end latency from the moment of submission of the job until all the results of the task are returned to the client, while the &lt;em&gt;y&lt;/em&gt;-axis shows the percentile – the fraction of jobs that completed within the indicated latency.&lt;/p&gt; 
&lt;p&gt;We performed these measurements using jobs containing 1, 10, and 100 zero-work tasks, and plotted them using full, dashed, and dotted lines respectively in Figure 2.&lt;/p&gt; 
&lt;p&gt;We also considered two distinct job-completion notification mechanisms. The first mechanism relies on YellowDog’s task-level REST API. This API allows us to check the status of individual tasks.&lt;/p&gt; 
&lt;p&gt;The second mechanism is a custom extension we built with Amazon ElastiCache. This mechanism relies on the worker to mark task completion directly in the cache, with the client application pulling results from the cache. This is more consistent with how real-world tasks would usually be managed.&lt;/p&gt; 
&lt;div id="attachment_3911" style="width: 956px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3911" loading="lazy" class="size-full wp-image-3911" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/24/IMG-2024-06-24-15.01.51.png" alt="Figure 2: End-to-end task completion time as perceived by a client. Median latency for different notification mechanisms is around 400ms." width="946" height="597"&gt;
 &lt;p id="caption-attachment-3911" class="wp-caption-text"&gt;Figure 2: End-to-end task completion time as perceived by a client. Median latency for different notification mechanisms is around 400ms.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The results indicated that YellowDog can provide an end-to-end scheduling overhead of a few hundred milliseconds, which is sufficiently low to be unnoticeable by a user like a trader, requiring a more or less real-time response.&lt;/p&gt; 
&lt;h2&gt;Efficiency with short duration tasks&lt;/h2&gt; 
&lt;p&gt;For ‘long’ running tasks (that is, &lt;em&gt;minutes to hours&lt;/em&gt;) a scheduling latency of one second might not be relevant. But this changes when processing time is measured in seconds: a low scheduling overhead directly translates into efficiency when processing short running tasks.&lt;/p&gt; 
&lt;p&gt;YellowDog’s &lt;em&gt;Worker&lt;/em&gt; polls the YellowDog &lt;em&gt;Scheduler&lt;/em&gt; for tasks to execute. If there are no tasks for a Worker to process it will sleep for a randomly chosen, but &lt;em&gt;configurable,&lt;/em&gt; duration which defaults to be between 0-60 seconds.&lt;/p&gt; 
&lt;p&gt;Once a task becomes available, the Worker will retrieve it, execute it, and will then attempt to retrieve the next task from the queue. If there are no further tasks waiting, the Worker will sleep again until another task becomes available.&lt;/p&gt; 
&lt;p&gt;The duration of the Worker sleep interval is only relevant when there are no tasks in the Worker’s queue, and it can be adjusted at a system-wide level if required, to reduce the average interval.&lt;/p&gt; 
&lt;p&gt;By using a prepopulated task queue, a single YellowDog Worker can efficiently process up to 25 tasks per second, implying a base scheduling overhead of only 40 milliseconds. This processing speed translates into an efficiency rate of 98% for tasks that complete within two seconds.&lt;/p&gt; 
&lt;p&gt;That means a single YellowDog Worker can efficiently consume short tasks from a queue and deliver results in a timely fashion.&lt;/p&gt; 
&lt;h2&gt;Throughput&lt;/h2&gt; 
&lt;p&gt;Throughput refers to the rate at which tasks can be processed. Throughput is particularly important for nightly batches when millions of tasks need to be processed as a part of a larger workload, as efficiently as possible. The ability to schedule a large volume of tasks quickly and efficiently is of paramount importance for a successful FSI grid scheduler.&lt;/p&gt; 
&lt;p&gt;More importantly, a fast scheduler can scale &lt;em&gt;horizontally&lt;/em&gt; to take full advantage of the compute resources available at AWS, allowing workloads to complete faster, thereby providing an opportunity for additional computation or to re-run workloads that required correction.&lt;/p&gt; 
&lt;p&gt;To measure YellowDog’s throughout we configured 2,000 YellowDog Workers (each with 1vCPU and 4GiB RAM), and we used 16 clients to submit zero-work jobs comprising of 5,000 to 10,000 tasks with 10,000,000 tasks in total.&lt;/p&gt; 
&lt;p&gt;We measured the total time from the moment we initiated submission, until the last task was complete.&lt;/p&gt; 
&lt;p&gt;Using this approach, we measured the average tasks per second (TPS) to be 3,000. This translates to 85 million tasks per typical overnight eight-hour window.&lt;/p&gt; 
&lt;h2&gt;Integration with AWS services&lt;/h2&gt; 
&lt;p&gt;When it comes to running large scale grids on AWS, it’s critical to follow the best practices and use the correct APIs to obtain the necessary compute capacity quickly – and cost effectively.&lt;/p&gt; 
&lt;p&gt;YellowDog supports the Amazon Elastic Computer Cloud (Amazon EC2) Fleet API, including important options like EC2 Spot, Allocation Strategies (including the recommended ‘price-capacity-optimized’). It also supports attribute-based instance selection, which greatly simplifies finding suitable Amazon EC2 instances for your workload to include in that mix for Spot.&lt;/p&gt; 
&lt;p&gt;Using ‘dynamic templates’ YellowDog automatically selects the best source of compute based on your requirements and preferences, especially useful when your workload characteristics can change. This automation is fed by YellowDog Insights which measures Amazon EC2 instance types across price, performance, benchmarking, location, and energy efficiency. Included in this data is a comparison of spot and on-demand pricing and YellowDog’s managed compute requirements ensure that if you experience spot interruptions, compute can be re-provisioned and tasks can be re-tried on other workers.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;We saw a median scheduling overhead of 400ms, and a throughput greater than 3,000 tasks per second. In our previous post we showed YellowDog scaling to more than 3.2 million vCPUs, and maintaining a high utilization for that fleet. Taken together, these results indicate that YellowDog has the capability to handle a wide range of FSI workloads.&lt;/p&gt; 
&lt;p&gt;Based on these results, we think YellowDog has the necessary scale, efficiency, and low scheduling latency to handle both batch &lt;em&gt;and&lt;/em&gt; intraday jobs effectively for financial services customers. It’s also well integrated with AWS services, which allows it to take full advantage of the power of AWS when provisioning compute resources at scale.&lt;/p&gt; 
&lt;p&gt;YellowDog’s existing FSI customers are made up of large asset managers and hedge funds running complex hybrid-cloud workloads, at scale. You can &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-q3kswq4btig6o?sr=0-1&amp;amp;ref_=beagle&amp;amp;applicationId=AWSMPContessa"&gt;access YellowDog&lt;/a&gt; through AWS Marketplace, which will include the cost of the solution in your monthly AWS bill.&lt;/p&gt; 
&lt;p&gt;You can &lt;a href="https://yellowdog.co/get-started/"&gt;get in touch with YellowDog&lt;/a&gt; directly to arrange a discovery call, discuss your business requirements, and learn how they can support you. Alternatively, reach out to the FSI HPC experts at AWS by emailing us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Improve HPC workloads on AWS for environmental sustainability</title>
		<link>https://aws.amazon.com/blogs/hpc/improve-hpc-workloads-on-aws-for-environmental-sustainability/</link>
		
		<dc:creator><![CDATA[Sam Mokhtari]]></dc:creator>
		<pubDate>Tue, 02 Jul 2024 12:35:15 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<category><![CDATA[visualization]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">da98b13a916f8f924ea8af83da8c8373caefe598</guid>

					<description>Need to cut your carbon footprint without sacrificing productivity? Migrating HPC workloads to the cloud allowed Baker Hughes to reduce emissions by 99%! Get tips for optimizing compute, storage, networking so you can do better.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3870" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/18/boofla88_a_supercomputer_sitting_in_the_middle_of_a_rainforest._013a2b55-5560-40a6-bf1e-9ee121f4dd3c.png" alt="Improve HPC workloads on AWS for environmental sustainability" width="380" height="212"&gt;HPC workloads are used across industries from finance and engineering to genomics and chemistry – all to enable opportunities for innovation and growth. Because the compute is so intensive, these systems can consume large amounts of energy and generate serious carbon emissions. Many businesses are looking to reduce their carbon footprint to meet the Paris accord, and on-premises HPC clusters constitute a noticeable portion of their total data center impact. But by migrating their HPC workloads to AWS &lt;a href="https://aws.amazon.com/solutions/case-studies/baker-hughes-case-study/"&gt;Baker Hughes&lt;/a&gt; was able to reduce their carbon footprint by 99%, setting an example for the community.&lt;/p&gt; 
&lt;p&gt;If you want to achieve a similar result, it’s critical to optimize your cluster for resource efficiency &lt;em&gt;and&lt;/em&gt; effectiveness. HPC architectures are built on five main pillars: compute, storage, networking, visualization and orchestration.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll focus on each one of these pillars, and discuss tips and best practices that can help you optimize resource usage and the environmental impact of your workloads, while also keeping your productivity goals.&lt;/p&gt; 
&lt;p&gt;To demonstrate these ideas, we’ll assume the use case of an automotive customer who needs to run computer-aided engineering (CAE). These types of jobs need large volumes of compute, so they’re representative of most simulations you’re likely to be doing in your environment, too.&lt;/p&gt; 
&lt;h2&gt;Compute&lt;/h2&gt; 
&lt;h3&gt;Choose the right instance type for your HPC workload&lt;/h3&gt; 
&lt;p&gt;In an on-premises cluster, the computing nodes are usually homogeneous – they have the same CPU type and the quantity of memory per core. However, HPC workloads don’t all have the same requirements. For example, structural analysis can benefit from having a low latency NVMe scratch disk to access, computational fluid dynamics (CFD) jobs usually span across a greater number of cores, and finite element analysis (FEA) simulations need more memory per core, compared to other types of simulations.&lt;/p&gt; 
&lt;p&gt;To reduce your carbon footprint, you need to optimize your consumption of computing resources, so it’s important to match your workload to the &lt;strong&gt;right compute instance types&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;In AWS there are more than 750 types of Amazon Elastic Compute Cloud (Amazon EC2) instances to choose from, each one with different hardware characteristics. Recently, we launched a new family of instances dedicated to HPC workloads.&lt;/p&gt; 
&lt;p&gt;For our CAE use case, you can use the new Amazon EC2 Hpc7a instance. We’ve seen them perform very well for workloads that can be accelerated by increasing the number of compute cores. You can read about some of our &lt;a href="https://aws.amazon.com/blogs/hpc/deep-dive-into-hpc7a-the-newest-amd-powered-member-of-the-hpc-instance-family/"&gt;benchmark tests in another post&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Use AWS Graviton instances if you can&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/ec2/graviton/"&gt;AWS Graviton&lt;/a&gt;&amp;nbsp;is a custom-designed processor created by AWS, based on the Arm64 architecture and which provides a balance of efficiency, performance, and cost. This makes it a good fit to lower the environmental impact of many HPC workloads. &lt;a href="https://aws.amazon.com/ec2/graviton/graviton-sustainability/"&gt;AWS Graviton-based Amazon EC2 instances use up to 60% less energy&lt;/a&gt; than comparable EC2 instances for the same performance. If your HPC application can be compiled for Arm64, you can use Graviton instances to enable greater energy efficiency for your compute intensive workloads.&lt;/p&gt; 
&lt;p&gt;Hpc7g Instances are powered by AWS Graviton3E processors, with up to 35% higher vector instruction processing performance than the Graviton3. They are designed to give you the best price/performance for tightly coupled compute-intensive HPC and distributed computing workloads, and deliver 200 Gbps of dedicated network bandwidth that’s optimized for traffic between instances in the same subnet.&lt;br&gt; For CAE, customers can take advantage of open source CFD applications, like OpenFOAM, that can be compiled for Arm architecture processors and are well suited for the hpc7g instance type. But ISVs like Siemens, Cadence, and Ansys, now make versions of their solvers available for Graviton in the same way they do for x86.&lt;/p&gt; 
&lt;p&gt;The table below summarizes some of the key HPC instance types with their characteristics, and provides a recommendation of the target workload.&lt;/p&gt; 
&lt;div id="attachment_3924" style="width: 1003px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3924" loading="lazy" class="size-full wp-image-3924" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/03/IMG-2024-07-03-12.57.09.png" alt="Table 1 – Comparison of Amazon EC2 HPC specific instance types showing example workloads and attributes" width="993" height="446"&gt;
 &lt;p id="caption-attachment-3924" class="wp-caption-text"&gt;Table 1 – Comparison of Amazon EC2 HPC specific instance types showing example workloads and attributes&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Choose the right AWS Region for your HPC workload&lt;/h3&gt; 
&lt;p&gt;AWS Well-Architected framework helps customers to build secure, highly-performant, resilient, and efficient architecture, and is built around six pillars: operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. The framework provides a consistent approach for customers and partners to evaluate architectures and implement scalable designs.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/region-selection.html"&gt;SUS01-BP01&lt;/a&gt; best practices in the AWS Well-Architected &lt;em&gt;Sustainability&lt;/em&gt; pillar, recommends that you choose the AWS Regions for your workload based on both business requirements and sustainability goals.&lt;br&gt; As explained in another &lt;a href="https://aws.amazon.com/blogs/architecture/how-to-select-a-region-for-your-workload-based-on-sustainability-goals/"&gt;blog post&lt;/a&gt;, this process includes two key steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Assess and shortlist potential Regions for your workload based on your business requirements.&lt;/li&gt; 
 &lt;li&gt;Choose Regions near Amazon renewable energy projects and Region(s) where the grid has a lower published carbon intensity.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As explained in the previous step, for the sample CAE workload, Hpc7a instances can accelerate this workload and help you to achieve your business goals.&lt;/p&gt; 
&lt;p&gt;By using this simple script, you can assess that this instance type is available, for example, in the Stockholm Region (eu-north-1):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;#!/bin/sh
instance_type=hpc7a.96xlarge

# Get a list of all AWS regions
regions=$(aws ec2 describe-regions --all-regions --query 'Regions[].RegionName' --output text)

echo "AWS Regions where $instance_type is available:"

# Loop through each region and check if the instance type is available
for region in $regions; do
  available=$(aws ec2 describe-instance-type-offerings --filters Name=instance-type,Values=$instance_type --region $region --query 'InstanceTypeOfferings[].InstanceType' --output text 2&amp;gt;/dev/null)

  if [ "$available" = "$instance_type" ]; then
    echo "- $region"
  fi
done
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Moreover, the electricity consumed in the Stockholm Region is attributable to 100% renewable energy as listed on the &lt;a href="https://sustainability.aboutamazon.com/environment/the-cloud?energyType=true#renewable-energy-map"&gt;Amazon sustainability website&lt;/a&gt;. So, this Region is a good candidate for running your CAE workloads, and also achieving your sustainability goals.&lt;/p&gt; 
&lt;h3&gt;Use a mix of EC2 purchase options&lt;/h3&gt; 
&lt;p&gt;HPC workloads usually have different resource requirements and business priorities. You can use a combination of purchase options to address your specific business needs, balancing instance flexibility, scalability, and efficiency.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html"&gt;Compute Savings Plans&lt;/a&gt; for predictable, steady-state workloads that allow flexibility if your needs (like target Region, instance families, or instance types) change frequently.&lt;/li&gt; 
 &lt;li&gt;Use&amp;nbsp;&lt;a href="https://alpha-docs-aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html"&gt;On-Demand Instances&lt;/a&gt; for new, and unpredictable workloads&lt;/li&gt; 
 &lt;li&gt;Use&amp;nbsp;&lt;a href="https://alpha-docs-aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html"&gt;Spot Instances&lt;/a&gt; to supplement the other options for applications that are fault tolerant and flexible.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Our example use case may require periods of intense activity to meet project milestones or client deadlines, where the elasticity of the cloud facilitates access to large amounts of On-Demand compute resources in order to meet tight deadlines. AWS HPC orchestration tools like AWS Batch and AWS ParallelCluster both have support for Spot, On-demand and Saving Plans resources.&lt;/p&gt; 
&lt;h2&gt;Storage&lt;/h2&gt; 
&lt;p&gt;AWS provides several types of storage and file system technologies that can be used for HPC workloads. Similar to compute, optimizing your storage choices can help you to accelerate your workload &lt;em&gt;and&lt;/em&gt; help to reduce your carbon emissions.&lt;/p&gt; 
&lt;p&gt;Every workload has a different data strategy, but we can split them across different stages:&lt;/p&gt; 
&lt;h4&gt;On-premises storage&lt;/h4&gt; 
&lt;p&gt;Amazon’s goal is to power our operations with 100% renewable energy by 2025 – five years ahead of our original 2030 target, so migrating your entire portfolio of workloads to AWS is usually a great option to reduce your carbon footprint.&lt;/p&gt; 
&lt;p&gt;In some cases, this is not possible because your data is generated from on-site instruments or labs. In this case, you need an efficient mechanism to move data to AWS to process in the cloud. Some of our managed file system offerings, such as &lt;a href="https://docs.aws.amazon.com/fsx/latest/FileCacheGuide/what-is.html"&gt;Amazon File Cache&lt;/a&gt;, allow you to mount the on-premises file system to the cloud. By doing that, you’ll avoid unnecessarily replicating your data in the cloud and the results of your simulations will also be accessible on-premises.&lt;/p&gt; 
&lt;h3&gt;Local Disk&lt;/h3&gt; 
&lt;p&gt;Use EBS volumes to store your data on the computing nodes. If your applications need access to high-speed, low-latency local storage for scratch files, you can use local NVMe-based SSD block storage physically connected to the host server. Many Amazon EC2 instances – like the Hpc6id – have multiple NVMe disks to accelerate the performance of the most IO demanding workloads. Keep in mind that these local disks are ephemeral, and their content will be automatically erased when you stop the instances. You need to identify which files you need to save when the job has been completed and move them to a persistent storage area, like a shared POSIX file system or Amazon S3.&lt;/p&gt; 
&lt;h3&gt;Shared POSIX file system&lt;/h3&gt; 
&lt;p&gt;If your applications need to access a shared file system, you can use one of the AWS managed file systems. For example, Amazon FSx for Lustre is a fully-managed service that provides high-performance storage for compute workloads. Powered by Lustre, the world’s most popular high-performance file system, FSx for Lustre offers shared storage with sub-millisecond latencies, up to &lt;em&gt;terabytes per second&lt;/em&gt; of throughput, and millions of IOPS.&lt;/p&gt; 
&lt;h3&gt;Long term storage&lt;/h3&gt; 
&lt;p&gt;Once the jobs are completed, you can migrate your data to a long-term storage in Amazon S3. This will help you to reduce the amount of data in the shared file system. FSx for Lustre also integrates with Amazon S3 via the Data Repository mechanism that allows seamless access to objects in Amazon S3 that can be lazy loaded into the high-performance file system layer. This approach delivers the required access semantics and performance for scalable HPC applications when they’re needed, while also providing the usual capacity, cost, data protection, lifecycle, and sustainability benefits of Amazon S3.&lt;/p&gt; 
&lt;h3&gt;Use life cycle policies to move and delete data&lt;/h3&gt; 
&lt;p&gt;HPC workloads have datasets with different retention and access requirements. For example, your HPC application may need frequent access to&amp;nbsp;some datasets for a limited period of time. After that, those datasets can be archived or deleted.&lt;/p&gt; 
&lt;p&gt;To efficiently manage your HPC datasets, you can configure lifecycle policies, which are rules that define how to handle datasets. You can set automated lifecycle policies to enforce lifecycle rules. You can set up automated lifecycle policies for &lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html"&gt;Amazon S3&lt;/a&gt;, &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html"&gt;Elastic Block Storeage (EBS)&lt;/a&gt;, and &lt;a href="https://docs.aws.amazon.com/efs/latest/ug/lifecycle-management-efs.html"&gt;Amazon EFS&lt;/a&gt;&lt;u&gt;.&lt;/u&gt;&lt;/p&gt; 
&lt;p&gt;Our example CAE application is well suited to the FSx for Lustre file system since it allows high performance concurrent access from multiple compute instances. You can use the &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data-repo.html"&gt;S3 Data Repository&lt;/a&gt; settings to automate the migration of the data to Amazon S3 and then the S3 lifecycle policies to move the data from different&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html"&gt;storage classes&lt;/a&gt;&amp;nbsp;like&amp;nbsp;&lt;strong&gt;S3 Standard-IA &lt;/strong&gt;and&amp;nbsp;&lt;strong&gt;S3 Glacier Deep Archive.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Networking&lt;/h2&gt; 
&lt;p&gt;For tightly coupled workloads, it’s important to remove all network bottlenecks to guarantee the best utilization of your computing resources. These applications are sensitive to network latency so we recommended using &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"&gt;&lt;strong&gt;Cluster Placement Group&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;s (CPG)&lt;/strong&gt; and &lt;a href="https://aws.amazon.com/hpc/efa/"&gt;&lt;strong&gt;Elastic Fabric Adapter (EFA)&lt;/strong&gt;&lt;/a&gt;. Deploying your compute nodes in the same &lt;strong&gt;CPG&lt;/strong&gt;, will allow for reliable low-latency communication between the instances, and will help your tightly-coupled application to scale as desired.&lt;br&gt; &lt;strong&gt;Elastic Fabric Adapter (EFA)&lt;/strong&gt; is a network interface for Amazon EC2 instances that enables customers to run HPC applications requiring high levels of inter-instance communications, like computational fluid dynamics, weather modelling, and reservoir simulation, at scale on AWS.&lt;/p&gt; 
&lt;p&gt;Coming back to our example, most of the HPC applications used in the automotive industry implement distributed memory parallelism using the Message Passing Interface (MPI), allowing a single simulation to employ parallel processing using the cores and memory of multiple instances to increase simulation speed. MPI applications perform best using instances which feature EFA because it reduces latency and increases throughput for MPI parallel communications.&lt;/p&gt; 
&lt;h2&gt;Orchestration&lt;/h2&gt; 
&lt;p&gt;If you want to reduce the environmental impact of HPC workloads, it’s important to right size your compute resources and deploy only when needed. There is usually no need to have all your compute always up and running when there are no jobs to run.&lt;/p&gt; 
&lt;p&gt;On AWS, you can start and stop the computing nodes based on your users’ needs: they submit their jobs and AWS HPC infrastructure scales capacity up (or down) to aim for the right capacity when needed.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; is a fully managed batch computing service that plans, schedules, and runs your containerized HPC or Machine Learning workloads across the full range of AWS compute offerings, such as Amazon ECS, Amazon EKS, and AWS Fargate, using Spot or On-Demand instances. If you select the allocation strategy, AWS Batch will automatically select the Spot instances that are large enough to meet the requirements of the jobs and are less likely to be interrupted. Using Spot instances is a great mechanism to run your compute intensive workload leveraging our unused capacity.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; is an open-source cluster management tool that makes it easy for you to deploy and manage HPC clusters on AWS. ParallelCluster uses a graphical interface to let you model and provision all the resources needed for your HPC applications in an automated and secure manner. It supports multiple instance types and job submission queues using SLURM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example, automotive customers tend to deploy their HPC infrastructure using AWS ParallelCluster because it can configure EFA automatically and mount an FSx for Lustre file system for the high-performance scratch area.&lt;/p&gt; 
&lt;h2&gt;Remote visualization&lt;/h2&gt; 
&lt;p&gt;If you need an interactive graphical application to generate the input files for your jobs, it’s more convenient to also run the pre-processing GUI on AWS. Customers use remote visualization technologies such as &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;NICE DCV&lt;/a&gt; to stream the graphics from EC2 nodes back to the end-user laptop. By using a remote visualization technology, you remove the need to copy large datasets to and from the cloud.&lt;/p&gt; 
&lt;p&gt;For our automotive use case, NICE DCV is an excellent choice for the graphics intensive pre-processing and post-processing stages of the HPC workflows. NICE DCV allows you to use graphics instances with powerful GPUs and direct access to the same data used by the simulation stages, allowing high performance and interactive model setup and visualization of results.&lt;/p&gt; 
&lt;h2&gt;Evaluating sustainability improvements for your workloads&lt;/h2&gt; 
&lt;p&gt;The best way to evaluate success when optimizing HPC workloads for sustainability is to use&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/evaluate-specific-improvements.html"&gt;proxy measures and unit of work KPIs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The aggregate cost of your computational resources can be a good “&lt;em&gt;rule of thumb&lt;/em&gt;” proxy for their energy consumption. Generally, compute resources (EC2 instance types and sizes) with more processing power, use commensurately more energy and mostly have a higher price.&lt;/p&gt; 
&lt;p&gt;To get a measure of where you currently stand, you could potentially analyze the total volume of simulation or modelling work that you have accomplished within say a typical one-month period, from which you can derive a metric for the average cost per job. Of course, different applications and workloads can have quite different computational resource requirements, so you might be better served deriving an average &lt;em&gt;cost per job&lt;/em&gt; for &lt;em&gt;each job type&lt;/em&gt;. Your challenge is to then try to reduce this average cost per job, by trying different EC2 instance types or sizes that can deliver the required performance at further reduced cost.&lt;/p&gt; 
&lt;p&gt;For each job type, it’s also important to define a service level agreement (SLA) represented by the maximum allowable run time (cut-off time) to maintain business operations or staff productivity objectives. This defines the practical limit, beyond which further parallelism, or performance optimization doesn’t help.&lt;/p&gt; 
&lt;p&gt;For the type and quantity of computing resources that you &lt;em&gt;do&lt;/em&gt; select, it’s also important to pay attention to their resource utilization while running the workload. For compute-intensive HPC workloads, you should generally not have any idle instances, and all CPU cores should generally be running at close to 100%, unless explainable by phases of parallel communication or storage I/O. Remember that if network transfers or I/O time are significant it may be beneficial to select EC2 instances with higher speed networking, or perhaps faster local or shared storage options.&lt;/p&gt; 
&lt;p&gt;To better understand how to use proxy measures to optimize workloads for efficiency, you can read&amp;nbsp;&lt;a href="https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/"&gt;Sustainability Well-Architected Lab&lt;/a&gt;&amp;nbsp;on&amp;nbsp;&lt;em&gt;Turning the Cost and Usage Report into Efficiency Reports&lt;/em&gt;.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Reducing the environmental impact of HPC workloads is a journey. Moving your workloads to AWS will help you to accelerate the reduction of your carbon footprint emissions quickly. In this post, we provided guidance and recommendations on how to improve your HPC workload on AWS without sacrificing your business outcomes.&lt;/p&gt; 
&lt;p&gt;To learn more, check out the&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html"&gt;Sustainability Pillar of the AWS Well-Architected Framework&lt;/a&gt;&amp;nbsp;and other blog posts on&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/architecture/tag/sustainability/"&gt;architecting for sustainability&lt;/a&gt;. For more architecture content, refer to&amp;nbsp;the &lt;a href="https://aws.amazon.com/architecture/"&gt;AWS Architecture Center&lt;/a&gt;&amp;nbsp;for reference architecture diagrams, vetted architecture solutions,&amp;nbsp;&lt;a href="https://aws.amazon.com/architecture/well-architected/"&gt;Well-Architected&lt;/a&gt;&amp;nbsp;best practices, patterns, icons, and more.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Simulating autonomous mining operations using Robotec.ai on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/simulating-autonomous-mining-operations-using-robotec-ai-on-aws/</link>
		
		<dc:creator><![CDATA[Matt Hansen]]></dc:creator>
		<pubDate>Mon, 01 Jul 2024 16:28:55 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">7e189a5a1112ae1b83b595746cad451281851d11</guid>

					<description>Big changes are underway in mining - see how the Boliden Group simulates fleets of autonomous trucks using AWS Batch - for safety and efficiency.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3749" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/30/Simulating-autonomous-mining-operations-using-Robotec.ai-on-AWS-1.png" alt="" width="380" height="212"&gt;This post was contributed by Matthew Hansen, Principal SA, Advanced Computing &amp;amp; Simulation, Ryan Qi, Principal BD/GTM, Advanced Computing from AWS and Dominik Jargot, Software Engineer, Bartłomiej Boczek, Senior Machine Learning Engineer from Robotec.ai, and Peter Burman, Program Manager, Boliden.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Autonomous mining operations represent a significant leap forward in the mining industry, offering enhanced safety, efficiency, and productivity. The introduction of autonomous trucks is at the forefront of this transformation. &lt;a href="https://www.boliden.com/"&gt;Boliden AB&lt;/a&gt;, a Swedish multinational company specializing in precious metals, is an AWS customer using autonomous trucking technology for mining operations. These self-driving heavy-duty vehicles are equipped with advanced sensors and navigation systems, and operate around the clock without human intervention – reducing the risk of accidents and increasing uptime. These trucks follow pre-determined routes to move materials from the mining site to storage or processing sites, communicating real-time data back to a central control system.&lt;/p&gt; 
&lt;p&gt;Simulations are instrumental in developing and optimizing autonomous mining operations. They create a virtual environment for testing various scenarios and conditions, minimizing the risks and costs of real-world trials. Engineers at Boliden use simulations to evaluate the operations and performance of autonomous trucks, refine control strategies, and ensure systems can adapt to unexpected events or changes in the mining environment. This predictive capability is essential for maintaining safety, enhancing efficiency, and ensuring continuous production, all of which contribute to the long-term sustainability and profitability of mining operations.&lt;/p&gt; 
&lt;p&gt;In this post, you’ll learn how Boliden simulates multiple autonomous trucks in a mine, and scales those simulations to run dozens of scenarios simultaneously using the recently added multi-container jobs feature in AWS Batch.&lt;/p&gt; 
&lt;h2&gt;Building autonomous mining simulations&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://robotec.ai/"&gt;Robotec.ai&lt;/a&gt; is a technology leader in the realm of robotics and autonomous vehicles. They have a diverse team of experts in robotics, electrical engineering, software development, AI/ML, and human-technology interaction. Their simulation platform, &lt;a href="https://robotec.ai/products/rosi-simulation-platform/"&gt;RoSi&lt;/a&gt;, is engineered to create hyper-realistic simulations of confined spaces such as mining sites, warehouses, and large agricultural fields. The simulation of virtual mining environments enables accurate modeling of mining equipment and experimentation with many traffic scenarios.&lt;/p&gt; 
&lt;p&gt;Boliden uses RoSi to simulate the real-time operations of their mines over 40 distinct scenarios that cover a wide spectrum of machine interactions. This rigorous testing regime demands in excess of 1,000 hours of computational simulation every time Boliden changes their mining operations, to ensure robustness and reliability.&lt;/p&gt; 
&lt;p&gt;Figure 1 illustrates a scenario where two autonomous mining trucks, operating underground and simulated using the RoSi platform, navigate a narrow tunnel while avoiding obstacles and each other. One truck is using an underground turnout to wait for another to pass going the opposite direction. The time spent waiting is idle time that Boliden wants to reduce. The number of turnouts is also an expense they want to minimize. They run scenarios with varying numbers of trucks and turnouts and measure the idle time of the trucks, to find the optimal number of turnouts in the mine.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 1 - Underground autonomous mining simulations using Robotec.aiâ&#128;&#153;s RoSi platform" width="500" height="375" src="https://www.youtube-nocookie.com/embed/wWVtjBrhf_g?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 1 – Underground autonomous mining simulations using Robotec.ai’s RoSi platform&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Multi-container simulations&lt;/h2&gt; 
&lt;p&gt;Traditionally, when Boliden used &lt;a href="https://aws.amazon.com/batch"&gt;AWS Batch&lt;/a&gt; to simulate these types of scenarios, the service allowed only jobs with a single container. Consequently, engineers had to spend time on extra job preparation steps, merging all of the simulation components into a large, complex, monolithic container which also made division of work across teams difficult. Any code changes required engineers to rebuild the entire monolithic container, complicating software development (Dev), IT operations (Ops), and debugging.&lt;/p&gt; 
&lt;p&gt;Recently, though, AWS Batch added multi-container jobs, a feature that streamlines the process for customers to conduct large-scale simulation for complex systems in autonomous vehicles and robotics. This enhancement allows teams like those at Robotec.ai and Boliden to leverage Batch’s sophisticated scaling, scheduling, and cost management features without the added complexity of converting their systems into monolithic containers. Figure 2 illustrates the architecture for simulating mining operations with RoSi, using these multi-container job features in Batch.&lt;/p&gt; 
&lt;div id="attachment_3724" style="width: 918px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3724" loading="lazy" class="size-full wp-image-3724" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.23.17.png" alt="Figure 2 - Architecture for running mining operation simulations on RoSi and AWS Batch multi-container feature" width="908" height="544"&gt;
 &lt;p id="caption-attachment-3724" class="wp-caption-text"&gt;Figure 2 – Architecture for running mining operation simulations on RoSi and AWS Batch multi-container feature&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As shown in Figure 2, engineers can use an array of smaller, independent containers, each representing a mining truck, alongside the RoSi simulator. This approach not only accelerates development by eliminating redundant steps in job setup, but also reduces the need for engineers to develop additional in-house tools, and eases collaboration between software development and IT operations teams.&lt;/p&gt; 
&lt;h2&gt;How you can run simulations on AWS Batch with RoSi&lt;/h2&gt; 
&lt;h3&gt;Step 1 – Configure your Job inputs in RoSi&lt;/h3&gt; 
&lt;p&gt;Using RoSi, you can configure experiment properties specific to each use case. In the Boliden example, the configuration includes the main scenario setup, traffic properties, events, and safety system parameters. Configuration enables testing of the entire underground mine in the simulated environment against the complex system of systems, which typically consists of autonomous vehicles, a traffic management system, and a safety system.&lt;/p&gt; 
&lt;p&gt;RoSi offers a lot of flexibility in the system configuration, allowing engineers to create isolated tests for individual system parameters, or large-scale system-wide stress tests. The typical flow of batch experiments configuration in RoSi goes like this:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Configure the types and paths of vehicles in the &lt;strong&gt;Events&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Configure traffic properties in the &lt;strong&gt;Traffic&lt;/strong&gt; tab, for example, the localization of the turnouts (places where vehicles can freely pass each other) in the narrow tunnels of the mine.&lt;/li&gt; 
 &lt;li&gt;In the &lt;strong&gt;Main&lt;/strong&gt; tab, configure the scenario details, like the number of vehicles, their positions, and speeds, to simulate various situations.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Figures 3 and 4 show the detail for some of the configuration capabilities of RoSi.&lt;/p&gt; 
&lt;div id="attachment_3725" style="width: 1195px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3725" loading="lazy" class="size-full wp-image-3725" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.23.56.png" alt="Figure 3 – Turnouts and other traffic parameters configuration in the underground mine." width="1185" height="622"&gt;
 &lt;p id="caption-attachment-3725" class="wp-caption-text"&gt;Figure 3 – Turnouts and other traffic parameters configuration in the underground mine.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3726" style="width: 1198px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3726" loading="lazy" class="size-full wp-image-3726" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.24.28.png" alt="Figure 4 – A scenario configuration with six autonomous mine vehicles starting in different points of the mine." width="1188" height="623"&gt;
 &lt;p id="caption-attachment-3726" class="wp-caption-text"&gt;Figure 4 – A scenario configuration with six autonomous mine vehicles starting in different points of the mine.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Step 2 – Submit multi-container jobs from RoSi&lt;/h3&gt; 
&lt;p&gt;RoSi lets you to run multiple real-time experiments in parallel, which greatly reduces the time needed to test a large number of scenarios, significantly improving efficiency. Engineers can select the configurations required for the batch of experiments from the simulation setup screen (Figure 5).&lt;/p&gt; 
&lt;p&gt;For convenience, a set of configurations can be stored as a preset that can be reused. After pressing the &lt;strong&gt;Start&lt;/strong&gt; button, RoSi Simulations Manager will perform a cartesian product of selected configs, configure a batch of experiments, and submit them to the AWS Batch job queue. By increasing the number of workers, you can speed up the experiment execution rate – by running them in parallel. Once the jobs are submitted, you can also view them in the Batch console (Figure 6).&lt;/p&gt; 
&lt;div id="attachment_3727" style="width: 1193px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3727" loading="lazy" class="size-full wp-image-3727" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.24.56.png" alt="Figure 5 – The simulation setup screen, where users can select configurations and start a batch of experiments." width="1183" height="621"&gt;
 &lt;p id="caption-attachment-3727" class="wp-caption-text"&gt;Figure 5 – The simulation setup screen, where users can select configurations and start a batch of experiments.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3728" style="width: 1249px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3728" loading="lazy" class="size-full wp-image-3728" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.25.25.png" alt="Figure 6 - AWS Batch console while a multi-container job is running." width="1239" height="654"&gt;
 &lt;p id="caption-attachment-3728" class="wp-caption-text"&gt;Figure 6 – AWS Batch console while a multi-container job is running.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Step 3 – Viewing the simulation data&lt;/h3&gt; 
&lt;p&gt;Viewing real-time simulation data from the RoSi experiment manager (Figure 7) helps you gain insights before the experiments end. In the mining example, Boliden can evaluate the productivity of each experiment to assess the impact of changing mining variables. Data is streamed, so when the multi-container job is submitted to Batch, you can monitor the job status, and view the logs from each container. All the data is stored in log files saved in an Amazon Simple Storage Service (Amazon S3) bucket, which you can use for post-processing, analysis, visualization, and debugging.&lt;/p&gt; 
&lt;div id="attachment_3729" style="width: 1193px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3729" loading="lazy" class="size-full wp-image-3729" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.25.53.png" alt="Figure 7 - RoSi dashboard provides status of each experiment in real-time" width="1183" height="670"&gt;
 &lt;p id="caption-attachment-3729" class="wp-caption-text"&gt;Figure 7 – RoSi dashboard provides status of each experiment in real-time&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Step 4 – Viewing and analyzing the results&lt;/h3&gt; 
&lt;p&gt;RoSi enables viewing real-time data to automate and optimize multiple aspects of mining operations.&lt;/p&gt; 
&lt;p&gt;Boliden can track metrics they need in RoSi, and post-process them on AWS. This enables them to effectively manage the mining fleet and reduce the time vehicles wait between operations. The result is a cost-effective use of fuel and optimization of working shifts. Figure 8 depicts wait times (in minutes) of six vehicles running across multiple simulation experiments.&lt;/p&gt; 
&lt;div id="attachment_3730" style="width: 1214px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3730" loading="lazy" class="size-full wp-image-3730" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.26.18.png" alt="Figure 8 - Waiting times distribution across the mining tunnel with six turnouts." width="1204" height="369"&gt;
 &lt;p id="caption-attachment-3730" class="wp-caption-text"&gt;Figure 8 – Waiting times distribution across the mining tunnel with six turnouts.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In this sample data, the vehicles spend the most time waiting at turnout ID 1. In a 24-hour simulation, vehicle six spent the most time waiting at turnout ID 1, waiting approximately 55 minutes of total time.&lt;/p&gt; 
&lt;p&gt;We can compare this data with other scenarios – with different locations and numbers of turnouts – to find the optimal turnout configuration within the mine, &lt;em&gt;before creating the mine in the real world&lt;/em&gt;, thus ensuring smooth operation and minimizing costs.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we’ve explored running multi-container simulations on AWS to enhance autonomous mining operations, using the advanced capabilities of Robotec.ai’s RoSi simulator, in tandem with AWS Batch.&lt;/p&gt; 
&lt;p&gt;By partitioning complex systems into containers, such as individual mining trucks, we can analyze their interactions to increase efficiency. The integration of RoSi with Batch means simulation engineers and DevOps teams can orchestrate large-scale simulations, scaling of their compute resources more easily, and getting fast results that help them optimize their operations.&lt;/p&gt; 
&lt;p&gt;Most of all, this approach paves the way for more streamlined underground mining operations by reducing waiting times and operational costs in &lt;em&gt;the real world&lt;/em&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>How vertical scaling and GPUs can accelerate mixed media modelling for marketing analytics</title>
		<link>https://aws.amazon.com/blogs/hpc/how-vertical-scaling-and-gpus-can-accelerate-mixed-media-modelling-for-marketing-analytics/</link>
		
		<dc:creator><![CDATA[Niro Amerasinghe]]></dc:creator>
		<pubDate>Wed, 26 Jun 2024 13:58:36 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">d3547a4030d4f5ce527c3b726e59fa0d2c80abc1</guid>

					<description>In marketing analytics, mixed media modeling (MMM) is a machine learning technique that combines information from various sources, like TV ads, online ads and social media to measure the impact of marketing and advertising campaigns. By using these techniques, businesses can make smarter decisions about where to invest their money for advertising, helping them get […]</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3812" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/AdobeStock_814815853.png" alt="How vertical scaling and GPUs can accelerate mixed media modelling for marketing analytics " width="380" height="212"&gt;In marketing analytics, &lt;em&gt;mixed media modeling&lt;/em&gt; (MMM) is a machine learning technique that combines information from various sources, like TV ads, online ads and social media to measure the impact of marketing and advertising campaigns. By using these techniques, businesses can make smarter decisions about where to invest their money for advertising, helping them get the best return on investment. It’s a bit like having a treasure map that guides you to the most valuable marketing strategies.&lt;/p&gt; 
&lt;p&gt;A key challenge for marketing analytics teams is the compute-heavy requirement for running these models. With popular libraries like &lt;a href="https://github.com/google/lightweight_mmm"&gt;LightweightMMM&lt;/a&gt;, &lt;a href="https://facebookexperimental.github.io/Robyn/"&gt;Robyn&lt;/a&gt;, and &lt;a href="https://www.pymc-marketing.io/en/stable/"&gt;PyMC Marketing&lt;/a&gt; not being designed to scale horizontally, jobs with a granular breakdown of demographic and geographic regions will take more than 24 hours to complete on workstations with limited compute, leading to delayed insight and hence delayed marketing optimization.&lt;/p&gt; 
&lt;p&gt;In this blog post, I’ll demonstrate how to accelerate your mixed media modeling (MMM) jobs using AWS Batch. Using the &lt;a href="https://github.com/google/lightweight_mmm"&gt;LightweightMMM&lt;/a&gt; open-source library as an example, I’ll use AWS Batch to help with two key challenges: 1) give teams access to larger GPU and CPU compute resources; and 2) efficiently provision and manage infrastructure while reducing the risk of cost overruns. Using larger compute instances overcomes the inability of these MMM libraries to scale horizontally and results in a big reduction in model training time. At the end of the post, I’ll also go through a time and cost analysis to show you the benefits of using more expensive vertically-scaled instances for these types of workloads.&lt;/p&gt; 
&lt;h2&gt;Overview of our sample application&lt;/h2&gt; 
&lt;p&gt;First let’s take a look at a sample application (Figure 1) that is also published in &lt;a href="https://github.com/aws-samples/mixed-media-model-portal"&gt;AWS Samples repo here&lt;/a&gt; (see the link for the deployment method).&lt;/p&gt; 
&lt;p&gt;You can quickly submit training jobs for MMMs through this application while AWS Batch takes care of the provisioning and, importantly, the removal and cleanup of compute resource. You submit jobs using the &lt;strong&gt;Train new model&lt;/strong&gt; button. The &lt;strong&gt;Details&lt;/strong&gt; button shown in the completed jobs table allows the user to visualize the results and run further inference to optimize a proposed marketing budget.&lt;/p&gt; 
&lt;div id="attachment_3805" style="width: 950px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3805" loading="lazy" class="size-full wp-image-3805" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-08.53.08.png" alt="Figure 1 – The web frontend for our sample application. Data scientists can submit training jobs for mixed media models leveraging high end compute and obtain results for further analysis after completion." width="940" height="601"&gt;
 &lt;p id="caption-attachment-3805" class="wp-caption-text"&gt;Figure 1 – The web frontend for our sample application. Data scientists can submit training jobs for mixed media models leveraging high end compute and obtain results for further analysis after completion.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Architecture: AWS Batch integrated with web frontend&lt;/h2&gt; 
&lt;p&gt;The architectural diagram for the sample application is shown in Figure 2. A &lt;a href="#_Web_front-end"&gt;web application&lt;/a&gt; provides users with a simple user interface (1). When you submit a job, the web application calls an API in the processing layer with the model-training parameters. The processing layer uses AWS Batch to run a training job (2). AWS Batch will automatically provision the required compute and execute the code for the training job. The training job accesses the specified tables in the &lt;a href="#_Data_lake_for"&gt;data lake&lt;/a&gt; to acquire its training data (3). On completion, the trained model is saved back to the data lake (4). Once training is complete, AWS Batch will make sure the compute resources are shutdown so you don’t pay for compute that is not being used. Finally, the processing layer provides an API for inference requests back to the web application by loading the saved model from Amazon Simple Storage Service (Amazon S3) on demand (5).&lt;/p&gt; 
&lt;div id="attachment_3806" style="width: 952px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3806" loading="lazy" class="size-full wp-image-3806" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-08.53.40.png" alt="Figure 2 - Architectural diagram of a web application to train Mixed Media Models. When a training request is submitted from the web frontend, the job is submitted to AWS Batch via Amazon API Gateway/AWS Lambda. Amazon Athena provides the training data directly to the training job. All the components are serverless with no requirement for pre provisioned unmanaged infrastructure." width="942" height="403"&gt;
 &lt;p id="caption-attachment-3806" class="wp-caption-text"&gt;Figure 2 – Architectural diagram of a web application to train Mixed Media Models. When a training request is submitted from the web frontend, the job is submitted to AWS Batch via Amazon API Gateway/AWS Lambda. Amazon Athena provides the training data directly to the training job. All the components are serverless with no requirement for pre provisioned unmanaged infrastructure.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Deep dive on components in the architecture&lt;/h2&gt; 
&lt;h3&gt;Web front-end&lt;/h3&gt; 
&lt;p&gt;We used Amazon CloudFront and Amazon S3 to provide a simple, cost-effective way to serve a &lt;a href="https://react.dev/"&gt;React&lt;/a&gt; based front end, with Amazon Cognito providing authentication and authorization to control access. The combination of Amazon API Gateway, AWS Lambda, and Amazon DynamoDB provide a job management function exposed as a REST API that can be called by the web application.&lt;/p&gt; 
&lt;h3&gt;Data lake for storage&lt;/h3&gt; 
&lt;p&gt;By using Amazon Athena and Amazon S3, a data lake allows team members to prepare variations of the model input data using standard SQL queries. We also have a Lambda function that contains code to generate sample data that is a feature of the &lt;a href="https://github.com/google/lightweight_mmm"&gt;LightweightMMM&lt;/a&gt; framework.&lt;/p&gt; 
&lt;h3&gt;Processing layer&lt;/h3&gt; 
&lt;p&gt;The processing layer provides the compute required to both train the model’s and run inference requests. By using Batch, we create a number of different job types, queues, and compute environments to allow training the model on small / medium and large GPU/CPU instances. Batch will automatically provision the required compute using the Amazon Elastic Compute Cloud (Amazon EC2) service with the specified &lt;a href="https://aws.amazon.com/ec2/instance-types/"&gt;instance types&lt;/a&gt; per job. This allows us to expose a simple job submission system via the web front end while using Batch to manage the complexity of the underlying compute infrastructure. The trained jobs are saved into the S3 bucket in the data lake, and we use Lambda to expose inference requests on these trained models.&lt;/p&gt; 
&lt;h2&gt;The LightweightMMM framework&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://github.com/google/lightweight_mmm"&gt;LightweightMMM&lt;/a&gt; library is written in Python and uses open-source frameworks Jax and NumPyro to create a Bayesian approach to MMM using the &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; method.&lt;/p&gt; 
&lt;p&gt;There are some key input parameters for training the model that can influence the run time.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Historical Data and Granularit&lt;strong&gt;y&lt;/strong&gt; – the length and granularity of historical data used can impact the time it takes a model to complete. For this blog post we will use 3 years’ worth of data with a weekly granularity leading to 160 data points per feature.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ul&gt; 
 &lt;li&gt;Number of Geographies – the number of Geographies multiplies the complexity of the calculation as each data point above must be repeated for each geography as well as the number of Marketing channels. For this blog post we will use different geography numbers to show the impact on run time.&lt;/li&gt; 
 &lt;li&gt;Marketing Channels – each data point multiplied by the number of Geographies is further multiplied for each marketing channel included in the analysis. For this blog post we will compare 3 and 6 channels.&lt;/li&gt; 
 &lt;li&gt;Chains – each additional chain requires running the MCMC algorithm independently. This means that you need to perform the sampling and calculations for each chain separately. For this blog post we will use a varying number of Chains to demonstrate the impact on run time.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to maximize GPU memory&lt;/h3&gt; 
&lt;p&gt;The current implementation of LightweightMMM runs each &lt;em&gt;Chain&lt;/em&gt; on a dedicated GPU, the default settings will pre-allocate GPU memory for each chain and we found that this can lead to &lt;code&gt;Out of Memory&lt;/code&gt; errors when running large MMM models where each chain has a large number of data points (calculated as &lt;code&gt;historical data x geographies x channels&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-python"&gt;os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = ".50"
os.environ["XLA_PYTHON_CLIENT_ALLOCATOR"] = "platform"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This configuration inside you python application code can help maximize the GPUs for larger model training jobs. You can refer to &lt;a href="https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html"&gt;Jax Documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;CUDA and cuDNN on AWS Batch&lt;/h3&gt; 
&lt;p&gt;For JAX to work with an NVIDIA GPU we needed to ensure:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The host machine’s CUDA drivers are on the same major version as the container image, and the minor version must be the same or newer.&lt;/li&gt; 
 &lt;li&gt;The newer NVIDIA GPUs (e.g. H100) perform much better when using CUDA 12.&lt;/li&gt; 
 &lt;li&gt;Jax has a prebuilt &lt;code&gt;jaxlib&lt;/code&gt; python wheel to support CUDA 12.2 with cuDNN 8.9 and so it is best to align a container image that contains this.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The simplest solution is to use a pre-built NVIDIA container image with the required libraries, and compiler tools. You can find these in &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda"&gt;the NVIDIA container catalog&lt;/a&gt;. We recommend using the developer releases with cuDNN included.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;FROM nvcr.io/NVIDIA/cuda:12.2.2-cudnn8-devel-ubuntu22.04&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;You can find a full example of the Docker file we used in our sample application in our &lt;a href="http://github/"&gt;AWS samples repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Time and cost analysis&lt;/h2&gt; 
&lt;p&gt;To understand the impact of scaling compute when training the models, we ran an experiment with LightweightMMM on a number of different compute configurations and model complexities. The results below have been compiled based on average run times and approximate costs (using On Demand pricing for a consistent baseline) for Amazon EC2 C6i and C7i instance types for CPU, and G5 and P5 instance types for GPUs. We used the &lt;code&gt;us-east-1&lt;/code&gt; Region.&lt;/p&gt; 
&lt;p&gt;For our experiment, we compared the results against a base configuration of 16-cores which is reflective of the performance of a typical desktop workstation.&lt;/p&gt; 
&lt;div id="attachment_3807" style="width: 956px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3807" loading="lazy" class="size-full wp-image-3807" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-08.56.37.png" alt="Figure 3 – Shows the average runtime of training a smaller model with 3 marketing channels, 100 geographies with 2 chains using different levels of compute" width="946" height="537"&gt;
 &lt;p id="caption-attachment-3807" class="wp-caption-text"&gt;Figure 3 – Shows the average runtime of training a smaller model with 3 marketing channels, 100 geographies with 2 chains using different levels of compute&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For smaller models, increasing the level of processing power using a greater number of CPU cores did not have a significant impact on training time. We think this is due to the overheads of splitting the processing across a larger number of threads. We found that on the extreme end, a 192-core instance with high-parallelism can actually have a &lt;em&gt;negative impact&lt;/em&gt; on the training time.&lt;/p&gt; 
&lt;p&gt;Using GPUs had a significant positive impact on the training job: a G5 instance with 4 x NVIDIA A10 GPUs ran 4x faster compared to a typical cloud desktop configuration despite being about the same cost to train the model.&lt;/p&gt; 
&lt;div id="attachment_3808" style="width: 959px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3808" loading="lazy" class="size-full wp-image-3808" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-08.57.03.png" alt="Figure 4 – Shows the average runtime of training a larger model with 6 Marketing channels, 300 Geographies using different levels of compute" width="949" height="545"&gt;
 &lt;p id="caption-attachment-3808" class="wp-caption-text"&gt;Figure 4 – Shows the average runtime of training a larger model with 6 Marketing channels, 300 Geographies using different levels of compute&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For larger complex models, a 128-core C6i CPU-based instance provided more than a 2x time reduction before higher parallelism started to become detrimental. We found that G5 instances with A10 GPUs didn’t have enough memory to run the larger training configuration. However, the P5 instances with NVIDIA H100 GPUs ran &lt;em&gt;58x faster&lt;/em&gt;, with training completing in approximately 45 minutes. For an approximate cost increase of $50 the run time dropped from 1.5 days to 45 mins. This is a great trade-off from if you’re short on time.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post I covered how you can use AWS Batch to run mixed media models (MMM) faster with Amazon EC2 &lt;a href="https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing"&gt;accelerated compute&lt;/a&gt; instances. I explained how to configure NVIDIA CUDA libraries to enable frameworks like Jax to work on Batch. And I walked you through a time and cost analysis, showing that while larger CPU and GPU instances do come with a higher per-hour price tag than the cheapest options, the time savings can enable data scientists to iterate faster – and build better MMM models – more efficiently.&lt;/p&gt; 
&lt;p&gt;To get started running mixed media models on AWS Batch, have a look at our &lt;a href="https://github.com/aws-samples/mixed-media-model-portal"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Harnessing the scale of AWS for financial simulations</title>
		<link>https://aws.amazon.com/blogs/hpc/harnessing-the-scale-of-aws-for-financial-simulations/</link>
		
		<dc:creator><![CDATA[Yusong Wang]]></dc:creator>
		<pubDate>Tue, 25 Jun 2024 14:57:18 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[Financial Services]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Industries]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[FSI]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">80cee173cb118b7e6b59dd5783c9980d9be60f84</guid>

					<description>Struggling with long compute times for numerical simulations in finance? See how AWS makes it simple to leverage the cloud for large-scale financial modeling. We walk through a real example using QuantLib and Monte Carlo methods.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3768" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/05/boofla88_a_room_of_financial_traders._Its_chaotic_and_busy_with_802574df-b747-498d-9a1e-d94cae8b61a0-copy.png" alt="Harnessing the scale of AWS for financial simulations" width="380" height="212"&gt;In the dynamic world of finance, numerical simulations have become indispensable tools for tackling complex problems where analytical solutions are elusive. However, these simulations can be incredibly time-consuming and computationally intensive, especially when dealing with large-scale tasks.&lt;/p&gt; 
&lt;p&gt;This is where the benefits of cloud computing come into play. Clouds like AWS offer a plethora of advantages that have made us increasingly popular in the financial industry. Flexible pricing models, massive compute environments, and versatile instance choices allow financial institutions to scale their simulations seamlessly and make use of multiple AWS Regions and Availability Zones for both scale and resiliency.&lt;/p&gt; 
&lt;p&gt;In this blog post, we’ll show how you can use AWS to execute large-scale financial simulations more easily. We’ll focus on a specific example: calculating option prices using Monte Carlo simulations with the &lt;a href="https://www.quantlib.org/"&gt;QuantLib&lt;/a&gt; open-source library. QuantLib is a powerful tool for modelling, trading, and risk management in the financial industry. Finally, there’s a link to a workshop you can follow – step-by-step – to deploy this in your own AWS account.&lt;/p&gt; 
&lt;h2&gt;Optimizing financial simulations with AWS Batch and AWS Lambda&lt;/h2&gt; 
&lt;p&gt;When it comes to running large-scale financial simulations, the choice of compute service can have a significant impact on the overall efficiency and cost-effectiveness of the workflow. In this regard, &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; and &lt;a href="https://aws.amazon.com/pm/lambda"&gt;AWS Lambda&lt;/a&gt; offer complementary solutions that can be tailored to different workload requirements.&lt;/p&gt; 
&lt;p&gt;AWS Batch, a job scheduler and resource orchestrator, is well-suited for running batch-oriented workflows at scale. By automatically provisioning the appropriate compute resources and managing the execution of jobs, Batch enables financial institutions to run their simulations in a cost-effective and efficient manner, particularly for workloads that require longer durations or need to handle high concurrency.&lt;/p&gt; 
&lt;p&gt;On the other hand, for smaller jobs that require a fast turnaround, typically within a couple of minutes, AWS Lambda can be the preferred choice. Lambda’s serverless architecture and rapid response times make it an ideal solution for scenarios where the low latency of less than a second is a critical requirement, like in real-time risk analysis or portfolio optimization.&lt;/p&gt; 
&lt;p&gt;The selection of the appropriate compute service depends on the specific workload requirements. For fast response time (&amp;lt;1s) or high throughput (over 500 transactions per second), Lambda is the choice we’d recommend. However, for workloads with durations longer than 5 minutes, or the need to scale quickly to over 20,000 concurrent jobs, Batch is a more suitable option. Additionally, for managing multiple workloads with a scheduler, Batch is the recommended service.&lt;/p&gt; 
&lt;h2&gt;Solution architecture&lt;/h2&gt; 
&lt;p&gt;Figure 1 illustrates a solution architecture for running cloud-native financial simulations on AWS in multiple regions. The simulation workflow to calculate &lt;em&gt;American Option&lt;/em&gt; prices using QuantLib is described like this:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Upload an input CSV file with financial asset information to the &lt;em&gt;input&lt;/em&gt; Amazon Simple Storage Service (Amazon S3) buckets for AWS Batch or AWS Lambda to compute.&lt;/li&gt; 
 &lt;li&gt;Apply &lt;a href="https://aws.amazon.com/pm/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt; to monitor the designated Amazon S3 buckets.&lt;/li&gt; 
 &lt;li&gt;Once the input data has been fed into the Amazon S3 input bucket, an AWS Batch job is triggered through EventBridge, or a Lambda job is invoked through event-driven process of the S3 bucket, depending on the location of the input file.&lt;/li&gt; 
 &lt;li&gt;The job splits the input file to multiple files and put on S3 to run in parallel. For Lambda, we use the same event-driven process to run the job. For Batch, we run jobs in parallel using Array Jobs, for efficiency. The input file is processed directly if the number of assets is under a threshold (configurable through an environment variable).&lt;/li&gt; 
 &lt;li&gt;The result files are put on the &lt;em&gt;result&lt;/em&gt; S3 bucket with the same path as the input file. Users get the result back by copying from the result S3 bucket.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_3765" style="width: 934px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3765" loading="lazy" class="size-full wp-image-3765" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/05/IMG-2024-06-05-09.13.33.png" alt="Figure 1: Reference solution architecture for running cloud-native financial simulations" width="924" height="641"&gt;
 &lt;p id="caption-attachment-3765" class="wp-caption-text"&gt;Figure 1: Reference solution architecture for running cloud-native financial simulations&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The common workflow in financial simulations is to keep experimenting with – and optimizing – a compute algorithm. Adopting continuous integration and continuous deployment (CI/CD) is a natural fit. Figure 2 illustrates the high-level workflow for CI/CD development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/codepipeline/"&gt;AWS CodePipeline&lt;/a&gt; is a continuous delivery service that allows you to model, visualize, and automate the steps required to release application software. With it, we model the full release process for building the code, deploying to pre-production environments, testing the application and releasing it to production every time there is a code change.&lt;/p&gt; 
&lt;div id="attachment_3766" style="width: 899px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3766" loading="lazy" class="size-full wp-image-3766" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/05/IMG-2024-06-05-09.14.06.png" alt="Figure 2: High-level workflow for the continuous integration and continuous deployment. When a developer pushes code changes to an AWS CodeCommit repository, the AWS CodePipeline will automatically start to build the container image with AWS CodeBuild and push the image to the Amazon ECR. The same image is used by both AWS Lambda and AWS Batch at runtime. " width="889" height="350"&gt;
 &lt;p id="caption-attachment-3766" class="wp-caption-text"&gt;Figure 2: High-level workflow for the continuous integration and continuous deployment. When a developer pushes code changes to an AWS CodeCommit repository, the AWS CodePipeline will automatically start to build the container image with AWS CodeBuild and push the image to the Amazon Elastic Container Registry (ECR). The same image is used by both AWS Lambda and AWS Batch at runtime.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;You can do this&lt;/h2&gt; 
&lt;p&gt;We’ve &lt;a href="https://catalog.workshops.aws/batch-lambda-fsi"&gt;published a workshop&lt;/a&gt; that offers a quick and effective implementation of this, which will take you about 20 minutes to set up.&lt;/p&gt; 
&lt;p&gt;You’ll have the opportunity to see for yourself the impressive performance of AWS Lambda to process &lt;strong&gt;hundreds of equities in about a minute&lt;/strong&gt;, and AWS Batch processing &lt;strong&gt;tens of thousands of equities in less than ten minutes&lt;/strong&gt; by chomping through 10 to 100 equities for each individual job.&lt;/p&gt; 
&lt;h2&gt;Unlock the future of financial simulations with AWS&lt;/h2&gt; 
&lt;p&gt;The cloud-native Monte Carlo simulation for &lt;em&gt;American Option&lt;/em&gt; pricing using QuantLib we’ve outlined here is just the beginning. By using AWS, financial institutions can unlock new levels of efficiency, scalability, and innovation in their simulation workflows.&lt;/p&gt; 
&lt;p&gt;Ready to get started? Check out &lt;a href="https://catalog.workshops.aws/batch-lambda-fsi"&gt;the published workshop&lt;/a&gt; to dive deeper into the step-by-step implementation details and see how you can apply these cloud-native techniques to your own financial simulation.&lt;/p&gt; 
&lt;p&gt;This hands-on experience empowers financial professionals to harness the full potential of cloud technologies, revolutionizing the approaches to complex financial simulations.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>A library of HPC Applications Best Practices on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/a-library-of-hpc-applications-best-practices-on-aws/</link>
		
		<dc:creator><![CDATA[Nicola Venuti]]></dc:creator>
		<pubDate>Mon, 24 Jun 2024 12:35:25 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[FEA]]></category>
		<category><![CDATA[Finite Element Analysis]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">8da82d09564c8acf86902c1e2a56895df6c02376</guid>

					<description>Want insights on running HPC codes efficiently on AWS? Our HPC specialists compiled their know-how into a new public GitHub repo. Get best practices, templates, scripts and more to optimize your workloads.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3836" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/HPC-Applications-Best-Practices-on-AWS-1.png" alt="" width="380" height="213"&gt;Being an HPC specialist in AWS comes with some critical responsibilities. Key among them is to help customers run their applications as fast as possible, but also in the most cost efficient way, too. We aim to help customers find the most appropriate services for every workload with optimal drivers, settings, and options to get great outcomes.&lt;/p&gt; 
&lt;p&gt;But the number of HPC related services – and their capabilities sometimes grows fast enough that it’s not easy for you to ensure you’re using AWS effectively.&lt;/p&gt; 
&lt;p&gt;So today we’re announcing a resource containing the best practices from our HPC Specialist Solution Architect (SSA) community to help you get the most from running your workloads on AWS. We’re hosting these in a &lt;a href="https://github.com/aws-samples/hpc-applications"&gt;GitHub repository&lt;/a&gt; that is publicly available starting today. In addition to Application Best Practices, this repo also contains CloudFormation Templates to create clusters, and launch scripts (with some benchmark results) for selected applications.&lt;/p&gt; 
&lt;p&gt;We expect to be regularly updating and expanding the list of HPC applications included in the repo, based on your feedback, and participation form our teams. You can propose new applications that would benefit from being included using &lt;a href="https://github.com/aws-samples/hpc-applications/issues"&gt;GitHub Issues&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;Today AWS has more than &lt;a href="https://aws.amazon.com/ec2/"&gt;750 different instance types&lt;/a&gt;, but only some of these are helpful for HPC applications. Even if you just include the &lt;a href="https://www.youtube.com/watch?v=i2T7Hi2Yjoc"&gt;HPC-specific instances&lt;/a&gt;, and the &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types"&gt;instance types that support EFA&lt;/a&gt; (the Elastic Fabric Adapter), there’s more than a hundred options.&lt;/p&gt; 
&lt;p&gt;Customers often benchmark their HPC applications on AWS to understand which of these instance types is best for their code, and to ensure that the way the application is installed and run is aligned with their business needs (e.g., runs as fast as possible, lowest price, and so forth).&lt;/p&gt; 
&lt;p&gt;Sometimes customers wonder whether they are running a specific application ‘properly’ – are they achieving the best performance?&lt;/p&gt; 
&lt;p&gt;Sometimes it’s more than idle interest – customers often request benchmarks as part of formal procurement exercises, or before they start a &lt;em&gt;Proof of Concept&lt;/em&gt; (PoC) – with help from our teams.&lt;/p&gt; 
&lt;p&gt;Running HPC application benchmarks properly – and at scale – is a complex task. It requires preparation, experience, and strong domain knowledge. It’s more complex if you have to leave your comfort-zone and run applications in a new, possibly &lt;em&gt;unknown&lt;/em&gt; environment, too, because this might call for a deep understanding of how the new infrastructure works.&lt;/p&gt; 
&lt;p&gt;This is why we’ve created the resource we’re launching today. It’s maintained by AWS HPC Specialists Solution Architects, who will take care of updating and improving it as our services evolve, new application versions are released, or more optimal ways are found for running applications.&lt;/p&gt; 
&lt;p&gt;We’re starting with the most common applications for the computer-aided engineering (CAE) community. These are the codes most requested by our customers.&lt;/p&gt; 
&lt;h2&gt;Our goals for HPC Applications Best Practices on AWS&lt;/h2&gt; 
&lt;p&gt;Here’s what we’re aiming to accomplish with this initiative:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure our customers can achieve the best price/performance using our infrastructure and services for their HPC Applications&lt;/li&gt; 
 &lt;li&gt;Ensure you have references (like time to completion) and datapoints (benchmark metrics) for running these applications using public datasets.&lt;/li&gt; 
 &lt;li&gt;Share general guidance, settings, and tips that can be applicable to other applications, too.&lt;/li&gt; 
 &lt;li&gt;Lowering the level of cloud expertise needed to run these workloads in the cloud.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;While the repo isn’t a supported product or service, we’re aiming to give you access to our best thinking – and our experience – on the topics it covers.&lt;/p&gt; 
&lt;h2&gt;Let’s tour the GitHub repo&lt;/h2&gt; 
&lt;p&gt;At launch, we’ve structured the repo as follows:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;/apps&lt;/code&gt; contains a folder for the best practices for each of the included applications. Within this folder you’ll find:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;an &lt;strong&gt;example launch script&lt;/strong&gt;, in some cases multiple ones to cover different architectures (x86 vs GPUs vs Graviton), or different application versions (in case they require different settings in the launch script). The example launch scripts are working examples, that can be executed with minor changes, but we also know every end-user has their own peculiarities and might want to run a given application in a custom way. In this case we’d recommend starting from the working example launch script provided and adapt it to your specific needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;short documentation&lt;/strong&gt; about the best practice (README.md file + a few assets). It typically comprises of an introduction, tips and tricks, a deep dive into the architectural choices and the most important application and environment settings aiming at tuning the performance, and finally the benchmark results shared with one or more charts.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;/docs&lt;/code&gt; contains documentation, images, and charts. This doesn’t replace our official application, but complements them with explanations about architectural choices and details about specific applications and environment settings we’ve used.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;/ParallelCluster&lt;/code&gt; contains simple example config files for building an HPC cluster &lt;a href="https://aws.amazon.com/it/hpc/parallelcluster/"&gt;using AWS ParallelCluster&lt;/a&gt;. As we release new services or features for managing HPC resources, we’ll also update this section, accordingly. We’ve included some automated &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/cloudformation-v3.html"&gt;CloudFormation-based&lt;/a&gt; procedures to deploy a basic cluster (using ParallelCluster) in selected AWS Regions. This structure will change over time as our capabilities grow, and as we gather new material.&lt;/p&gt; 
&lt;p&gt;Each application included in the repository will be supported by slightly different sets of assets (for example, launch scripts, documentation, performance charts). But there’s a minimum set you’ll see for every included application.&lt;/p&gt; 
&lt;p&gt;Beginning with the &lt;code&gt;/apps&lt;/code&gt; folder, you’ll find the list of Best Practices available in the repo.&lt;/p&gt; 
&lt;div id="attachment_3827" style="width: 953px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3827" loading="lazy" class="size-full wp-image-3827" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-15.52.33.png" alt="Figure 1. The list of available best practices you can find in the GitHub repository under /apps/" width="943" height="559"&gt;
 &lt;p id="caption-attachment-3827" class="wp-caption-text"&gt;Figure 1. The list of available best practices you can find in the GitHub repository under /apps/&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Inside any application directory, you’ll find one or more launch scripts that you can use as-is – or customize based on your needs. In some cases, the launch scripts are in sub-directories, broken down by CPU or GPU architectures.&lt;/p&gt; 
&lt;div id="attachment_3828" style="width: 929px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3828" loading="lazy" class="size-full wp-image-3828" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-15.53.00.png" alt="Figure 2. Shows a typical example of the assets you can expect to find for an application." width="919" height="849"&gt;
 &lt;p id="caption-attachment-3828" class="wp-caption-text"&gt;Figure 2. Shows a typical example of the assets you can expect to find for an application.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Simple documentation, typically in a Readme.md file (or additional documents linked to it).&lt;/p&gt; 
&lt;div id="attachment_3829" style="width: 937px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3829" loading="lazy" class="size-full wp-image-3829" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-15.53.43.png" alt="Figure 3. An example of the documentation available." width="927" height="788"&gt;
 &lt;p id="caption-attachment-3829" class="wp-caption-text"&gt;Figure 3. An example of the documentation available.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The documentation is not meant to be exhaustive – it focuses on what is relevant for running the application in the most optimal way on AWS. For general purpose application documentation (like end-user or admin guides), refer to the official guides that come with the app itself.&lt;/p&gt; 
&lt;p&gt;Typically, the documentation in the repo will include the list of versions and architectures we’ve tested, application installation tips, general tips, and key settings relevant for tuning the performance, and some performance-related information with charts and metrics for the most relevant Instance types.&lt;/p&gt; 
&lt;div id="attachment_3830" style="width: 680px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3830" loading="lazy" class="size-full wp-image-3830" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-15.54.43.png" alt="Figure 4. An example of the performance charts we will provide." width="670" height="845"&gt;
 &lt;p id="caption-attachment-3830" class="wp-caption-text"&gt;Figure 4. An example of the performance charts we will provide.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;How to use these best practices&lt;/h2&gt; 
&lt;p&gt;If you already have a cluster up and running, you can try these best practices by cloning this repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;git clone https://github.com/aws-samples/hpc-applications.git &lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In case you don’t have an existing HPC Cluster, or if you want to deploy a new one for the scope of these tests, then you can follow &lt;a href="https://github.com/aws-samples/hpc-applications/blob/main/ParallelCluster/README.md"&gt;the guide to launch a new ParallelCluster&lt;/a&gt;. We’ve included a few simple CloudFormation templates that help you to create a new test HPC cluster with just a few clicks. If you want to assemble a more complex environment for testing, using modular templates designed to work together, check out the HPC Recipes Library, which is also available &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/"&gt;as a GitHub repo&lt;/a&gt; (and is well explained in our &lt;a href="https://hpc.news/recipes"&gt;blog post&lt;/a&gt; when we announced it).&lt;/p&gt; 
&lt;p&gt;To deploy one of the one-click stacks, select your preferred AWS Region among the ones shown in the table, and click the appropriate launch button. You’ll be asked a few questions about networking and storage. If you don’t know how to answer these, just leave the default values: AUTO. The one-click deployment procedure will take care of creating everything needed for your HPC Cluster to run properly.&lt;/p&gt; 
&lt;div id="attachment_3831" style="width: 459px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3831" loading="lazy" class="size-full wp-image-3831" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-15.55.37.png" alt="Figure 5. the links to the 1-Click CloudFormation templates." width="449" height="409"&gt;
 &lt;p id="caption-attachment-3831" class="wp-caption-text"&gt;Figure 5. the links to the 1-Click CloudFormation templates.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;After the CloudFormation stack is deployed, you can go to the &lt;strong&gt;Output&lt;/strong&gt; tab in the CloudFormation console and click on the &lt;strong&gt;SystemManagerUrl&lt;/strong&gt; link. This will let you securely access the head node using AWS Systems Manager (SSM), without needing a password or certificate. You’ll find a clone of the GitHub HPC Applications Best Practices repo under &lt;code&gt;/fsx&lt;/code&gt; on the cluster.&lt;/p&gt; 
&lt;div id="attachment_3832" style="width: 922px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3832" loading="lazy" class="size-full wp-image-3832" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-15.56.42.png" alt="Figure 6. The Outputs tab in the CloudFormation console shows a link to securely connect to the cluster through AWS Systems Manager (SSM), without needing a password or certificate." width="912" height="305"&gt;
 &lt;p id="caption-attachment-3832" class="wp-caption-text"&gt;Figure 6. The Outputs tab in the CloudFormation console shows a link to securely connect to the cluster through AWS Systems Manager (SSM), without needing a password or certificate.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Fine tuning your HPC applications to run well on any cluster is a complex task. We aim to keep this repository up-to-date with future versions of common applications and AWS services to provide you with the best experience possible with the fewest steps.&lt;/p&gt; 
&lt;p&gt;We’d love to receive your feedback (using &lt;a href="https://github.com/aws-samples/hpc-applications/issues"&gt;GitHub Issues&lt;/a&gt;) to let us know if this is useful for you, or if you need something else to support your business.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Job queue snapshots: see what’s at the head of your queues in AWS Batch</title>
		<link>https://aws.amazon.com/blogs/hpc/job-queue-snapshots-see-whats-at-the-head-of-your-queues-in-aws-batch/</link>
		
		<dc:creator><![CDATA[Angel Pizarro]]></dc:creator>
		<pubDate>Thu, 20 Jun 2024 14:52:30 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[EC2 Spot]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">f22c191e048a0cdce1210fc4a7b44187ffd3d053</guid>

					<description>AWS Batch just grew a neat new feature: Job queue snapshots. This gives you the visibility you need for managing throughput in a dynamic environment - with competing priorities - and across multiple queues and workloads. Today we give you the inside scoop on how this feature works - especially when you’re using fair share scheduling.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="size-full wp-image-3893 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/20/boofla88_Rows_of_mice_wearing_lab_coats_lined_up_in_orderly_que_05a3c6ff-1f52-4929-b5a0-d81619ecacd2.png" alt="Job queue snapshots: see what’s at the head of your queues in AWS Batch" width="380" height="212"&gt;In 2021, AWS Batch introduced &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/job_scheduling.html"&gt;fair share job queues&lt;/a&gt; which allowed customers to create &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/scheduling-policies.html"&gt;scheduling policies&lt;/a&gt; for a job queue so you can control how to split resources and prioritize jobs &lt;em&gt;that belong to different workloads&lt;/em&gt; (differentiated by their jobs having different &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/scheduling-policy-parameters.html"&gt;share identifiers&lt;/a&gt;). Before this, all Batch job queues acted as independent first-in-first-out (FIFO) queues. That meant if you had multiple groups or workloads in the same AWS account, you needed separate job queues (JQs) and compute environments (CEs) for each business need.&lt;/p&gt; 
&lt;p&gt;This in turn meant you needed to manage the distribution of the underlying compute resource across all these CEs. By introducing Fair Share Scheduling (FSS), customers like Amazon Search could consolidate their environments, which reduced their operational overhead, drove up their fleet utilization – and greatly improved their throughput.&lt;/p&gt; 
&lt;p&gt;But moving away from FIFO queues introduced a different challenge — how to reliably tell what was going to run next across the set of possible jobs at the head of the queue across different share identifiers.&lt;/p&gt; 
&lt;p&gt;Today we’ll explain a recent addition to AWS Batch that we think will address this: Job queue snapshots. This is a new API &lt;a href="https://aws.amazon.com/about-aws/whats-new/2024/06/aws-batch-job-queue-snapshot-jobs-front-job-queues/"&gt;we launched a few weeks ago&lt;/a&gt; to query the jobs that are at the head of the job queue. We’ll work through the details and give you an practical example of how to use this new information.&lt;/p&gt; 
&lt;h2&gt;Inspecting the head of the queue&lt;/h2&gt; 
&lt;p&gt;Using the &lt;a href="https://console.aws.amazon.com/batch/home?#queues"&gt;AWS Batch management console&lt;/a&gt;, the AWS &lt;a href="https://docs.aws.amazon.com/batch/latest/APIReference/API_GetJobQueueSnapshot.html"&gt;SDK&lt;/a&gt;, or AWS &lt;a href="https://docs.aws.amazon.com/cli/latest/reference/batch/get-job-queue-snapshot.html"&gt;CLI&lt;/a&gt;, you can now list the first 100 &lt;code&gt;RUNNABLE&lt;/code&gt; jobs for a single job queue by calling the &lt;code&gt;&lt;a href="https://docs.aws.amazon.com/batch/latest/APIReference/API_GetJobQueueSnapshot.html"&gt;GetJobQueueSnapshot&lt;/a&gt;&lt;/code&gt; API. For FIFO job queues, jobs are ordered based on their submission time. For FSS job queues, jobs are ordered based on their share’s usage and, within a share, the job share priority. You can read more about how job priority and share usage effect job scheduling in our &lt;a href="https://aws.amazon.com/blogs/hpc/deep-dive-on-fair-share-scheduling-in-aws-batch/"&gt;fair share deep dive&lt;/a&gt; blog post.&lt;/p&gt; 
&lt;p&gt;Job queue snapshots are a great visibility tool for customers that need to make on-the-fly modifications to jobs in the queue. Let’s take a look at a practical example.&lt;/p&gt; 
&lt;p&gt;We’ve created a &lt;em&gt;fair share job queue&lt;/em&gt; that uses AWS Fargate for the compute environment. For the purposes of this experiment, we’ve temporarily disabled the compute environment so that jobs stay in the job queue and we can see the results of the queue manipulations. Finally, the fair share policy gives equal weight to the two active shares, “pink” and “blue”, meaning that you should see an interleaving of set of jobs with each share (Figure 1).&lt;/p&gt; 
&lt;div id="attachment_3881" style="width: 909px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3881" loading="lazy" class="size-full wp-image-3881" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/19/IMG-2024-06-19-09.35.15.png" alt="Figure 1:&amp;nbsp; The AWS Batch management console showing the Job queue snapshot tab. The tab lists an interleaving of pink and blue jobs that are at the head of the job queue." width="899" height="456"&gt;
 &lt;p id="caption-attachment-3881" class="wp-caption-text"&gt;Figure 1: The AWS Batch management console showing the Job queue snapshot tab. The tab lists an interleaving of pink and blue jobs that are at the head of the job queue.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Bob from team &lt;em&gt;pink&lt;/em&gt; comes in with an urgent request to run high-priority jobs as soon as possible to meet an important deadline. You submit these jobs with a priority=10, and this results in the high priority jobs moving ahead of all other &lt;em&gt;pink&lt;/em&gt; jobs, but there are still some &lt;em&gt;blue&lt;/em&gt; jobs ahead of one or more high priority &lt;em&gt;pink&lt;/em&gt; jobs (Figure 2). That’s because job priority is only applicable &lt;em&gt;within each share&lt;/em&gt;, and doesn’t affect the overall placement of jobs &lt;em&gt;across shares&lt;/em&gt;. At this point you can determine whether the high-priority pink jobs can finish by the deadline without affecting team &lt;em&gt;blue’s&lt;/em&gt; workload.&lt;/p&gt; 
&lt;div id="attachment_3882" style="width: 899px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3882" loading="lazy" class="size-full wp-image-3882" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/19/IMG-2024-06-19-09.35.50.png" alt="Figure 2: The AWS Batch management console showing the job queue snapshot tab. The tab shows that high-priority pink jobs have moved ahead of lower priority pink jobs, but are still interleaved with blue jobs." width="889" height="454"&gt;
 &lt;p id="caption-attachment-3882" class="wp-caption-text"&gt;Figure 2: The AWS Batch management console showing the job queue snapshot tab. The tab shows that high-priority pink jobs have moved ahead of lower priority pink jobs, but are still interleaved with blue jobs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;If you think that the high priority jobs will still not finish by the deadline, then you have two options:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;You can temporarily adjust the share policy to prefer &lt;em&gt;pink&lt;/em&gt; jobs over &lt;em&gt;blue&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;You can cancel team &lt;em&gt;blue&lt;/em&gt;’s jobs and then resubmit them once the high priority jobs are &lt;code&gt;RUNNING&lt;/code&gt;. Note that, in this case, &lt;em&gt;blue&lt;/em&gt; jobs will keep their place in the queue, even though Batch has marked them for cancellation. When they reach the head of the queue, their state will immediately switch to &lt;code&gt;FAILED&lt;/code&gt; and won’t take up any compute resources.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Since option 1 is less destructive, you change the scheduling policy to preference placement of team &lt;em&gt;pink&lt;/em&gt; jobs by adjusting the weight factor to a lower value (lower weight factor &lt;a href="https://docs.aws.amazon.com/batch/latest/APIReference/API_ShareAttributes.html#Batch-Type-ShareAttributes-weightFactor"&gt;means&lt;/a&gt; a share gets more compute resources over time). This places &lt;em&gt;most&lt;/em&gt; of the high priority pink jobs ahead of any &lt;em&gt;blue&lt;/em&gt; ones (Figure 3).&lt;/p&gt; 
&lt;div id="attachment_3883" style="width: 911px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3883" loading="lazy" class="size-full wp-image-3883" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/19/IMG-2024-06-19-09.36.16.png" alt="Figure 3: The AWS Batch management console showing the job queue snapshot tab. The tab shows that high-priority pink jobs have moved ahead of lower priority pink jobs, but are still interleaved with blue jobs." width="901" height="459"&gt;
 &lt;p id="caption-attachment-3883" class="wp-caption-text"&gt;Figure 3: The AWS Batch management console showing the job queue snapshot tab. The tab shows that high-priority pink jobs have moved ahead of lower priority pink jobs, but are still interleaved with blue jobs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The underlying reason &lt;em&gt;blue&lt;/em&gt; still gets some allocation before pink is that the &lt;em&gt;fair share&lt;/em&gt; algorithm will try to give blue jobs &lt;em&gt;some&lt;/em&gt; allocation of resources (it’s called “fair share” for a reason). Figure 3 shows that most of the upcoming resources are for &lt;em&gt;pink&lt;/em&gt; jobs, even the regular priority ones. At the low weight factor we set, the next &lt;em&gt;blue&lt;/em&gt; job comes after the last &lt;em&gt;pink&lt;/em&gt; job in our queue (not shown in the figure).&lt;/p&gt; 
&lt;p&gt;If at this point you’re still not sure that the &lt;em&gt;pink&lt;/em&gt; jobs will complete by your deadline, you may need to take the more drastic option number 2. In either case, once the high-priority workloads are &lt;code&gt;RUNNING&lt;/code&gt; be sure to re-instate the previous share policy allocations. Otherwise Batch will keep prioritizing team &lt;em&gt;pink&lt;/em&gt;’s jobs over team blue’s – forever.&lt;/p&gt; 
&lt;p&gt;Before this new feature to see what’s at the head of the queue, you may have needed to be indiscriminate about “clearing the queue” – cancelling all scheduled jobs to make room for the high priority request.&lt;/p&gt; 
&lt;p&gt;Now with job queue snapshots you can be more prescriptive about how to adjust the job queue to allow high-priority workloads to run in the time you need.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post we described a new feature for AWS Batch: &lt;em&gt;job queue snapshots&lt;/em&gt;. This feature improves the customer and user experience with job queues by giving insight into what is at the head of the queue for both FIFO and fair share job queues. We also described a scenario to use job queue snapshots to help make decisions about managing a queue to reorder workloads based on an urgent priority.&lt;/p&gt; 
&lt;p&gt;Job queue snapshots are available now in to the &lt;a href="https://console.aws.amazon.com/batch/home?"&gt;AWS Batch console&lt;/a&gt;, or through the CLI, or API – whichever you prefer. We think you’ll love this new feature and welcome feedback on how you use it. Also &lt;a href="mailto:ask-hpc@amazon.com?subject=Re:%20GetJobQueueuSnapshot%20and%20job%20cancellation%20features"&gt;let us know&lt;/a&gt; if there’s more we can do to make the job of managing jobs easier.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>An agent-based simulation of Amazon’s inbound supply chain</title>
		<link>https://aws.amazon.com/blogs/hpc/an-agent-based-simulation-of-amazons-inbound-supply-chain/</link>
		
		<dc:creator><![CDATA[Siva Veluchamy]]></dc:creator>
		<pubDate>Wed, 19 Jun 2024 14:47:06 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">897cb050a52ce3f07883cdad8b87aedea2e5ba9c</guid>

					<description>Hundreds of millions of products, the entire *first-mile* of distribution - learn how Amazon simulated their massive US supply chain, end-to-end, with help from a company called Simudyne.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright wp-image-3762 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/30/An-agent-based-simulation-of-Amazons-inbound-supply-chain-2.png" alt="An agent-based simulation of Amazon's inbound supply chain" width="380" height="212"&gt;&lt;/p&gt; 
&lt;p&gt;Amazon has a complex inbound supply chain network for our retail operations in the United States, and we’re always seeking innovative ways to make it more efficient.&lt;/p&gt; 
&lt;p&gt;Recently, we embarked on an ambitious project: simulating our entire US inbound supply chain. To put this into perspective, this is the entire “first-mile” of distribution, tracking the movement of hundreds of millions of individual products through the network. This was a significant first step towards enhancing the network’s efficiency, and was made possible by using &lt;a href="https://docs.simudyne.com/overview"&gt;Simudyne’s Software Development Kit&lt;/a&gt; and the compute power of AWS.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk you through the story of our supply chain, and how we came to pull of what we think is quite an amazing feat. The supply chain toolkit used to build this is available on AWS Marketplace, so if you’re reading this – and interested in supply chain simulation – you can just dive right in.&lt;/p&gt; 
&lt;h2&gt;Amazon’s inbound network: a complex system&lt;/h2&gt; 
&lt;p&gt;Amazon’s inbound network in the US is a multi-stage, complex, and dynamic system where goods flow from a myriad of suppliers to strategically-located cross-dock facilities (we call them IXDs), and ultimately to Fulfillment Centers.&lt;/p&gt; 
&lt;p&gt;Suppliers range from local startups to international conglomerates, each feeding into the diverse Amazon inventory. The IXDs are vital in this system, acting as high-efficiency transit hubs where products are sorted and prepped for their next leg to the Fulfillment Centers.&lt;/p&gt; 
&lt;p&gt;Fulfillment Centers are large facilities, outfitted with advanced technologies like artificial intelligence (AI) systems and robotics. Items are stored, cataloged, and eventually dispatched to customers from these locations. This segment of the supply chain exemplifies the need for balance in optimization – a blend of efficiency, cost management, and technological innovation.&lt;/p&gt; 
&lt;p&gt;Optimizing a network like this is challenging. We need to balance cost efficiency and customer satisfaction, align operations with environmental sustainability, ensure adaptability to future market shifts, and deal manage technological advancements.&lt;/p&gt; 
&lt;p&gt;There must also be an understanding of the interface between the &lt;em&gt;micro&lt;/em&gt; aspects of fulfillment centers (i.e., what’s happening within individual centers/facilities) and the &lt;em&gt;macro&lt;/em&gt; aspects of a supply chain network (i.e., phenomena that emerge when examining the web of relationships between facilities).&lt;/p&gt; 
&lt;p&gt;One way to approach this optimization problem is by building and executing computer simulations, or mathematical models that aim to mimic systems, like supply chain networks, so the simulations can be used to study how those systems work. Building a supply chain simulation that can represent this variety of scales needs an intricate understanding of the supply chain’s dynamics – from warehousing logistics to the last-mile delivery intricacies.&lt;/p&gt; 
&lt;h2&gt;Solving for complexity: the power of agent-based models&lt;/h2&gt; 
&lt;p&gt;Simudyne’s Agent-Based Modeling (ABM) technology and SDK turned out to be a game-changer in our optimization toolkit. This technology allows us to simulate the Amazon supply chain with high fidelity, with each agent representing a distinct component like a warehouse or delivery vehicle. By mimicking real-world behavior and decision-making processes, we can analyze and optimize various aspects of supply chains like topology, inventory placement, resource allocation, and so forth.&lt;/p&gt; 
&lt;p&gt;Simudyne’s SDK is designed to strike a balance between handling the multi-scale system representation problem, the effort needed to build and implement simulations, and the ability of simulations to take advantage of large-scale computing that can speed up the business decision-making process.&lt;/p&gt; 
&lt;p&gt;Key to this balance is Simudyne’s advanced approach to agent-based modeling, which leverages a graph-based approach designed for more accurate and efficient simulations. Unlike traditional agent-based modeling tools that might use graph representation primarily for data visualization, Simudyne’s method involves using the graph as the core computational structure.&lt;/p&gt; 
&lt;p&gt;In this setup, each agent, whether a facility, vehicle, or another entity, is a node with connections to other nodes, representing their interactions and relationships. This structure allows agents to make independent observations and decisions based on their interactions, leading to a more dynamic and realistic simulation of the real world.&lt;/p&gt; 
&lt;h2&gt;Sumdyne’s SDK&lt;/h2&gt; 
&lt;p&gt;Simudyne uses a &lt;a href="https://docs.simudyne.com/features/graph-computation"&gt;Pregel-like framework for processing large-scale graphs&lt;/a&gt;. Pregel supports parallelized processing of massive graphs and graph computations by distributing the workload across multiple cores, for instance across large Amazon EC2 instances like the r6a.32xlarge, with 128 vCPUs. This technique is highly efficient for handling the complex, large-scale supply chain simulations that Amazon requires.&lt;/p&gt; 
&lt;p&gt;Simudyne’s Java-based SDK employs structures like &lt;em&gt;Parallel Streams&lt;/em&gt; and &lt;em&gt;Concurrent Lists&lt;/em&gt; that unlock additional intra-simulation parallelization potential. The SDK is designed for scalability from small-scale tests to large-scale production without needing significant code rewrites, ensuring that our simulation models are not only accurate, but flexible and adaptable.&lt;/p&gt; 
&lt;p&gt;Simudyne recently released a &lt;a href="https://docs.simudyne.com/supply_chain_toolkit"&gt;Supply Chain Toolkit&lt;/a&gt; that specifically enhances the SDK with specialized tools for modeling supply chain networks. The new toolkit includes some useful classes for representing the network:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Facility&lt;/code&gt; agent&lt;strong&gt; &amp;nbsp;&lt;/strong&gt;– a class which &lt;a href="https://docs.simudyne.com/supply_chain_toolkit/facility"&gt;users can extend&lt;/a&gt; to represent various types of facilities within a supply chain. This class includes essential functions for managing the flow of products, and users can add multiple &lt;code&gt;LoadingBay&lt;/code&gt; classes for different types of docks.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;TransportMessages&lt;/code&gt; – a class that &lt;a href="https://docs.simudyne.com/supply_chain_toolkit/transport_message"&gt;facilitates the movement&lt;/a&gt; of inventory between facilities, with options for cargo to be represented either as &lt;code&gt;Product&lt;/code&gt; class or as a string for memory efficiency.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;LoadingBay&lt;/code&gt; – a class that &lt;a href="https://docs.simudyne.com/supply_chain_toolkit/loading_bay"&gt;represents loading docks&lt;/a&gt; in the simulation, with a transport queue and a specified capacity, allowing for various types of docks and custom priority handling.&lt;/p&gt; 
&lt;p&gt;For visualizing supply chain networks, the &lt;a href="https://docs.simudyne.com/supply_chain_toolkit/python_visualization"&gt;Simudyne SDK integrates with Python tools like Plotly&lt;/a&gt;, so we can create interactive maps to display facility information. We can customize this visualization to display additional data points and network connections.&lt;/p&gt; 
&lt;p&gt;Overall, the toolkit is designed to provide a comprehensive suite for simulating and optimizing supply chain networks, offering operational researchers and supply chain analysts a powerful tool for modeling complex dynamics and managing high-throughput scenarios.&lt;/p&gt; 
&lt;div id="attachment_3794" style="width: 1396px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3794" loading="lazy" class="size-full wp-image-3794" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/07/IMG-2024-06-07-14.51.33.png" alt="Fig 1: Sample Dashboard Connecting Simudyne SDK to Custom Visualization Layer" width="1386" height="581"&gt;
 &lt;p id="caption-attachment-3794" class="wp-caption-text"&gt;Fig 1: Sample Dashboard Connecting Simudyne SDK to Custom Visualization Layer&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Running supply chain simulations on AWS&lt;/p&gt; 
&lt;p&gt;Using the Simudyne SDK and the Supply Chain Toolkit, Amazon’s Worldwide Design Engineering team, Simudyne, and AWS’s Emerging Technologies &amp;amp; Workloads team worked together to build a simulation of Amazon’s US inbound supply chain on AWS.&lt;/p&gt; 
&lt;p&gt;We’ve shown the reference architecture in Figure 2. The simulation code connects to an Amazon Redshift data warehouse to pull relevant supply chain data that’s used in the simulations.&lt;/p&gt; 
&lt;p&gt;Typically, a full-scale simulation of the US inbound supply chain can be executed on a r6a.32xlarge EC2 instance, which not only has 128 vCPUs, but also comes with over 1 TiB of RAM. Agent-based simulations are often memory-intensive applications and more performant when the state of agents during simulations can be saved and operated in memory, making high-memory instances found on AWS very handy. On an r6a.32xlarge instance, three (3) simulation weeks of the full US inbound supply chain ran in about an hour.&lt;/p&gt; 
&lt;div id="attachment_3755" style="width: 1004px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3755" loading="lazy" class="size-full wp-image-3755" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/30/IMG-2024-05-30-10.36.03.png" alt="Figure 2: AWS reference architecture used to run simulations of the Amazon US inbound supply chain." width="994" height="599"&gt;
 &lt;p id="caption-attachment-3755" class="wp-caption-text"&gt;Figure 2: AWS reference architecture used to run simulations of the Amazon US inbound supply chain.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Using Simudyne’s technology to simulate our supply chain has opened new avenues for us. We think It can provide insights into cost reduction, service improvement, environmental impact, and strategic adaptability.&lt;/p&gt; 
&lt;p&gt;This project is not just a testament to Amazon’s commitment to innovation but also an example for people exploring the frontiers of supply chain optimization. We’ll continue to refine our models and strategies because the potential for groundbreaking improvements in e-commerce logistics remains vast and exciting.&lt;/p&gt; 
&lt;p&gt;Now that we’ve successfully modeled our inbound supply chain network helping with the Inbound regionalization initiative as outlined in our CEO’s letter to shareholders, our focus is now shifting to simulating the outbound network – the critical journey of products from our Fulfillment Centers to customers. This new phase of modeling will let us explore and experiment with various strategies – including intermodal transport – within the simulated environment.&lt;/p&gt; 
&lt;p&gt;This allows us to refine and validate our strategies before we implement them in the real world. In our upcoming posts, we plan to share insights from our experience extending our simulations and using AI to further optimize the network, offering a glimpse into the cutting-edge methods driving supply chain efficiency.&lt;/p&gt; 
&lt;p&gt;Simudyne currently offers a 30-day free trial of its SDK, which you can find in &lt;a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-2hr6jozikusxs"&gt;AWS Marketplace&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Call for participation: HPC tutorial series from the HPCIC</title>
		<link>https://aws.amazon.com/blogs/hpc/call-for-participation-hpc-tutorial-series-from-the-hpcic/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Tue, 11 Jun 2024 17:09:26 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Public Sector]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[visualization]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">7634a0bf6ed82010fb48611fdb16217bc1cbb185</guid>

					<description>Interested in getting hands-on experience with cutting-edge HPC tools? Check out this blog post on an upcoming virtual training series from @LLNL and @AWSCloud. Learn emerging technologies from the experts this August.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3849" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-16.56.36.png" alt="Call for participation: HPC tutorial series from the HPCIC" width="380" height="161"&gt;Lawrence Livermore National Laboratory (LLNL) and AWS are joining forces to provide a training opportunity for emerging HPC tools and application. This takes the form of a series of tutorials over the summer by the HPC Innovation Center (HPCIC) at the Lab, which focuses on a broad suite of open-source software projects originating from LLNL. The series aims to give attendees experience with these cutting-edge technologies.&lt;/p&gt; 
&lt;p&gt;This &lt;a href="https://hpcic.llnl.gov/tutorials/2024-hpc-tutorials"&gt;virtual series&lt;/a&gt; consists of nine distinct tutorials through the month of August. Each tutorial focuses on a specific package from the HPCIC. You’ll get hands-on experience with each project and learn more about running HPC on AWS. Each event will be hosted on AWS resources and use &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; to provide the environment for attendees.&lt;/p&gt; 
&lt;p&gt;LLNL has a long history of delivering some of the world’s largest supercomputers and the supporting software to effectively use them. This collaboration between LLNL and AWS brings these projects and industry best practices to a broader community. Together we are democratizing access to next generation computational research tools and HPC resources.&lt;/p&gt; 
&lt;h2&gt;How to Participate&lt;/h2&gt; 
&lt;p&gt;Come join us in learning how these projects can help advance your HPC research and improve efficiency. These tutorials provide a great opportunity to learn modern techniques and tools in an adaptive cloud environment.&lt;/p&gt; 
&lt;p&gt;To register your interest, use the &lt;a href="https://llnlfed.webex.com/webappng/sites/llnlfed/webinar/webinarSeries/register/f0f129eba81946dc8a30552fc657ee94"&gt;signup form&lt;/a&gt;. Details of each event and registration deadlines are listed in the following table.&lt;/p&gt; 
&lt;table style="width: 90%"&gt; 
 &lt;thead&gt; 
  &lt;tr style="background-color: #000000;height: 20px"&gt; 
   &lt;th style="width: 20%"&gt;&lt;span style="color: #ffffff"&gt;Date&lt;/span&gt;&lt;/th&gt; 
   &lt;th style="width: 20%" width="72"&gt;&lt;span style="color: #ffffff"&gt;&lt;strong&gt;Time (PDT, GMT-7)&lt;/strong&gt;&lt;/span&gt;&lt;/th&gt; 
   &lt;td width="406"&gt;&lt;span style="color: #ffffff"&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu Aug 1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;BLT:&lt;/strong&gt;&amp;nbsp;build, link, and test large-scale applications&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tue Aug 6&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;8–11:30am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Spack (part 1 of 2):&lt;/strong&gt;&amp;nbsp;learn to install your software quickly&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Wed Aug 7&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;8–11:30am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Spack (part 2 of 2)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu Aug 8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–12pm&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Caliper:&lt;/strong&gt;&amp;nbsp;integrate performance profiling capabilities into your applications&lt;br&gt; &lt;strong&gt;Hatchet:&lt;/strong&gt;&amp;nbsp;analyze hierarchical performance data&lt;br&gt; &lt;strong&gt;Thicket:&lt;/strong&gt;&amp;nbsp;optimize application performance on supercomputers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tue Aug 13&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9– 11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;RAJA:&lt;/strong&gt;&amp;nbsp;run and port codes across different GPUs&lt;br&gt; &lt;strong&gt;Umpire:&lt;/strong&gt;&amp;nbsp;discover, provision, and manage HPC memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu Aug 15&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Axom:&lt;/strong&gt;&amp;nbsp;leverage robust, flexible software components for scientific applications&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tue Aug 20&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Ascent:&lt;/strong&gt;&amp;nbsp;visualize and analyze your simulations in situ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu Aug 22&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;MFEM:&lt;/strong&gt;&amp;nbsp;use scalable finite element discretizations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tue Aug 27&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;WEAVE:&lt;/strong&gt;&amp;nbsp;analyze runs of your code&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu 29&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Flux:&lt;/strong&gt;&amp;nbsp;run thousands of jobs in a workflow&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;We’re excited about this opportunity to give back to the HPC community alongside LLNL and look forward to meeting you all over the duration of the event.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Integrating Research and Engineering Studio with AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/research-and-engineering-studio-integration-with-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Doug Morand]]></dc:creator>
		<pubDate>Tue, 11 Jun 2024 13:38:50 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[DCV]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[VDI]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">975dc9ecff3f91bdf9cbec7ea6b4ab9cc4b3a431</guid>

					<description>Researchers, engineers &amp;amp; scientists - learn how to leverage AWS ParallelCluster with Research &amp;amp; Engineering Studio for a full-featured cloud workspace. Read this post for details on this new integration.</description>
										<content:encoded>&lt;p&gt;&lt;a href="https://aws.amazon.com/hpc/res/"&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3787" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/06/Integrating-Research-and-Engineering-Studio-with-AWS-ParallelCluster-1.png" alt="Integrating Research and Engineering Studio with AWS ParallelCluster" width="380" height="213"&gt;Research and Engineering Studio on AWS (RES)&lt;/a&gt; is an easy-to-use self-service portal for researchers and engineers to access and manage their cloud-based workspaces with persistent storage and secure virtual desktops to access their data and run interactive applications.&lt;/p&gt; 
&lt;p&gt;The people who use RES are also frequently users of HPC clusters. RES today does not ship with direct integration to other AWS solutions for HPC, like &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;So today, we’re happy to announce a new HPC recipe which creates a RES-compatible &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/login-nodes-v3.html"&gt;ParallelCluster login node&lt;/a&gt; software stack. This solution uses new features added to RES 2024.04 including &lt;a href="https://docs.aws.amazon.com/res/latest/ug/res-ready-ami.html"&gt;RES-ready AMIs&lt;/a&gt; and project &lt;a href="https://docs.aws.amazon.com/res/latest/ug/projects.html#project-launch-template"&gt;launch templates&lt;/a&gt;. The RES-ready AMI allows you to pre-install RES dependencies for your virtual desktop instances (VDIs) to pre-bake software and configuration into your images, and improve boot times for your end users.&lt;/p&gt; 
&lt;p&gt;These AMIs can be registered as a RES &lt;a href="https://docs.aws.amazon.com/res/latest/ug/evdi.html#software-stacks"&gt;software stack&lt;/a&gt; for your end users to create a VDI, which joins a ParallelCluster to become their own personal login node. Using this recipe, you’ll see how to create customized software stacks that can be used as conduits to access other native AWS cloud services.&lt;/p&gt; 
&lt;h2&gt;ParallelCluster login node for RES&lt;/h2&gt; 
&lt;p&gt;Starting from version 3.7.0, ParallelCluster provides login nodes for users to access the Slurm-based environment in the cloud. Both RES and ParallelCluster support multi-user access using Active Directory (AD) integration. With that mechanism in place, shared storage and user authentication becomes easier to manage.&lt;/p&gt; 
&lt;p&gt;We start with an existing ParallelCluster that’s integrated with the same AD and using the same &lt;code&gt;ldap_id_mapping&lt;/code&gt; setting. We take a snapshot of a login node of the cluster, turn it into an Amazon Machine Image (AMI), and use as the Base image within an EC2 Image Builder recipe to create a RES-ready AMI. The &lt;code&gt;ldap_ip_mapping&lt;/code&gt; setting is an installation-level setting used by System Security Services Daemon (SSSD). This setting (when enabled) allows SSSD to generate a UID/GID.&lt;/p&gt; 
&lt;p&gt;This setting is used in both RES and ParallelCluster. In RES, it’s enabled through the &lt;code&gt;EnableLdapIDMapping&lt;/code&gt; AWS CloudFormation parameter which you use to &lt;a href="https://docs.aws.amazon.com/res/latest/ug/launch-the-product.html"&gt;launch the product&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In ParallelCluster, it’s configured in the &lt;code&gt;DirectoryService&lt;/code&gt; section of the configuration file, as part of &lt;code&gt;AdditionalSssdConfigs&lt;/code&gt; (more on this &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/DirectoryService-v3.html#yaml-DirectoryService-AdditionalSssdConfigs"&gt;in our docs&lt;/a&gt;), like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;DirectoryService:
…
  AdditionalSssdConfigs:
    ldap_id_mapping: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The RES-compatible login node AMI is created through an &lt;a href="https://aws.amazon.com/systems-manager/"&gt;AWS Systems Manager&lt;/a&gt; (SSM) automation document. SSM allows you to gather insights and automate tasks across your AWS accounts.&lt;/p&gt; 
&lt;p&gt;This automation process performs two tasks:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It creates an Amazon Machine Image (AMI) from the login node&lt;/li&gt; 
 &lt;li&gt;Updates the ParallelCluster &lt;em&gt;head node security group&lt;/em&gt; by adding ingress rules to allow connections from RES virtual desktops. Slurm (6819-6829) and NFS (2049) ingress connections are allowed via the &lt;code&gt;RESPCLoginNodeSG&lt;/code&gt; security group. This group will be used later to associate to RES project(s) when creating login node VDI instances.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;After the RES-compatible login node AMI has been created, we can create the RES-ready AMI using &lt;a href="https://aws.amazon.com/image-builder/"&gt;EC2 Image Builder&lt;/a&gt;. EC2 Image Builder simplifies the build process of building and maintaining “golden images”. Our SSM automation process will create an Image Builder recipe that includes the base AMI, and an additional component for the RES login node to configure the image to work as a RES VDI. The resulting AMI from the Image Builder pipeline will be used to create a RES software stack.&lt;/p&gt; 
&lt;p&gt;Once the RES-ready AMI has been created by Image Builder, a RES administrator can login to create the Project and Software Stack by in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Update a RES Project to modify the Launch template (or create a project if it’s their first time using RES). The &lt;code&gt;RESPCLoginNodeSG&lt;/code&gt; must be added to any project that requires access to the ParallelCluster. This can be added in the RES project &lt;em&gt;Resource Configurations -&amp;gt; Advanced Options -&amp;gt; Add Security Groups&lt;/em&gt; configuration section.&lt;/li&gt; 
 &lt;li&gt;Create a new Software Stack using the RES-ready AMI created from the Image Builder pipeline.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Once the software stack has been created, end-users that have access to it as part of a Project can create their own, dedicated, login node virtual desktops.&lt;/p&gt; 
&lt;h2&gt;Virtual desktop login node in action&lt;/h2&gt; 
&lt;p&gt;End-users will access the login node virtual desktop in the same way they access other virtual desktops.&lt;/p&gt; 
&lt;div id="attachment_3776" style="width: 957px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3776" loading="lazy" class="size-full wp-image-3776" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/06/IMG-2024-06-06-13.46.01.png" alt="Figure 1 – The Virtual Desktops contain a virtual desktop (PC-LoginNode372) which is a based on a LoginNode instance compatible with ParallelCluster." width="947" height="550"&gt;
 &lt;p id="caption-attachment-3776" class="wp-caption-text"&gt;Figure 1 – The Virtual Desktops contain a virtual desktop (PC-LoginNode372) which is a based on a LoginNode instance compatible with ParallelCluster.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Once end users have access to the login node VDI they can interact with ParallelCluster in the same way they’re accustomed. They’ll have access to the same shared storage in the ParallelCluster Login Node &lt;em&gt;and&lt;/em&gt; in the RES VDI. The next couple of screenshots show examples of shared storage accessed from both the Login Node and RES VDI.&lt;/p&gt; 
&lt;div id="attachment_3777" style="width: 998px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3777" loading="lazy" class="size-full wp-image-3777" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/06/IMG-2024-06-06-13.46.34.png" alt="Figure 2 – A terminal session on a ParallelCluster Login Node showing the user shared storage directory listing" width="988" height="257"&gt;
 &lt;p id="caption-attachment-3777" class="wp-caption-text"&gt;Figure 2 – A terminal session on a ParallelCluster Login Node showing the user shared storage directory listing&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3778" style="width: 996px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3778" loading="lazy" class="size-full wp-image-3778" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/06/IMG-2024-06-06-13.46.54.png" alt="Figure 3 – A RES virtual desktop session showing the same shared storage directory accessible from the ParallelCluster Login Node" width="986" height="428"&gt;
 &lt;p id="caption-attachment-3778" class="wp-caption-text"&gt;Figure 3 – A RES virtual desktop session showing the same shared storage directory accessible from the ParallelCluster Login Node&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;End users can now interact with a ParallelCluster (PC) from a RES VDI. This VDI acts similarly to a ParallelCluster login node with the added benefit that end-users in the RES environment can launch their own login node, with VDI, any time they need one.&lt;/p&gt; 
&lt;div id="attachment_3791" style="width: 1448px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3791" loading="lazy" class="size-full wp-image-3791" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/07/res-loginnode-screenvideo.gif" alt="Figure 4 – A virtual desktop session showing examples of Slurm commands demonstrating the integration with ParallelCluster." width="1438" height="544"&gt;
 &lt;p id="caption-attachment-3791" class="wp-caption-text"&gt;Figure 4 – A virtual desktop session showing examples of Slurm commands demonstrating the integration with ParallelCluster.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Getting started with RES-compatible ParallelCluster login nodes&lt;/h2&gt; 
&lt;p&gt;You can follow the steps to create a RES-compatible login node for your ParallelCluster by heading to the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/login_node_for_res"&gt;Login Node for Research and Engineering Studio&lt;/a&gt; recipe that’s part of the &lt;a href="https://hpc.news/recipes"&gt;HPC Recipes Library&lt;/a&gt; (a great resource, if you’re unfamiliar with it until now).&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Integrating AWS ParallelCluster and Research and Engineering Studio unlocks the ability for end users using interactive desktops to process large amounts of data when HPC is necessary. It’s a great experience, because not only does this put large-scale computational power in the hands of scientists, it does so in a way that’s friendly to use, and accessible any time they need it.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Securing HPC on AWS: implementing STIGs in AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/securing-hpc-on-aws-implementing-stigs-in-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Alex Domijan]]></dc:creator>
		<pubDate>Tue, 04 Jun 2024 11:35:47 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Security]]></category>
		<category><![CDATA[Security, Identity, & Compliance]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">a5572dfa24f9456e8fb3e01a8a7cc20766ce2442</guid>

					<description>Want to accelerate creating compliant Amazon EC2 images? Learn how HPC users can leverage cloud-native methods for applying STIG security standards.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3698" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/Securing-HPC.png" alt="" width="380" height="212"&gt;&lt;/em&gt;Today, we’ll discuss cloud-native methods that HPC customers can use to accelerate their process for creating Amazon Elastic Compute Cloud (Amazon EC2) images for AWS ParallelCluster that are compliant with Security Technical Implementation Guides (STIGs), a set of standards maintained by the US government.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk you through the process of applying STIGs to your ParallelCluster environment, help you identify the decisions you need to make on the way, and show you some of the tools you can use to make it all easier.&lt;/p&gt; 
&lt;h2&gt;What’s a STIG?&lt;/h2&gt; 
&lt;p&gt;STIGs are maintained by &lt;a href="https://public.cyber.mil/stigs/downloads/"&gt;a US government organization&lt;/a&gt;, and are simply a set of security standards that can be applied to different environments, like &lt;a href="https://aws.amazon.com/pm/ec2/?gclid=Cj0KCQjwiMmwBhDmARIsABeQ7xQS_X3FGRFdQeMzTWl84Gi1PUFg8U7-Y2A8g30P269Hh4BW5wxBXvIaAmfOEALw_wcB&amp;amp;trk=9cd376cd-1c18-46f2-9f75-0e1cdbca94c5&amp;amp;sc_channel=ps&amp;amp;ef_id=Cj0KCQjwiMmwBhDmARIsABeQ7xQS_X3FGRFdQeMzTWl84Gi1PUFg8U7-Y2A8g30P269Hh4BW5wxBXvIaAmfOEALw_wcB:G:s&amp;amp;s_kwcid=AL!4422!3!651751059309!e!!g!!amazon%20ec2!19852662176!145019189697"&gt;Amazon EC2&lt;/a&gt;. Think of STIGs as a checklist of items to apply to your EC2 instances where each checklist item has a corresponding severity level attached to it that says, “&lt;em&gt;the risk of not doing x is a low, medium, or high security risk&lt;/em&gt;”. You’ll also see these security levels referred to as Category Codes (CAT) where CAT 1 corresponds to a high security risk, CAT 2 to medium, and CAT 3 to low.&lt;/p&gt; 
&lt;p&gt;For example, one high security-risk STIG checklist item for Red Hat Enterprise Linux (RHEL) 8 is to not allow accounts configured with blank or null passwords. To resolve this, an administrator can login to the operating system and manually configure accounts to not have a blank or null password. With hundreds of checklist items it is easy to see why this can quickly become a burdensome task. The process described in this post automates up to 87% of the otherwise manual STIG remediation process.&lt;/p&gt; 
&lt;h2&gt;Why do customers want to implement STIGs?&lt;/h2&gt; 
&lt;p&gt;In short, some want to, and some need to. Customers such as the U.S. Department of Defense (DoD) must adhere to stringent compliance standards for operating system hardening. Other customers may prefer to use STIGs as a benchmark to improve their security posture.&lt;/p&gt; 
&lt;p&gt;Customers like the DoD often operate in AWS without any access to the Internet. Organizational policy dictates the reason for why, which is usually to reduce the risk of sensitive data going places it shouldn’t. We address how customers with these network restrictions can accelerate STIG hardening using AWS cloud native tools.&lt;/p&gt; 
&lt;p&gt;Once you’ve “&lt;em&gt;STIG’d&lt;/em&gt;” your ParallelCluster instances, how can you verify which checklist items you have crossed off? This is where&amp;nbsp;&lt;a href="https://www.open-scap.org/"&gt;OpenSCAP&lt;/a&gt;, an open-source security and compliance tool, comes into play. OpenSCAP automates continuous monitoring, vulnerability management, and reporting of security policy compliance data. While OpenSCAP is primarily designed to align with DoD security standards, it’s used to establish security baselines across many industries.&lt;/p&gt; 
&lt;p&gt;This post will focus on some supported ParallelCluster operating systems (OS): RHEL8, Amazon Linux 2 (AL2), and Ubuntu 20.04 (at the time of writing this, the &lt;a href="https://public.cyber.mil/stigs/downloads/"&gt;DISA STIG document library&lt;/a&gt; didn’t contain a benchmark for Ubuntu 22.04 – which is why it’s not mentioned).&lt;/p&gt; 
&lt;p&gt;We worked through the process defined in this post using the &lt;a href="https://aws.amazon.com/govcloud-us/?whats-new-ess.sort-by=item.additionalFields.postDateTime&amp;amp;whats-new-ess.sort-order=desc"&gt;AWS GovCloud West&lt;/a&gt; region, but you should be able to repeat it in other AWS regions.&lt;/p&gt; 
&lt;p&gt;For HPC customers completely new to AWS, we recommend reviewing this &lt;a href="https://aws.amazon.com/blogs/hpc/the-plumbing-best-practice-infrastructure-to-facilitate-hpc-on-aws/"&gt;blog post&lt;/a&gt; which speaks about best practices for setting up a foundation in AWS to build your HPC workloads on.&lt;/p&gt; 
&lt;h2&gt;AMIs for AWS ParallelCluster&lt;/h2&gt; 
&lt;p&gt;An &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"&gt;Amazon Machine Image&lt;/a&gt; (AMI) is a template that contains a software configuration (for example, an OS, an application server, and applications). From an AMI, you launch an&amp;nbsp;EC2 instance, which is a copy of the AMI running as a virtual server in the cloud. AMIs used for ParallelCluster are unique because they have software installed on them necessary for operating the cluster management tool.&lt;/p&gt; 
&lt;p&gt;Customers can optionally choose to create custom AMIs for ParallelCluster using &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/custom-ami-v3.html"&gt;two methods&lt;/a&gt;, both of which we can use for achieving STIG compliance, depending on factors like Internet connectivity and OS choice.&lt;/p&gt; 
&lt;p&gt;The first option is the build image configuration process which you can trigger from a ParallelCluster CLI &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcluster.build-image-v3.html"&gt;command&lt;/a&gt;: &lt;code&gt;pcluster build-image&lt;/code&gt;. This process uses &lt;a href="https://aws.amazon.com/image-builder/"&gt;Amazon EC2 Image Builder&lt;/a&gt; to launch a build instance, apply the &lt;a href="https://github.com/aws/aws-parallelcluster-cookbook"&gt;ParallelCluster cookbook&lt;/a&gt;, install the ParallelCluster software stack, and perform other necessary configuration tasks.&lt;/p&gt; 
&lt;p&gt;The second option involves taking a baseline ParallelCluster AMI (one produced by the ParallelCluster team themselves) and customizing it by performing manual modifications through &lt;a href="https://aws.amazon.com/systems-manager/"&gt;AWS Systems Manager&lt;/a&gt; (SSM).&lt;/p&gt; 
&lt;h2&gt;Process comparison&lt;/h2&gt; 
&lt;p&gt;Should you take a baseline ParallelCluster image and then apply STIGs, or take an image that already has STIGs applied (a “golden image”), and then install ParallelCluster on top? The end result is fundamentally similar, but there are some trade-offs depending on which route you choose.&lt;/p&gt; 
&lt;p&gt;The benefit of applying STIGs after a ParallelCluster image is created is that you can minimize permissions attached to the EC2 instance’s role. There &lt;em&gt;are&lt;/em&gt; additional &lt;a href="https://aws.amazon.com/iam/"&gt;AWS Identity and Access Management&lt;/a&gt; (IAM) permissions required to trigger the build image process and you can find them in &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/iam-roles-in-parallelcluster-v3.html#iam-roles-in-parallelcluster-v3-user-policy-build-image"&gt;our documentation&lt;/a&gt;. The tradeoff you’re making is that you would be standing up a new image build pipeline to accommodate security policy (STIG) enforcement starting from a baseline ParallelCluster image.&lt;/p&gt; 
&lt;p&gt;An advantage of taking a golden image and installing ParallelCluster is that you can maintain an already established image build pipeline that may accelerate internal compliance processes. However, this would require a wider permissions boundary in comparison to the previous example. There’s also a chance that installing new software could impact how STIG compliant your images are. For customers interested in trying this process on your own AMIs, you can follow along with any of the sections below depending on Internet connectivity and operating system requirements as the process is the same. In either case, we recommend performing compliance scans on your images.&lt;/p&gt; 
&lt;h2&gt;Accelerating RHEL8, AL2, and Ubuntu 20.04 STIG compliance&lt;/h2&gt; 
&lt;p&gt;Apart from the OS your use cases require, the process to achieve&amp;nbsp;STIG compliance is determined by whether your Amazon EC2 instances have Internet connectivity or not. If your compliance requirements allow you the flexibility to choose, then it’s easier with Internet connectivity.&lt;/p&gt; 
&lt;p&gt;For users &lt;strong&gt;with&lt;/strong&gt; Internet connectivity who want to use RHEL8 or AL2 operating systems, refer to the instructions in our &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/stig#rhel8-and-al2-instances-with-internet-connectivity"&gt;GitHub repo&lt;/a&gt; that’s part of the HPC Recipes Library which will guide you through the EC2 Image Builder process.&lt;/p&gt; 
&lt;p&gt;For users &lt;strong&gt;without&lt;/strong&gt; Internet connectivity who want to use RHEL8 or AL2 operating systems, refer to &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/stig#rhel8-and-al2-instances-without-internet-connectivity"&gt;these instructions&lt;/a&gt; in the same repository. This type of connectivity scenario is perhaps more common amongst customers with STIG requirements. These customers can take advantage of &lt;a href="https://aws.amazon.com/privatelink/"&gt;AWS PrivateLink&lt;/a&gt; which is a feature of Virtual Private Cloud (&lt;a href="https://aws.amazon.com/vpc/"&gt;VPC&lt;/a&gt;) and allows for private connectivity to AWS services. To take advantage of this technology for purposes of accelerating STIG compliance, ensure that you configure the &lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html"&gt;required VPC endpoints&lt;/a&gt; to allow connectivity from your private subnet to SSM. You’ll also need the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/network-configuration-v3.html#aws-parallelcluster-in-a-single-public-subnet-no-internet-v3"&gt;required VPC endpoints&lt;/a&gt; for ParallelCluster which will be used to launch your cluster with the resulting AMI.&lt;/p&gt; 
&lt;p&gt;The process for Ubuntu 20.04 includes an extra step compared to RHEL8 and AL2 operating systems because there are a couple of findings that Systems Manager cannot rectify during its run command. Due to this, we launch a baseline ParallelCluster Ubuntu 20.04 EC2 instance with a user data script that resolves findings &lt;a href="https://www.stigviewer.com/stig/canonical_ubuntu_18.04_lts/2022-08-25/finding/V-219166"&gt;V-219166&lt;/a&gt;, &lt;a href="https://www.stigviewer.com/stig/canonical_ubuntu_20.04_lts/2023-09-08/finding/V-238237"&gt;V-238237&lt;/a&gt;, and &lt;a href="https://www.stigviewer.com/stig/canonical_ubuntu_20.04_lts/2021-03-23/finding/V-238218"&gt;V-238218.&lt;/a&gt; As with RHEL8 and AL2 operating systems, customers without Internet connectivity should ensure they configure the required VPC endpoints to allow connectivity from your private subnet to SSM, and the required VPC endpoints for ParallelCluster. Instructions for Ubuntu 20.04 can be found in our &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/stig#ubuntu-2004-instances-with-or-without-internet-connectivity"&gt;HPC samples repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;As previously mentioned, there are corresponding severity levels (high, medium, low) associated with STIG checklist items. Customers can choose which security level they want to apply to their Amazon EC2 instances which is described in our &lt;a href="https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/awsec2-configure-stig.html"&gt;SSM documentation&lt;/a&gt;. We used the STIG High baseline which includes any vulnerability that can result in loss of confidentiality, availability, or integrity. Customers can optionally choose to make additional modifications to the AMIs after the STIG process of their choosing has been performed. In any event, we recommend testing AMI compatibility with your application prior to deploying to a production environment.&lt;/p&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;p&gt;Customers may be interested to find out what the effects of running the EC2 Image Builder STIG High component or Systems Manager STIG High document has on their respective operating systems.&lt;/p&gt; 
&lt;p&gt;We used OpenSCAP to perform compliance scanning to assess the security posture of our instances. It also uses the concept of profiles to determine which checks it will run and the profile can vary on mission requirements and OS.&lt;/p&gt; 
&lt;p&gt;For the purposes of maintaining a consistent benchmark for before and after assessments, we used the &lt;code&gt;xccdf_mil.disa.stig_profile_MAC-2_Sensitive&lt;/code&gt; profile for RHEL8 and Ubuntu 20.04 operating systems, and &lt;code&gt;stig-rhel7-disa&lt;/code&gt; on AL2.&lt;/p&gt; 
&lt;p&gt;Each of the ‘Baseline’ AMIs in the screenshots that follow refer to the baseline ParallelCluster AMI. In other words, these are the AMIs you would find by typing the CLI &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcluster.list-official-images-v3.html"&gt;command&lt;/a&gt;: &lt;code&gt;pcluster list-official-images&lt;/code&gt;. Note that the baseline and subsequent STIG high AMI results may change in future ParallelCluster releases.&lt;/p&gt; 
&lt;div id="attachment_3684" style="width: 1253px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3684" loading="lazy" class="size-full wp-image-3684" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.46.20.png" alt="Figure 1 - RHEL8 baseline ParallelCluster AMI OpenSCAP results from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 83 checks and failing 148 for a result of being 35.93% compliant with this profile." width="1243" height="451"&gt;
 &lt;p id="caption-attachment-3684" class="wp-caption-text"&gt;Figure 1 – RHEL8 baseline ParallelCluster AMI OpenSCAP results from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 83 checks and failing 148 for a result of being 35.93% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3685" style="width: 1270px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3685" loading="lazy" class="size-full wp-image-3685" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.46.41.png" alt="Figure 2 - RHEL8 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 201 checks and failing 30 for a result of being 87.01% compliant with this profile." width="1260" height="447"&gt;
 &lt;p id="caption-attachment-3685" class="wp-caption-text"&gt;Figure 2 – RHEL8 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 201 checks and failing 30 for a result of being 87.01% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3686" style="width: 1243px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3686" loading="lazy" class="size-full wp-image-3686" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.47.16.png" alt="Figure 3 - Amazon Linux 2 baseline ParallelCluster AMI OpenSCAP results from running the stig-rhel7-disa profile. This shows the EC2 instance as passing 54 checks and failing 160 for a result of being 58.88% compliant with this profile." width="1233" height="446"&gt;
 &lt;p id="caption-attachment-3686" class="wp-caption-text"&gt;Figure 3 – Amazon Linux 2 baseline ParallelCluster AMI OpenSCAP results from running the stig-rhel7-disa profile. This shows the EC2 instance as passing 54 checks and failing 160 for a result of being 58.88% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3687" style="width: 1263px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3687" loading="lazy" class="size-full wp-image-3687" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.47.33.png" alt="Figure 4 - Amazon Linux 2 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the stig-rhel7-disa profile. This shows the EC2 instance as passing 142 checks and failing 72 for a result of being 68.09% compliant with this profile." width="1253" height="449"&gt;
 &lt;p id="caption-attachment-3687" class="wp-caption-text"&gt;Figure 4 – Amazon Linux 2 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the stig-rhel7-disa profile. This shows the EC2 instance as passing 142 checks and failing 72 for a result of being 68.09% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3688" style="width: 1273px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3688" loading="lazy" class="size-full wp-image-3688" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.48.01.png" alt="Figure 5 - Ubuntu 20.04 baseline ParallelCluster AMI OpenSCAP results from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 17 checks and failing 92 for a result of being 15.6% compliant with this profile." width="1263" height="454"&gt;
 &lt;p id="caption-attachment-3688" class="wp-caption-text"&gt;Figure 5 – Ubuntu 20.04 baseline ParallelCluster AMI OpenSCAP results from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 17 checks and failing 92 for a result of being 15.6% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3689" style="width: 1259px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3689" loading="lazy" class="size-full wp-image-3689" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.48.17.png" alt="Figure 6 - Ubuntu 20.04 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 42 checks and failing 67 for a result of being 38.53% compliant with this profile." width="1249" height="446"&gt;
 &lt;p id="caption-attachment-3689" class="wp-caption-text"&gt;Figure 6 – Ubuntu 20.04 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 42 checks and failing 67 for a result of being 38.53% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Running your own OpenSCAP scans&lt;/h2&gt; 
&lt;p&gt;If you want to perform additional STIGs on ParallelCluster AMIs, you may want to run those images through the same OpenSCAP profiles used for this blog post.&lt;/p&gt; 
&lt;p&gt;We’ve stored the scripts for &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/blob/main/recipes/pcluster/stig/assets/EC2_RHEL8_SCAP_Assessment.sh"&gt;RHEL8&lt;/a&gt;, &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/blob/main/recipes/pcluster/stig/assets/EC2_AL2_SCAP_Assessment.sh"&gt;AL2&lt;/a&gt;, and &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/blob/main/recipes/pcluster/stig/assets/EC2_UBUNTU_2004_SCAP_Assessment.sh"&gt;Ubuntu 20.04&lt;/a&gt; in our GitHub repo. These scripts &lt;em&gt;do&lt;/em&gt; require Internet connectivity to run because they download a series of tools like the AWS CLI and OpenSCAP, and STIG benchmarks to the EC2 instance being evaluated.&lt;/p&gt; 
&lt;p&gt;You’ll need to &lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html"&gt;create an S3 bucket&lt;/a&gt;, and update the name of the bucket inside the script where it saves the results of the evaluations. The scripts use EC2 instance &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html"&gt;metadata&lt;/a&gt;&amp;nbsp;to dynamically name the output files in Amazon S3 after the instance, so they’re not overwritten as new instances are tested.&lt;/p&gt; 
&lt;p&gt;To run these scripts with minimal effort, you can run them as a user-data script upon launch and have the HTML results automatically sent to your S3 bucket. Inputting a user-data script follows the same logic as described under step 3 of the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/stig#ubuntu-2004-instances-with-or-without-internet-connectivity"&gt;Ubuntu 20.04 section&lt;/a&gt;. For RHEL8 and Ubuntu 20.04 operating systems, it takes approximately 10 minutes from instance launch to see the results uploaded to your Amazon S3 bucket. AL2 takes approximately 20-25 minutes.&lt;/p&gt; 
&lt;h2&gt;Using the resulting images&lt;/h2&gt; 
&lt;p&gt;The STIG’d AMIs can be found in the EC2 section of the Management Console and referenced in a ParallelCluster configuration file. You can create clusters using the ParallelCluster &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-configuring.html"&gt;CLI&lt;/a&gt; or the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/configure-create-pcui-v3.html"&gt;UI&lt;/a&gt;. For purposes of this post, we’ll show an example of placing the STIG’d AMI ID into the ParallelCluster configuration file for a cluster in GovCloud West.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Region: us-gov-west-1
Image:
  Os: rhel8
HeadNode:
  InstanceType: c5a.4xlarge
  Networking:
    SubnetId: {your-subnet-id}
  Ssh:
    KeyName: {your-keypair}
  Image:
    CustomAmi: {your-AMI-id}
SharedStorage:
  - MountDir: /fsx  
    Name: FSxExtData
    StorageType: FsxLustre
    FsxLustreSettings:
      StorageCapacity: 1200
      DeploymentType: PERSISTENT_1
      PerUnitStorageThroughput: 50
      DeletionPolicy: Delete
Scheduling:
  Scheduler: slurm
  SlurmSettings:
    QueueUpdateStrategy: DRAIN
  SlurmQueues:
  - Name: queue1
    ComputeResources:
    - Name: compute
      Instances:
      - InstanceType: hpc7a.48xlarge
      MinCount: 1
      MaxCount: 10
      Efa:
       Enabled: true
    Networking:
      SubnetIds:
      - {your-subnet-id}
      PlacementGroup:
        Enabled: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should edit the Items enclosed in the {} to include your identifiers.&lt;/p&gt; 
&lt;p&gt;Once you’ve created the file you can launch the cluster using the command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster --cluster-name &amp;lt;name&amp;gt; --cluster-configuration &amp;lt;file-name&amp;gt;.yml&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see validation warning messages because you are using a custom AMI, however these messages can be ignored and will not impact the creation of the cluster. You can track the cluster creation status through the &lt;a href="https://aws.amazon.com/cloudformation/"&gt;AWS CloudFormation&lt;/a&gt; console or by using the ParallelCluster CLI command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster list-clusters&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Verifying levels of compliance for compute resources is a requirement in some industries, and desired in others. Throughout this post, we’ve discussed several different cloud-native methods HPC customers with compliance requirements can choose from to accelerate their STIG process in AWS ParallelCluster depending on their Internet connectivity (or lack thereof) and operating system choice.&lt;/p&gt; 
&lt;p&gt;We recommend validating that your output images work with your application in a development environment prior to running in production.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Large scale training with NVIDIA NeMo Megatron on AWS ParallelCluster using P5 instances</title>
		<link>https://aws.amazon.com/blogs/hpc/large-scale-training-with-nemo-megatron-on-aws-parallelcluster-using-p5-instances/</link>
		
		<dc:creator><![CDATA[Aman Shanbhag]]></dc:creator>
		<pubDate>Wed, 29 May 2024 13:52:05 +0000</pubDate>
				<category><![CDATA[Amazon Machine Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[FEA]]></category>
		<category><![CDATA[GPU]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<guid isPermaLink="false">9f2f940ec2831d85025f4b4f1d755f1419ed79d4</guid>

					<description>Launching distributed GPT training? See how AWS ParallelCluster sets up a fast shared filesystem, SSH keys, host files, and more between nodes. Our guide has the details for creating a Slurm-managed cluster to train NeMo Megatron at scale.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Akshit Arora (NVIDIA), Peter Dykas (NVIDIA), Aman Shanbhag (AWS), Sean Smith (AWS), Pierre-Yves (AWS)&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Today we’ll take you on a step-by-step guide to help you to create a cluster of p5.48xlarge instances, using &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html"&gt;AWS ParallelCluster&lt;/a&gt; to launch GPT training through the NVIDIA NeMo Megatron framework, using Slurm. We’ve put detailed information about this &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher"&gt;in our GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We use ParallelCluster to execute NVIDIA NeMo Megatron across multiple nodes, because it takes care of mounting a fast shared filesystem between the nodes, synchronizing the SSH keys, creating a host file, and all the other overheads that make job submission possible.&lt;/p&gt; 
&lt;p&gt;&lt;a class="c-link" href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html" target="_blank" rel="noopener noreferrer" data-stringify-link="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html" data-sk="tooltip_parent"&gt;AWS ParallelCluster&lt;/a&gt; is a supported, open-source cluster management tool that makes it easy to create, scale, and manage clusters of accelerated&amp;nbsp;instances based on the open-source Slurm scheduler. It uses a YAML configuration file to stand up a head node,&amp;nbsp;accelerated&amp;nbsp;compute nodes, and a file system. Users can login and submit jobs to pre-provisioned nodes, or dynamically spin-up Amazon Elastic Compute Cloud (Amazon EC2) instances using On-Demand or Spot. ParallelCluster also offers a&amp;nbsp;&lt;a class="c-link" href="https://aws.amazon.com/blogs/hpc/large-scale-training-with-nemo-megatron-on-aws-parallelcluster-using-p5-instances/ParallelCluster%20also%20offers%20a%20web-based%20user%20interface%20that%20serves%20as%20a%20dashboard%20for%20creating,%20monitoring,%20and%20managing%20clusters." target="_blank" rel="noopener noreferrer" data-stringify-link="https://aws.amazon.com/blogs/hpc/large-scale-training-with-nemo-megatron-on-aws-parallelcluster-using-p5-instances/ParallelCluster%20also%20offers%20a%20web-based%20user%20interface%20that%20serves%20as%20a%20dashboard%20for%20creating,%20monitoring,%20and%20managing%20clusters." data-sk="tooltip_parent"&gt;web-based user interface&lt;/a&gt;&amp;nbsp;that serves as a dashboard for creating, monitoring, and managing clusters.&lt;/p&gt; 
&lt;h2&gt;Introducing NVIDIA NeMO Framework&lt;/h2&gt; 
&lt;p&gt;The NVIDIA NeMo Framework (or just &lt;em&gt;NeMo FW&lt;/em&gt; for the rest of this post) focuses on foundation model-training for generative AI models. Large language model (LLM) pre-training typically needs a lot of compute and model parallelism to efficiently scale training. NeMo FW’s model training scales to thousands of NVIDIA GPUs and can be used for training LLMs on trillions of tokens.&lt;/p&gt; 
&lt;p&gt;The &lt;em&gt;&lt;a href="https://github.com/NVIDIA/NeMo-Megatron-Launcher"&gt;NVIDIA NeMo Megatron Launcher&lt;/a&gt;&lt;/em&gt; &lt;em&gt;(NeMo Launcher)&lt;/em&gt; is a cloud-native tool for launching end-to-end NeMo FW training jobs. The Launcher is designed to be a simple and easy-to-use tool for launching NeMo FW training jobs on CSPs or on-prem clusters.&lt;/p&gt; 
&lt;p&gt;The launcher is typically used from a head node and only requires a minimal python installation. Launcher will generate (and launch) the submission scripts needed by the cluster scheduler and will also organize and store the job results. Launcher includes tested configuration files, but anything in a configuration file can be modified by the user. Launcher supports many functionalities, from cluster setup and configuration, data downloading, curating and model training setup, evaluation and deployment.&lt;/p&gt; 
&lt;h2&gt;Steps to create cluster and launch jobs&lt;/h2&gt; 
&lt;p&gt;This guide assumes that Amazon EC2 P5 instances are available in us-east-2 for you, but this may vary depending on your account / capacity reservation and the region you plan to use.&lt;/p&gt; 
&lt;h3&gt;Step 0: Install ParallelCluster CLI&lt;/h3&gt; 
&lt;p&gt;Installing CLI Instructions: &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-virtual-environment.html"&gt;https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-virtual-environment.html&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If &lt;code&gt;virtualenv&lt;/code&gt; is not installed, install &lt;code&gt;virtualenv&lt;/code&gt; using &lt;code&gt;pip3&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;python3 -m pip install --upgrade pip
python3 -m pip install --user --upgrade virtualenv
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Create a virtual environment, name it, and activate it.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;python3 -m virtualenv ~/apc-ve
source ~/apc-ve/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Install ParallelCluster into your virtual environment.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;(apc-ve)~$ python3 -m pip install "aws-parallelcluster" --upgrade --user&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Install Node Version Manager (&lt;code&gt;nvm&lt;/code&gt;) and the latest Long-Term Support (LTS) &lt;code&gt;Node.js&lt;/code&gt; version.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash
$ chmod ug+x ~/.nvm/nvm.sh
$ source ~/.nvm/nvm.sh
$ nvm install --lts
$ node --version
$ export PATH=$PATH:~/.local/bin
$ pcluster version
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 1: Create a VPC and Security Groups&lt;/h3&gt; 
&lt;div id="attachment_3704" style="width: 931px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3704" loading="lazy" class="size-full wp-image-3704" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/22/IMG-2024-05-22-13.06.54.png" alt="Figure 1 – A VPC configuration in a new account with one public subnet and one private subnet in the target region. The P5 instance topology is defined to have 32 ENI cards of 100Gbps each." width="921" height="433"&gt;
 &lt;p id="caption-attachment-3704" class="wp-caption-text"&gt;Figure 1 – A VPC configuration in a new account with one public subnet and one private subnet in the target region. The P5 instance topology is defined to have 32 ENI cards of 100Gbps each.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;If you’re using a new AWS account, your VPC configuration will consist of one public subnet and a private subnet in the target region. We &lt;a href="https://github.com/aws/aws-ofi-nccl/blob/master/topology/p5.48xl-topo.xml"&gt;define the P5 instance topology&lt;/a&gt; to have a total of 32 Elastic Network Interfaces (ENI) cards of 100Gbps each. To handle 32 ENIs, compute instances need to be placed into a private subnet, otherwise your cluster will fail in creation because a public IP is not automatically assigned on instances with multiple NICs. You can find more information about deploying a VPC for ParallelCluster &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/1.architectures/1.vpc_network#vpc-cloudformation-stacks"&gt;in our GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unless you’re comfortable deploying a private subnet and setting the routes and security groups, we recommend that you deploy a custom VPC using the CloudFormation template called ML-VPC. This template is region-agnostic and enables you to create a VPC with the required network architecture to run your workloads.&lt;/p&gt; 
&lt;p&gt;You can follow the steps to deploy your new VPC:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Deploy this &lt;a href="https://console.aws.amazon.com/cloudformation/home?#/stacks/quickcreate?templateURL=https%3A%2F%2Fawsome-distributed-training.s3.amazonaws.com%2Ftemplates%2F1.vpc-multi-az.yaml&amp;amp;stackName=ML-VPC"&gt;CloudFormation template&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;You’ll see a list of parameters: 
  &lt;ul&gt; 
   &lt;li&gt;In &lt;strong&gt;Name of your VPC&lt;/strong&gt;, you can leave it as default LargeScaleVPC.&lt;/li&gt; 
   &lt;li&gt;For &lt;strong&gt;Availability zones&lt;/strong&gt; (AZ’s), select your desired AZ. This will deploy a public and private subnet in that AZ. &lt;em&gt;If you’re using a capacity reservation (CR), use the AZ specific to the CR.&lt;/em&gt;&lt;/li&gt; 
   &lt;li&gt;Keep the &lt;strong&gt;S3 Endpoint&lt;/strong&gt;, &lt;strong&gt;Public Subnet&lt;/strong&gt; and &lt;strong&gt;DynamoDB Endpoint&lt;/strong&gt; as true.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Check the acknowledgement box in the &lt;strong&gt;Capabilities&lt;/strong&gt; section and create the stack.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;It’ll take a few minutes to deploy your network architecture. The stack outputs tab will contain IDs of your security groups and subnets. You’ll need to keep this information handy for the next step.&lt;/p&gt; 
&lt;h3&gt;Step 2: Build ParallelCluster custom AMI&lt;/h3&gt; 
&lt;p&gt;We used the following configuration (which we saved as image_build.yaml), for adding ParallelCluster dependencies on top of the &lt;a href="https://aws.amazon.com/releasenotes/aws-deep-learning-base-gpu-ami-ubuntu-22-04/"&gt;AWS Deep Learning Base GPU AMI&lt;/a&gt;. This Deep Learning AMI page also contains a command for retrieving the AMI ID (search for “Query AMI-ID with AWSCLI”). You can specify which AMI to use as base depending on your requirements, and then install ParallelCluster dependencies on top of it following &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/building-custom-ami-v3.html"&gt;the tutorial in our service documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Now ensure that the &lt;code&gt;SubnetId&lt;/code&gt;, &lt;code&gt;ParentImage&lt;/code&gt;, and &lt;code&gt;SecurityGroupIds&lt;/code&gt; are set to the values exported when deploying your network architecture in Step 1. Save the configuration to the file &lt;code&gt;image_build.yaml&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Build:
 InstanceType: p5.48xlarge
 SubnetId: subnet-xxxxxx
 ParentImage: ami-xxxxxx
 SecurityGroupIds:
 - sg-xxxxxx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We specify security groups and subnet specific to private subnet (in the required AZ) created as a result of Step 1.&lt;/p&gt; 
&lt;p&gt;Now launch the ParallelCluster custom AMI creation job like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster build-image --image-id p5-pcluster-dlgpu-baseami --image-configuration image_build.yaml --region us-east-2&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This step takes about 30-45 minutes to complete.&lt;/p&gt; 
&lt;h3&gt;Step 3: Launch ParallelCluster&lt;/h3&gt; 
&lt;p&gt;Once the AMI is ready, it’s time to launch your cluster.&lt;/p&gt; 
&lt;p&gt;Here’s a reference configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Image:
 Os: ubuntu2204
HeadNode:
 InstanceType: m5.8xlarge
 LocalStorage:
   RootVolume:
     Size: 200
     DeleteOnTermination: true
 Networking:
   SubnetId: subnet-xxxxxx
 Ssh:
   KeyName: &amp;lt;key-name&amp;gt;
 Iam:
   S3Access:
     - BucketName: &amp;lt;s3-bucket-name&amp;gt;
 CustomActions:
   OnNodeConfigured:
     Script: https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/authentication-credentials/multi-runner/postinstall.sh
     Args:
       - https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/authentication-credentials/pyxis/postinstall.sh
       - -/fsx
       - https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh
Scheduling:
 Scheduler: slurm
 SlurmQueues:
 - Name: compute
   ComputeSettings:
     LocalStorage:
       RootVolume:
         Size: 200
   ComputeResources:
   - Name: compute
     InstanceType: p5.48xlarge
     MinCount: 2
     MaxCount: 2
     CapacityReservationTarget:
           CapacityReservationId: cr-xxxxxx
     Efa:
       Enabled: true
   Networking:
     PlacementGroup:
       Enabled: true
     SubnetIds:
       - subnet-xxxxxx
   CustomActions:
     OnNodeConfigured:
       Script: https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/authentication-credentials/multi-runner/postinstall.sh
       Args:
         - https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/authentication-credentials/pyxis/postinstall.sh
         - -/fsx
   Image:
     CustomAmi: ami-xxxxxx
SharedStorage:
 - MountDir: /fsx
   Name: FSxDataMount
   StorageType: FsxLustre
   FsxLustreSettings:
     StorageCapacity: 1200
     DeploymentType: PERSISTENT_2
Monitoring:
 DetailedMonitoring: true
 Logs:
   CloudWatch:
     Enabled: true # good for debug
 Dashboards:
   CloudWatch:
     Enabled: false # provide basic dashboards
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, you should:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Update the &lt;code&gt;Region&lt;/code&gt; to your intended region&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;Networking:SubnetId&lt;/code&gt; (for the head node) to the &lt;strong&gt;public subnet&lt;/strong&gt; you created in Step 1.&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;Ssh:KeyName&lt;/code&gt;, to your specific key.&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;MinCount&lt;/code&gt; and &lt;code&gt;MaxCount&lt;/code&gt; to the desired numbers of instances you’d like in the cluster.&lt;/li&gt; 
 &lt;li&gt;Set the &lt;code&gt;CapacityReservationId&lt;/code&gt;, if any.&lt;/li&gt; 
 &lt;li&gt;Update the compute node &lt;code&gt;Networking:SubnetId&lt;/code&gt; to private subnet you created in Step 1.&lt;/li&gt; 
 &lt;li&gt;Optionally: 
  &lt;ol&gt; 
   &lt;li&gt;Set the &lt;code&gt;Iam:S3Access:BucketName&lt;/code&gt;, if you’d like the compute instances to be able to access an Amazon Simple Storage Service (Amazon S3) bucket.&lt;/li&gt; 
   &lt;li&gt;Update the &lt;code&gt;ImportPath&lt;/code&gt; within &lt;code&gt;SharedStorage&lt;/code&gt; to an Amazon S3 bucket URI, if you’d like to initialize your Lustre storage with data from an S3 bucket.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There’s more information about postinstall scripts and a library of especially useful ones &lt;a href="https://github.com/aws-samples/aws-parallelcluster-post-install-scripts/tree/main"&gt;in our GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can launch the cluster-creation process using a command like this once you’ve chosen a name for your cluster:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster --cluster-name &amp;lt;cluster-name&amp;gt; --cluster-configuration pcluster_config.yaml --region us-east-2  --rollback-on-failure False&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Some additional commands you’ll need later:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To destroy a cluster: &lt;code&gt;pcluster delete-cluster -n &amp;lt;cluster-name&amp;gt; -r us-east-2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;To SSH to the head node: &lt;code&gt;pcluster ssh -n &amp;lt;cluster-name&amp;gt; -r us-east-2 -i ssh_key.pem&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;You can use &lt;code&gt;sinfo&lt;/code&gt; on the head node to validate the cluster.&lt;/li&gt; 
 &lt;li&gt;You can get cluster status, too: &lt;code&gt;pcluster describe-cluster -n &amp;lt;cluster-name&amp;gt; -r us-east-2&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once your cluster is launched, you can validate some important elements by checking package versions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Elastic fabric adapter (EFA) – this is the custom-built, high-speed network interface into the Amazon EC2 fabric for running HPC and distributed machine-learning codes:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ fi_info --version
fi_info: 1.18.2amzn1.0
libfabric: 1.18.2amzn1.0
libfabric api: 1.18
 
$ cat /opt/amazon/efa_installed_packages
# EFA installer version: 1.24.1
# Debug packages installed: no
# Packages installed:
efa-config_1.15_all efa-profile_1.5_all libfabric-aws-bin_1.18.1_amd64 libfabric-aws-dev_1.18.1_amd64 libfabric1-aws_1.18.1_amd64 openmpi40-aws_4.1.5-1_amd64 ibacm_46.0-1_amd64 ibverbs-providers_46.0-1_amd64 ibverbs-utils_46.0-1_amd64 infiniband-diags_46.0-1_amd64 libibmad-dev_46.0-1_amd64 libibmad5_46.0-1_amd64 libibnetdisc-dev_46.0-1_amd64 libibnetdisc5_46.0-1_amd64 libibumad-dev_46.0-1_amd64 libibumad3_46.0-1_amd64 libibverbs-dev_46.0-1_amd64 libibverbs1_46.0-1_amd64 librdmacm-dev_46.0-1_amd64 librdmacm1_46.0-1_amd64 rdma-core_46.0-1_amd64 rdmacm-utils_46.0-1_amd64 efa_2.5.0-1.amzn1_amd64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We have a &lt;a href="https://github.com/aws/aws-ofi-nccl/blob/master/doc/efa-env-var.md"&gt;document&lt;/a&gt; to help streamline EFA environment variables in your Docker image and scripts, along with some additional guidance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Message-passing interface (MPI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ mpirun --version
mpirun (Open MPI) 4.1.6&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;AWS-OFI-NCCL plugin&lt;/li&gt; 
 &lt;li style="list-style-type: none"&gt; 
  &lt;ol&gt; 
   &lt;li&gt;NCCL is the NVIDIA Collective Communications Library; it provides inter-GPU communication primitives. &lt;a href="https://github.com/aws/aws-ofi-nccl"&gt;AWS-OFI-NCCL&lt;/a&gt; is a plug-in that enables developers on AWS to use libfabric as a network provider while running NCCL based applications.&lt;/li&gt; 
   &lt;li&gt;The plugin is the easiest way to get the version is by running a NCCL (NVIDIA Collective Communications Library) test. You can build the tests using &lt;a href="https://github.com/NVIDIA/nccl-tests"&gt;these instructions&lt;/a&gt;, and look for logs reporting the version. You should see something like: &lt;code&gt;Initializing aws-ofi-nccl 1.7.4-aws&lt;/code&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;NCCL version – you should also find &lt;code&gt;NCCL version 2.18.5+cuda12.2&lt;/code&gt; in the logs of the NCCL test.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To resolve issues regarding Cluster Creation, please refer &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/troubleshooting-v3-cluster-deployment.html"&gt;to our troubleshooting documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 4: Cluster validation&lt;/h3&gt; 
&lt;p&gt;NeMo Launcher offers a &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/cloudserviceproviders.html#cluster-validation"&gt;cluster validation script&lt;/a&gt; which runs NVIDIA &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cloud-native/containers/dcgm"&gt;DCGM&lt;/a&gt; (Data Center GPU Manager) tests and NCCL tests. The DCGM is a suite of tools for managing and monitoring NVIDIA GPUs in cluster environments. All DCGM functionality is available via the dcgmi, which is the DCGM command-line utility.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd /path/to/NeMoMegatronLauncher/csp_tools/aws &amp;amp;&amp;amp; bash cluster_validation.sh --nodes=2&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To resolve issues regarding Cluster Validation, please refer to the “Troubleshooting” section later.&lt;/p&gt; 
&lt;h3&gt;Step 5: Launch GPT training job&lt;/h3&gt; 
&lt;p&gt;Use &lt;code&gt;enroot&lt;/code&gt; to import the container to local:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;enroot import --output nemo_megatron_training.sqsh dockerd://nvcr.io/ea-bignlp/nemofw-training:23.07-py3&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, download the vocab and merges files:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json
wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here is the &lt;code&gt;config.yaml&lt;/code&gt; for a GPT 20B training job. Make a copy of this file and make changes as we describe next:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;defaults:
  - _self_
  - cluster: bcm  # Leave it as bcm even if using bcp. It will be ignored for bcp.
  - data_preparation: gpt3/download_gpt3_pile
  - training: gpt3/20b
  - conversion: gpt3/convert_gpt3
  - fine_tuning: null
  - prompt_learning: null
  - adapter_learning: null
  - ia3_learning: null
  - evaluation: gpt3/evaluate_all
  - export: gpt3/export_gpt3
  - override hydra/job_logging: stdout

hydra:
  run:
    dir: .
  output_subdir: null

debug: False

stages:
  - training
  # - conversion
  # - evaluation
  # - export

cluster_type: bcm  # bcm or bcp. If bcm, it must match - cluster above.
launcher_scripts_path: /home/ubuntu/NeMo-Megatron-Launcher/launcher_scripts  # Path to NeMo Megatron Launch scripts, should ends with /launcher_scripts
data_dir: /fsx/gpt3_dataset  # Location to store and read the data.
base_results_dir: /fsx/gpt3_dataset/results  # Location to store the results, checkpoints and logs.
container_mounts:
  - /home/ubuntu/NeMo-Megatron-Launcher/csp_tools/aws/:/nccl
container: /home/ubuntu/NeMo-Megatron-Launcher/nemo_megatron_training.sqsh

wandb_api_key_file: null  # File where the w&amp;amp;B api key is stored. Key must be on the first line.

env_vars:
  NCCL_TOPO_FILE: /nccl/topo.xml # Should be a path to an XML file describing the topology
  UCX_IB_PCI_RELAXED_ORDERING: null # Needed to improve Azure performance
  NCCL_IB_PCI_RELAXED_ORDERING: null # Needed to improve Azure performance
  NCCL_IB_TIMEOUT: null # InfiniBand Verbs Timeout. Set to 22 for Azure
  NCCL_DEBUG: INFO # Logging level for NCCL. Set to "INFO" for debug information
  NCCL_PROTO: simple # Protocol NCCL will use. Set to "simple" for AWS
  TRANSFORMERS_OFFLINE: 1
  NCCL_AVOID_RECORD_STREAMS: 1

# GPU Mapping
numa_mapping:
  enable: True  # Set to False to disable all mapping (performance will suffer).
  mode: unique_contiguous  # One of: all, single, single_unique, unique_interleaved or unique_contiguous.
  scope: node  # Either node or socket.
  cores: all_logical  # Either all_logical or single_logical.
  balanced: True  # Whether to assing an equal number of physical cores to each process.
  min_cores: 1  # Minimum number of physical cores per process.
  max_cores: 8  # Maximum number of physical cores per process. Can be null to use all available cores.

# Do not modify below, use the values above instead.
data_preparation_config: ${hydra:runtime.choices.data_preparation}
training_config: ${hydra:runtime.choices.training}
fine_tuning_config: ${hydra:runtime.choices.fine_tuning}
prompt_learning_config: ${hydra:runtime.choices.prompt_learning}
adapter_learning_config: ${hydra:runtime.choices.adapter_learning}
ia3_learning_config: ${hydra:runtime.choices.ia3_learning}
evaluation_config: ${hydra:runtime.choices.evaluation}
conversion_config: ${hydra:runtime.choices.conversion}
export_config: ${hydra:runtime.choices.export}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this &lt;code&gt;config.yaml&lt;/code&gt; file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Update the &lt;code&gt;launcher_scripts_path&lt;/code&gt; to the absolute path for NeMo Launcher’s launcher_scripts&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;data_dir&lt;/code&gt; to wherever the data is residing.&lt;/li&gt; 
 &lt;li&gt;Update container with path to &lt;code&gt;sqsh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;base_results_dir&lt;/code&gt; to point to the directory where you’d like to organize the results.&lt;/li&gt; 
 &lt;li&gt;Update &lt;code&gt;NCCL_TOPO_FILE&lt;/code&gt; to point to a xml &lt;a href="https://github.com/aws/aws-ofi-nccl/blob/master/topology/p5.48xl-topo.xml"&gt;specific to P5&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Optionally update &lt;code&gt;container_mounts&lt;/code&gt; to mount a specific directory from host into container.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can find some example configuration files &lt;a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/config.yaml"&gt;in our GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To launch the job:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd /path/to/NeMoMegatronLauncher/launcher_scripts &amp;amp;&amp;amp; python main.py &amp;amp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once you launch this job, you can look at the &lt;code&gt;.log&lt;/code&gt; file (of format &lt;code&gt;log-nemo-megatron-&amp;lt;model_name&amp;gt;_&amp;lt;date&amp;gt;.log&lt;/code&gt;) to track the logs of the training job. Additionally, you can use a &lt;code&gt;.err&lt;/code&gt; file (of format &lt;code&gt;log-nemo-megatron-&amp;lt;model_name&amp;gt;_&amp;lt;date&amp;gt;.err&lt;/code&gt;) to track the errors and warnings, if any, of your training job. If you set up TensorBoard, you can also check the events file (of format &lt;code&gt;events.out.tfevents.&amp;lt;compute_details&amp;gt;&lt;/code&gt;) to look over the loss curves, learning rates and other parameters that NeMo tracks. For more information on this, refer to &lt;a href="https://www.tensorflow.org/tensorboard/get_started"&gt;the TensorBoard documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note: The files mentioned above are located in the directory specified by you in the &lt;code&gt;base_results_dir&lt;/code&gt; field in the &lt;code&gt;config.yaml&lt;/code&gt; file above.&lt;/p&gt; 
&lt;p&gt;This is what an example &lt;code&gt;.log&lt;/code&gt; file looks like. Note, this is only a part of the entire log file (“…” below entails omitted parts of the output), until step 3 of the training job (the actual logs contain logs until step 60000000):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;    ************** Experiment configuration ***********
…
[NeMo I 2024-01-18 00:20:43 exp_manager:394] Experiments will be logged at /shared/backup120820231021/gpt_results/gpt3_126m_8_fp8_01172024_1619/results
[NeMo I 2024-01-18 00:20:43 exp_manager:835] TensorboardLogger has been set up
…
[NeMo I 2024-01-18 00:21:13 lr_scheduler:910] Scheduler "&amp;lt;nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fe75b3387f0&amp;gt;" 
    will be used during training (effective maximum steps = 60000000) - 
    Parameters : 
…
Sanity Checking DataLoader 0:   0%|                | 0/2 [00:00&amp;lt;?, ?it/s]
Sanity Checking DataLoader 0: 100%|████████| 2/2 [00:07&amp;lt;00:00,  3.92s/it]
…                                                      
Epoch 0: :   0%| | 1/60000000 [00:35&amp;lt;583621:51:56, v_num=, reduced_train_
Epoch 0: :   0%| | 2/60000000 [00:36&amp;lt;303128:00:21, v_num=, reduced_train_
Epoch 0: :   0%| | 3/60000000 [00:36&amp;lt;202786:11:11, v_num=, reduced_train_
…
&lt;/code&gt;&lt;/pre&gt; 
&lt;div id="attachment_3705" style="width: 949px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3705" loading="lazy" class="size-full wp-image-3705" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/22/IMG-2024-05-22-13.25.45.png" alt="Figure 2 – Sample output graphs from our run using TensorBoard." width="939" height="301"&gt;
 &lt;p id="caption-attachment-3705" class="wp-caption-text"&gt;Figure 2 – Sample output graphs from our run using TensorBoard.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Cluster creation failed&lt;/h3&gt; 
&lt;p&gt;Bringing up a cluster can fail for many reasons. The easiest way to debug is to create a cluster with &lt;code&gt;--rollback-on-failure False&lt;/code&gt;. Then you can see information in the AWS CloudFormation console detailing why the cluster creation failed. Even more detailed information will be in the logs on the head node which you can find in: &lt;code&gt;/var/log/cfn-init.log /var/log/cloud-init.log /var/log/cloud-init-output.log&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The most common reason for cluster failure is that you may not have access to the target EC2 capacity. You’ll see this in the &lt;code&gt;/var/log/parallelcluster/clustermgtd&lt;/code&gt; log on the head node, or in CloudFormation.&lt;/p&gt; 
&lt;h3&gt;Cluster Validation Issues&lt;/h3&gt; 
&lt;h3&gt;1 – DCGMI output&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;Error: Unable to complete diagnostic for group 2147483647. Return: (-21) Host engine connection invalid/disconnected.
srun: error: compute-st-compute-1: task 0: Exited with exit code 235
Error: Unable to complete diagnostic for group 2147483647. Return: (-21) Host engine connection invalid/disconnected.
srun: error: compute-st-compute-2: task 1: Exited with exit code 235
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Resolution&lt;/strong&gt;: The DCGM container may not be accessible from &lt;a href="https://docs.nvidia.com/ngc/index.html"&gt;NGC&lt;/a&gt;. Try converting the DCGM container to a local &lt;code&gt;.sqsh&lt;/code&gt; file using &lt;code&gt;enroot&lt;/code&gt; and pointing the validation script (&lt;code&gt;csp_tools/aws/dcgmi_diag.sh&lt;/code&gt;) to this local file, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;enroot import --output dcgm.sqsh 'docker://$oauthtoken@nvcr.io#nvidia/cloud-native/dcgm:2.3.5-1-ub
i8' 
srun --container-image=dcgm.sqsh bash -c "dcgmi diag -r 3"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2 – PMIX Error in NCCL Logs&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;[compute-st-compute-2:201457] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Resolution&lt;/strong&gt;: This is a non-fatal error. Try adding export &lt;code&gt;PMIX_MCA_gds=^ds12&lt;/code&gt; to the &lt;code&gt;csp_tools/aws/nccl.sh&lt;/code&gt; script.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog post, we’ve shown you how to leverage the AWS ParallelCluster and the NVIDIA NeMo Megatron Framework to enable large-scale Large Language Model (LLM) training on AWS P5 instances. Together, AWS ParallelCluster and the NVIDIA NeMo Megatron Framework can empower researchers and developers to train LLMs on trillions of tokens, scaling to thousands of GPUs, which means accelerating time-to-market for cutting-edge natural language processing (NLP) applications.&lt;/p&gt; 
&lt;p&gt;To learn more about training GPT3 NeMo Megatron on Slurm, refer to &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher"&gt;AWS Sample&lt;/a&gt;s. To learn more about ParallelCluster and Nemo Megatron, check out the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html"&gt;ParallelCluster User Guide&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA/NeMo-Megatron-Launcher"&gt;NeMo Megatron Launcher&lt;/a&gt;, and &lt;a href="https://www.mlworkshops.com/01-getting-started/05-pcui-connect.html"&gt;Parallel Cluster UI&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Building an AI simulation assistant with agentic workflows</title>
		<link>https://aws.amazon.com/blogs/hpc/building-an-ai-simulation-assistant-with-agentic-workflows/</link>
		
		<dc:creator><![CDATA[Sam Bydlon]]></dc:creator>
		<pubDate>Tue, 28 May 2024 14:57:10 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">a1c68c7ee69e75af5f1ce9d202a580c62bf691db</guid>

					<description>Simulations provide critical insights but running them takes specialized people, which can slow everyone down. We show how a Simulation Assistant can use LLMs and agents to start these workflows via chat so you can get results sooner.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3673" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/20/building-an-AI-simulation-assistant.png" alt="" width="380" height="212"&gt;Simulations have become indispensable tools which enable organizations to predict outcomes, evaluate risks, and make informed decisions. Simulations provide valuable insights that drive strategic decision-making – running the gamut from supply chain optimization to exploration of design alternatives for products and processes.&lt;/p&gt; 
&lt;p&gt;But running and analyzing simulations can be a time-consuming task because it requires specialized teams of data scientists, analysts, and subject matter experts. In the manufacturing sector, these experts are in high demand to model and optimize complex production processes. This regularly leads to backlogs and delays in obtaining critical insights. In the healthcare industry, specialized teams of epidemiologists and statisticians run the simulations for infectious disease modeling that public health officials need to make decisions. The limited bandwidth of these specialists creates bottlenecks and inefficiencies that impact the ability to rapidly respond to emerging health crises in a data-driven manner.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll examine an generative AI-based “Simulation Assistant” demo application built using &lt;a href="https://python.langchain.com/v0.1/docs/modules/agents/"&gt;LangChain Agents&lt;/a&gt; and &lt;a href="https://www.anthropic.com/"&gt;Anthropic’s&lt;/a&gt; recently released Claude V3 large language model (LLM) on Amazon Bedrock.&lt;/p&gt; 
&lt;p&gt;By leveraging the latest advancements in LLMs and AWS, we’ll show you how to streamline and democratize your simulation workflows using a scalable and serverless architecture for an application with a chatbot-style interface. This will allow users to launch and interact with simulations using natural language prompts.&lt;/p&gt; 
&lt;h2&gt;How does this help experts?&lt;/h2&gt; 
&lt;p&gt;The Simulation Assistant demo offers a blueprint for providing significant value to organizations in two key ways. It:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Democratizes simulation-driven problem solving: &lt;/strong&gt;While regulated industries may still require certified personnel for final sign-off, this solution demonstrates a way to democratize simulation use beyond specialist teams. By enabling knowledgeable personnel across functions, such as analysts, managers, and decision-makers, to launch and analyze simulations under the guidance of experts, organizations can increase the utilization of simulation capabilities and free up the bandwidth of their simulation experts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhances efficiency for simulation experts: &lt;/strong&gt;Allowing a wider user-base to run routine simulations lets experts focus on high value tasks like performance tuning or building new simulations. Streamlined, automated workflows accessible through a single chatbot interface improves productivity, standardizes processes, enables knowledge sharing, and increases result reliability.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Whether you’re a business analyst, product manager, researcher, or a simulation expert, this demonstration offers an intuitive and efficient way to harness the power of simulations by leveraging the capabilities of generative AI through Amazon Bedrock and the scalability of AWS – driving innovation and operational excellence across diverse industries.&lt;/p&gt; 
&lt;h2&gt;Solution overview&lt;/h2&gt; 
&lt;div id="attachment_3742" style="width: 1511px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3742" loading="lazy" class="size-full wp-image-3742" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/Sim_Assist_Demo_May24update-AWS-Architecture.drawio.png" alt="Figure 1 – Architectural diagram for Simulation Assistant application. A containerized Streamlit web app is deployed via a load-balanced AWS Fargate service. The web app instantiates an LLM-based agent with access to seven tools. The retriever tool provides RAG capabilities using Amazon Kendra. Others tools enable the agent to perform a custom mathematical transform, invoke a simple inflation simulation executed using AWS Lambda, invoke a set of containerized investment portfolio simulations using AWS Batch that store results in an Amazon DynamoDB table, and analyze a batch of simulation results by generating plots." width="1501" height="1163"&gt;
 &lt;p id="caption-attachment-3742" class="wp-caption-text"&gt;Figure 1 – Architectural diagram for Simulation Assistant application. A containerized Streamlit web app is deployed via a load-balanced AWS Fargate service. The web app instantiates an LLM-based agent with access to seven tools. The retriever tool provides RAG capabilities using Amazon Kendra. Others tools enable the agent to perform a custom mathematical transform, invoke a simple inflation simulation executed using AWS Lambda, invoke a set of containerized investment portfolio simulations using AWS Batch that store results in an Amazon DynamoDB table, and analyze a batch of simulation results by generating plots.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 1 depicts the architecture of the Simulation Assistant application. A web application, built using &lt;a href="https://streamlit.io/"&gt;Streamlit&lt;/a&gt;, serves as the user interface. Streamlit is an open-source Python library that allows you to create interactive web applications for machine learning and data science use cases. We’ve containerized this app using Docker and stored it in an &lt;a href="https://aws.amazon.com/ecr/"&gt;Amazon Elastic Container Registry (ECR)&lt;/a&gt; repository.&lt;/p&gt; 
&lt;p&gt;The containerized web application is deployed as a load-balanced AWS Fargate Service within an &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon Elastic Container Service (ECS)&lt;/a&gt; cluster. &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt; is a serverless compute engine that allows you to run containers without managing servers or clusters. By using Fargate, the Simulation Assistant application can scale its compute resources up or down automatically based on the incoming traffic, ensuring optimal performance and cost-efficiency.&lt;/p&gt; 
&lt;p&gt;The web application is fronted by an &lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html"&gt;Application Load Balancer (ALB)&lt;/a&gt;. The ALB distributes incoming traffic across multiple targets, like Fargate tasks, in a balanced manner. This load balancing mechanism ensures that user requests are efficiently handled, even during periods of high traffic, by dynamically routing requests to available container instances.&lt;/p&gt; 
&lt;h2&gt;Life cycle of a request&lt;/h2&gt; 
&lt;p&gt;When a user accesses the Simulation Assistant application, their request is received by the ALB, which then forwards the request to one of the healthy Fargate tasks running the Streamlit web application. This serverless deployment approach, combined with the load-balancing capabilities of the ALB, provides a highly available and scalable architecture for the Simulation Assistant, allowing it to handle varying levels of user traffic without the need for manually provisioning and managing servers.&lt;/p&gt; 
&lt;p&gt;The Streamlit web application acts as the central hub, orchestrating the interaction between different AWS services to enable seamless simulation capabilities for users. Within the Streamlit app, we’ve used &lt;a href="https://aws.amazon.com/bedrock/"&gt;Amazon Bedrock&lt;/a&gt; to process user queries by leveraging state-of-the-art language models. Bedrock is a fully-managed service that makes these art foundation models from both Amazon and leading AI startups available via a unified API, while abstracting away complex model management.&lt;/p&gt; 
&lt;p&gt;For simple simulations, like price inflation scenarios, the Streamlit app integrates with &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; functions. These serverless functions can encapsulate lightweight simulation logic, allowing for efficient execution and scalability without the need for provisioning and managing dedicated servers.&lt;/p&gt; 
&lt;p&gt;Additionally, we’re also leveraging &lt;a href="https://aws.amazon.com/kendra/"&gt;Amazon Kendra&lt;/a&gt;, an intelligent search service, to enable retrieval augmented generation (RAG). Amazon Kendra indexes and searches through documents stored in an &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon S3&lt;/a&gt; bucket, acting as a source repository. This integration empowers the application to provide relevant information from existing documents, enhancing the simulation capabilities and enabling more informed decision-making.&lt;/p&gt; 
&lt;p&gt;For more computationally intensive simulations, like running sets of investment portfolio simulations in a Monte Carlo-style manner, the Simulation Assistant uses &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;. AWS Batch is a fully managed batch processing service that efficiently runs batch computing workloads across AWS resources. The Simulation Assistant submits jobs to AWS Batch, which then dynamically provisions the compute resources needed to run them in parallel, enabling faster execution times and scalability.&lt;/p&gt; 
&lt;p&gt;Once the simulations are complete, the results are stored in an &lt;a href="https://aws.amazon.com/dynamodb/"&gt;Amazon DynamoDB&lt;/a&gt; database, a fully managed NoSQL database service. DynamoDB provides fast and predictable performance with seamless scalability, making it well-suited for storing and retrieving simulation data efficiently. Furthermore, the application integrates with &lt;a href="https://aws.amazon.com/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt;, a serverless event bus service. When a simulation batch is finished, EventBridge triggers a notification, which is sent to the user via email using &lt;a href="https://aws.amazon.com/sns/"&gt;Amazon Simple Notification Service (SNS)&lt;/a&gt;. This notification system keeps users informed about the completion of their simulation requests, allowing them to promptly access and analyze the results.&lt;/p&gt; 
&lt;h2&gt;LLM-based “agents” with “tools”&lt;/h2&gt; 
&lt;p&gt;The Streamlit web application houses the logic of the key technological concept underlying the Simulation Assistant application, the enablement of what is called “&lt;em&gt;agentic behavior&lt;/em&gt;” of an LLM. The precise definition of an LLM agent is elusive, because the field is relatively new and rapidly evolving. But the general idea is to augment the capabilities of LLMs by enabling the models to break down tasks into individual steps, make plans, and take actions including the use of tools to solve specific tasks, and even work together as a team of multiple agents that can collaborate and influence each other.&lt;/p&gt; 
&lt;p&gt;One design pattern for enabling agentic behavior of is called “&lt;a href="https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/?ref=dl-staging-website.ghost.io"&gt;Tool Use&lt;/a&gt;”, in which an LLM is taught (through prompt engineering or fine-tuning) how to trigger pieces of additional software, wrapped in a standardized form like a function call. These additional pieces of software are called “tools”. The Simulation Assistant employs tools to augment the behavior of an underlying foundation model. In our demo, the underlying foundation model is &lt;a href="https://www.anthropic.com/news/claude-3-family"&gt;Claude V3 Sonnet&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Tools help LLMs solve problems that are not reliably solved by direct generation using the underlying transformer network. LLMs are demonstrating incredible ability at solving problems and performing mathematical reasoning – try asking Claude V3 Sonnet to “simulate the price of milk over the next 20 years with a dynamic inflation rate where the mean is 4% and standard deviation 2%”. But their ability to simulate more complex systems like financial markets or the spread of wildfires is still (for now) best left to trusted simulation codes. By introducing tools that can execute those trusted codes and instructing the LLM on how and when tools should be used, LLMs can gain profound new abilities.&lt;/p&gt; 
&lt;p&gt;There are many possibilities for what tools can &lt;em&gt;be&lt;/em&gt; and &lt;em&gt;do&lt;/em&gt;, and the application of tools and agentic behavior in general is a concept that will have wide ranging uses cases far beyond simulation. Tools can help LLMs to perform web searches, query a database, or schedule a meeting using your calendar system, just to name a few options.&lt;/p&gt; 
&lt;h2&gt;How we applied agents and tools&lt;/h2&gt; 
&lt;p&gt;The Simulation Assistant instantiates a LangChain agent, based on Claude V3 Sonnet. &lt;a href="https://aws.amazon.com/what-is/langchain/"&gt;LangChain&lt;/a&gt; is an open-source framework for building applications with LLMs. With LangChain &lt;a href="https://python.langchain.com/v0.1/docs/modules/agents/"&gt;Agents&lt;/a&gt; and &lt;a href="https://python.langchain.com/v0.1/docs/modules/tools/"&gt;Tools&lt;/a&gt;, developers can create powerful generative AI-based applications that leverage LLM agentic behavior and integrate with existing systems and workflows.&lt;/p&gt; 
&lt;p&gt;There are &lt;a href="https://python.langchain.com/docs/modules/agents/agent_types/"&gt;several kinds of agents&lt;/a&gt; that can be constructed with LangChain, but not all agent types support tools with multiple inputs. Since simulations often require users to specify an array of input parameters to define the system mechanics to be simulated, we’ll want to choose an agent type that supports multi-input tools and can be used in concert with Amazon Bedrock. The Simulation Assistant demo uses a &lt;a href="https://python.langchain.com/docs/modules/agents/agent_types/structured_chat/"&gt;&lt;em&gt;structured chat agent&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;which satisfies both of these requirements.&lt;/p&gt; 
&lt;p&gt;Building the Simulation Assistant involved addressing several key technical challenges:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Designing and implementing an agent-tool architecture:&lt;/strong&gt; To create an effective agent-tool system, careful design of the tool interfaces and integration with the LangChain agent framework was required. We handled multi-input tools by defining structured input schemas. We enabled communication between the LLM and the tools using LangChain’s &lt;em&gt;tool abstraction layer&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt engineering for agentic behavior:&lt;/strong&gt; Crafting prompts that guide the LLM to exhibit desired agentic behavior was an iterative process. The approach involved exploring the capabilities and limitations of Claude V3 Sonnet, designing prompts that promoted appropriate tool selection and usage, and refining the prompts based on performance in test scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scaling and managing simulation workloads:&lt;/strong&gt; To handle computationally intensive simulations, we designed a scalable architecture using AWS Batch for running simulation jobs. This allowed us to efficiently manage compute resources and reliably execute simulations with varying workloads.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interpreting and visualizing simulation results:&lt;/strong&gt; To help users interpret and visualize simulation outputs, we developed tools that process and summarize simulation data, and integrated visualizations within Simulation Assistant.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We gave the Simulation Assistant agent access to seven tools, including ones designed to perform a custom mathematical transform defined within the Streamlit application, perform simple inflation simulations housed in an AWS Lambda function, launch Monte Carlo-style simulation ensembles via AWS Batch to mimic an investment portfolio and visualize results, and to perform RAG over an internal database of documents.&lt;/p&gt; 
&lt;p&gt;These are simple examples, showing how tools can be built to handle some common elements of simulation workflows. But tools can do a lot more: they can also be designed to prepare configuration files, trigger pre- or post-processing jobs on related data – or even call other specialized LLMs that are able to interpret and summarize the results of simulations.&lt;/p&gt; 
&lt;h2&gt;A sample workflow&lt;/h2&gt; 
&lt;p&gt;When a user enters a natural language query into Simulation Assistant like, “&lt;em&gt;run a set of 100 investment simulations starting at $10,000, with cash flow of $250, for 50 steps&lt;/em&gt;,” we pass the query to Amazon Bedrock inside a prompt &lt;em&gt;specifically engineered&lt;/em&gt; to work well in an agent-tool setting. You can find examples of polished prompts you can use at the &lt;a href="https://smith.langchain.com/hub"&gt;LangChain Hub&lt;/a&gt; – including prompts that &lt;a href="https://smith.langchain.com/hub/hwchase17/structured-chat-agent?organizationId=6e7cb68e-d5eb-56c1-8a8a-5a32467e2996"&gt;work well with structured chat agents&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Behind the scenes, the agentic LLM breaks down the request into steps, decides whether each step can be completed with “bare hands” (i.e. without tools), or whether one of its tools can be used to complete the step. For our example request, the agentic LLM determines that it needs to use a tool that allows it to run batches of investment portfolio simulations, performs the entity extraction of the key parameters like cash flow, and uses these parameters to trigger an AWS Batch job to run the simulations. It does this completely independently.&lt;/p&gt; 
&lt;p&gt;The agent then responds to the user telling them they’ll receive an email when the simulations are complete and provides some helpful information that can be used to analyze the simulation results.&lt;/p&gt; 
&lt;p&gt;Figure 2 is a graphical depiction of this process.&lt;/p&gt; 
&lt;div id="attachment_3671" style="width: 1198px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3671" loading="lazy" class="size-full wp-image-3671" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/20/IMG-2024-05-20-09.11.00.png" alt="Figure 2 – Workflow of an LLM-based agent with tools for batch simulation execution. User requests are formatted into prompts by the application and sent to the LLM, which decomposes the request, selects and triggers the appropriate tool with extracted parameters (run_batch_simulations tool with simulation inputs). Tool results are returned to the LLM for generating a final response displayed to the user, including information for accessing and analyzing simulation outputs." width="1188" height="627"&gt;
 &lt;p id="caption-attachment-3671" class="wp-caption-text"&gt;Figure 2 – Workflow of an LLM-based agent with tools for batch simulation execution. User requests are formatted into prompts by the application and sent to the LLM, which decomposes the request, selects and triggers the appropriate tool with extracted parameters (run_batch_simulations tool with simulation inputs). Tool results are returned to the LLM for generating a final response displayed to the user, including information for accessing and analyzing simulation outputs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For a deeper look into how we used these tools and to see the Simulation Assistant demo in action (including the batch simulation query described above) you can check out the video demo below. This will walk you through some potential interactions with the application, and shows the LLM-based agent interacting with various tools representing different aspects of a simulation workflow. You’ll see the agent list the available tools and provide instructions on how to use them when prompted.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 3 â&#128;&#147; A video showing the Simulation Assistant demo in action. Click to play." width="500" height="281" src="https://www.youtube-nocookie.com/embed/chmq52YX84A?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 3 – A video showing the Simulation Assistant demo in action. Click to play.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The core portion of the demo involves the user asking the agent to run a set of 100 investment portfolio simulations with specific parameters like starting amount, cash flow, and number of steps. The agent interprets this natural language query, extracts the necessary parameters, and triggers an AWS Batch job to execute the simulations in parallel. Once the simulations complete, the agent retrieves the results from a database and visualizes them using another tool.&lt;/p&gt; 
&lt;p&gt;Additionally, the video contrasts the agent’s capabilities with a standard LLM’s response to the same query, so you can see the enhanced abilities provided by the agent-tool architecture. You’ll also notice the agent breaking down a complex problem into steps and leveraging multiple tools in different orders to solve it, demonstrating the flexibility of the approach.&lt;/p&gt; 
&lt;h2&gt;Future Work&lt;/h2&gt; 
&lt;p&gt;While the Simulation Assistant demo achieves a lot, we want to extend the demo in the future to tackle some more technical challenges:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Integrating with existing simulation codebases:&lt;/strong&gt; A key goal of ours is to integrate existing simulation codebases as tools within the agent-tool architecture. This will require a deep understanding of the codebases and, in some cases, modifying them to fit the tool interface requirements. For example, to integrate &lt;a href="https://www.openfoam.com/"&gt;OpenFOAM&lt;/a&gt; (a popular open-source computational fluid dynamics software) as a tool, the approach would involve wrapping OpenFOAM’s solver and utility executables as Python functions, defining input and output schemas, and potentially modifying the codebase to enable programmatic execution and data exchange.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ensuring simulation reproducibility and traceability:&lt;/strong&gt; We’d like to enable comprehensive reproducibility and traceability of the simulation runs by implementing logging mechanisms that track input parameters, intermediate steps, and provide detailed documentation for each simulation execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Establishing guardrails:&lt;/strong&gt; Guardrails and safeguards are crucial to ensure the secure and responsible use of the simulation framework. This may involve setting limits on compute resources, enforcing access controls, and implementing validation checks to prevent potential misuse or unintended consequences. Additionally, ethical considerations should be considered, like ensuring privacy and data protection, avoiding biases, and promoting transparency in the simulation processes.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;This post introduces an AWS-native Simulation Assistant demo that we hope will provide inspiration, and a blueprint, for organizations looking to leverage generative AI and other cutting-edge techniques like agentic LLM behavior to help their businesses.&lt;/p&gt; 
&lt;p&gt;The demo shows the potential of these technologies for revolutionizing simulation workflows across various industries. Using LangChain Agents, Amazon Bedrock, and the scalability of AWS services, the Simulation Assistant demo can offer a glimpse into a future where simulations might be more accessible and interactive. We hope this will let organizations unlock new insights and drive better decision-making.&lt;/p&gt; 
&lt;p&gt;The application of agentic LLM frameworks extend far beyond simulations, and the concept of tools can be applied to a countless number of domains or workflows. By enabling LLMs to interact with external systems, perform computations, and trigger actions, organizations can augment their existing processes in this way, fostering innovation and operational excellence.&lt;/p&gt; 
&lt;p&gt;Solutions like this can also pave the way for new paradigms in human-machine collaboration, amplifying human capabilities and accelerating the pace of discovery.&lt;/p&gt; 
&lt;p&gt;If your organization is interested in exploring how to implement these techniques in concert with your workflows, we encourage you to reach out to your AWS account team, or send an email to &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Announcing: Seqera Containers for the bioinformatics community</title>
		<link>https://aws.amazon.com/blogs/hpc/announcing-seqera-containers-for-the-bioinformatics-community/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Thu, 23 May 2024 15:35:06 +0000</pubDate>
				<category><![CDATA[Featured]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Life Sciences]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Bioinformatics]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[Genomics]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<guid isPermaLink="false">771f9023418ab35b6435bfa6a8543c7119414297</guid>

					<description>Genomics community: rejoice! Seqera and AWS have teamed up to announce Seqera Containers, an open-source, no cost, reliable way to generate containers.</description>
										<content:encoded>&lt;div id="attachment_3815" style="width: 390px" class="wp-caption alignright"&gt;
 &lt;a href="https://youtu.be/SE6ZCvh1ThQ" target="_new" rel="noopener"&gt;&lt;img aria-describedby="caption-attachment-3815" loading="lazy" class="wp-image-3815 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/YT-Containers-just-got-easier.png" alt="Announcing: Seqera Containers for the bioinformatics community" width="380" height="213"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-3815" class="wp-caption-text"&gt;You can watch a demo led by Phil Ewels on our Tech Shorts channel on YouTube.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;This post was contributed by Brendan Bouffler, Head of Developer Relations, HPC Engineering at AWS, Phil Ewels, Snr Product Manager for Open Source at Seqera, and Paolo Di Tommaso, the CTO &amp;amp; co-founder of Seqera.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Life sciences is a rapidly-moving area of research, and you don’t need to look too far for amazing examples where the practitioners of this field have adopted new tools and techniques to solve ever harder problems. Within this domain, bioinformatics has emerged as a crucial discipline, bridging the gap between biology and computer science. Many of the features in AWS Batch have been driven by this community, leading to a close relationship between our teams and the people working at the front lines of research.&lt;/p&gt; 
&lt;p&gt;But while biological data has become more complex and massive, it has also become increasingly personal, and inevitably regulated. The world needed researchers across borders and geographies to share insights and techniques without necessarily moving the data, or sharing the same infrastructure. The conditions were set for another uptake of new technology.&lt;/p&gt; 
&lt;p&gt;Adopting containers for packaging applications turned out to be pivotal for this shift: it revolutionized the way bioinformatics workflows are &lt;a href="https://seqera.io/blog/nextflow-and-aws-batch-inside-the-integration-part-1-of-3/"&gt;developed&lt;/a&gt;, &lt;a href="https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-1-of-2/"&gt;deployed&lt;/a&gt;, &lt;em&gt;and &lt;/em&gt;&lt;a href="https://nf-co.re/"&gt;&lt;em&gt;shared&lt;/em&gt;&lt;/a&gt; – increasing the reproducibility of an analysis. But while they’ve unquestionably simplified the life of bioinformaticians, their creation and usage is not without some friction.&lt;/p&gt; 
&lt;p&gt;Today, in conjunction with our friends at &lt;a href="https://seqera.io/about/"&gt;Seqera&lt;/a&gt;, we’re announcing a project we’re supporting called &lt;strong&gt;Seqera Containers. &lt;/strong&gt;This is a freely-available resource for the entire bioinformatics community (in the cloud or not) which simplifies the container experience – allowing researchers to generate a container for any combination of Conda and PyPI packages at the click of a button. Best of all, Seqera Containers are publicly accessible to everyone, at no-cost.&lt;/p&gt; 
&lt;h2&gt;Building containers on-demand – powered by Wave and AWS&lt;/h2&gt; 
&lt;p&gt;Seqera Containers is not a traditional container registry: users are not expected to browse existing container images or push local images to a remote. Instead, Seqera Containers provides a simple form to choose a combination of packages from Conda or the Python Package Index (PyPI). Clicking “Get Container” returns a Docker or Singularity image URI which can be used immediately.&lt;/p&gt; 
&lt;div id="attachment_3716" style="width: 1328px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3716" loading="lazy" class="size-full wp-image-3716" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/23/IMG-2024-05-23-07.44.30-1.png" alt="Figure 1 - the community wave registry is easy to access, and even easier to use." width="1318" height="942"&gt;
 &lt;p id="caption-attachment-3716" class="wp-caption-text"&gt;Figure 1 – the community wave registry is easy to access, and even easier to use.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Wave containerization service&lt;/h2&gt; 
&lt;p&gt;The beating heart of Seqera Containers is &lt;a href="https://seqera.io/wave/"&gt;Wave&lt;/a&gt; – an &lt;a href="https://github.com/seqeralabs/wave"&gt;open-source&lt;/a&gt; technology built by Seqera for next-generation container provisioning that aims to simplify the usage of containers. Instead of writing Dockerfile scripts to build images, a developer or end user can request a just-in-time container image tailored for the target execution platform using only package names and version numbers from popular software packaging tools &lt;a href="https://conda.io/"&gt;Conda&lt;/a&gt; (including &lt;a href="https://bioconda.github.io/"&gt;Bioconda&lt;/a&gt;&lt;u&gt;),&lt;/u&gt; and the &lt;a href="https://pypi.org/"&gt;Python Package Index&lt;/a&gt; and later, &lt;a href="https://packages.spack.io/"&gt;Spack&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Wave can also compile images on the fly to match your compute infrastructure – including x86 and Arm64-based processors (where upstream packages are available).&lt;/p&gt; 
&lt;p&gt;Wave supports both Docker and Singularity images – and thus any other container technologies that use Docker images, like Podman, Shifter, Sarus, or Charliecloud. Seqera Containers provides an OCI compliant registry for native Singularity image builds and even allows direct .sif image download as a flat file, without needing Singularity installed locally.&lt;/p&gt; 
&lt;p&gt;As part of the build process, Wave conducts a vulnerability scan using the &lt;a href="https://trivy.dev/"&gt;Trivy&lt;/a&gt; security scanner and is able to generate &lt;a href="https://en.wikipedia.org/wiki/Software_supply_chain"&gt;SBOM manifests&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Reproducible container URIs&lt;/h2&gt; 
&lt;p&gt;Building containers on demand is convenient, but for the sake of performance, reproducibility, and provenance, it’s important to use the exact same image for every run. To do this, Wave generates a checksum for the build and can push the built image to a traditional OCI registry. Subsequent requests with the same checksum will be returned directly from the registry.&lt;/p&gt; 
&lt;p&gt;So, when you request an image through the Seqera Containers web interface, most of the time the images will come from the cache – meaning virtually instant access and consistent reuse by you, or anyone in the community. That image cache doesn’t expire, so those images will still be there when you need to reproduce that analysis in a few years’ time.&lt;/p&gt; 
&lt;h2&gt;Built-in support for Nextflow&lt;/h2&gt; 
&lt;p&gt;We hope that this service and these containers will be of use to the entire bioinformatics community. However, the experience of using Seqera Containers will be particularly good for Nextflow users.&lt;/p&gt; 
&lt;p&gt;Using the &lt;a href="https://nextflow.io/docs/latest/wave.html"&gt;Nextflow wave plugin&lt;/a&gt;, pipeline developers can avoid specifying container URIs in their pipeline code entirely. Instead, just naming the software packages in the conda (or, later, spack) declarations and then setting wave.enabled = true and wave.freeze = true is all you need. This instructs Nextflow to request an image from Wave for these packages, store it in the Seqera Containers public registry, and then use this at run time.&lt;/p&gt; 
&lt;p&gt;Wave isn’t restricted to working with Nextflow, there is also a &lt;a href="https://github.com/seqeralabs/wave-cli"&gt;Wave CLI&lt;/a&gt; which anyone can use to generate containers as well as an &lt;a href="https://docs.seqera.io/wave/api"&gt;API&lt;/a&gt;, both of which have all the same functionality as the Nextflow plugin and Seqera Containers web interface.&lt;/p&gt; 
&lt;h2&gt;It’s available now&lt;/h2&gt; 
&lt;div id="attachment_3818" style="width: 390px" class="wp-caption alignright"&gt;
 &lt;a href="https://youtu.be/HjNwpJPMvWQ" target="new" rel="noopener"&gt;&lt;img aria-describedby="caption-attachment-3818" loading="lazy" class="size-full wp-image-3818" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/YT-on-demand-containers-for-bioinformatics.png" alt="Paolo Di Tommaso talks about Nextflow and the revolution it started." width="380" height="213"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-3818" class="wp-caption-text"&gt;Paolo Di Tommaso talks about Nextflow and the revolution it started.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The impact of container technology on the progress of life sciences research just can’t be overstated.&lt;/p&gt; 
&lt;p&gt;It addresses big challenges in bioinformatics, like reproducibility, scalability, and collaboration. It eases software management, and empowers researchers to focus on their core scientific endeavors. This has accelerated the pace of discoveries, fostered collaboration, and enhanced the overall quality and reproducibility of bioinformatics research.&lt;/p&gt; 
&lt;p&gt;We think today’s announcement pushes containers, and bioinformatics one step further, by making it dramatically easier to get your hands on containers in the right shape, size, and form-factor to meet the needs of your pipelines. Batch users will definitely enjoy using this. But, we’re pretty sure this will be a community resource everyone will love, and AWS is thrilled to be able to support the Seqera team to deliver this for the entire community.&lt;/p&gt; 
&lt;p&gt;If you still need convincing, try it out yourself now – head to &lt;a href="https://seqera.io/containers/"&gt;https://seqera.io/containers/&lt;/a&gt; and type in some of your favorite bioinformatics tool names before clicking “Build”. Pull your Docker image and get to work.&lt;/p&gt; 
&lt;p&gt;And if you have ideas for Seqera or AWS, don’t hesitate to reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Using machine learning to drive faster automotive design cycles</title>
		<link>https://aws.amazon.com/blogs/hpc/using-machine-learning-to-drive-faster-automotive-design-cycles/</link>
		
		<dc:creator><![CDATA[Steven Miller]]></dc:creator>
		<pubDate>Mon, 13 May 2024 14:40:10 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AI]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">2c1df43a0e4299fe6554a7b7f672836e27c25a46</guid>

					<description>Aerospace and automotive companies are speeding up their product design using AI. In this post we'll discuss how they're using machine learning to shift design cycles from hours to seconds using surrogate models.</description>
										<content:encoded>&lt;p&gt;The automotive product engineering process involves months or years of iterative design reviews and refinement, with back-and-forth feedback between stakeholders regularly to adjust designs and evaluate the impact of design changes on engineering metrics like the coefficient of drag. Between each design iteration, engineers wait hours or days for simulations to complete, which means they can only execute a handful of design decisions each week.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll show how automakers can reduce cycle times from hours to seconds by leveraging surrogate machine-learning (ML) models in place of HPC, physics-based simulations and create subtle design variations for non-parametric geometries.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;In short&lt;/strong&gt;: automotive engineers can use emerging ML methodologies to speed up the product engineering process.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;Typically, the automotive product design process involves conceptualization, computer-aided design (CAD) geometry creation, and engineering simulations for validating the designs to ensure aerodynamic efficiency and desired structural stability.&lt;/p&gt; 
&lt;p&gt;Recent advances in AI/ML and Generative AI technology including algorithms such as MeshGraphNets, U-Nets and Variational Autoencoders, have provided a path towards accelerating the design process through fast ML inferences which allow us to run fewer full-fidelity physics-based HPC simulations, which can take hours, while gathering insight into large design spaces.&lt;/p&gt; 
&lt;p&gt;In this work, we’ll focus on computational fluid dynamics (CFD) simulations for intelligent aerodynamic surface design, though there are often other design considerations like impact response, noise, vibration and harshness.&lt;/p&gt; 
&lt;p&gt;The goal of this work is &lt;em&gt;not&lt;/em&gt; to develop a comprehensive toolkit for aerodynamic design – we want to present a simple user experience to show how engineers can use scientific ML methods, together with AWS services to deliver business value. We’ll mainly use &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;Nice DCV&lt;/a&gt;, &lt;a href="https://aws.amazon.com/pm/serv-s3/"&gt;Amazon S3&lt;/a&gt; and &lt;a href="https://aws.amazon.com/pm/sagemaker/"&gt;Amazon SageMaker&lt;/a&gt;. We’ll also explain how to build a web application that uses ML and generative design to enhance these development processes.&lt;/p&gt; 
&lt;div id="attachment_3604" style="width: 857px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3604" loading="lazy" class="size-full wp-image-3604" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.33.24.png" alt="Figure 1 – Overview of Workflow Structure incorporating Web Application UI overview, AI/ML Methodology and AWS Implementation" width="847" height="433"&gt;
 &lt;p id="caption-attachment-3604" class="wp-caption-text"&gt;Figure 1 – Overview of Workflow Structure incorporating Web Application UI overview, AI/ML Methodology and AWS Implementation&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Roadmap for this post&lt;/h2&gt; 
&lt;p&gt;Much like the workflow chart in Figure 1, we’ll divide this post into three parts.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, we’ll start by explaining how to build a minimally viable web application including the end user experience and core components starting from an AWS architecture blueprint, highlighting some key components.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Next&lt;/strong&gt;, we’ll dive deep into the underlying ML models including the training methodology and inference deployment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Finally&lt;/strong&gt;, we’ll wrap up with a brief discussion about how we imagine automakers using workflows like this in practice to augment and accelerate the product design lifecycles.&lt;/p&gt; 
&lt;h2&gt;Cloud implementation and AWS architecture&lt;/h2&gt; 
&lt;p&gt;Let’s look at how to quickly and securely implement a minimally-viable web application. Using that, we’ll generate ground truth meshes, run OpenFOAM simulations, and train the machine learning models using AWS HPC services.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AWS architecture.&lt;/strong&gt; We don’t want to completely replace HPC simulations – these are necessary for verification and validation. We want to enable engineers to quickly build a web application inside their environment, access it securely, and store all their related artifacts. For this reason, we’ve chosen a minimalist architecture and we’ll leave it up to the automaker to decide how to integrate this into their existing environment.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 2: AWS Architecture diagram for deployment on a g5 instance using DCV, with endpoints to retrieve data or optionally submit compute jobs on self-managed ML endpoints (AWS Batch) or Amazon SageMaker&lt;/em&gt;&lt;/p&gt; 
&lt;div id="attachment_3605" style="width: 665px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3605" loading="lazy" class="size-full wp-image-3605" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.34.05.png" alt="Figure 2: AWS Architecture diagram for deployment on a g5 instance using DCV, with endpoints to retrieve data or optionally submit compute jobs on self-managed ML endpoints (AWS Batch) or Amazon SageMaker" width="655" height="345"&gt;
 &lt;p id="caption-attachment-3605" class="wp-caption-text"&gt;Figure 2: AWS Architecture diagram for deployment on a g5 instance using DCV, with endpoints to retrieve data or optionally submit compute jobs on self-managed ML endpoints (AWS Batch) or Amazon SageMaker&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We’re hosting the web application on a g5.12xlarge Amazon Elastic Compute Cloud (Amazon EC2) instance. The &lt;a href="https://aws.amazon.com/ec2/instance-types/g5/"&gt;G5&lt;/a&gt; has 4 x A10G NVIDIA GPUs to serve the machine learning models and we’re using the &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;NICE DCV&lt;/a&gt; streaming protocol to securely stream the desktop to the end.&lt;/p&gt; 
&lt;p&gt;We’re also keeping the EC2 instance secure inside of a VPC and we only allow TCP traffic to flow from the instance to a single end user.&lt;/p&gt; 
&lt;p&gt;We train drag prediction models using Amazon SageMaker, and we securely store the inputs and outputs (including model artifacts) in Amazon Simple Storage Service (Amazon S3).&lt;/p&gt; 
&lt;p&gt;We use AWS Batch to launch the CFD simulations to create the ground truth data.&lt;/p&gt; 
&lt;p&gt;To keep all communications secure, we use VPC endpoints for all key services.&lt;/p&gt; 
&lt;h2&gt;Application workflow and user experience&lt;/h2&gt; 
&lt;p&gt;Now we can build the minimally viable application to show this new workflow. The web application layer consists of five main modules: application, inference configuration, pretrained models, processed meshes, and utilities.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Application.&lt;/strong&gt; The core application, built on the open-source &lt;a href="https://streamlit.io/"&gt;Streamlit&lt;/a&gt; library, serves the user interface and all the orchestration to deliver the user experience. We used a 3D plotting library (&lt;a href="https://docs.pyvista.org/version/stable/"&gt;PyVista&lt;/a&gt;) to display 3D visualizations and enable user interaction with STL meshes (original and design targets) and ML and physics-based computational fluid dynamics results, including pressure cross-sections and 3D streamlines.&lt;/p&gt; 
&lt;p&gt;In this example, we’ll begin with a choice of a base car model (coupe or estate/station-wagon), and then provide the user with design modification options. In this case, some features of the car (like front bumper shape, windscreen angle, trunk depth, and cabin height) are provided, but we could easily extend this to other design features we’re interested in.&lt;/p&gt; 
&lt;p&gt;For each of these combinations, the design variations are produced through non-parametric “morphing” of the base meshes using &lt;em&gt;radial basis function (RBF) interpolation&lt;/em&gt;. If we wanted to explore parametric design, and the base CAD geometries were available, we could potentially incorporate them into the workflow. The user interface is shown in Figure 3.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure3 - Using machine learning to drive faster automotive design cycles." width="500" height="375" src="https://www.youtube-nocookie.com/embed/2Z06wjAPVkA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Radial basis function (RBF) – &lt;/strong&gt;RBF interpolation lets us distort meshes at specific parts of the car, while keeping other features constant – including the chassis platform and tire radius. RBF relies on the concept of a deformation map between the original mesh and the deformed mesh and we get to control the extent of deformation using the slider in the user interface.&lt;/p&gt; 
&lt;p&gt;To enable engineers to show specifically how the mesh has been deformed, we’ve provided a side-by-side comparison of the original mesh and the new mesh in the UI (which you can see in Figure 4) before we pass the deformed mesh along to the ML model for inference.&lt;/p&gt; 
&lt;div id="attachment_3607" style="width: 831px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3607" loading="lazy" class="size-full wp-image-3607" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.34.57.png" alt="Figure 4 – Deformed geometry (left) with a wireframe overlay of original mesh highlighting change and original mesh (right)" width="821" height="348"&gt;
 &lt;p id="caption-attachment-3607" class="wp-caption-text"&gt;Figure 4 – Deformed geometry (left) with a wireframe overlay of original mesh highlighting change and original mesh (right)&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Inference configuration.&lt;/strong&gt; We trained a hierarchical machine learning model for a CFD flow field and drag predictions – we’ll explain that more soon. At runtime, we hosted these on different GPUs which allowed us to parallelize the inference of individual sub-models. To get inferences from the ML model, we specify the key inputs and parameters using inference configurations. These inputs include the decimated surface meshes of the car and a geometry for volume calculations. Additional input parameters include the batch size for inference, number of workers and other neural network associated hyperparameters.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Processed mesh.&lt;/strong&gt; When a user requests an inference, the application generates a processed &lt;a href="https://www.hdfgroup.org/solutions/hdf5/"&gt;HDF5&lt;/a&gt; file which contains the new mesh and the corresponding flow fields. These HDF5 files are then exposed to the end user via an interactive 3D visualization. There are some extra components that support the application: a function to convert meshes to HDF5 files, another function to apply the RBF to morph meshes based on user inputs, and finally a function to run inferences and gather outputs.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deployment.&lt;/strong&gt; Since this application is meant for demonstration purposes only, we designed it to run on a single compute instance, and chose a g5.12xlarge that includes 4 x GPUs that’s able to run the entire application. To speed the application, we distribute individual ML sub-models across the GPUs. To keep it all secure, we isolate the instances from the public internet in a private VPC. And we used NICE DCV to access the instance for an easier remote desktop experience.&lt;/p&gt; 
&lt;p&gt;Geometric and ML models&lt;/p&gt; 
&lt;p&gt;To enable engineers to rapidly iterate, potentially even &lt;em&gt;during&lt;/em&gt; design conversations, our web application predicts the &lt;em&gt;coefficient of drag&lt;/em&gt; (Cd) and the flow fields on unseen geometries. We used a hierarchy of ML models for the flow fields and drag predictions.&lt;/p&gt; 
&lt;p&gt;In both cases, we trained our models using synthetic data that we generated by morphing the open source &lt;a href="https://www.epc.ed.tum.de/en/aer/research-groups/automotive/drivaer/"&gt;DrivAer&lt;/a&gt; data set, and then ran full-fidelity CFD simulations on all these geometries using AWS Batch with the open-source &lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Synthetic data generation&lt;/strong&gt;. To create training data for this project, we used the RBF to strategically morph the underlying mesh of both the coupe and the SUV. This method has broad applicability and can be generically applied to many parametric and non-parametric STL meshes.&lt;/p&gt; 
&lt;p&gt;For each area of the car, we specified points in an overlayed 1000-point (10x10x10) cubic lattice, bounded by the dimensions of the car. This allowed us to create 400 variations (100 for each of the area of the car) that we used as the basis for ground-truth simulations. Figure 5 shows how we used the RBF to deform the mesh &lt;em&gt;and&lt;/em&gt; create the training data for the ML model. Each time frame in the video corresponds to a different deformed mesh.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Using machine learning to drive faster automotive design cycles - Figure5" width="500" height="281" src="https://www.youtube-nocookie.com/embed/aBcO03OBTm0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p style="text-align: left"&gt;&lt;em&gt;Figure 5 – Synthetic mesh generation using morphing of individual features of a car shown in sequence&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;After we created 400 variations of the base meshes, we used OpenFOAM to create CFD simulations with steady-state RANS simulations using the k-ω steady-state turbulence model with a fixed inlet velocity of 20m/s.&lt;/p&gt; 
&lt;p&gt;We ran these simulations in parallel using AWS Batch on c5.24xlarge instances using MPI for parallelism – this took around 7 hours to run. We could have chosen larger mesh sizes, but we settled on a mesh size that allowed resolving fine geometry – in a reasonable compute time.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Coefficient of drag with MeshGraphNets (MGN) –&lt;/strong&gt; MGN is a deep-learning framework for learning mesh-based simulations which represents the mesh as a graph where information is propagated across the graph’s nodes and edges. MGN excels at efficiently capturing unstructured mesh topologies and it’s a great fit for predicting Cd values. To train the MGN model, we used the synthetic data that we generated using the RBF, and OpenFOAM. During training, we provided the underlying mesh and the Coefficient of Drag (Cd) metrics as inputs.&lt;/p&gt; 
&lt;p&gt;We then trained a model on 320 samples, with each mesh decimated while preserving topological features for computational efficiency, containing approximately 6,000 nodes. We used a p3.2xlarge instance for 500 epochs, with a total training time of 46 minutes.&lt;/p&gt; 
&lt;p&gt;The model achieved a MAPE (&lt;em&gt;mean absolute percentage error&lt;/em&gt;) of 2.5% in drag coefficient (Cd) when predicting on unseen data, with an average inference time of 0.028 seconds per sample. Figure 6 shows the predicted vs. actual drag coefficient (Cd) plot for train, validation, and unseen test data (car meshes).&lt;/p&gt; 
&lt;div id="attachment_3609" style="width: 718px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3609" loading="lazy" class="size-full wp-image-3609" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.36.06.png" alt="Figure 6: Predicted vs. actual drag coefficient (Cd) plot for train, validation, and unseen test data (i.e. car meshes)." width="708" height="426"&gt;
 &lt;p id="caption-attachment-3609" class="wp-caption-text"&gt;Figure 6: Predicted vs. actual drag coefficient (Cd) plot for train, validation, and unseen test data (i.e. car meshes).&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Although the model didn’t match the ground truth CFD simulations exactly, this level of agreement from the MGN surrogate model may prove acceptable during the initial design iterations leading up to a final, high-fidelity simulation. After that comes real-world wind tunnel testing, which has an inherent uncertainty in measurements.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Flow Fields using deep convolutional neural networks (CNNs).&lt;/strong&gt; To predict pressure distributions and velocity fields for full 3D flow, we used a CNN architecture called &lt;a href="https://github.com/wolny/pytorch-3dunet"&gt;U-Net&lt;/a&gt; which has shown promise in high-resolution volume segmentation and regression tasks. This architecture included encoding and decoding convolutional layers with skip connections. We trained individual U-Nets for each of the pressure and velocity primary variables.&lt;/p&gt; 
&lt;p&gt;We converted the 3D volume outputs from 400 x OpenFOAM simulations into the training (375) and validation (25) ground-truth datasets. We used a p5.48xlarge instance with 8 x H100 GPUs for approximately 28 hours. We saw a volume RSME (&lt;em&gt;root-mean-square-error&lt;/em&gt;) of around 3% for pressure, and 5% for combined velocity magnitudes, for unseen data not in the training or validation sets – although this is dependent on the degree of mesh warping away from the initial baseline mesh. Figure 7 shows the differences in velocity slices an unseen mesh.&lt;/p&gt; 
&lt;div id="attachment_3610" style="width: 680px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3610" loading="lazy" class="size-full wp-image-3610" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.36.52.png" alt="Figure 7: ML Predicted (Top) vs Ground Truth OpenFOAM (Bottom) X-Component Velocity" width="670" height="516"&gt;
 &lt;p id="caption-attachment-3610" class="wp-caption-text"&gt;Figure 7: ML Predicted (Top) vs Ground Truth OpenFOAM (Bottom) X-Component Velocity&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Business value creation&lt;/h2&gt; 
&lt;p&gt;We’ve now shown how to rapidly build a minimally viable web interface that engineers can use to take advantage of these ML models to speed up the product engineering process. But now we need to couple this with a new business process so automakers can realize actual business value.&lt;/p&gt; 
&lt;div id="attachment_3611" style="width: 855px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3611" loading="lazy" class="size-full wp-image-3611" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.37.12.png" alt="Figure 8: Overall product design iterations using machine learning surrogate models to accelerate the cycles" width="845" height="168"&gt;
 &lt;p id="caption-attachment-3611" class="wp-caption-text"&gt;Figure 8: Overall product design iterations using machine learning surrogate models to accelerate the cycles&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Using the speed of the ML surrogate models, it’s possible to explore hundreds of designs per week, and we can now make meetings more collaborative.&lt;/p&gt; 
&lt;p&gt;Instead of reviewing results from prior runs, engineers can solicit feedback from their peers real-time0, and get the results in 20-60 seconds, avoiding costly wait times. This will require engineers to think differently about product design reviews and meetings. Engineers will need to set expectations accordingly and be prepared to guide conversations. But this enables engineers to explore a greater design space than they otherwise could – especially given budget or time constraints. We’re confident this approach will lead to new innovations.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;With the right combination of machine-learning and process change, automakers can use the architecture explained here to build tailored solutions to speed up the product engineering process, reduce the cost of compute, and explore a greater number of design options in a shorter period of time. If you or your team want to discuss the business case, a recommended implementation path with our Professional Services team, or technical solution blueprint in more detail, you can reach out to us at ask-hpc@amazon.com.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Announcing the High Performance Software Foundation (HPSF)</title>
		<link>https://aws.amazon.com/blogs/hpc/announcing-the-high-performance-software-foundation/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Mon, 13 May 2024 05:55:13 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">e91b47faf0b755001ca4998fd17206fcbdaf12f6</guid>

					<description>We're excited to share how we’re involved in launching the High Performance Software Foundation to increase access to and adoption of HPC. By bringing together key players to collaborate, we can lower barriers and accelerate development of portable HPC software stacks.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="size-full wp-image-3644 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/09/AdobeStock_144582125-resized.png" alt="Announcing the High Performance Software Foundation" width="380" height="253"&gt;&lt;/em&gt;In high performance computing (HPC), where speed and efficiency are paramount, open-source software provides the load-bearing, structural support. Our community has long recognized the immense potential of collaborative development, sharing of resources, and the power of community-driven innovation.&lt;/p&gt; 
&lt;p&gt;So we’re excited to announce that we’re a Premier founding member of the new &lt;strong&gt;High Performance Software Foundation&lt;/strong&gt; (HPSF). The HPSF was launched today in Hamburg at &lt;a href="https://www.isc-hpc.com/"&gt;ISC’24&lt;/a&gt;, by the Linux Foundation, the nonprofit organization that enables mass innovation through open-source. The HPSF has strong support across the HPC landscape.&lt;/p&gt; 
&lt;p&gt;Through a series of technical projects, the HPSF aims to build, promote, and advance a portable core software stack for HPC to increase adoption by lowering barriers to contribution, supporting open-source development efforts, and making HPC more accessible to all of us. We’ve seen from our work with RIKEN to &lt;a href="https://aws.amazon.com/blogs/publicsector/why-fugaku-japans-fastest-supercomputer-went-virtual-on-aws/"&gt;extend Fugaku to the cloud&lt;/a&gt;&amp;nbsp;how impactful this can be.&lt;/p&gt; 
&lt;p&gt;Recent years have seen extraordinary demand for HPC across domains which are critical to all of us, from climate modeling and genomics to drug-discovery and engineering design. With the emergence of machine learning and AI, there’s never been a more important task than to reinforce the structures which have made all of this possible.&lt;/p&gt; 
&lt;h2&gt;How can HPSF help?&lt;/h2&gt; 
&lt;p&gt;The HPSF aims to make life easier for HPC developers through a number of focused initiatives, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Continuous Integration (CI) resources tailored for HPC projects&lt;/li&gt; 
 &lt;li&gt;Continuously built, turnkey software stacks&lt;/li&gt; 
 &lt;li&gt;Architecture support&lt;/li&gt; 
 &lt;li&gt;Performance regression testing and benchmarking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AWS has been supporting many of these efforts for the &lt;a href="https://spack.io/"&gt;Spack&lt;/a&gt; community since 2021, leading to the launch of the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-the-spack-rolling-binary-cache/"&gt;Spack Rolling Binary Cache&lt;/a&gt; two years ago. We’ve all learned a lot from this relationship, which we hope the wider HPC open-source community will be able to benefit from, through the foundation.&lt;/p&gt; 
&lt;h2&gt;First steps&lt;/h2&gt; 
&lt;p&gt;One of HPSF’s first jobs is to set up the technical advisory committee (TAC) which will manage working groups tackling a variety of HPC topics. Drawing from member organizations and community participants, the TAC will follow a governance model based on the &lt;a href="https://www.cncf.io/"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF).&lt;/p&gt; 
&lt;p&gt;The HPSF launches today with the following technical projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Spack&lt;/strong&gt;: the HPC package manager&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Kokkos&lt;/strong&gt;: a performance-portable programming model for writing modern C++ applications in a hardware-agnostic way&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Viskores (formerly VTK-m)&lt;/strong&gt;: a toolkit of scientific visualization algorithms for accelerator architectures&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HPCToolkit&lt;/strong&gt;: performance measurement and analysis tools for computers ranging from laptops to GPU-accelerated supercomputers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Apptainer&lt;/strong&gt;: Formerly known as Singularity, Apptainer is a Linux Foundation project providing a high performance, full featured HPC and computing optimized container subsystem&lt;/li&gt; 
 &lt;li&gt;&lt;b&gt;E4S: &lt;/b&gt;a curated, hardened distribution of scientific software packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting involved&lt;/h2&gt; 
&lt;p&gt;The HPSF welcomes organizations from across the HPC community to become involved and help drive innovation in open-source HPC solutions.&lt;/p&gt; 
&lt;p&gt;To learn more about the HPSF, to contribute, or to become a member, you can visit the &lt;a href="https://hpsf.io/"&gt;HPSF website&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The HPSF’s commitment to supporting and nurturing these vital software packages will enable the HPC community to have access to the tools and resources necessary to push the boundaries of scientific discovery. We’re thrilled to be part of it.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Best practices for running molecular dynamics simulations on AWS Graviton3E</title>
		<link>https://aws.amazon.com/blogs/hpc/best-practices-for-running-molecular-dynamics-simulations-on-aws-graviton3e/</link>
		
		<dc:creator><![CDATA[Nathaniel Ng]]></dc:creator>
		<pubDate>Tue, 07 May 2024 13:55:18 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[GROMACS]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">1e63bf1a121b398b36fb135821ca6835db681e7f</guid>

					<description>If you run molecular dynamics simulations, you need to read this. We walk through running benchmarks of popular apps like GROMACS and LAMMPS on new Hpc7g instances and Graviton3E processors. The results - up to 35% better vector performance versus Graviton3! Learn how to optimize your own workflows.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Nathaniel Ng, Shun Utsui, and James Chen from AWS Solution Architecture&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Last year, &lt;a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc7g-instances-powered-by-aws-graviton3e-processors-optimized-for-high-performance-computing-workloads/"&gt;we announced&lt;/a&gt; the general availability of Hpc7g instances, the instance type focused on HPC workloads powered by AWS Graviton3E. Graviton3E processors deliver up to 35% higher vector-instruction performance &lt;a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc7g-instances-powered-by-aws-graviton3e-processors-optimized-for-high-performance-computing-workloads/"&gt;compared to Graviton3&lt;/a&gt;, providing higher performance benefits for HPC applications.&lt;/p&gt; 
&lt;p&gt;Molecular dynamics (MD) is a domain that frequently leverages HPC resources. Previously, customers ran their MD workloads using predominantly x86 architectures, but we’ve heard that many are interested in understanding the performance they can get on Graviton3E.&lt;/p&gt; 
&lt;p&gt;So, in this post, we’ll show how you can run MD workloads on Hpc7g instances using AWS ParallelCluster, a supported open-source cluster management tool that allows you to deploy a scalable HPC environment on AWS in a matter of minutes. We’ll use &lt;a href="https://www.gromacs.org/"&gt;GROMACS&lt;/a&gt; and &lt;a href="https://www.lammps.org/"&gt;LAMMPS&lt;/a&gt; as examples – two very popular MD applications. And we’ll highlight the best practices for the tools – and the compiler flags – we used to achieve optimal performance.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Key to the architecture is AWS ParallelCluster. Customers can install the ParallelCluster CLI on their laptops using Python’s pip package manager, and use it to deploy a cluster in the cloud from their laptops by supplying a short configuration file.&lt;/p&gt; 
&lt;p&gt;Through the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcui-using-v3.html"&gt;ParallelCluster UI&lt;/a&gt;, you can use a wizard to configure a cluster without editing a configuration file or installing anything on your laptop. Details on how to set up a ParallelCluster environment can be found in &lt;a href="https://www.hpcworkshops.com/01-hpc-overview.html"&gt;this workshop&lt;/a&gt;. You can also find a &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/try_hpc7g"&gt;one-click launchable stack&lt;/a&gt; in the HPC Recipe Library (on GitHub), which will build an Hpc7g cluster for you after asking a minimum of questions.&lt;/p&gt; 
&lt;div id="attachment_3496" style="width: 831px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3496" loading="lazy" class="size-full wp-image-3496" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.40.13.png" alt=" Figure 1: Architecture using AWS ParallelCluster to run MD workloads with AWS Graviton 3E instances. " width="821" height="378"&gt;
 &lt;p id="caption-attachment-3496" class="wp-caption-text"&gt;&lt;br&gt;Figure 1: Architecture using AWS ParallelCluster to run MD workloads with AWS Graviton 3E instances.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;ParallelCluster uses Slurm for its job scheduler, to dynamically scale the number of Hpc7g.16xlarge compute instances, responding to the queue.&lt;/p&gt; 
&lt;p&gt;These instances are powered by custom-built AWS Graviton3E processors. They feature the latest DDR5 memory offering 50% more bandwidth compared to DDR4, and they carry 200Gbps network interfaces with the Elastic Fabric Adapter (EFA). Graviton3E processors implement Scalable Vector Extension (SVE) of the Neoverse V1 architecture and hence can deliver up to 2x better performance for floating point codes than Graviton2.&lt;/p&gt; 
&lt;p&gt;To achieve sufficient performance on storage I/O, we deployed 4.8 TB of Amazon FSx for Lustre – a fully-managed, high-performance Lustre file system. We selected the &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/using-fsx-lustre.html"&gt;PERSISTENT_2&lt;/a&gt; deployment type with a disk throughput of &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/performance.html"&gt;1000 MBps/TiB of storage provisioned&lt;/a&gt;, and backed it with an Amazon Simple Storage Service (Amazon S3) bucket.&lt;/p&gt; 
&lt;p&gt;We’ve documented all the details of our ParallelCluster configuration in our &lt;a href="https://github.com/aws-samples/aws-graviton-md-example"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Development tooling&lt;/h2&gt; 
&lt;p&gt;We tested several configurations and concluded that we recommend using the following compilers and libraries for most use cases. You can check our &lt;a href="https://github.com/aws-samples/aws-graviton-md-example"&gt;GitHub repository&lt;/a&gt; to find out best practices for compiler flags when building MD applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Compiler&lt;/strong&gt;: Arm compiler for Linux (ACfL) version 23.04 or later&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Library&lt;/strong&gt;: Arm performance libraries (ArmPL) version 23.04 or later, included in ACfL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MPI&lt;/strong&gt;: Open MPI version 4.1.5 or later&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It’s worth noting that Arm Compilers and Performance Libraries for HPC developers are &lt;a href="https://community.arm.com/arm-community-blogs/b/high-performance-computing-blog/posts/arm-compilers-and-libraries-for-hpc-now-free"&gt;now available for no cost&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In the rest of this post, we’ll explain why we preferred these tools, by diving deeper into the performance of GROMACS and LAMMPS with different compilers, compiler options, and input files.&lt;/p&gt; 
&lt;h2&gt;GROMACS&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.gromacs.org/about.html"&gt;GROMACS&lt;/a&gt; is an open-source software suite for high-performance molecular dynamics and output analysis. It’s widely adopted in the computer simulation fields not only for biochemical molecules but also for non-biological systems like polymers and fluid dynamics. The GROMACS community have optimized it to make great use of the SIMD capabilities of many modern HPC architectures.&lt;/p&gt; 
&lt;p&gt;For Arm architectures, the &lt;a href="https://manual.gromacs.org/current/install-guide/index.html#simd-support"&gt;code supports both NEON (ASIMD) and SVE instructions&lt;/a&gt;. In this post, we used GNU 12.2 and the Arm compiler for Linux (ACfL) 23.04 as the testing compiler suite, with Arm Performance Library (ArmPL) 23.04 for the math library, and Open MPI 4.1.5 linked with Libfabric to build different binary executables of GROMACS 2022.5.&lt;/p&gt; 
&lt;h3&gt;Build scripts&lt;/h3&gt; 
&lt;p&gt;Our main objective was to find out the best compiler suite – and SIMD support fit – for Graviton3E-based Hpc7g instances. We’ll discuss the build scripts, job submission scripts, and performance data here – and you can find all the scripts in our &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/tree/main/codes/GROMACS"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For GROMACS’s build script, there’s no difference in the CMake options for GNU and Arm compiler for Linux.&lt;/p&gt; 
&lt;p&gt;For GNU compilers, the Open MPI 4.1.5 environment is already installed in ParallelCluster’s machine images. We recommend anyone new to the cloud to use the system default Open MPI library. However, the system default Open MPI does &lt;em&gt;not&lt;/em&gt; support ACfL, so we’ve &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/setup/2-install-openmpi-with-acfl.sh"&gt;supplied a script&lt;/a&gt; which demonstrates how to compile and install Open MPI 4.1.5 with ACfL. Once this is installed, we can use the bash module environments to switch between the GNU and Arm compiler.&lt;/p&gt; 
&lt;p&gt;With this done, it’s now possible to build executables for GROMACS 2022.5 for both SVE or NEON/ASIMD instruction sets.&lt;/p&gt; 
&lt;p&gt;We’ve stored the procedures for this in yet another &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/tree/main/codes/GROMACS"&gt;reference script&lt;/a&gt;. We only need to change the parameter &lt;code&gt;GMX_SIMD&lt;/code&gt; in the configuration setup. For the SVE-enable binary, the parameter is –&lt;code&gt;DGMX_SIMD=ARM_SVE&lt;/code&gt;. For building the NEON/ASIMD-enabled binary, you’ll need to switch the parameter to &lt;code&gt;-DGMX_SIMD= ARM_NEON_ASIMD&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Test cases&lt;/h3&gt; 
&lt;p&gt;We applied three standard test cases for GROMACS from the &lt;a href="https://repository.prace-ri.eu/git/UEABS/ueabs"&gt;Unified European Application Benchmark Suite (UEABS)&lt;/a&gt;.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href="https://repository.prace-ri.eu/ueabs/GROMACS/2.2/GROMACS_TestCaseA.tar.xz"&gt;Test Case A &lt;/a&gt;is the ion channel system of the membrane protein GluCl embedded in a DOPC membrane and solvated in TIP3P water. This system contains 142k atoms.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://repository.prace-ri.eu/ueabs/GROMACS/2.2/GROMACS_TestCaseB.tar.xz"&gt;Test Case B&lt;/a&gt; is a model of cellulose and lignocellulosic biomass in an aqueous solution. The system has 3.3 million atoms.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://repository.prace-ri.eu/ueabs/GROMACS/2.2/GROMACS_TestCaseC.tar.xz"&gt;Test Case C&lt;/a&gt; is the standard test case for NAMD benchmarks. The system is a 3 x 3 x 3 replica of the STMV (&lt;em&gt;Satellite Tobacco Mosaic Virus&lt;/em&gt;). The size of model is about 28 million atoms.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For all these cases, we set the total simulation steps to 10k.&lt;/p&gt; 
&lt;p&gt;To find out the best combination of compiler and SIMD setup, we started to test the application performance on a single Hpc7g.16xlarge instance. Figure 2 charts the performance we saw for test case A (142K atoms). ACfL with SVE-enabled generated the best performance. The binary this generated was about 9-10% faster than the one using NEON/ASIMD. The binary produced by the ACfL with SVE ran 6% faster than when we used the GNU compiler with SVE.&lt;/p&gt; 
&lt;div id="attachment_3497" style="width: 714px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3497" loading="lazy" class="size-full wp-image-3497" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.41.42.png" alt="Figure 2: Performance of GROMACS 2022.5 for GluCl Ion Channel system (142K atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enable binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs. " width="704" height="466"&gt;
 &lt;p id="caption-attachment-3497" class="wp-caption-text"&gt;Figure 2: Performance of GROMACS 2022.5 for GluCl Ion Channel system (142K atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enable binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 3 charts the performance we saw for test case B (3.3M atoms). The best setup we found – again – was to use the ACfL with SVE enabled. We measured a 28% improvement for the SVE-enabled binary, compared with the NEON/ASIMD-enable binary when we used this compiler.&lt;/p&gt; 
&lt;div id="attachment_3498" style="width: 717px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3498" loading="lazy" class="size-full wp-image-3498" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.42.04.png" alt="Figure 3: Performance of GROMACS 2022.5 for cellulose and lignocellulosic biomass (3.3M atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enabled binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs. " width="707" height="465"&gt;
 &lt;p id="caption-attachment-3498" class="wp-caption-text"&gt;Figure 3: Performance of GROMACS 2022.5 for cellulose and lignocellulosic biomass (3.3M atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enabled binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The performance results for test case C (28M atoms) are shown in Figure 4. Again, we saw a similar pattern to before: the ACfL with SVE enabled was the best option for GROMACS running on Hpc7g instance. In this case, the performance delta was 19% compared to NEON-ASIMD. And again, the binary generated by ACfL was 6% faster than the one built by the GNU compiler.&lt;/p&gt; 
&lt;div id="attachment_3499" style="width: 716px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3499" loading="lazy" class="size-full wp-image-3499" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.42.44.png" alt="Figure 4: Performance of GROMACS 2022.5 for Satellite Tobacco Mosaic Virus, STMV (28M atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enable binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs. " width="706" height="462"&gt;
 &lt;p id="caption-attachment-3499" class="wp-caption-text"&gt;Figure 4: Performance of GROMACS 2022.5 for Satellite Tobacco Mosaic Virus, STMV (28M atoms) with different settings of compilers and SIMD using one Hpc7g.16xlarge instance. SVE-enable binary generated by ACfL produces the best performance. All the data points are based on the average of three individual runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Based on the results running on single Hpc7g instance, we concluded that ACfL with SVE-enabled SIMD, generated the best performance results.&lt;/p&gt; 
&lt;p&gt;By default, the configuration tool in GROMACS detected the CPU automatically, and selected the SIMD support correctly, too. GROMACS also detected the configuration of SIMD correctly for AWS Graviton3.&lt;/p&gt; 
&lt;p&gt;We chose test case C to highlight the scalability for performance on Hpc7g when we ran across multiple nodes. The result charted in Figure 5 confirms this – scalability of performance is near-linear with EFA enabled.&lt;/p&gt; 
&lt;div id="attachment_3500" style="width: 895px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3500" loading="lazy" class="size-full wp-image-3500" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.43.17.png" alt="Figure 5: Scalability of Test Case C running on the Hpc7g-based cluster with, and without, EFA. 200Gpbs EFA contributes the scalability in the cases running beyond 2 compute instances. The binary was compiled with ACfL with SVE-enabled. All the data points are based on the average of three individual runs. " width="885" height="579"&gt;
 &lt;p id="caption-attachment-3500" class="wp-caption-text"&gt;Figure 5: Scalability of Test Case C running on the Hpc7g-based cluster with, and without, EFA. 200Gpbs EFA contributes the scalability in the cases running beyond 2 compute instances. The binary was compiled with ACfL with SVE-enabled. All the data points are based on the average of three individual runs.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Conclusion for GROMACS&lt;/h3&gt; 
&lt;p&gt;Based on these performance results for three quite different, but well-known test cases, we found that SVE-enabled binary was faster than using ASIMD/Neon instructions. ACfL produced faster code than the GNU compiler. Specifically for Hpc7g, ACfL with SVE-enabled SIMD setting was the best configuration when paired with the latest ArmPL and Open MPI with EFA enabled.&lt;/p&gt; 
&lt;h2&gt;LAMMPS&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.lammps.org/"&gt;LAMMPS&lt;/a&gt; (Large-scale Atomic/Molecular Massively-Parallel Simulator) is a classical molecular dynamics simulator, used for particle-based modelling of materials. It was developed at Sandia National Laboratories, and is available as an open-source tool, distributed under GPLv2. You can download the source code from the &lt;a href="https://github.com/lammps/lammps/releases"&gt;LAMMPS GitHub repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In terms of algorithms, LAMMPS uses parallel spatial decomposition, as well as parallel FFTs for long-range Coloumbic interactions. The computational cost of scaling with the number of atoms is O(N) for short-range interactions, and O(N log N) when computations involve Coloumbic interactions with FFT-based methods.&lt;/p&gt; 
&lt;p&gt;To prepare for LAMMPS running on AWS, we can follow the same steps as before for GROMACS for the AWS ParallelCluster, GCC, ACfL, and Open MPI setup. The LAMMPS &lt;a href="https://docs.lammps.org/Install.html"&gt;install guide&lt;/a&gt; has several options for installation, but to use the latest version of LAMMPS, and more recent versions of GCC, ACfL, and Open MPI, LAMMPS must be compiled from source.&lt;/p&gt; 
&lt;p&gt;LAMMPS has specific Makefiles that are optimized for Arm architecture available: &lt;a href="https://github.com/lammps/lammps/blob/develop/src/MAKE/MACHINES/Makefile.aarch64_arm_openmpi_armpl"&gt;Makefile.aarch64_arm_openmpi_armpl&lt;/a&gt; and &lt;a href="https://github.com/lammps/lammps/blob/develop/src/MAKE/MACHINES/Makefile.aarch64_g++_openmpi_armpl"&gt;Makefile.aarch64_g++_openmpi_armpl&lt;/a&gt;, for use with ACfL and GCC compilers respectively. In the case of ACfL, we used &lt;code&gt;-march=armv8-a+sve&lt;/code&gt; &lt;a href="https://developer.arm.com/documentation/102476/0100/Programming-with-SVE/Auto-vectorization"&gt;to add the SVE instructions&lt;/a&gt;, and checked that switching between &lt;code&gt;-march=armv8-a+sve&lt;/code&gt; and &lt;code&gt;-march=armv8-a+simd&lt;/code&gt; did not impact performance. For &lt;em&gt;both&lt;/em&gt; ACfL and GCC Makefiles, we added &lt;code&gt;-fopenmp&lt;/code&gt; to &lt;code&gt;CCFLAGS&lt;/code&gt; and &lt;code&gt;LINKFLAGS&lt;/code&gt; to enable OpenMP. We’ve summarized the final settings in Table 1.&lt;/p&gt; 
&lt;div id="attachment_3501" style="width: 927px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3501" loading="lazy" class="wp-image-3501 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.44.36.png" alt="Table 1: Compiler settings for GCC and ACfL" width="917" height="247"&gt;
 &lt;p id="caption-attachment-3501" class="wp-caption-text"&gt;Table 1: Compiler settings for GCC and ACfL&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To compile LAMMPS, the final Makefiles available in our GitHub repo, were &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/LAMMPS/2a-compile-lammps-acfl-sve.sh"&gt;2a-compile-lammps-acfl-sve.sh&lt;/a&gt; for ACfL and &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/LAMMPS/2b-compile-lammps-gcc.sh"&gt;2b-compile-lammps-gcc.sh&lt;/a&gt; for GCC.&lt;/p&gt; 
&lt;p&gt;The key points to note in our LAMMPS compile script involve loading the correct environment modules – &lt;code&gt;libfabric-aws/1.17.1&lt;/code&gt; and &lt;code&gt;armpl/23.04.1&lt;/code&gt; for both compilers, &lt;code&gt;acfl/23.04.1&lt;/code&gt; for ACfL, and &lt;code&gt;gnu/12.2.0&lt;/code&gt; for GCC – and replacing the &lt;code&gt;CCFLAGS&lt;/code&gt; and &lt;code&gt;LINKFLAGS&lt;/code&gt; variables with those from Table 1. For ACfL, we set both &lt;code&gt;PATH&lt;/code&gt; and &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; to the Open MPI installation folders.&lt;/p&gt; 
&lt;p&gt;We pulled the LAMMPS source code from the git repository into the install folder (&lt;code&gt;~/software&lt;/code&gt; in our case), and checked out the 23 June 2022 branch for compilation (&lt;code&gt;git checkout stable_23Jun2022_update4&lt;/code&gt;). You can check out other releases, too. If you want the default stable branch, use &lt;code&gt;git checkout stable&lt;/code&gt;. Use &lt;code&gt;make clean-all&lt;/code&gt; to remove all the intermediate object files and executable files, and &lt;code&gt;make no-all&lt;/code&gt; to turn off all options. Next, we ran &lt;code&gt;make yes-most&lt;/code&gt; to install most of the packages, which was enough for us to test all five LAMMPS benchmarks.&lt;/p&gt; 
&lt;p&gt;Once we compiled LAMMPS, we were able to submit jobs using Slurm. We’ve included sample job scripts &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/LAMMPS/3a-lammps-acfl-sve.sh"&gt;3a-lammps-acfl-sve.sh&lt;/a&gt; (for ACfL-compiled LAMMPS) and &lt;a href="https://github.com/aws-samples/aws-graviton-md-example/blob/main/codes/LAMMPS/3b-lammps-gcc.sh"&gt;3b-lammps-gcc.sh&lt;/a&gt; (for GCC-compiled LAMMPS) in our repo. In the final command, you should execute LAMMPS using &lt;code&gt;mpirun&lt;/code&gt;. The &lt;code&gt;-var x&lt;/code&gt;, &lt;code&gt;-var y&lt;/code&gt;, and &lt;code&gt;-var z&lt;/code&gt; flags specify the &lt;code&gt;NX&lt;/code&gt;, &lt;code&gt;NY&lt;/code&gt;, and &lt;code&gt;NZ&lt;/code&gt; parameters passed to the LAMMPS input file, and the &lt;code&gt;-in&lt;/code&gt; parameter indicates the LAMMPS input file (&lt;code&gt;in.lj&lt;/code&gt; in our example).&lt;/p&gt; 
&lt;p&gt;For the LAMMPS test runs, we chose the five benchmark cases listed at the &lt;a href="https://www.lammps.org/bench.html"&gt;LAMMPS Benchmarks site&lt;/a&gt; as described in Table 2:&lt;/p&gt; 
&lt;div id="attachment_3502" style="width: 638px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3502" loading="lazy" class="size-full wp-image-3502" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.48.36.png" alt="Table 2: Description of the 5 LAMMPS benchmarks" width="628" height="353"&gt;
 &lt;p id="caption-attachment-3502" class="wp-caption-text"&gt;Table 2: Description of the 5 LAMMPS benchmarks&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We ran a comparison between LAMMPS compiled with the GCC, and LAMMPS compiled with ACfL for all of the 5 input files (&lt;code&gt;in.lj&lt;/code&gt;, &lt;code&gt;in.chain&lt;/code&gt;, &lt;code&gt;in.eam&lt;/code&gt;,&lt;code&gt; in.chute&lt;/code&gt;, and &lt;code&gt;in.rhodo&lt;/code&gt;), running on a single Hpc7g.16xlarge instance.&lt;/p&gt; 
&lt;p&gt;For each of the input files, we submitted three jobs with GCC and three jobs with ACfL, and we used the average of the runs to report the results. We took the speed from the &lt;code&gt;log.lammps&lt;/code&gt; output file, reported in tau/day for the &lt;code&gt;chain&lt;/code&gt;, &lt;code&gt;chute&lt;/code&gt;, and &lt;code&gt;lj&lt;/code&gt; test cases, and ns/day for the &lt;code&gt;eam&lt;/code&gt; and &lt;code&gt;rhodo&lt;/code&gt; test cases.&lt;/p&gt; 
&lt;p&gt;The LAMMPS benchmarks each contain 32,000 atoms (for &lt;code&gt;NX=NY=NZ=1&lt;/code&gt;). We chose &lt;code&gt;NX=NY=NZ=8&lt;/code&gt; as a balance between &lt;code&gt;NX=NY=NZ=1&lt;/code&gt; (with high variability in performance due to the short run time) and &lt;code&gt;NX=NY=NZ=32&lt;/code&gt; (resulting in out of memory errors).&lt;/p&gt; 
&lt;div id="attachment_3504" style="width: 896px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3504" loading="lazy" class="size-full wp-image-3504" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.51.47.png" alt="Figure 6. Performance of LAMMPS a single Hpc7g.16xlarge instance with NX=NY=NZ=8, when compiled using the GCC and ACfL compilers. Speed is in tau/day for the chain, chute, and lj test cases, and ns/day for the eam and rhodo test cases." width="886" height="562"&gt;
 &lt;p id="caption-attachment-3504" class="wp-caption-text"&gt;Figure 6. Performance of LAMMPS a single Hpc7g.16xlarge instance with NX=NY=NZ=8, when compiled using the GCC and ACfL compilers. Speed is in tau/day for the chain, chute, and lj test cases, and ns/day for the eam and rhodo test cases.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;ACfL consistently outperformed GCC for all the single-node runs, with improvements ranging from 2.3% to 46%.&lt;/p&gt; 
&lt;div id="attachment_3503" style="width: 885px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3503" loading="lazy" class="size-full wp-image-3503" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.50.35.png" alt="Table 3: Speedups with ACfL over GCC for the 5 benchmark cases on a single Hpc7g.16xlarge instance with NX=NY=NZ=8." width="875" height="124"&gt;
 &lt;p id="caption-attachment-3503" class="wp-caption-text"&gt;Table 3: Speedups with ACfL over GCC for the 5 benchmark cases on a single Hpc7g.16xlarge instance with NX=NY=NZ=8.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For the multi-node runs, we further increased &lt;code&gt;NX=NY=NZ&lt;/code&gt; to &lt;code&gt;32&lt;/code&gt;, to increase the runtime – and therefore reduce the variability – in the results. This brought the total number of atoms to 32,000 x 32 x 32 x 32 = ~ 1 billion atoms.&lt;/p&gt; 
&lt;p&gt;We chose to focus only on the Lennard Jones benchmark and tested both GCC- and ACfL- compiled LAMMPS with &lt;code&gt;OMP_NUM_THREADS=1,2,4&lt;/code&gt;. For 8 nodes (512 cores) and above, we observed better results with &lt;code&gt;OMP_NUM_THREADS=2&lt;/code&gt; for both compilers. &lt;em&gt;Unlike the single node runs&lt;/em&gt;, we observed that GCC-compiled LAMMPS outperformed its ACfL-compiled counterpart. We tested the GCC-compiled version up to 128 nodes.&lt;/p&gt; 
&lt;p&gt;We’ve plotted the results in Figure 7.&lt;/p&gt; 
&lt;div id="attachment_3505" style="width: 906px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3505" loading="lazy" class="size-full wp-image-3505" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/09/CleanShot-2024-04-09-at-12.53.46.png" alt="Figure 7: Scalability of GCC-compiled LAMMPS for Lennard Jones benchmark case (1 billion atoms) using multiple Hpc7g.16xlarge instances. The line for linear scaling is plotted in grey." width="896" height="617"&gt;
 &lt;p id="caption-attachment-3505" class="wp-caption-text"&gt;Figure 7: Scalability of GCC-compiled LAMMPS for Lennard Jones benchmark case (1 billion atoms) using multiple Hpc7g.16xlarge instances. The line for linear scaling is plotted in grey.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we showed how you can run GROMACS and LAMMPS on Hpc7g-based instance. We discussed the ideal toolchain and SIMD setup based on the results we saw.&lt;/p&gt; 
&lt;p&gt;We didn’t modify the source code of GROMACS and LAMMPS, choosing instead to leverage “auto-vectorization” implemented in the compilers.&lt;/p&gt; 
&lt;p&gt;Arm’s compiler for Linux (ACfL) with SVE enabled SIMD boosted GROMACS performance up to 22% compared with the GNU compiler and NEON/ASIMD on Hpc7g instances. Arm compiler for Linux sped up LAMMPS simulation between 2.3% to 46%, depending on the case, compared to the GNU compiler on a single node basis. For LAMMPS, we also saw performance improvements at larger core counts with hybrid MPI + OpenMP parallelization.&lt;/p&gt; 
&lt;p&gt;We’ve made all our build scripts – and job submission scripts – available in our GitHub samples repo, and we’d encourage you to use this if you want to build on this work for your own research workloads. If you need to discuss any of this, feel free to reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Accelerate drug discovery with NVIDIA BioNeMo Framework on Amazon EKS</title>
		<link>https://aws.amazon.com/blogs/hpc/accelerate-drug-discovery-with-nvidia-bionemo-framework-on-amazon-eks/</link>
		
		<dc:creator><![CDATA[Doruk Ozturk]]></dc:creator>
		<pubDate>Wed, 01 May 2024 15:01:56 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">6ce72440eac8d6fda6dcdd01cb3b62a20165c5d0</guid>

					<description>This post was contributed by Doruk Ozturk and Ankur Srivastava at AWS, and Neel Patel at NVIDIA. Introduction Drug discovery is a long and expensive process. Pharmaceutical companies must sift through thousands of compound possibilities to find potential new drugs to treat diseases. This process takes multiple years and costs billions of dollars, with the […]</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Doruk Ozturk and Ankur Srivastava at AWS, and Neel Patel at NVIDIA.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Drug discovery is a long and expensive process. Pharmaceutical companies must sift through thousands of compound possibilities to find potential new drugs to treat diseases. This process takes multiple years and costs billions of dollars, with the majority of the candidates failing during clinical trials.&lt;/p&gt; 
&lt;p&gt;As generative artificial intelligence (generative AI) continues to transform industries, the life sciences sector is leveraging these advanced technologies to accelerate drug discovery. Generative AI tools powered by deep learning models make it possible to analyze massive datasets, identify patterns, and generate insights to aid the search for new drug compounds. However, running these generative AI workloads requires a full-stack approach that combines robust computing infrastructure with optimized domain-specific software that can accelerate time to solution.&lt;/p&gt; 
&lt;p&gt;In this blog post, we’ll show you how to leverage the NVIDIA BioNeMo platform on Amazon Elastic Kubernetes Service (Amazon EKS) to accelerate drug discovery by using generative AI and other machine learning technologies.&lt;/p&gt; 
&lt;h2&gt;NVIDIA BioNeMo&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://www.nvidia.com/en-us/clara/bionemo/"&gt;NVIDIA BioNeMo&lt;/a&gt; is a generative AI platform for drug discovery that simplifies and accelerates the training of models using your own data. BioNeMo provides researchers and developers a fast and easy way to build and integrate state-of-the-art generative AI applications across the entire drug discovery pipeline—from target identification to lead optimization—with AI workflows for 3D protein structure prediction, de novo design, virtual screening, docking, and property prediction.&lt;/p&gt; 
&lt;p&gt;The BioNeMo framework facilitates centralized model training, optimization, fine-tuning, and inferencing for protein and molecular design. Researchers can build and train foundation models from scratch at scale, or use pre-trained model checkpoints provided with the BioNeMo Framework for fine-tuning for downstream tasks. Currently, BioNeMo supports models such as ESM1nv, ESM2nv, ProtT5nv, DNABERT, OpenFold, EquiDock, DiffDock, and MegaMolBART. To read more about BioNeMo, visit &lt;a href="https://docs.nvidia.com/bionemo-framework/latest/"&gt;the documentation page&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_3527" style="width: 1689px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3527" loading="lazy" class="wp-image-3527 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/15/HPCBlog-281-fig1-1.png" alt="Figure 1: This image shows the workflow for developing models on NVIDIA BioNeMo. The process is divided into phases for model development and customization and then fine-tuning and deployment." width="1679" height="691"&gt;
 &lt;p id="caption-attachment-3527" class="wp-caption-text"&gt;Figure 1: This image shows the workflow for developing models on NVIDIA BioNeMo. The process is divided into phases for model development and customization and then fine-tuning and deployment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In this post, we’ll walk through how to deploy the NVIDIA BioNeMo Framework on Amazon Elastic Kubernetes Service (Amazon EKS). Amazon EKS provides a fully managed Kubernetes service, making it simpler to run distributed, containerized generative AI workloads at scale. The process will cover:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Setting up an EKS cluster with NVIDIA GPU nodes&lt;/li&gt; 
 &lt;li&gt;Leveraging Amazon FSx for Lustre for high-performance data storage and sharing&lt;/li&gt; 
 &lt;li&gt;Downloading and ingesting Uniref-50 data in a machine learning friendly format&lt;/li&gt; 
 &lt;li&gt;Running a distributed pre-training job to train the ESM-1nv model&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;div id="attachment_3528" style="width: 1145px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3528" loading="lazy" class="size-full wp-image-3528" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/15/HPCBlog-281-fig2-1.png" alt="Figure 2:&amp;nbsp; This architecture diagram shows Amazon EKS cluster with GPU nodes, Amazon FSx for Lustre filesystem, and BioNeMo containers. GPU nodes are optimized for machine learning workloads. The FSx filesystem enables fast access to data needed for distributed training. Amazon CloudWatch is used for logging and monitoring." width="1135" height="1003"&gt;
 &lt;p id="caption-attachment-3528" class="wp-caption-text"&gt;Figure 2: This architecture diagram shows Amazon EKS cluster with GPU nodes, Amazon FSx for Lustre filesystem, and BioNeMo containers. GPU nodes are optimized for machine learning workloads. The FSx filesystem enables fast access to data needed for distributed training. Amazon CloudWatch is used for logging and monitoring.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We leveraged the &lt;a href="https://github.com/aws-ia/terraform-aws-eks-blueprints"&gt;Amazon EKS Blueprints for Terraform &lt;/a&gt;and &lt;a href="https://github.com/awslabs/data-on-eks"&gt;Data on EKS&lt;/a&gt; open source projects to build a robust Kubernetes infrastructure on EKS using Infrastructure as Code best practices. These projects enabled us to quickly stand up an EKS cluster while meeting security and operational excellence requirements.&lt;/p&gt; 
&lt;p&gt;In particular, the Terraform blueprints let us create a production-grade EKS cluster with networking, security groups, node groups, and other critical components out-of-the-box. The Data on EKS repository provided examples for running data-intensive workloads such as Apache Spark and TensorFlow on EKS. Together, these tools allowed us to launch an EKS cluster purpose-built for large-scale data processing in a reproducible and automated fashion. We can easily scale the cluster up to hundreds of nodes to handle compute-intensive jobs. Adopting these community-built modules accelerated our delivery timelines while ensuring the infrastructure remained secure, observable, and operationally robust as it scaled. The flexibility to customize the modules as needed also enabled us to tailor the infrastructure to the specific needs of our data workloads.&lt;/p&gt; 
&lt;h1&gt;The NVIDIA BioNeMo on EKS Blueprint&lt;/h1&gt; 
&lt;p&gt;We published all of the templates we used to deploy BioNeMo on EKS as a new Data on EKS &lt;a href="https://github.com/awslabs/data-on-eks/tree/main/ai-ml/bionemo"&gt;blueprint&lt;/a&gt; on GitHub. We’ll continue to iterate and improve on this blueprint over time, so you should bookmark and refer to the &lt;a href="https://awslabs.github.io/data-on-eks/docs/gen-ai/training/bionemo"&gt;official documentation&lt;/a&gt; on the Data on EKS website moving forward. However, we will discuss key configuration details and technical details here for reference.&lt;/p&gt; 
&lt;p&gt;Step one, as always, is to prepare the input data as a machine learning friendly structure. Uniref50 contains over 50 million unique protein sequences clustered from UniProt at 50% identity. This comprehensive set provides a foundation for vital tasks such as gene annotation and protein family prediction.&lt;/p&gt; 
&lt;p&gt;To leverage Uniref50’s scale, we downloaded and organized the data into training, validation, and test partitions. This layout enhances downstream machine learning by enabling effective model building, evaluation, and monitoring of overfitting. An Amazon FSx for Lustre shared filesystem provided the high-throughput storage needed for data sharing across the compute cluster nodes.&lt;/p&gt; 
&lt;p&gt;After data preparation, we are ready to run the pretraining job. It is important to leverage all available NVIDIA GPU resources for maximum performance. P5 instances powered by NVIDIA H100 Tensor Core GPUs offer the best performance, however, in this example, we used two p3.16xlarge instances with eight GPUs each for demonstration purposes. To utilize all 16 GPUs, we configured the &lt;code&gt;PytorchJob&lt;/code&gt; &lt;a href="https://github.com/awslabs/data-on-eks/blob/main/ai-ml/bionemo/examples/training/esm1nv_pretrain-job.yaml"&gt;custom resource&lt;/a&gt; with one GPU per replica:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-yaml"&gt;resources:
  requests:
    nvidia.com/gpu: 1
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;We set replicas to match the total number of available GPUs across the worker nodes:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-yaml"&gt;pytorchReplicaSpecs:
&amp;nbsp; Worker:
&amp;nbsp;&amp;nbsp;&amp;nbsp; replicas: 16&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;To allocate 8 processes per GPU node, we set:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-yaml"&gt;nprocPerNode: "8"&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;By default, workloads that don’t require GPUs won’t be scheduled on our GPU nodes. To make sure our BioNeMo pods were scheduled on the GPU nodes, we added tolerations:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-yaml"&gt;tolerations:
-  Key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;To utilize a larger cluster for BioNeMo analysis, change the above values to match the cluster size. For example, to fully leverage a cluster of 8 p3.8xlarge instances, each with 4 GPUs, you would need to change the following parameters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Set “&lt;code&gt;nprocPerNode&lt;/code&gt;” to 4 to indicate the number of GPUs available per node.&lt;/li&gt; 
 &lt;li&gt;Set “&lt;code&gt;replicas&lt;/code&gt;” to 32 to total the number of GPUs across the 8 nodes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As each p3.8xlarge instance contains 4 GPUs, 8 instances x 4 GPUs per instance = 32 GPUs in the cluster.&lt;/p&gt; 
&lt;p&gt;By properly configuring these parameters according to the resources provisioned, you can efficiently parallelize the training across the all-available GPUs in the cluster. For more information on configuring Pytorch and Kubeflow for your specific needs, read the &lt;a href="https://pytorch.org/docs/stable/distributed.html"&gt;Pytorch distributed module documentation&lt;/a&gt; and the &lt;a href="https://www.kubeflow.org/docs/components/training/"&gt;Kubeflow Training Operator’s documentation&lt;/a&gt;&lt;u&gt;,&lt;/u&gt; respectively.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In summary, leveraging technologies such as NVIDIA BioNeMo on Amazon EKS can accelerate AI-powered drug discovery by orders of magnitude. By combining the power of generative models with robust infrastructure for distributed training, researchers can rapidly analyze massive protein datasets to uncover new drug compound candidates. Automating infrastructure deployment using Terraform modules and best practices for maximum GPU utilization, storage, and monitoring enables workloads to benefit from security, scalability, and observability. Subject matter experts are a key resource for drug discover, and purpose-built generative AI platforms on cloud-native infrastructure can augment their creativity and intuition. As this technology continues maturing, we may see further breakthroughs in delivering life-saving treatments to patients faster.&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/15/nilkanthp-nvidia-profile.png" alt="Neel Patel" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Neel Patel&lt;/h3&gt; 
  &lt;p&gt;Neel Patel is a drug discovery scientist at NVIDIA, who focuses on cheminformatics and computational structural biology. Before joining NVIDIA, Patel was a computational chemist at Takeda Pharmaceuticals. He holds a Ph.D. from the University of Southern California. He lives in San Diego with his family and enjoys hiking and travelling.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Data, emerging technologies, and the circular economy: how Accenture and AWS are unlocking environmental and business impact</title>
		<link>https://aws.amazon.com/blogs/hpc/data-emerging-technologies-and-the-circular-economy-how-accenture-and-aws-are-unlocking-environmental-and-business-impact/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Tue, 30 Apr 2024 12:27:06 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">cfe27b15e171b02db52875e9424a371ce2457466</guid>

					<description>Realizing the $4.5 trillion circular economy opportunity requires accurate data, scalable HPC and agile tools. Read this post to discover how AWS and Accenture partner for real progress.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright wp-image-3572 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/Data-technology-and-the-circular-economy-how-Accenture-and-AWS-are-unlocking-environmental-and-business-impact.png" alt="Data, technology, and the circular economy: how Accenture and AWS are unlocking environmental and business impact" width="380" height="212"&gt;This post was contributed by Ilan Gleiser, Principal Specialist, Emerging Technologies at AWS, Joshua Curtis, Circular Intelligence Global Lead and Patrick Ford, Circular Intelligence North America Lead, Accenture Sustainability Services&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;It is well documented that the circular economy is an opportunity for positive impact on business and society. Accenture’s analysis presents an economic opportunity of $4.5tn value is at stake for the global economy to 2030 by departing from our current ‘&lt;em&gt;take-make-waste&lt;/em&gt;’ economic system [1]. Ellen MacArthur Foundation outlines the importance of circularity as a solution to climate change, with 45% of the required carbon emission reductions to achieve a 1.5-degree world coming from how we make and consume products [2].&lt;/p&gt; 
&lt;p&gt;It’s clear that resource use and circularity are critical to the creation of a sustainable, healthy economy. But how do we realize this value? How does a business identify where and how circular strategies can create financial and environmental impact?&lt;/p&gt; 
&lt;p&gt;In this post, we will explore the challenges to achieving accurate and actionable data on circular economy performance and impact, and the solutions that lie in emerging technologies. Join us as we explore opportunities for kicking off and accelerating the data transformation needed to drive authentic, impact-driven progress on circularity.&lt;/p&gt; 
&lt;h2&gt;Achieving data-driven circularity&lt;/h2&gt; 
&lt;p&gt;The importance of transitioning to circular business models is why the European Commission is, for the first time, making measurement and disclosure of resource use and circular economy impacts mandatory for companies. The newly-launched European Sustainability Reporting Standards (ESRS) include a requirement [3] for companies – where material – to report on circular economy metrics like:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;the percentage of material used for products and packaging that are renewable, recycled or re-used&lt;/li&gt; 
 &lt;li&gt;the volume of waste by stream that is recovered by destination&lt;/li&gt; 
 &lt;li&gt;the financial effects of material risks and opportunities arising from resource use&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;But how do we calculate these metrics? How do we collect, aggregate, and analyze the data in a way that doesn’t require significant time and resources year on year? How do we not only do this to understand where we are, but also to determine where we need to go?&lt;/p&gt; 
&lt;p&gt;While there is an increasing number of new sustainability measurement products being presented to the market, there is no one ‘tool’ to answer these questions. In fact, these questions themselves are not new — they are the essence of data-driven decision-making which is the foundation of any profitable business. The circular measurement challenge is a data challenge and must be approached as such.&lt;/p&gt; 
&lt;p&gt;What is new – and evolving – is the potential application of digital technologies to support the transformation of circular economy data management for companies. For example, the advancement of the Internet of Things (IoT) enables tracking of product movement and health through use phases; machine learning algorithms can help companies identify patterns in circular economy data, like trends in demand for certain recycled materials; and blockchain technology can be used to create a transparent and secure ledger of circular-related transactions, enabling stakeholders to track and verify the movement of materials and products through the value chain.&lt;/p&gt; 
&lt;p&gt;As we continue to tackle the circular measurement challenge, it is essential to approach it with a data-driven mindset. Digital technologies have the potential to revolutionize how we manage and analyze circular economy data, allowing us to create a more sustainable and efficient economy for the future. Accenture and AWS are collaborating to make these applications a reality.&lt;/p&gt; 
&lt;h2&gt;The challenges to circular data transformation&lt;/h2&gt; 
&lt;p&gt;To help ensure digital solutions are effective in managing circular economy performance, it’s crucial to design them to address specific challenges faced by businesses. Let’s begin by exploring these challenges.&lt;/p&gt; 
&lt;p&gt;First, selecting the right metrics themselves is not straightforward. We mentioned the European Commission’s regulations ESRS E5 on resource use and the Circular Economy. They provide headline metrics for business disclosure. The &lt;a href="https://pacecircular.org/sites/default/files/2023-01/CEIC_Circular%20Target%20Activation%20Guides_FINAL_01182023.pdf"&gt;Circular Target-Setting Guidance&lt;/a&gt; from the Circular Economy Indicators Coalition (CEIC), a partnership between The Platform for Accelerating the Circular Economy (PACE) and Circle Economy (supported by Accenture) provides an overview of leading measurement methodologies and approaches for business implementation.&lt;/p&gt; 
&lt;p&gt;These are important starting points for business across industries, but they don’t account for the specific value chains or functional priorities of businesses in different sectors. For example, the metrics to measure circular economy performance (and therefore the data required) vary significantly for a fashion retailer compared to an oil and gas major. Ultimately, selecting the right metrics must be led by each business, drawing on the wealth of supporting materials, best practices and market standards.&lt;/p&gt; 
&lt;p&gt;Next comes the hard part: identifying, collecting and transforming the data. Comprehensive circular measurement relies on data from across the value chain, often not tracked in existing enterprise systems. The foundational data itself is simple enough – what materials are being used and where do they come from; what waste is being produced and where is it going – are tangible examples. The challenge is collecting that data across product lines, business units, and geographies and then transforming the data to be usable. For example, when calculating your percentage of materials that are recycled, renewable or re-used (as ESRS E5 requires), materials data must be segmented in ways not currently built into enterprise data capture. Without technology, this requires line-by-line segmentation based on data that is available e.g. through supplier declarations. The bottom line is that collecting data to measure circular performance is an arduous process, requiring time and costs, and is hindered by data gaps. To do this at the business level, in a way that enables action, is not only helped by digital technologies, it depends upon them.&lt;/p&gt; 
&lt;p&gt;Finally, companies must transform this data into actionable insights to guide decision-making. Circularity is not an end, but a means to optimize planetary and business impact. To accelerate this impact, resource use data like the above example regarding materials that are recycled, renewable or re-used, must be connected with other internal and external data sets like sales data and emissions intensity factors. Companies must understand how different material choices impact carbon emissions, as well as procurement costs and business profitability. The true story of corporate circularity is of trade-offs and investment requirements to capture long-term value. Without a comprehensive approach to circular economy measurement and data transformation, understanding those trade-offs properly and making impact-driven decisions is impossible. This again adds complexity to data collection and analysis, with the only solution for ongoing insight generation being an automated, centralized approach, like a circular and/or sustainability data lake, which combines data sets and applies analytics solutions for calculation and visualization.&lt;/p&gt; 
&lt;div id="attachment_3564" style="width: 867px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3564" loading="lazy" class="size-full wp-image-3564" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-13.24.08.png" alt="Figure 1. The Circular Business Hub: Illustration of data visualization for circular economy performance management. Companies must overcome challenges to achieve measurement and visualization of circularity at each pillar of the business value chain." width="857" height="537"&gt;
 &lt;p id="caption-attachment-3564" class="wp-caption-text"&gt;Figure 1. The Circular Business Hub: Illustration of data visualization for circular economy performance management. Companies must overcome challenges to achieve measurement and visualization of circularity at each pillar of the business value chain.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;The role of emerging technologies&lt;/h2&gt; 
&lt;p&gt;Accenture and AWS are collaborating to bring the best of their combined data, technology and sustainability expertise to transform circular economy data management. AWS offers the broadest set of capabilities in artificial intelligence (AI), machine learning (ML), Internet of Things (IoT), big data analytics, and high-performance computing (HPC) in the market. Accenture is the world’s leading integrator of AWS solutions and technologies – they’ve completed over 1,100 projects with us over 15 years of partnership.&lt;/p&gt; 
&lt;p&gt;Teaming up on the circular economy, Accenture brings its 12+ years of client experience in circular economy strategy and implementation, and 100+ global circular economy experts, to guide the application of these digital solutions to unlock value for joint customers from data-driven circularity.&lt;/p&gt; 
&lt;p&gt;Accenture has developed core assets as part of a suite of circular intelligence solutions, powered by AWS. These include industry-specific KPI frameworks, a foundational data model and a proof-of-concept dashboard to act as a platform for client co-development and customization. Building upon these assets, a core priority of this collaboration is to enable the automation of circular data ingestion, transformation and analysis to enable ongoing performance measurement and generation of actionable insights for companies across the value chain. To do this, we’re leveraging &lt;a href="https://www.accenture.com/us-en/services/cloud/aws-business-group#block-velocity"&gt;Velocity&lt;/a&gt;, a co-funded and co-developed platform that adds new cloud innovations up to 50% faster, to develop the data architecture which pulls in resource use and circular economy-related datasets into a central circular data lake.&lt;/p&gt; 
&lt;p&gt;A circular data lake (Figure 2) works by taking raw data from siloed sources and bringing it into a unified, organized system. This process is made possible using the AWS Transfer family, which collects data and brings it into a raw layer.&lt;/p&gt; 
&lt;div id="attachment_3565" style="width: 835px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3565" loading="lazy" class="size-full wp-image-3565" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-13.24.48.png" alt="Figure 2. Reference Architecture of an automated circular data lake for managing circular economy measurement and insight generation" width="825" height="375"&gt;
 &lt;p id="caption-attachment-3565" class="wp-caption-text"&gt;Figure 2. Reference Architecture of an automated circular data lake for managing circular economy measurement and insight generation&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Once the data is in the raw layer, one or more &lt;em&gt;ETL&lt;/em&gt; (extract, transform, and load) workstreams are triggered. These ETL workstreams work together to clean, refine, and organize the data. To facilitate the ETL process, the reference architecture employs several AWS Glue jobs and AWS Lambda functions. These functions and jobs help to ensure that the ETL workflows are fully orchestrated and operate efficiently and effectively, to produce the refined data necessary for circular metrics and KPIs.&lt;/p&gt; 
&lt;p&gt;Once the data is fully curated and prepared, it’s ready for business intelligence and machine learning (AWS Athena, Amazon Redshift, and Amazon SageMaker). These services enable companies to derive value from the data by analyzing it, identifying patterns and trends. By doing so, it’s possible to transform siloed data from a wide range of sources into actionable insights that can drive circular business progress.&lt;/p&gt; 
&lt;p&gt;The circular data lake is designed to centralize all data related to a business’ resource use and subsequent impact. With this, it must be designed and implemented in connection with other environmental objectives like climate and nature-related data sources. Indeed, the circular data lake is designed to be a component of an organization’s overall sustainability data lake fabric, creating a single system of record for ESG data management.&lt;/p&gt; 
&lt;p&gt;Building upon the circular data architecture, created and tested with joint customers, Accenture and AWS are building solutions for companies focused on creating value from data in functional areas of circular transformation. This means going beyond just measuring circular economy performance (which we refer to as ‘foundational’ use cases) to applying digital solutions for value creation from circular insights (these are ‘advanced’ use cases).&lt;/p&gt; 
&lt;div id="attachment_3566" style="width: 836px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3566" loading="lazy" class="size-full wp-image-3566" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/17/CleanShot-2024-04-17-at-13.25.35.png" alt="Figure 3. Overview of foundational and advanced circular intelligence use cases at each stage of an industry-agnostic value chain " width="826" height="385"&gt;
 &lt;p id="caption-attachment-3566" class="wp-caption-text"&gt;Figure 3. Overview of foundational and advanced circular intelligence use cases at each stage of an industry-agnostic value chain&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 3 shows examples of circular economy use cases split between foundational and advanced. Circular intelligence helps companies measure the impact of their investments and compare results over time. For more examples of how digital technologies enable a circular economy, please see &lt;a href="https://aws.amazon.com/blogs/hpc/how-to-make-digital-technologies-for-the-circular-economy-work-for-your-business/"&gt;this&lt;/a&gt; blog post.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 3. Overview of foundational and advanced circular intelligence use cases at each stage of an industry-agnostic value chain &lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;The circular intelligence maturity journey&lt;/h2&gt; 
&lt;p&gt;Let’s look at an example of how AWS services and solutions are being used to enable foundational and advanced use cases in the procurement stage of the product development lifecycle.&lt;/p&gt; 
&lt;h3&gt;Example: how to transition away from virgin, non-renewable materials to optimize for decarbonization, cost reduction and risk management?&lt;/h3&gt; 
&lt;p&gt;The materials that companies buy for products and services are a critical component of their environmental impact, while the reliance on virgin, finite materials also presents a growing supply chain risk for many industries.&lt;/p&gt; 
&lt;p&gt;To achieve foundational circular procurement decision-making, machine learning algorithms can analyze past procurement data and identify patterns and insights to make better purchasing decisions. For example:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;With AWS’s &lt;a href="https://aws.amazon.com/solutions/ai-ml/intelligent-document-processing/"&gt;Intelligent Document Processing&lt;/a&gt; solutions, companies can streamline the ingestion of bills of materials across product lines. By automating the procurement data ingestion process, companies can measure their circular input footprint (share of circular materials per product and/or packaging type), as well as the cost and carbon impact of circular inputs. This allows for faster and more accurate decision-making when selecting sustainable alternatives.&lt;/li&gt; 
 &lt;li&gt;Another powerful tool is &lt;a href="https://aws.amazon.com/textract/"&gt;Amazon Textract&lt;/a&gt;, which can automatically ingest supplier declarations, saving time and increasing accuracy. By digitizing these declarations, companies can easily track progress toward their circular procurement goals and identify areas that require improvement.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These applications are a significant value add for companies in reducing the time required to assess circular performance in procurement, while enabling decisions that account for trade-offs, and can accelerate the highest value initiatives.&lt;/p&gt; 
&lt;p&gt;The value opportunity doesn’t end there, however.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;With generative AI services, like &lt;a href="https://aws.amazon.com/bedrock/"&gt;Amazon Bedrock&lt;/a&gt;, companies can build on this foundation for predictive analytics, real-time scenario modeling and advanced forecasting – AWS Partner &lt;a href="https://www.simudyne.com/"&gt;Simudyne&lt;/a&gt;, uses high-performance computing enabled simulations, coupled with LLM powered chatbots, to provide recommendations of how companies can decarbonize their supply chains, and therefore reduce their scope 3 emissions.&lt;/li&gt; 
 &lt;li&gt;By leveraging market data and research to identify opportunities for alternative materials and supply chain risk assessment, companies can gain an edge over the competition. For example, AWS partner &lt;a href="https://goodchemistry.com/"&gt;Good Chemistry&lt;/a&gt;, uses &lt;a href="https://aws.amazon.com/hpc/"&gt;HPC clusters&lt;/a&gt;, orchestrated by AWS Batch, to &lt;a href="https://aws.amazon.com/blogs/hpc/massively-scaling-quantum-chemistry-to-support-a-circular-economy/"&gt;design new materials&lt;/a&gt; and accelerate the identification of molecules to substitute or destroy toxic chemicals from the production process and environment.&lt;/li&gt; 
 &lt;li&gt;By using SageMaker for its &lt;a href="https://aws.amazon.com/blogs/machine-learning/remote-monitoring-of-raw-material-supply-chains-for-sustainability-with-amazon-sagemaker-geospatial-capabilities/"&gt;geospatial capabilities&lt;/a&gt;, companies are proactively addressing potential environmental risks. By monitoring deforestation in real-time with satellite data, IOT sensors and drones, coupled with machine learning algorithms, regulators, insurance companies and businesses can be aware of the risk of their current material footprint.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Overall, the use of AWS emerging technologies helps companies, governments and other stakeholders, transition towards circular materials in the short and long term, making sustainability a priority in their procurement process.&lt;/p&gt; 
&lt;h2&gt;Call to action&lt;/h2&gt; 
&lt;p&gt;Each company’s journey on circular economy measurement will look different, depending on industry, strategic priorities, and technical maturity. At Accenture and AWS, we believe solutions must be fit-for-purpose and co-designed for the specific business context. While there are growing options for ‘plug-and-play’ sustainability solutions, specifically in carbon management, developing the data foundation for automated baselining across the value chain and moving towards value-creating technology applications requires a level of customization and co-development using existing products and repeatable solutions where available.&lt;/p&gt; 
&lt;p&gt;The first step to get started is defining your circular economy blueprint with key metrics across each area of your business. This is the foundation for a data-driven strategy and many companies are working hard to collect data for reporting on circular performance on an annual basis. To then understand and plan for foundational and advanced circular measurement use cases, it is key to assess your current functional and technical maturity to measure and manage these metrics. Identifying where the raw data lives, how it is collected and stored, and who is responsible for it is critical to map the journey towards automation. From there, it is a case of prioritization of metrics and use cases through engaging with stakeholders across different business groups. The end-users of the data and insights must be involved in the full journey to ensure the applied solution serves the required needs.&lt;/p&gt; 
&lt;p&gt;The next step is to design, build and test. Cross-functional teams will need to work together to integrate solutions as part of a circular data lake, and develop the user interface for practical, day-to-day business decision making. Proof-of-Concepts (POCs) are valuable in building the foundational technical architecture, illustrating the value with a tangible output, and establishing buy-in from leadership and key stakeholders. This can then prime you for rapid deployment and scale.&lt;/p&gt; 
&lt;p&gt;From there, deployment must be thought of as a multi-generational journey. Companies should focus on their key requirements for a Minimum Viable Product (MVP) solution that supports their core business priorities. Be it cross-value chain performance management, or specific use case advancement, the MVP must also include solutions for data pipeline automation and a data governance strategy. Accenture and AWS are supporting companies across these phases of implementation, including the development of targeted POCs and MVPs to tackle the key challenges your business is facing on circular measurement and prove the value of emerging technologies applied to these challenges. This landscape is rapidly evolving and our joint assets are developing with every implementation; co-development with leading businesses is at the heart of this.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Circular economy measurement requires more than an off-the-shelf sustainability solution; it is a data and technology problem that requires a comprehensive approach. By leveraging the power of AWS and Accenture’s expertise, businesses can unlock the full potential of the circular economy. By capturing data into a purpose driven data lake and running advanced analytics, our joint approach provides businesses with actionable insights and recommendations for optimizing resource usage and reducing waste. By prioritizing sustainability and embracing greater intelligence, businesses can drive positive environmental and business impacts, paving the way for a more sustainable future.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;h4&gt;References&lt;/h4&gt; 
&lt;p&gt;[1] Lacy, Long and Spindler, The Circular Economy Handbook: Realizing the Circular Advantage (2019)&lt;br&gt; [2] Ellen MacArthur Foundation, Completing the Picture: How the Circular Economy Tackles Climate Change (2021)&lt;br&gt; [3]European Sustainability Reporting Standards in scope for companies with a turnover above €150 million in EU&lt;/p&gt;</content:encoded>
					
		
		
			</item>
	</channel>
</rss>