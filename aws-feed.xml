<?xml version="1.0" encoding="UTF-8" standalone="no"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" version="2.0">

<channel>
	<title>AWS HPC Blog</title>
	<atom:link href="https://aws.amazon.com/blogs/hpc/feed/" rel="self" type="application/rss+xml"/>
	<link>https://aws.amazon.com/blogs/hpc/</link>
	<description>Just another Amazon Web Services site</description>
	<lastBuildDate>Tue, 23 Jan 2024 16:01:54 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>Leveraging Seqera Platform on AWS Batch for machine learning workflows – Part 2 of 2</title>
		<link>https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-2-of-2/</link>
		
		<dc:creator><![CDATA[Ben Sherman]]></dc:creator>
		<pubDate>Tue, 23 Jan 2024 16:01:54 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Genomics]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<guid isPermaLink="false">0082e40ba30d67f985dcb877d3cb64053a9b9f0a</guid>

					<description>In this second part of using Nextflow for machine learning for life science workloads, we provide a step-by-step guide, explaining how you can easily deploy a Seqera environment on AWS to run ML and other pipelines.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img class="alignright size-full wp-image-3236" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/18/Batch-and-Seqera.png" alt="Batch and Seqera" width="380" height="190"&gt;This post was contributed by Dr Ben Sherman (Seqera) and Dr Olivia Choudhury (AWS), Paolo Di Tomasso, and Gord Sissons from Seqera, and Aniket Deshpande and Abhijit Roy from AWS.&lt;br&gt; &lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;In &lt;a href="https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-1-of-2/"&gt;part one of this two-part series&lt;/a&gt;, we explained how Nextflow and &lt;a href="https://seqera.io/platform/"&gt;Seqera Platform&lt;/a&gt; work with AWS, and we provided a sample Nextflow pipeline that performs ML model training and inference for image analysis to illustrate how Nextflow can support custom ML-based workflows. We also discussed how AWS customers are using this today.&lt;/p&gt; 
&lt;p&gt;In this second post, we will provide a step-by-step guide explaining how users new to Seqera Platform can rapidly get started on AWS, maximizing the use of &lt;a href="https://aws.amazon.com/batch"&gt;AWS Batch&lt;/a&gt;, &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service (Amazon S3&lt;/a&gt;), and other AWS services.&lt;/p&gt; 
&lt;p&gt;Depending on your familiarity with AWS, these steps should take around an hour to complete.&lt;/p&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;Before following the step-by-step guide here, readers should:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Have an AWS account and be familiar with using the &lt;a href="https://aws.amazon.com/console/"&gt;AWS Management Console&lt;/a&gt; or &lt;a href="https://aws.amazon.com/cli/"&gt;A&lt;/a&gt;&lt;a href="https://aws.amazon.com/cli/"&gt;WS&lt;/a&gt;&lt;a href="https://aws.amazon.com/cli/"&gt; Command Line Interface&lt;/a&gt; (CLI).&lt;/li&gt; 
 &lt;li&gt;Have a high-level understanding of &lt;a href="https://aws.amazon.com/iam"&gt;AWS Identity and Access Management&lt;/a&gt; (IAM) users and roles.&lt;/li&gt; 
 &lt;li&gt;Obtain a free &lt;a href="https://tower.nf"&gt;Seqera Cloud&lt;/a&gt; account for testing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Step-by-step guide&lt;/h2&gt; 
&lt;p&gt;This section explains how to set up an AWS environment to support pipeline execution, obtain a free Seqera Cloud account, and configure Seqera to launch and manage the ML pipeline introduced in part 1 of this series.&lt;/p&gt; 
&lt;h3&gt;Create an Amazon S3 bucket&lt;/h3&gt; 
&lt;p&gt;The first step is to create an S3 bucket in your preferred AWS region (we used &lt;em&gt;us-east&lt;/em&gt;).&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Once logged into the AWS console, navigate to the &lt;strong&gt;Amazon S3&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Click on the &lt;strong&gt;Create bucket&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Give the bucket a globally unique name (for example, we used &lt;code&gt;seqera-work-bucket&lt;/code&gt;, but you will need to choose something else) and select the AWS region where the bucket will reside.&lt;/li&gt; 
 &lt;li&gt;You can leave ACLs disabled and block all public access to the bucket since Seqera Platform will use this bucket internally. Others do not need to see the contents.&lt;/li&gt; 
 &lt;li&gt;Accept the default for the remaining settings and select &lt;strong&gt;Create bucket&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once the bucket is created, select it and record its &lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html"&gt;Amazon Resource Name&lt;/a&gt; (ARN) by clicking &lt;strong&gt;Copy ARN&lt;/strong&gt;. In our example, the ARN is &lt;code&gt;arn:aws:s3:::seqera-work-bucket&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You will need the name of the ARN to set up an AWS policy in the following steps.&lt;/p&gt; 
&lt;h3&gt;Setup your IAM credentials&lt;/h3&gt; 
&lt;p&gt;To create an AWS Batch environment and marshal other AWS cloud resources, Seqera will need AWS credentials. We’ll use the &lt;a href="https://docs.aws.amazon.com/iam/"&gt;AWS Identity and Access Management&lt;/a&gt; (IAM) service to create appropriate IAM policies and attach these policies to an AWS IAM user or role.&lt;/p&gt; 
&lt;p&gt;First, create a policy with sufficient privileges to manage the AWS Batch environment and support pipeline execution. Follow the steps below to create a custom “&lt;em&gt;seqera_forge_policy&lt;/em&gt;.”&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Navigate to the IAM console by searching for the &lt;strong&gt;IAM&lt;/strong&gt;&lt;strong&gt; service&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Access Management&lt;/strong&gt;, select &lt;strong&gt;Policies&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Click on &lt;strong&gt;Create policy&lt;/strong&gt; to create a new IAM policy&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Specify permissions&lt;/strong&gt;, select the &lt;strong&gt;JSON&lt;/strong&gt;&lt;strong&gt; policy editor&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Copy the contents of the default &lt;code&gt;forge-policy.json&lt;/code&gt; file &lt;a href="https://github.com/seqeralabs/nf-tower-aws/blob/master/forge/forge-policy.json"&gt;located on GitHub&lt;/a&gt; into the policy editor&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Next&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Give the policy a name (like &lt;code&gt;Seqera_Forge_Policy&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Select &lt;strong&gt;Create policy&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that this is a sample policy and you should review the policy based on your security requirements, particularly if you want to use it for production.&lt;/p&gt; 
&lt;p&gt;With these steps, you have created a policy to give Seqera sufficient permissions to deploy an AWS Batch environment on your behalf.&lt;/p&gt; 
&lt;p&gt;Next, you’ll need to create a similar policy that gives your IAM user or role permission to access the Amazon S3 bucket you created earlier.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Repeat the steps in IAM as before to create a new policy for accessing the S3 bucket by clicking &lt;strong&gt;Create policy&lt;/strong&gt; from the &lt;strong&gt;Policies&lt;/strong&gt; screen&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Specify permissions&lt;/strong&gt;, select the &lt;strong&gt;JSON&lt;/strong&gt;&lt;strong&gt; policy editor&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Copy the contents of the default &lt;code&gt;s3-bucket-write.json&lt;/code&gt; file &lt;a href="https://github.com/seqeralabs/nf-tower-aws/blob/master/launch/s3-bucket-write.json"&gt;located on GitHub&lt;/a&gt; into the policy editor&lt;/li&gt; 
 &lt;li&gt;Update the ARN in the &lt;code&gt;S3-bucket-write&lt;/code&gt; policy to reflect the ARN for the S3 bucket you created&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Next&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Give the policy a name (like &lt;code&gt;Seqera_S3_Bucket_Write&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Select &lt;strong&gt;Create policy&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You should now have two new customer-managed policies, as shown in Figure 1.&lt;/p&gt; 
&lt;div id="attachment_3221" style="width: 1089px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3221" loading="lazy" class="size-full wp-image-3221" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.31.29.png" alt="Figure 1: Create IAM policies to manage AWS Batch environment and execute pipelines." width="1079" height="367"&gt;
 &lt;p id="caption-attachment-3221" class="wp-caption-text"&gt;Figure 1: Create IAM policies to manage AWS Batch environment and execute pipelines.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Next, attach these policies to an IAM user or IAM role. You may already have your own IAM users or roles defined, in which case you can bind the policies to existing users or roles.&lt;/p&gt; 
&lt;p&gt;If you are doing this for the first time, follow these steps to create a new IAM user called &lt;code&gt;seqera-user,&lt;/code&gt; and bind the policies created above to that user like this:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;From the AWS console, navigate to the &lt;strong&gt;IAM&lt;/strong&gt;&lt;strong&gt; console&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Users&lt;/strong&gt;, select &lt;strong&gt;Create user&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Give the user a name like &lt;code&gt;seqera-user&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;There is no need to provide this IAM user with access to the AWS Management Console&lt;/li&gt; 
 &lt;li&gt;Select &lt;strong&gt;Attach policies directly&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Search for the two customer-managed policies you created earlier by searching on the string Seqera and then select these two policies to apply them to the new IAM user&lt;/li&gt; 
 &lt;li&gt;After attaching the policies, select &lt;strong&gt;Create user&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The final step is to create an access key based on the new IAM user that will be used by Seqera:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Under &lt;strong&gt;Access management&lt;/strong&gt;&lt;strong&gt; / &lt;/strong&gt;&lt;strong&gt;Users&lt;/strong&gt;, select &lt;strong&gt;seqera-user&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Record the user’s ARN for future reference&lt;/li&gt; 
 &lt;li&gt;Click on &lt;strong&gt;Create Access key&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Record the &lt;strong&gt;Access key&lt;/strong&gt; and &lt;strong&gt;Secret access key&lt;/strong&gt; and &lt;em&gt;store them in a safe place&lt;/em&gt;. You’ll need these credentials to create a compute environment in the Seqera Platform.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Create a free Seqera Cloud account&lt;/h3&gt; 
&lt;p&gt;The next step is to create a free Seqera Cloud account. Navigate to &lt;a href="https://seqera.io"&gt;https://seqera.io&lt;/a&gt; and click on &lt;strong&gt;Login / Sign up&lt;/strong&gt;. You can create a free account and sign into the Seqera cloud environment hosted on AWS infrastructure using GitHub or Google credentials – or by providing an email address.&lt;/p&gt; 
&lt;p&gt;After logging in, you’ll be directed to the Seqera Platform console.&lt;/p&gt; 
&lt;h3&gt;Setup an AWS Batch Compute Environment in Seqera&lt;/h3&gt; 
&lt;p&gt;On initial login to the Seqera Platform, you’ll be taken to a community showcase workspace with ready-to-run pipelines and preconfigured AWS compute environments. New users can experiment with community showcase pipelines and Datasets and enjoy up to 100 hours of free AWS cloud credits.&lt;/p&gt; 
&lt;div id="attachment_3222" style="width: 1095px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3222" loading="lazy" class="size-full wp-image-3222" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.32.39.png" alt="Figure 2: Log in to Seqera Platform to use community showcase pipelines or add your own pipelines." width="1085" height="680"&gt;
 &lt;p id="caption-attachment-3222" class="wp-caption-text"&gt;Figure 2: Log in to Seqera Platform to use community showcase pipelines or add your own pipelines.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;You can also watch a &lt;a href="https://www.youtube.com/watch?v=bZUHuCSKFpA"&gt;short introductory video&lt;/a&gt; that explains how to get started with the Seqera interface.&lt;/p&gt; 
&lt;p&gt;To add your own compute environments and pipelines, you’ll need to navigate to your personal workspace, as shown in Figure 2. Click on the selector next to &lt;strong&gt;community | showcase&lt;/strong&gt; and select your personal workspace by selecting your name where it appears under &lt;strong&gt;user&lt;/strong&gt;. You can optionally create a new workspace and store your compute environments and pipelines there.&lt;/p&gt; 
&lt;p&gt;The first time you login, your personal workspace will have no compute environments, datasets, or pipelines. You can create a new AWS Batch Compute Environment (CE) by following these steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Select the &lt;strong&gt;Compute Environments&lt;/strong&gt; tab from within your personal workspace&lt;/li&gt; 
 &lt;li&gt;Click on &lt;strong&gt;Add Compute Environment&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Define a new AWS Batch compute environment, by completing the form, as shown in Figure 3:&lt;/p&gt; 
&lt;div id="attachment_3223" style="width: 1102px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3223" loading="lazy" class="size-full wp-image-3223" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.33.12.png" alt="Figure 3: Create a new compute environment." width="1092" height="309"&gt;
 &lt;p id="caption-attachment-3223" class="wp-caption-text"&gt;Figure 3: Create a new compute environment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The first time you add user credentials, you’ll be asked to supply the IAM Access key and Secret access key you generated earlier.&lt;/p&gt; 
&lt;p&gt;Seqera Platform will store these credentials securely and ask you to assign a name to the credentials for future reference. Your AWS credentials will &lt;u&gt;not&lt;/u&gt; be visible to you, or other users.&lt;/p&gt; 
&lt;p&gt;Continue filling in the form, as illustrated in Figure 4:&lt;/p&gt; 
&lt;div id="attachment_3224" style="width: 1036px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3224" loading="lazy" class="size-full wp-image-3224" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.33.49.png" alt="Figure 4: Add credentials to store access keys and tokens for your compute environment." width="1026" height="541"&gt;
 &lt;p id="caption-attachment-3224" class="wp-caption-text"&gt;Figure 4: Add credentials to store access keys and tokens for your compute environment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Nextflow pipelines running on AWS Batch typically use Amazon S3 storage. Nextflow supports S3 natively, automatically copying data in and out of S3 and making it accessible to containerized bioinformatics tools.&lt;/p&gt; 
&lt;p&gt;To provide more efficient data handling, we use Seqera’s &lt;a href="https://seqera.io/fusion/"&gt;Fusion file system&lt;/a&gt; (Fusion v2) when setting up the pipeline. Fusion is a virtual, lightweight, distributed filesystem that allows any existing application to access object storage using the standard POSIX interface. Using the Fusion file system is optional but it’s recommended because it reduces the pipeline execution time and your cost by avoiding overhead related to data movement. If you use the Fusion file system, you must also enable &lt;a href="https://seqera.io/wave/"&gt;Wave containers&lt;/a&gt; in the compute environment since the Fusion client is deployed in a Wave container.&lt;/p&gt; 
&lt;p&gt;You can learn more about Fusion file system in the whitepaper &lt;a href="https://seqera.io/whitepapers/breakthrough-performance-and-cost-efficiency-with-the-new-fusion-file-system/"&gt;Breakthrough performance and cost-efficiency with the new Fusion file system&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Under &lt;strong&gt;Config mode&lt;/strong&gt;, select &lt;strong&gt;Batch Forge&lt;/strong&gt; to have Seqera automatically configure the AWS Batch account on your behalf.&lt;/p&gt; 
&lt;p&gt;For the provisioning model, select &lt;strong&gt;Spot&lt;/strong&gt;, as shown in Figure 5. Since Nextflow pipeline tasks can tolerate Amazon Elastic Compute Cloud (Amazon EC2) Spot Instances being interrupted, we recommend you use Spot Instances in most cases.&lt;/p&gt; 
&lt;div id="attachment_3225" style="width: 1082px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3225" loading="lazy" class="size-full wp-image-3225" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.34.17.png" alt="Figure 5: Select Batch Forge for Config mode and Spot for Provisioning model." width="1072" height="256"&gt;
 &lt;p id="caption-attachment-3225" class="wp-caption-text"&gt;Figure 5: Select Batch Forge for Config mode and Spot for Provisioning model.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3226" style="width: 1068px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3226" loading="lazy" class="size-full wp-image-3226" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.34.49.png" alt="Figure 6: Select default values for the rest of the configuration options." width="1058" height="566"&gt;
 &lt;p id="caption-attachment-3226" class="wp-caption-text"&gt;Figure 6: Select default values for the rest of the configuration options.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Accept the defaults for the remainder of the configuration options, as shown in Figure 6.&lt;/p&gt; 
&lt;p&gt;Once you’ve provided details of your Batch Compute Environment, click &lt;strong&gt;Add&lt;/strong&gt; to add the CE.&lt;/p&gt; 
&lt;p&gt;Assuming you have valid AWS credentials and have filled in the form correctly, you should see a new Compute Environment appear in your Seqera workspace.&lt;/p&gt; 
&lt;p&gt;After creating the Compute Environment in Seqera, you can login into the AWS Console and navigate to AWS Batch. Assuming you selected Spot provisioning, two new AWS Batch CEs – and queues – will have been created on your behalf.&lt;/p&gt; 
&lt;p&gt;Seqera Forge configures a &lt;em&gt;Head queue&lt;/em&gt; that dispatches Nextflow head jobs to a compute environment configured to use EC2 On-Demand instances. This prevents the Nextflow head job from being interrupted during execution.&lt;/p&gt; 
&lt;p&gt;A separate &lt;em&gt;Work queue&lt;/em&gt; and Spot-based CE are also automatically configured to support Nextflow compute tasks during pipeline execution.&lt;/p&gt; 
&lt;p&gt;The names of the queues and compute environments are assigned by Seqera Forge, like in Figure 7:&lt;/p&gt; 
&lt;div id="attachment_3227" style="width: 1098px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3227" loading="lazy" class="size-full wp-image-3227" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.35.34.png" alt="Figure 7: Assign queues and compute environments with Seqera Forge." width="1088" height="373"&gt;
 &lt;p id="caption-attachment-3227" class="wp-caption-text"&gt;Figure 7: Assign queues and compute environments with Seqera Forge.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Add the ML pipeline to the Launchpad&lt;/h3&gt; 
&lt;p&gt;Now that we have an AWS Batch CE set up in our Seqera workspace, we can add the ML pipeline to the Seqera Launchpad.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;From your workspace in the Seqera UI, select the &lt;strong&gt;Launchpad&lt;/strong&gt; tab&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Add pipeline&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Give the pipeline a name like &lt;code&gt;ML-hyperopt-pipeline&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Select the Compute environment created above&lt;/li&gt; 
 &lt;li&gt;Supply the pipeline’s repository URI – for our machine learning example, use &lt;code&gt;https://github.com/nextflow-io/hyperopt&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Click on the &lt;strong&gt;Revision number&lt;/strong&gt; field, and select the tag or branch name retrieved from the source code manager to select a particular pipeline branch or version. Use &lt;strong&gt;master&lt;/strong&gt; in this example.&lt;/li&gt; 
 &lt;li&gt;Specify the S3 bucket you created earlier as the work directory (e.g. &lt;code&gt;s3://seqera-work-bucket&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Config profiles&lt;/strong&gt;, select &lt;strong&gt;wave&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Under &lt;strong&gt;Advanced Options&lt;/strong&gt; in the Nextflow config file field, optionally set &lt;code&gt;dag.enabled=true&lt;/code&gt; if you would like Nextflow to generate a DAG (directed acyclic graph) file&lt;/li&gt; 
 &lt;li&gt;Accept the defaults for the rest of the pipeline configuration options&lt;/li&gt; 
 &lt;li&gt;Click &lt;strong&gt;Add&lt;/strong&gt; to add the pipeline to the Launchpad&lt;/li&gt; 
 &lt;li&gt;You should see the new &lt;code&gt;ML-hyperopt-pipeline&lt;/code&gt; appear in the Seqera Launchpad in your workspace&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div id="attachment_3228" style="width: 1112px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3228" loading="lazy" class="size-full wp-image-3228" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.37.06.png" alt="Figure 8: Add ML pipeline to Seqera Launchpad." width="1102" height="441"&gt;
 &lt;p id="caption-attachment-3228" class="wp-caption-text"&gt;Figure 8: Add ML pipeline to Seqera Launchpad.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Run the pipeline&lt;/h2&gt; 
&lt;p&gt;To run the &lt;code&gt;ML-hyperopt-pipeline&lt;/code&gt;, click on &lt;strong&gt;Launch&lt;/strong&gt; beside the &lt;strong&gt;ML-hyperopt-pipeline&lt;/strong&gt; on the Launchpad.&lt;/p&gt; 
&lt;p&gt;After launching the pipeline, you can monitor execution under the &lt;strong&gt;Runs&lt;/strong&gt; tab. The pipeline will take a few minutes to start the Amazon EC2 instances supporting the AWS Batch CEs and begin dispatching tasks to Batch.&lt;/p&gt; 
&lt;p&gt;Assuming the pipeline runs successfully, you should see the following output in the &lt;strong&gt;Execution log&lt;/strong&gt; accessible through the Seqera interface.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;N E X T F L O W  ~  version 23.10.0
Pulling nextflow-io/hyperopt ...
 downloaded from https://github.com/nextflow-io/hyperopt.git
Launching `https://github.com/nextflow-io/hyperopt` [mighty_koch] DSL2 - revision: eb2b84a91a [master]

    M L - H Y P E R O P T   P I P E L I N E
    =======================================
    fetch_dataset   : true
    dataset_name    : wdbc

    visualize       : true

    train           : true
    train_data      : data/*.train.txt
    train_meta      : data/*.meta.json
    train_models    : [dummy, gb, lr, mlp, rf]

    predict         : true
    predict_models  : data/*.pkl
    predict_data    : data/*.predict.txt
    predict_meta    : data/*.meta.json

    outdir          : results
    
Monitor the execution with Nextflow Tower using this URL: https://tower.nf/user/gordon-sissons/watch/5MUVbQjbhEFjRu
[da/4b6a7f] Submitted process &amp;gt; fetch_dataset (wdbc)
[c5/78cd5e] Submitted process &amp;gt; split_train_test (wdbc)
[68/29cb37] Submitted process &amp;gt; train (wdbc/dummy)
[01/0b89d1] Submitted process &amp;gt; train (wdbc/mlp)
[37/7e1cc8] Submitted process &amp;gt; train (wdbc/lr)
[22/8452c6] Submitted process &amp;gt; train (wdbc/rf)
[6e/763ce3] Submitted process &amp;gt; visualize (1)
[2a/3f3d84] Submitted process &amp;gt; train (wdbc/gb)
[5a/1c71a0] Submitted process &amp;gt; visualize (2)
[cc/7dd9ad] Submitted process &amp;gt; predict (wdbc/dummy)
[b5/f30153] Submitted process &amp;gt; predict (wdbc/mlp)
[de/340f09] Submitted process &amp;gt; predict (wdbc/lr)
[e5/d205ef] Submitted process &amp;gt; predict (wdbc/rf)
[38/6a6eaf] Submitted process &amp;gt; predict (wdbc/gb)
The best model for ‘wdbc’ was ‘mlp’, with accuracy = 0.991

Done!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the pipeline trains each model, runs each model against the dataset, and determines the model with the best predictive accuracy.&lt;/p&gt; 
&lt;p&gt;If you’ve gotten this far: Congratulations! You’ve successfully set up your first compute environment in Seqera and used AWS Batch to execute a machine learning pipeline.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;For organizations collaborating on large-scale data analysis and ML workloads, Seqera Platform running on AWS can be an excellent solution. You can more easily deploy powerful AWS compute and storage resources at scale, while also reducing your costs through optimized resource usage, and managing spend across projects and teams. You can also leverage best-in-class curated bioinformatics pipelines and modules from &lt;a href="https://nf-co.re"&gt;&lt;em&gt;nf-core&lt;/em&gt;&lt;/a&gt; and other sources.&lt;/p&gt; 
&lt;p&gt;This can help improve research productivity with a unified view of data, pipelines, and results, while collaborating more effectively with local and remote teams.&lt;/p&gt; 
&lt;p&gt;We’re excited to know how this works out for you, or if you have other challenges that AWS or Seqera can help you meet. Contact us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt; if you want to discuss these with us.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Leveraging Seqera Platform on AWS Batch for machine learning workflows – Part 1 of 2</title>
		<link>https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-1-of-2/</link>
		
		<dc:creator><![CDATA[Ben Sherman]]></dc:creator>
		<pubDate>Tue, 23 Jan 2024 15:59:51 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Genomics]]></category>
		<guid isPermaLink="false">4153d168ace17c15a8bf48f252882386c3f15251</guid>

					<description>Nextflow is popular workflow framework for genomics pipelines, but did you know you can also use it for machine-learning? ML is already being used for medical imaging, protein folding, drug discovery, and gene editing. In this post, we explain how to build an example Nextflow pipeline that performs ML model-training and inference for image analysis.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3236" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/18/Batch-and-Seqera.png" alt="Batch and Seqera" width="380" height="190"&gt;This post was contributed by Dr Ben Sherman (Seqera) and Dr Olivia Choudhury (AWS), Paolo Di Tomasso, and Gord Sissons from Seqera, and Aniket Deshpande and Abhijit Roy from AWS.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Machine learning (ML) is used for multiple healthcare and life sciences (HCLS) applications, including medical imaging, protein folding, drug discovery, and gene editing. While &lt;a href="https://nextflow.io/"&gt;Nextflow&lt;/a&gt; pipelines (like those in &lt;a href="https://nf-co.re"&gt;nf-core&lt;/a&gt;) are commonly used for genomics, they are also being adopted for machine learning workloads.&lt;/p&gt; 
&lt;p&gt;Nextflow is an excellent solution for many ML-based scenarios. Sometimes you need to continuously (and automatically) retrain your models based on rapidly-changing datasets from external sources such as sequencers. Sometimes you need training and inference resources sporadically but you face constraints getting GPUs or FPGAs – even for short periods. And often pipelines like &lt;a href="https://nf-co.re/proteinfold/1.0.0"&gt;nf-core/proteinfold&lt;/a&gt; have compute and data-intensive inference steps where many samples need to be processed in parallel.&lt;/p&gt; 
&lt;p&gt;In the next two posts, we’ll show you how these kinds of challenges can be addressed using Nextflow and the &lt;a href="https://seqera.io/platform/"&gt;Seqera Platform&lt;/a&gt; integrated with AWS.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;In part one&lt;/strong&gt; of this two-part blog series, we explain how to build an example Nextflow pipeline that performs ML model-training and inference for image analysis, illustrating how Nextflow supports custom ML-based workflows. We also discuss how health care and life science customers are using this today.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;In &lt;a href="https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-2-of-2/"&gt;part two&lt;/a&gt;&lt;/strong&gt;, we’ll provide a step-by-step guide explaining how users new to the Seqera Platform can rapidly get started with AWS, maximizing the use of &lt;a href="https://aws.amazon.com/batch"&gt;AWS Batch&lt;/a&gt;, &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service (Amazon S3&lt;/a&gt;), and other AWS services.&lt;/p&gt; 
&lt;h2&gt;Seqera on AWS&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://seqera.io/platform/"&gt;Seqera Platform&lt;/a&gt; (previously &lt;em&gt;Nextflow Tower&lt;/em&gt;) is a comprehensive bioinformatics data analysis platform deeply integrated with AWS. Seqera is used by leading biotechnology and pharmaceutical companies globally, including 10 of the top 20 global BioPharmas and roughly 10,000 bioinformaticians across hundreds of organizations.&lt;/p&gt; 
&lt;p&gt;Seqera Platform has several key features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It is explicitly designed for Nextflow pipelines.&lt;/li&gt; 
 &lt;li&gt;It is cross platform – Seqera works with AWS and other cloud and HPC providers, including on-premises systems. It supports customer’s preferred container runtimes, registries, source code managers, and it uses multiple AWS services, including AWS Batch, Amazon S3, Amazon FSx for Lustre, Amazon Elastic File System (EFS), Amazon Elastic Kubernetes Service (EKS), AWS Secrets Manager and others.&lt;/li&gt; 
 &lt;li&gt;It has a large, active, user and developer community which provide high-quality curated &lt;a href="https://nf-co.re"&gt;nf-core&lt;/a&gt; pipelines and modules.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href="https://seqera.io/platform/"&gt;Seqera Platform&lt;/a&gt; can be deployed in two different ways.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Seqera Cloud&lt;/strong&gt; is a fully managed service hosted exclusively on AWS infrastructure. Presently, there are 8,000+ corporate and research Seqera Cloud users. Researchers can use Seqera Cloud for free and progress to paid plans as their needs evolve.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Seqera Enterprise&lt;/strong&gt; is a customer-managed version of the Seqera platform that is deployable on-premises or on a customer’s preferred cloud. Some customers install Seqera on-premises, while others deploy Seqera on AWS using Docker Compose or the &lt;a href="https://aws.amazon.com/eks/"&gt;Amazon Elastic Kubernetes Service&lt;/a&gt; (EKS).&lt;/p&gt; 
&lt;p&gt;Seqera employs a “&lt;em&gt;bring your own credentials&lt;/em&gt;” model. As illustrated in the architecture diagram in Figure 1, users log into Seqera and add &lt;a href="https://help.tower.nf/23.2/compute-envs/overview/"&gt;compute environments&lt;/a&gt; by supplying credentials for their preferred cloud or HPC workload manager.&lt;/p&gt; 
&lt;div id="attachment_3210" style="width: 1102px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3210" loading="lazy" class="size-full wp-image-3210" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.10.09.png" alt="Figure 1: High-level architecture of Seqera on the AWS Cloud." width="1092" height="591"&gt;
 &lt;p id="caption-attachment-3210" class="wp-caption-text"&gt;Figure 1: High-level architecture of Seqera on the AWS Cloud.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Seqera Platform users have a private workspace and can be assigned to various shared workspaces, each with its own pipelines, datasets, and compute environments. Seqera sidesteps the complexity of running in different cloud or HPC environments by providing a consistent user experience regardless of the underlying infrastructure.&lt;/p&gt; 
&lt;p&gt;While most workloads run on-premises or on AWS infrastructure, this ability to deploy to different compute environments is useful for several reasons:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Customers can leverage on-premises HPC clusters and tap cloud capacity when their own resources are fully utilized.&lt;/li&gt; 
 &lt;li&gt;Research frequently involves datasets hosted on third-party clouds, making it more cost-effective to &lt;em&gt;bring the compute to the data&lt;/em&gt; rather than transferring large datasets to a local execution environment.&lt;/li&gt; 
 &lt;li&gt;Academic and research efforts frequently involve collaboration among institutions using different infrastructure. Seqera allows these users to seamlessly share pipelines, datasets, computing infrastructure, and research results without exposing private cloud credentials.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Seqera Forge&lt;/h2&gt; 
&lt;p&gt;While users can choose to run pipelines on pre-existing AWS Batch environments, Seqera Forge fully automates creating and configuring AWS Batch compute environments and queues for Nextflow pipelines, following best practices. Seqera can also dispose of cloud resources when they’re not in use, helping reduce costs.&lt;/p&gt; 
&lt;p&gt;By leveraging AWS APIs, Forge dramatically simplifies the deployment, configuration, and teardown of AWS infrastructure, making it possible for researchers with minimal knowledge of “CloudOps” to deploy cloud infrastructure themselves.&lt;/p&gt; 
&lt;h2&gt;A sample training dataset&lt;/h2&gt; 
&lt;p&gt;To illustrate how machine learning and inference workloads can be run on AWS using Seqera Platform, we used the &lt;a href="https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic"&gt;Wisconsin Diagnostic Breast Cancer&lt;/a&gt; (WDBC) dataset. This is a well-known dataset, often used as an example for learning or comparing different ML techniques, specifically for image classification. It consists of 589 samples, each with a set of 30 features taken from an image of a breast tissue. The diagnosis column in the data indicates whether the sample was benign (B) or malignant (M), as illustrated in Figure 2.&lt;/p&gt; 
&lt;div id="attachment_3211" style="width: 1131px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3211" loading="lazy" class="size-full wp-image-3211" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.10.50.png" alt="Figure 2: Images from Breast Cancer Wisconsin (Diagnostic) Dataset aligning with tabular data showing that samples are either malignant or benign." width="1121" height="328"&gt;
 &lt;p id="caption-attachment-3211" class="wp-caption-text"&gt;Figure 2: Images from Breast Cancer Wisconsin (Diagnostic) Dataset aligning with tabular data showing that samples are either malignant or benign.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In the sample pipeline, we will train and evaluate multiple models to classify these breast samples as benign or malignant. In a real-world scenario, we can also use k-fold cross-validation to evaluate each model on several randomized partitions of the dataset and use multiple performance metrics with minimum requirements to determine whether a model is “good enough” to be used in production.&lt;/p&gt; 
&lt;p&gt;For our purposes here, we will simply evaluate each model on a single 80/20 train/test split and select the model with the highest test accuracy.&lt;/p&gt; 
&lt;h2&gt;A sample pipeline&lt;/h2&gt; 
&lt;p&gt;For illustration purposes, we use a simple proof-of-concept pipeline called &lt;a href="https://github.com/nextflow-io/hyperopt"&gt;Hyperopt&lt;/a&gt; developed by Seqera Labs. The pipeline takes any tabular dataset as input (or the name of a dataset on &lt;a href="https://www.openml.org/"&gt;OpenML&lt;/a&gt;). It then trains and evaluates a set of ML models on the dataset, reporting the model that achieved the highest test accuracy. You can learn more about this pipeline in the article &lt;a href="https://seqera.io/blog/nextflow-and-tower-for-machine-learning/"&gt;Nextflow and Tower for Machine Learning&lt;/a&gt;. The pipeline code is available on &lt;a href="https://github.com/nextflow-io/hyperopt"&gt;GitHub&lt;/a&gt;. Figure 3 shows a &lt;a href="https://mermaid.js.org/"&gt;Mermaid diagram&lt;/a&gt;&lt;u&gt;, automatically generated by Nextflow,&lt;/u&gt; of the overall pipeline.&lt;/p&gt; 
&lt;div id="attachment_3212" style="width: 643px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3212" loading="lazy" class="size-full wp-image-3212" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/17/CleanShot-2024-01-17-at-18.11.37.png" alt="Figure 3: The pipeline steps are implemented as Python scripts that use several common packages for ML, including numpy, pandas, scikit-learn, and matplotlib. These dependencies are defined in a Conda environment file called conda.yml." width="633" height="784"&gt;
 &lt;p id="caption-attachment-3212" class="wp-caption-text"&gt;Figure 3: The pipeline steps are implemented as Python scripts that use several common packages for ML, including numpy, pandas, scikit-learn, and matplotlib. These dependencies are defined in a Conda environment file called conda.yml.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;By default, the pipeline uses the &lt;a href="https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic"&gt;WDBC&lt;/a&gt; dataset described above and evaluates five different classification models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"&gt;baseline model&lt;/a&gt; (i.e. “dummy” model) that simply predicts the most common label&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier"&gt;Gradient Boosting&lt;/a&gt; (gb)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"&gt;Logistic Regression&lt;/a&gt; (lr)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier"&gt;Multi-layer Perceptron&lt;/a&gt; (mlp) (i.e. neural network)&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn-ensemble-randomforestclassifier"&gt;Random Forest&lt;/a&gt; (rf)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When you run the pipeline, you should see something like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ nextflow run hyperopt -profile wave
[...]
The best model for ‘wdbc’ was ‘mlp’, with accuracy = 0.991
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This shows that Nextflow ran a pipeline that trained different ML models on the WDBC dataset and evaluated their performance during model inference. Multi-layer perceptron was most accurate in classifying breast tumor images as benign or malignant. For further details of the pipeline and its deployment, refer to part two of this blog series.&lt;/p&gt; 
&lt;p&gt;While the hyperopt pipeline implements a simple classification model, it provides all the building blocks you need to create your own ML pipelines with Nextflow.&lt;/p&gt; 
&lt;p&gt;Seqera is also an excellent solution for deploying GPU-based workloads in the AWS cloud. For a hands-on tutorial, see the article &lt;a href="https://seqera.io/blog/running-ai-workloads-in-the-cloud-with-nextflow-tower-a-step-by-step-guide/"&gt;Running AI workloads in the cloud with Nextflow Tower — a step-by-step guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;It’s the customers that matter most&lt;/h2&gt; 
&lt;p&gt;Seqera is used by hundreds of pharmaceutical, healthcare, and biotech companies to run data analysis pipelines in the AWS Cloud. According to the latest &lt;a href="https://seqera.io/blog/the-state-of-the-workflow-2023-community-survey-results/"&gt;2023 State of the Workflow Survey&lt;/a&gt;, AWS is the most popular cloud environment among Nextflow users, with 49% of all Nextflow users surveyed already using or planning to use AWS within the next two years and 35.1% of survey respondents using AWS Batch [1,2]. The survey results showed strong cloud adoption, with the percentage of Nextflow users running in the cloud up 20% over 2022 [2].&lt;/p&gt; 
&lt;p&gt;Among the customers running Seqera and Nextflow on AWS are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://arcusbio.com/"&gt;Arcus Biosciences&lt;/a&gt;—Arcus Biosciences is at the forefront of designing combination therapies, with best-in-class potential, in the pursuit of cures for cancer. By using Seqera Platform on AWS, Arcus was able to improve productivity, ensure pipeline traceability, and use cloud resources more efficiently. They were also able to prepare for future growth by scaling capacity for research and clinical trials while providing an intuitive, collaborative user experience to researchers and clinicians. &lt;a href="https://seqera.io/case-studies/arcus-biosciences/"&gt;Read the case study here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://gritstonebio.com/"&gt;Gritstone Bio&lt;/a&gt;—Gritstone Bio is developing targeted immunotherapies for cancer and infectious disease. Gritstone’s approach seeks to generate a therapeutic immune response by leveraging insights into the immune system’s ability to recognize and destroy diseased cells by targeting select antigens. Their workloads involve massive compute requirements for analysis of individual biopsies and makes extensive use of machine learning for tumor classification models. Gritstone use Seqera and multiple AWS cloud services to manage their bioinformatics pipelines. &lt;a href="https://seqera.io/case-studies/gritstone/"&gt;Read the case study here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://www.tesseratherapeutics.com/"&gt;Tessera Therapeutics&lt;/a&gt;—Tessera Therapeutics are pioneers in a new category of genetic medicine and rely heavily on genomic analysis pipelines to identify promising new treatments. By using Seqera Platform to manage analysis pipelines on AWS, Tessera increased its analysis throughput and research productivity while simultaneously containing cloud spending by using resources more efficiently. You can &lt;a href="https://seqera.io/case-studies/tessera-therapeutics/"&gt;read the case study here&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;For organizations collaborating on large-scale data analysis and ML workloads, Seqera on AWS is an excellent solution. You can easily deploy powerful AWS compute and storage resources at scale, reduce costs through optimized resource usage, and manage spending across projects and teams.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;In &lt;a href="https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-2-of-2/"&gt;part two&lt;/a&gt;&lt;/strong&gt; of this blog series, we will provide a step-by-step guide, explaining how you can easily deploy a Seqera environment on AWS to run ML pipelines like the one above, and other Nextflow pipelines.&lt;/p&gt; 
&lt;h3&gt;References&lt;/h3&gt; 
&lt;p&gt;[1] &lt;a href="https://seqera.io/blog/the-state-of-the-workflow-2023-community-survey-results/"&gt;The State of the Workflow 2023: Community Survey Results&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;[2] &lt;a href="https://seqera.io/blog/state-of-the-workflow-2022-results/"&gt;The State of the Workflow 2022: Community Survey Results&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Save up to 90% using EC2 Spot, even for long-running HPC jobs</title>
		<link>https://aws.amazon.com/blogs/hpc/save-up-to-90-using-ec2-spot-even-for-long-running-hpc-jobs/</link>
		
		<dc:creator><![CDATA[Eran Brown]]></dc:creator>
		<pubDate>Tue, 16 Jan 2024 16:08:38 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[EC2]]></category>
		<category><![CDATA[EC2 Spot]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">17797086c47ab3a6385b921c1983c198c3da1c2c</guid>

					<description>New OS-level checkpointing tools can let you run existing HPC codes on EC2 Spot instances with minimal impact from interruptions. Read on for the details.</description>
										<content:encoded>&lt;p&gt;Amazon Elastic Compute Cloud (Amazon EC2) &lt;a href="https://aws.amazon.com/ec2/spot/"&gt;Spot Instances&lt;/a&gt; enable AWS customers to save up to 90% of their compute cost by using spare EC2 capacity, and are really popular with HPC customers who use a lot of compute every day. But long running HPC jobs often can’t survive an &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-interruptions.html"&gt;EC2 Spot interruption&lt;/a&gt; which makes it hard to benefit from all this low cost and large volume of compute.&lt;/p&gt; 
&lt;p&gt;Amazon EC2 Spot interruptions do provide a 2-minute warning, though, that you can detect from within your running instance, or externally using &lt;a href="https://aws.amazon.com/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt;. Some HPC tools are starting to handle these interruptions by saving their state and allowing the next instance to resume work from that point.&lt;/p&gt; 
&lt;p&gt;But what about the much larger number of HPC tools that don’t have this capability? Can we add them into the mix – to make them fault-tolerant without changing the applications? A few AWS technology partners (&lt;a href="https://aws.amazon.com/marketplace/seller-profile?id=3b7c724c-fae7-4187-ae45-de1625e51395"&gt;MemVerge&lt;/a&gt;, &lt;a href="https://aws.amazon.com/marketplace/seller-profile?id=80332cc4-940c-4903-848c-ddb937c49afe"&gt;Exostellar&lt;/a&gt;) are offering solutions for checkpointing at the OS level which capture the process tree and the working directory (like local temp files). By handling this at the OS level, they allow your existing HPC tools to run to completion unaware of spot interruptions. This opens savings opportunities for AWS customers using EC2 Spot instances.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll show you how these new technologies can help you benefit from all these amazing Spot rates – &lt;em&gt;and&lt;/em&gt; you won’t need to modify your applications to do it. We’ll cover the underlying technologies, their operational implications, and their limitations.&lt;/p&gt; 
&lt;h2&gt;Outline of the checkpoint/restore process&lt;/h2&gt; 
&lt;p&gt;Figure 1 describes the process: When a spot interruption happens, EC2 sends an event to Amazon CloudWatch. A monitoring script inside the instance identifies this from the instance metadata (1). This creates a checkpoint, stored in a shared file system (2). The HPC scheduler will detect the execution node was terminated (3). It’ll call the EC2 API to launch a new Spot instance (5). The new instance will detect a checkpoint exists (6), and will restore it (instead of starting the job from scratch). The job now continues, and communicates with the license server (7) for the duration of the job.&lt;/p&gt; 
&lt;div id="attachment_3199" style="width: 1094px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3199" loading="lazy" class="size-full wp-image-3199" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/12/CleanShot-2024-01-12-at-14.28.33.png" alt="Figure 1 – The checkpoint / restore process, which uses the Spot interruption notification to force a checkpoint to take place and recycles the job onto a new instance when one is available." width="1084" height="585"&gt;
 &lt;p id="caption-attachment-3199" class="wp-caption-text"&gt;Figure 1 – The checkpoint / restore process, which uses the Spot interruption notification to force a checkpoint to take place and recycles the job onto a new instance when one is available.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;Figure 1 – The checkpoint / restore process, which uses the Spot interruption notification to force a checkpoint to take place and recycles the job onto a new instance when one is available.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;How we tested this&lt;/h2&gt; 
&lt;p&gt;This blog post is the result of running a tool just like this with an HPC code over a few weeks, slowly incrementing the complexity of the solution. Our goal was to minimize changes to the existing HPC workflow.&lt;/p&gt; 
&lt;p&gt;Where changes are required we’ll mention them as we go along.&lt;/p&gt; 
&lt;p&gt;Our test introduced one new element at each step to be able to identify which steps impacted the runtime the most. We repeated each step at least 15 times to identify variability in the results:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Baseline: test a “clean” copy of the environment (no additional checkpointing tools installed)&lt;/li&gt; 
 &lt;li&gt;Test the checkpointing tool binaries installed, but not running&lt;/li&gt; 
 &lt;li&gt;Test with the tool running, but without creating checkpoints&lt;/li&gt; 
 &lt;li&gt;Test with a checkpoint, restore on the &lt;em&gt;same&lt;/em&gt; node&lt;/li&gt; 
 &lt;li&gt;Test with a checkpoint restore on a &lt;em&gt;different&lt;/em&gt; node&lt;/li&gt; 
 &lt;li&gt;Create multiple checkpoints in the same run, resume each one on a different node&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With two exceptions (which we’ll discuss in minute), we didn’t experience longer runtimes from checkpointing. We chose to limit testing to a single worker node. See “&lt;em&gt;Workloads spanning multiple nodes&lt;/em&gt;” near the end of this post for more on that.&lt;/p&gt; 
&lt;h2&gt;Operational considerations&lt;/h2&gt; 
&lt;p&gt;We learned a lot from this lab work – mainly things that you can work around with adequate planning.&lt;/p&gt; 
&lt;p&gt;For the sake of what follows, we’ll refer to the first EC2 Spot Instance that gets interrupted as the &lt;em&gt;old node&lt;/em&gt;. The new instance launched to continue the job is the &lt;em&gt;new node&lt;/em&gt;.&lt;/p&gt; 
&lt;h3&gt;Compute&lt;/h3&gt; 
&lt;p&gt;You must restart your job on a new host with the same CPU type as the old host. This includes the CPU manufacturer (AMD, Intel, Arm), memory size, and the CPU generation. Your HPC tool may check for specific instruction-set support and rely on it. If this instruction set is not available in the next new node, it may fail to complete the job. It’s possible that you can move up to newer generations with backwards compatible instruction sets, but we didn’t test that.&lt;/p&gt; 
&lt;p&gt;Since instances can be interrupted, it’s best to run each worker in its own instance. This limits the number of impacted workers. It also prevents them from competing for network bandwidth when they need to write the checkpoint.&lt;/p&gt; 
&lt;h3&gt;Scheduler integration&lt;/h3&gt; 
&lt;p&gt;Since the scheduler is the one restarting the interrupted job, it’s important to understand its role in this.&lt;/p&gt; 
&lt;p&gt;Most schedulers offer a mechanism for pre-execution scripts (to setup the environment for execution) and post-execution scripts (to clean up or capture logs). Pre-execution scripts are repeated on the new node too, so they’ll need to detect the existing of a checkpoint and restore it. You’ll need to check with your technology provider if they’re able to capture environment variables. If not, those may need to move to your pre-execution script.&lt;/p&gt; 
&lt;p&gt;Another aspect of the scheduler integration is the locality of the workload. While relevant to all HPC jobs, it’s worth highlighting it in this context. If the old node ran in one Availability Zone (AZ), the new node &lt;em&gt;really&lt;/em&gt; should be launched in the same AZ. This allows low-latency communication with the shared storage or other workers, and it’ll save you from paying for cross-AZ network traffic charges.&lt;/p&gt; 
&lt;p&gt;And, of course, the scheduler should not attempt to resume the job if a license isn’t available to run it (more on licensing later).&lt;/p&gt; 
&lt;h3&gt;Shared storage – performance&lt;/h3&gt; 
&lt;p&gt;The checkpoint has to be stored in some shared storage so it can outlive the old node and be restored on the new node. This’ll generate a “spike” of writes in an effort to write the checkpoint as quickly as possible (before the 2 minutes is up – that’s quite important). The size of the checkpoint is mostly the process memory and the size of the working directory that needs to be backed up.&lt;/p&gt; 
&lt;p&gt;However, in a large environment running many spot instances, you may encounter more than one interruption at once. Storage performance sizing should account for the published spot interruption rates in your region. AWS publishes these rates, see the &lt;a href="https://aws.amazon.com/ec2/spot/instance-advisor/"&gt;Spot Instance Advisor&lt;/a&gt; for more details. You’ll need a shared volume to host these checkpoints and it has to be sized to handle these peaks.&lt;/p&gt; 
&lt;p&gt;If storage is a performance bottleneck, some checkpoints might fail to complete within 2 minutes. Your job will then have to be restarted or failback to the previous checkpoint.&lt;/p&gt; 
&lt;h3&gt;Shared storage – permissions&lt;/h3&gt; 
&lt;p&gt;To capture the entire process-tree of the job, the checkpoint software will also need to have root permissions. This means checkpoint storage writes will come from the root user. Since most HPC environments use &lt;code&gt;root_squash&lt;/code&gt; for security reasons, you’ll need to decide how to allow these writes. You can setup a dedicated volume for checkpoints with &lt;code&gt;no_root_squash&lt;/code&gt;, or map the root user to another user. We recommend consulting your security team because any root access should be thoroughly reviewed.&lt;/p&gt; 
&lt;h3&gt;Shared storage – capacity&lt;/h3&gt; 
&lt;p&gt;Beyond the obvious capacity requirements for the checkpoints, you’ll need to consider how you clean up old checkpoints from successfully completed jobs. This minimizes storage capacity over time. You can use the post-execution script to handle that.&lt;/p&gt; 
&lt;p&gt;This script does &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; run on the old node, where the job never completed, but &lt;strong&gt;&lt;em&gt;does&lt;/em&gt;&lt;/strong&gt; run on the new node when it &lt;em&gt;is&lt;/em&gt; completed. However, you might want to keep them available for a short period of time for troubleshooting. If so, consider a queue of these jobs that gets cleaned up periodically.&lt;/p&gt; 
&lt;h3&gt;Licensing impact&lt;/h3&gt; 
&lt;p&gt;If your HPC job relies on a license checked out from a license server, you’ll need to consider how this impacts the license count. Licenses bound to MAC addresses may cause you to double up on checked-out licenses – until the first one is released.&lt;/p&gt; 
&lt;p&gt;Some licensed tools allow the client to set a keep-alive interval forcing it to communicate with the license servers to keep the license assignment. Setting the keep-alive be below the time it takes to restart the job can resolve this. However, setting it too low can overwhelm the license server with requests.&lt;/p&gt; 
&lt;p&gt;Another possible approach is to have the MAC address move with the job between nodes. You can achieve this using a second &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html"&gt;Elastic Network Interface&lt;/a&gt; (ENI) on the old node. ENIs can be migrated to another node. This avoids the MAC address change. You can achieve this in your pre-execution script, for example. Another way is to use a &lt;a href="https://aws.amazon.com/lambda/"&gt;Lambda function&lt;/a&gt; called by the scheduler. This minimizes access to this permission (“&lt;a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege"&gt;least privilege&lt;/a&gt;” – one of our guiding principles for designing anything in AWS).&lt;/p&gt; 
&lt;h3&gt;Incremental checkpoints&lt;/h3&gt; 
&lt;p&gt;To minimize network traffic and storage throughput bottlenecks, you can use incremental checkpoints.&lt;/p&gt; 
&lt;p&gt;By periodically checkpointing your process you reduce the amount of data that needs to be written during the 2-minute spot interruption. The tradeoff is that you’ll write &lt;strong&gt;more&lt;/strong&gt; data in total, because your memory changes throughout the job’s execution. Writing more frequently means you’ll write data in checkpoint 1, 2 and 3 that’s not relevant to the last checkpoint (the one that’s restored).&lt;/p&gt; 
&lt;p&gt;It’s hard to recommend a good “rule of thumb” for this, but we recommend measuring this tradeoff for the specific application you’re using, and considering these questions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;What is the cost of a longer recovery point (less frequent checkpoints)? &lt;/strong&gt;A high-cost license may make you prefer to checkpoint more frequently to minimize amount of work that can be lost in a worst-case scenario.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;How long does a checkpoint take? &lt;/strong&gt;The checkpoint might need to freeze the process to take a consistent checkpoint, which means your job takes longer as you add checkpoints. Here too you’ll need to balance the protection against lost work with the impact on the time to results. In our lab, a 32GB machine was able to complete the checkpoint in 3-4 seconds (negligible).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;What’s the memory size used? &lt;/strong&gt;The larger the memory model, the more data you’ll need to write. At some point the data volume becomes too great to write in two minutes, and incremental checkpoints become mandatory.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;How long is your job?&lt;/strong&gt; This defines your worst-case-scenario for losing work. For a job that takes 5 days to complete, checkpointing daily means you may lose a full day of work. If that’s not acceptable – consider more frequent checkpoints.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Repeated spot interruptions&lt;/h3&gt; 
&lt;p&gt;Some customers need to guarantee a Service Level Agreement (SLA) for jobs to complete. If you’re going to use EC2 Spot instances in a scenario like this, you’ll need limit the number of stop interruption to meet that SLA. You can limit this by keeping an interruption counter per job. When exceeded, the job will move from a queue using Spot to a queue using On-Demand instances (perhaps one of the &lt;a href="mailto:https://aws.amazon.com/blogs/hpc/deep-dive-into-hpc7a-the-newest-amd-powered-member-of-the-hpc-instance-family/"&gt;HPC-specific instances&lt;/a&gt; which have low prices and are built for HPC).&lt;/p&gt; 
&lt;p&gt;This model ensures you benefit from the discounted Spot rate for most of the jobs, while keeping your SLA.&lt;/p&gt; 
&lt;h3&gt;Checkpointing tool specific constraints&lt;/h3&gt; 
&lt;p&gt;The checkpointing tool may include additional requirements (like the need to run as root to capture the process tree). However, it may also introduce new requirements as well on &lt;em&gt;how&lt;/em&gt; you run your job.&lt;/p&gt; 
&lt;p&gt;These may include running them under a dedicated cgroup / container / namespace, which will introduce additional time to start / teardown the job. In our test we found a 20-second average setup time to create a new namespace, which for a long running job was acceptable.&lt;/p&gt; 
&lt;p&gt;Other requirements might include specific user permissions for running your tools, which will need to be evaluated against your security guidelines.&lt;/p&gt; 
&lt;p&gt;Some tools offer the ability to preempt the spot interruption and be able to move the workload in advance of this happening. This is an alternative to incremental checkpoints, but you can also use these two techniques together.&lt;/p&gt; 
&lt;h3&gt;Workloads spanning multiple nodes&lt;/h3&gt; 
&lt;p&gt;While we didn’t test workloads spanning multiple hosts in this lab, we &lt;em&gt;will&lt;/em&gt; offer two insights:&lt;/p&gt; 
&lt;p&gt;If your workers are &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/tightly-coupled-scenarios.html"&gt;tightly-coupled&lt;/a&gt;, and rely on one another, consider if this is a good use case for spot. A single spot instance interruption in a 100-instance workload will leave 99 hosts stuck waiting for the job to restart. HPC instances (like the &lt;a href="mailto:https://aws.amazon.com/blogs/hpc/deep-dive-into-hpc7a-the-newest-amd-powered-member-of-the-hpc-instance-family/"&gt;Hpc7a&lt;/a&gt;, &lt;a href="mailto:https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/"&gt;Hpc7g&lt;/a&gt;, or &lt;a href="mailto:https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc6id-instances-optimized-for-high-performance-computing/"&gt;Hpc6id&lt;/a&gt;) might be a better approach, since they’re very low cost, and built for exactly these kinds of workloads.&lt;/p&gt; 
&lt;p&gt;If your workers are &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/loosely-coupled-scenarios.html"&gt;loosely-coupled&lt;/a&gt;, you should still look at the head-node that manages them. (1) This node should &lt;strong&gt;not&lt;/strong&gt; be using Spot so it’s not interrupted. (2) You should also check how this node will react to an instance disappearing and returning with a new hostname. This is tool-specific, and you might need to do some testing to validate this.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Checkpoint/restore solutions offer a new way of reducing the cost of long running HPC jobs, but introduces some new requirements. For competitive markets like semiconductor and healthcare, this can mean faster time to market without a similar increase in the R&amp;amp;D budget.&lt;/p&gt; 
&lt;p&gt;It requires some planning and implementation to meet your specific software needs. For more information, visit the &lt;a href="https://aws.amazon.com/marketplace/search/results?searchTerms=spot+instances"&gt;AWS marketplace&lt;/a&gt; to explore partner solutions for using EC2 spot with your HPC workload, or reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Enhancing ML workflows with AWS ParallelCluster and Amazon EC2 Capacity Blocks for ML</title>
		<link>https://aws.amazon.com/blogs/hpc/enhancing-ml-workflows-with-aws-parallelcluster-and-amazon-ec2-capacity-blocks-for-ml/</link>
		
		<dc:creator><![CDATA[Austin Cherian]]></dc:creator>
		<pubDate>Thu, 11 Jan 2024 14:50:51 +0000</pubDate>
				<category><![CDATA[Amazon Machine Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<guid isPermaLink="false">f588e38a5ba88c676ee43776193f2153800b334e</guid>

					<description>No more guessing if GPU capacity will be available when you launch ML jobs! EC2 Capacity Blocks for ML let you lock in GPU reservations so you can start tasks on time. Learn how to integrate Caacity Blocks into AWS ParallelCluster to optimize your workflow in our latest technical blog post.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3192" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/09/boofla88_capacity_blocks_like_a_tetris_game_in_3d_f07df4ca-4b16-4d57-8d7a-2a8ab7030ad6.png" alt="" width="380" height="212"&gt;Have you ever geared up to run machine learning (ML) tasks with GPU instances, only to hit a roadblock? It’s a common scenario: you spend hours meticulously preparing your training sessions or simulations, tackling the intricate setup from fine-tuning application parameters to organizing job output directories, and managing the detailed process of data set cleaning and pre-processing.&lt;/p&gt; 
&lt;p&gt;But just when you’re ready to launch, you hit an unexpected snag – the GPU capacity you need isn’t available. This leaves you with two less-than-ideal options: wait for the GPU instances to become available &lt;strong&gt;or&lt;/strong&gt; reshape and resize your job to fit the limited resources at hand.&lt;/p&gt; 
&lt;p&gt;Recently Amazon EC2 announced the availability of Capacity Blocks for ML which allows you to reserve blocks of GPU time with specific start and end dates enabling you to launch your jobs when those reservations start. This approach effectively eliminates uncertainties around job start times.&lt;/p&gt; 
&lt;p&gt;AWS ParallelCluster now supports integrating these reservation blocks with your cluster and in this post, we show you how to setup a cluster to use them.&lt;/p&gt; 
&lt;h2&gt;A word or two about Capacity Blocks&lt;/h2&gt; 
&lt;p&gt;Amazon EC2 Capacity Blocks are carved out of Amazon EC2 UltraClusters. These Capacity Blocks enable users to reserve GPU instances for one to 14 days and can support a maximum of 64 instances or 512 GPUs, making them ideal for ML workloads ranging from small-scale experiments to extensive, distributed training sessions.&lt;/p&gt; 
&lt;p&gt;Their integrated, low-latency, and high-throughput network connectivity simplifies the setup by removing the need for managing cluster placement groups – streamlining the architecture, and improving the performance of distributed applications. EC2 Capacity Blocks currently support Amazon EC2 P5 instances, which are powered by NVIDIA H100 Tensor Core GPUs integrated into UltraClusters with advanced second-generation Elastic Fabric Adapter (EFA) networking. This enables remarkable connectivity and scalability up to hundreds of GPUs, making them well-suited for the most resource-intensive ML workloads. For more information, refer to the AWS official documentation on &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-blocks.html"&gt;Capacity Blocks for ML&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;AWS ParallelCluster integration with EC2 Capacity Blocks&lt;/h2&gt; 
&lt;p&gt;With &lt;strong&gt;AWS&lt;/strong&gt; &lt;strong&gt;ParallelCluster 3.8&lt;/strong&gt; you can specify and assign EC2 Capacity Blocks to be used by queues when launching jobs. ParallelCluster spins up the instances when the reserved capacity becomes active – enabling Slurm to launch jobs as soon as those instances are ready. In this way, you can setup your complex workflows ahead of time, submit them into the queue, and be assured that they’ll run as soon as the reserved capacity you purchased becomes available.&lt;/p&gt; 
&lt;h3&gt;Setting up clusters to use EC2 Capacity Blocks&lt;/h3&gt; 
&lt;p&gt;Setting up clusters to use Capacity Blocks and using them is largely a three-step process that involves, &lt;strong&gt;reserving the Capacity Block, integrating the reservations with the cluster and, and aligning and launching&lt;/strong&gt; &lt;strong&gt;your workloads&lt;/strong&gt; to use the capacity when it becomes available. Let’s have a closer look at these steps.&lt;/p&gt; 
&lt;h3&gt;Reserving a Capacity Block&lt;/h3&gt; 
&lt;p&gt;To reserve a Capacity Block, you start by defining the number of instances you need, the amount of time you need them for, and the date window when you want the capacity to run your jobs. You can use the AWS EC2 console to search for a Capacity Block based on your requirements. Once you find the ideal match, you can reserve the Capacity Block with just a couple clicks. You can also find and reserve a Capacity Block using the &lt;a href="https://aws.amazon.com/cli/"&gt;AWS Command Line Interface (AWS CLI)&lt;/a&gt; and &lt;a href="http://docs.aws.amazon.com/AWSJavaScriptSDK/latest/AWS/EC2.html"&gt;AWS SDKs&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_3185" style="width: 1102px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3185" loading="lazy" class="size-full wp-image-3185" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/09/CleanShot-2024-01-09-at-16.06.48.png" alt="Figure 1 – To get started with Capacity Blocks in the AWS EC2 console, start by navigating to the US East 1 (Ohio) Region, select Capacity Reservations in the navigation pane, and then click Purchase Capacity Blocks for ML to see available capacity." width="1092" height="509"&gt;
 &lt;p id="caption-attachment-3185" class="wp-caption-text"&gt;Figure 1 – To get started with Capacity Blocks in the AWS EC2 console, start by navigating to the US East 1 (Ohio) Region, select Capacity Reservations in the navigation pane, and then click Purchase Capacity Blocks for ML to see available capacity.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3186" style="width: 896px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3186" loading="lazy" class="size-full wp-image-3186" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/09/CleanShot-2024-01-09-at-16.07.34.png" alt="Figure 2 – After you specify your capacity requirements, click Find Capacity Block to see an available Capacity Block offering that matches your inputs, including the exact start and end times, Availability Zone, and total price of the reservation. Once you find a Capacity Block offering that you want to reserve, click Next to proceed with purchasing the reservation." width="886" height="725"&gt;
 &lt;p id="caption-attachment-3186" class="wp-caption-text"&gt;Figure 2 – After you specify your capacity requirements, click Find Capacity Block to see an available Capacity Block offering that matches your inputs, including the exact start and end times, Availability Zone, and total price of the reservation. Once you find a Capacity Block offering that you want to reserve, click Next to proceed with purchasing the reservation.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Capacity Blocks are delivered as a type of Capacity Reservation, and when you reserve a Capacity Block you get a reservation ID which is required for the next step of using the Capacity Block with your cluster. You can find your &lt;strong&gt;Capacity Block reservation ID&lt;/strong&gt; in the &lt;strong&gt;Capacity Reservations&lt;/strong&gt; resource table in the AWS EC2 console. For more information about finding and reserving Capacity Blocks, refer to the &lt;a href="https://docs.aws.amazon.com/en_us/AWSEC2/latest/UserGuide/capacity-blocks-using.html#capacity-blocks-purchase"&gt;Find and Purchase Capacity Blocks&lt;/a&gt; section of the EC2 Capacity Blocks documentation.&lt;/p&gt; 
&lt;h3&gt;Configuring a cluster to use a Capacity Block&lt;/h3&gt; 
&lt;p&gt;With the Capacity Block reservation ID in hand, you can now configure an existing cluster or create a &lt;em&gt;new&lt;/em&gt; cluster to target instance launches from the Capacity Block you purchased when it becomes active.&lt;/p&gt; 
&lt;p&gt;ParallelCluster previously already supported On-Demand Capacity Reservations (ODCRs). The same set of configuration parameters can now support Capacity Reservations based on EC2 Capacity Blocks with the addition of a newly supported capacity type: CAPACITY_BLOCK.&lt;/p&gt; 
&lt;p&gt;The cluster configuration file snippet that follows shows the highlighted parameters that need to be set in the ParallelCluster configuration file to configure a cluster to use a Capacity Block.&lt;/p&gt; 
&lt;div id="attachment_3187" style="width: 672px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3187" loading="lazy" class="size-full wp-image-3187" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/01/09/CleanShot-2024-01-09-at-16.08.11.png" alt="Figure 3 - The cluster configuration that is needed to configure the use of a Capacity Block." width="662" height="326"&gt;
 &lt;p id="caption-attachment-3187" class="wp-caption-text"&gt;Figure 3 – The cluster configuration that is needed to configure the use of a Capacity Block.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Let’s assume we already have an existing cluster and we want to configure one of its queues to use the Capacity Block we just reserved – for all jobs submitted to this queue. With the Capacity Block reservation ID, we obtained in step 1, we can now proceed as follows:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;You can skip this step if you’ve set &lt;em&gt;QueueUpdateStrategy&lt;/em&gt; property to &lt;code&gt;DRAIN&lt;/code&gt; or &lt;code&gt;TERMINATE&lt;/code&gt; in your ParallelCluster configuration file. However, if you’ve set the &lt;em&gt;QueueUpdateStrategy&lt;/em&gt; property to &lt;code&gt;COMPUTE_FLEET_STOP&lt;/code&gt; or do not use the property at all, you must stop the cluster’s compute fleet using pcluster update-compute-fleet command as follows:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-compute-fleet --cluster-name &amp;lt;cluster-name&amp;gt; --status STOP_REQUSTED&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Using the &lt;em&gt;QueueUpdateStrategy&lt;/em&gt; property set to &lt;code&gt;DRAIN&lt;/code&gt; or &lt;code&gt;TERMINATE&lt;/code&gt; avoids the need to stop your compute fleet when updating queue or storage configurations. Refer to the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/Scheduling-v3.html#yaml-Scheduling-SlurmSettings-QueueUpdateStrategy"&gt;official ParallelCluster documentation&lt;/a&gt; for more information about the &lt;em&gt;QueueUpdateStrategy&lt;/em&gt; parameter.&lt;/p&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;In the cluster’s configuration file, for the selected queue, update its parameters to point to the Capacity Block by setting its &lt;em&gt;CapacityType&lt;/em&gt; parameter to &lt;code&gt;CAPACITY_BLOCK&lt;/code&gt; and update the &lt;em&gt;CapacityReservationId&lt;/em&gt; property at the queue level to the reservation ID of the Capacity Block.&lt;/li&gt; 
 &lt;li&gt;Configuring Capacity Blocks also requires setting the &lt;em&gt;MinCount&lt;/em&gt; and &lt;em&gt;MaxCount&lt;/em&gt; properties to be the same value because these counts specify the number of static nodes that would be launched from the available capacity of the Capacity Block for jobs submitted to the queue. Make sure the counts don’t exceed the size of the Capacity Block.&lt;/li&gt; 
 &lt;li&gt;When specifying Capacity Blocks for a queue, you need to specify the &lt;em&gt;InstanceType&lt;/em&gt; property, setting it to the instance type of the Capacity Block. As of this writing, EC2 Capacity Blocks only support P5 instances. The &lt;em&gt;InstanceType &lt;/em&gt;set needs to match the instance type of the Capacity Block or you’ll get a validation error during the configuration of the cluster.&lt;/li&gt; 
 &lt;li&gt;With these configurations, we’ll now update the cluster configuration using &lt;code&gt;pcluster update-cluster&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-cluster --cluster-configuration &amp;lt;cluster-config-file&amp;gt; --cluster-name &amp;lt;cluster-name&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you’ve stopped the compute fleet as directed in step 1 then after the re-configuration of the cluster completes you’ll need to enable the compute fleet again, to allow for job submission. This can be done by the running the &lt;code&gt;pcluster update-compute-fleet&lt;/code&gt; command again, but this time with the &lt;code&gt;START_REQUESTED&lt;/code&gt; status as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-compute-fleet --cluster-name &amp;lt;cluster-name&amp;gt; --status START_REQUESTED&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;However, if you’ve set the &lt;em&gt;QueueUpdateStrategy&lt;/em&gt; property you won’t need to enable the compute fleet and the Capacity Block will be configured after the existing instances have been Drained or Terminated depending on the setting of the property.&lt;/p&gt; 
&lt;p&gt;And that’s it! Your cluster is now configured to use an EC2 Capacity Block. When the Capacity Block becomes active, Parallelcluster will launch the EC2 instances as static compute nodes for the configured queue. If you were starting from scratch and creating a new cluster, you could modify the configuration file as illustrated and use it when creating the cluster. &amp;nbsp;If you’d like to learn more about creating and updating clusters to use Capacity Blocks, you can refer to the ParallelCluster &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html"&gt;official documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Running jobs to use the configured Capacity Block&lt;/h4&gt; 
&lt;p&gt;With the cluster configured to use a Capacity Block reservation, submitting jobs is straight forward. You prepare your jobs and submit them to run in the configured queue. You don’t need to wait for the Capacity Block to become active. All the instances associated with the Capacity Block will be associated with a Slurm reservation. Instances in Slurm reservations continue to accept jobs and keep them pending. As soon as your Capacity Block becomes active, the instances will become available as static nodes, and the job will begin.&lt;/p&gt; 
&lt;p&gt;Jobs that exceed the Capacity Blocks duration will fail, because EC2 will terminate compute instances when the &lt;em&gt;end time&lt;/em&gt; of the Capacity Block is reached. To avoid job termination, you’ll need to estimagfe your job runtime and ensure it doesn’t exceed the Capacity Block’s duration &lt;em&gt;before&lt;/em&gt; launching the job.&lt;/p&gt; 
&lt;p&gt;You can check a Capacity Block’s current validity using the AWS console, CLI, or programmatically using the SDK before launching the job. Capacity Blocks also support &lt;a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-events.html"&gt;EventBridge&lt;/a&gt; notifications sent out as the Capacity Blocks nears expiry – you can use this to save jobs states before they are preempted. For more information on how to monitor your Capacity Blocks refer to &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/capacity-blocks-monitor.html"&gt;Monitor Capacity Blocks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We recommended enabling checkpointing for your job (if that’s available) so you can restart it from a more proximate point to when it was terminated, rather than starting it from the very beginning.&lt;/p&gt; 
&lt;p&gt;When you’re done using the Capacity Block, we recommend deactivating the queue or updating the queue to use another Capacity Block or a different Capacity Type. To deactivate the queue, you can run the Slurm Scontrol command as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;scontrol update partitionname=&amp;lt;queuename&amp;gt; state=INACTIVE&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To update the queue with the configuration of another Capacity Block, you just need to update the &lt;em&gt;CapacityReservationId&lt;/em&gt; parameter with the ID of the new Capacity Block and update the cluster as shown earlier. Alternatively, to setup the queue with a different capacity type you can delete the &lt;em&gt;CapacityReservationTarget&lt;/em&gt; and &lt;em&gt;CapacityReservationId&lt;/em&gt; parameters and set the &lt;em&gt;CapacityType&lt;/em&gt; to &lt;code&gt;ONDEMAND&lt;/code&gt; or &lt;code&gt;SPOT&lt;/code&gt; as you prefer. When changing the capacity type, you may want to set the &lt;em&gt;InstanceType&lt;/em&gt; parameter of the associated compute resource to another instance type (other than P5) depending on the configuration you prefer.&lt;/p&gt; 
&lt;h3&gt;Maximizing on your Capacity Blocks utilization&lt;/h3&gt; 
&lt;p&gt;Given that Capacity Blocks are intentionally ephemeral with a finite lifetime, maximizing their usage is crucial. Here are two key considerations for achieving that.&lt;/p&gt; 
&lt;h4&gt;Considerations for GPU failures&lt;/h4&gt; 
&lt;p&gt;If you’ve been running long-running ML training jobs, there’s a good chance you’ve faced GPU failures midway through the job, resulting in failed jobs and requiring job restarts. You may have faced this problem enough to have introduced checkpointing. To lower the probability of being affected by GPU failures, ParallelCluster already supports doing GPU health checks on the compute nodes assigned for a job &lt;em&gt;before the job starts&lt;/em&gt;. For more on using this feature, check out our post “&lt;a href="https://aws.amazon.com/blogs/hpc/introducing-gpu-health-checks-in-aws-parallelcluster-3-6/"&gt;Introducing GPU health check in ParallelCluster 3.6&lt;/a&gt;”.&lt;/p&gt; 
&lt;p&gt;These GPU health checks can be used with Capacity Block reservations, too. In the case of GPU failures when using Capacity Blocks, unhealthy instances are terminated and new instances are provisioned within a few minutes. Jobs are launched when the health checks on all instances pass.&lt;/p&gt; 
&lt;h4&gt;Multi-queue and multi-compute-resource configurations&lt;/h4&gt; 
&lt;p&gt;You may be using Slurm partitions to organize your cluster’s usage. For example, you may assign users or groups to a queue that they need to use for their jobs, or you might have multiple projects in flight using the same cluster, where each project gets its own queue. The permutations and combinations can work well for your organization, but when it comes to capacity utilization – especially with reserved capacity that you book and pay for – your cluster’s organization must have considerations to maximize utilization.&lt;/p&gt; 
&lt;p&gt;To help match your cluster organization while using Capacity Blocks, ParallelCluster supports sharing the same Capacity Block across multiple queues and compute resources. To share the Capacity Block across multiple queues, you simply need to configure the queues to use the same Capacity Block just like we showed earlier in the single queue case. However, take care to split the capacity between the queues where the Min/Max counts of &lt;em&gt;all the queues combined&lt;/em&gt; is &lt;em&gt;less than or equal to&lt;/em&gt; the size of the Capacity Block.&lt;/p&gt; 
&lt;p&gt;The reason this is required is that Capacity Blocks are spun up as &lt;strong&gt;static capacity&lt;/strong&gt; and assigned to a queue, and the static capacity assigned to a queue is not available in another queue.&lt;/p&gt; 
&lt;p&gt;In addition, the flexibility of multiple queues allows you to reserve and specify multiple Capacity Blocks on the same queue. You’d do this if you wanted to use the same queue for all your jobs but keep it replenished with Capacity Blocks that you keep purchasing as and when you’re able to reserve them.&lt;/p&gt; 
&lt;h3&gt;Conclusion&lt;/h3&gt; 
&lt;p&gt;To wrap up, combining AWS ParallelCluster with EC2 Capacity Blocks is an effective solution to solving the GPU availability constraints we are all facing in this AI enabled era. This combination helps with aligning your workload to the required GPU power it requires, getting you the crucial determinism you need to effectively plan your workflows.&lt;/p&gt; 
&lt;p&gt;You can reserve the GPU power you need ahead of time, submit your job, and it’ll run with the required capacity when the capacity arrives, which means less waiting and fewer project delays.&lt;/p&gt; 
&lt;p&gt;In this post, we briefly introduced EC2 Capacity Blocks and how they integrate with AWS ParallelCluster. We also illustrated a step-by-step walkthrough of how you can configure a cluster to use a Capacity Block. We talked about some smart ways to get the most out of these Capacity Blocks, like how to handle GPU issues and how to set up your projects in a smart way.&lt;/p&gt; 
&lt;p&gt;These tips aren’t just helpful – they’re crucial for anyone who wants to use Capacity Blocks with ParallelCluster to their full potential.&lt;/p&gt; 
&lt;p&gt;Lastly, the integration EC2 Capacity Blocks and AWS Parallelcluster is a glimpse into the future of how AWS services integrate to produce solutions to specific problems like the GPU capacity crunch we all face today. As more compute power is needed to solve tomorrow’s problems, these kinds of smart solutions are important. If you’d like to learn more about creating and updating clusters to use Capacity Blocks, you can refer to the ParallelCluster &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html"&gt;official documentation&lt;/a&gt;. If you have ideas or feedback that can help us make these features better , reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Create a Slurm cluster for semiconductor design with AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/create-a-slurm-cluster-for-semiconductor-design-with-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Allan Carter]]></dc:creator>
		<pubDate>Wed, 10 Jan 2024 15:46:22 +0000</pubDate>
				<category><![CDATA[Best Practices]]></category>
		<category><![CDATA[Hi-Tech and Electronics]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Industries]]></category>
		<category><![CDATA[Semiconductor]]></category>
		<category><![CDATA[Technical How-to]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">cdc660c35037e33dd83c140e30c100fc2cd1cd3a</guid>

					<description>If you work in the semiconductor industry with electronic design automation tools and workflows, this guide will help you build an HPC cluster on AWS specifically configured for your needs. It covers AWS ParallelCluster and customizations specifically to cater to EDA.</description>
										<content:encoded>&lt;p&gt;Chips are fingernail-sized pieces of glass that power all your electronic devices like cell phones, computers, and TVs. Chip designers use electronic design automation (EDA) tools to draw billions of microscopic switches and connect them with wires that are smaller than the finest hair.&lt;/p&gt; 
&lt;p&gt;EDA engineers measure device sizes in nanometers (1×10&lt;sup&gt;-9 &lt;/sup&gt;m or 1 billionth of a meter) and time in picoseconds (1×10&lt;sup&gt;-12&lt;/sup&gt; s, or &lt;em&gt;one-trillionth&lt;/em&gt; of a second). They analyze their designs to ensure that they meet power, performance, and area (PPA) goals and that they meet rigorous design rules so that silicon foundries can manufacture them. The designers send the completed drawings to a foundry that manufactures them in automated factories that reliably create structures at atomic scale. The entire design and manufacturing process requires vast amounts of high-performance storage and high-performance computing (HPC) to run millions of EDA jobs for large teams of engineers.&lt;/p&gt; 
&lt;p&gt;Today, we’re going to show you how to create a cluster on AWS, using AWS ParallelCluster, that’s purpose-built for this environment, and ready for the kinds of workloads that EDA users work with every day.&lt;/p&gt; 
&lt;h2&gt;A world of complex constraints&lt;/h2&gt; 
&lt;p&gt;EDA workflows impose complex requirements on the compute cluster. A workflow may consist of hundreds of different licensed EDA tools with very different requirements. The tool licenses are typically much more expensive than the infrastructure they run on so the scheduler must ensure that jobs only run when a license and the required compute resources are available.&lt;/p&gt; 
&lt;p&gt;Large teams must share these critical resources so it’s also essential that the scheduler can enforce a fair-share allocation policy to prevent some users from monopolizing resources at the expense of others. The EDA jobs themselves have widely differing requirements, so the compute cluster must support a very diverse set of Amazon EC2 instance sizes and families from compute-optimized to high-memory variations.&lt;/p&gt; 
&lt;h2&gt;A day in the life&lt;/h2&gt; 
&lt;p&gt;Figure 1 shows the basic workflow for designing a chip. The front-end and back-end tasks have quite distinct requirements for compute and storage. An advantage of AWS is the diversity of compute and storage solutions available so design teams can tailor their infrastructure for the needs of each step in the workflow.&lt;/p&gt; 
&lt;p&gt;The process is also extremely iterative. If engineers find a problem in the late stages of the project, the fix may require architectural changes that require the team to rerun all the steps of the workflow. This is where design teams can get into capacity crunches – and risk catastrophic schedule delays if they can’t access enough compute and storage capacity.&lt;/p&gt; 
&lt;div id="attachment_3159" style="width: 567px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3159" loading="lazy" class="size-full wp-image-3159" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/14/CleanShot-2023-12-14-at-12.33.30.png" alt="Figure 1 - Chip design workflow diagram. Architects create and edit the architecture. Designers create and edit the design and create RTL and circuit diagrams. Design verification engineers run simulations to verify that the design works and gather coverage information to verify completeness of tests. Back end design engineers synthesize RTL to create netlists. Then they do physical layout that converts the netlists to layout and GDSII. Then they run physical verification and power and signal analysis to make sure the layout meets power, performance, and area requirements and that the GDSII meets all design rules. After verification is complete, the developers tape out the GDSII to a silicon foundry for manufacturing." width="557" height="1055"&gt;
 &lt;p id="caption-attachment-3159" class="wp-caption-text"&gt;Figure 1 – Chip design workflow diagram. Architects create and edit the architecture. Designers create and edit the design and create RTL and circuit diagrams. Design verification engineers run simulations to verify that the design works and gather coverage information to verify completeness of tests. Back end design engineers synthesize RTL to create netlists. Then they do physical layout that converts the netlists to layout and GDSII. Then they run physical verification and power and signal analysis to make sure the layout meets power, performance, and area requirements and that the GDSII meets all design rules. After verification is complete, the developers tape out the GDSII to a silicon foundry for manufacturing.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;At the beginning of a project, compute usage is typically low and sporadic. Usage peaks around project milestones and at the end of a project when it runs hot for several months – and is usually in the critical path for project completion. Figure 2 shows the number of jobs running on different instance types in an EDA cluster over a typical 24-hour window. Utilization would likely be higher and more sustained near the end of a project.&lt;/p&gt; 
&lt;p&gt;Notice the variability and the diverse mix of instance types that the cluster uses ranging from high-frequency general purpose m5zn to memory-optimized and high-memory r6i and x2iezn instances. The scalability of AWS can ensure that jobs are able to run on instance types that are ideal for the jobs without infrastructure capacity constraints.&lt;/p&gt; 
&lt;div id="attachment_3160" style="width: 887px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3160" loading="lazy" class="size-full wp-image-3160" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/14/CleanShot-2023-12-14-at-12.34.21.png" alt="Figure 2 - The EDA job profile is highly variable. This graph shows peaks and troughs of usage throughout a 24-hour window. Engineers run jobs that use different instance types as required by the job requirements." width="877" height="790"&gt;
 &lt;p id="caption-attachment-3160" class="wp-caption-text"&gt;Figure 2 – The EDA job profile is highly variable. This graph shows peaks and troughs of usage throughout a 24-hour window. Engineers run jobs that use different instance types as required by the job requirements.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Enter Slurm and AWS ParallelCluster&lt;/h2&gt; 
&lt;p&gt;Design teams have traditionally used commercial schedulers, but are increasingly showing interest in Slurm. Slurm is open-source, free to use, and meets all the requirements for running EDA workloads. &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; is an AWS service that allows Slurm to automatically scale up and down fleets of compute instances on AWS. It supports high job throughput and job capacity for even the most demanding workloads. It also supports the high security demands of the semiconductor industry by not requiring any internet access for its functionality.&lt;/p&gt; 
&lt;p&gt;Starting with version 3.7.0, AWS ParallelCluster has all the features required to easily configure and deploy an EDA Slurm cluster that takes full advantage of the scalability of AWS to design the most advanced chips. It offers Slurm accounting, which allows administrators to configure license-sharing and enables fair-share allocation for cluster users. It can schedule jobs based on the number of cores and the amount of memory that the job requires, too. ParallelCluster also increased the number of instance types that it can support in a cluster. It added support for Redhat Enterprise version 8 which all EDA tools will require, starting in 2024. It added support for custom &lt;em&gt;instance type weighting&lt;/em&gt; so Slurm can schedule the lowest cost instance type that meets a job’s specific requirements. And finally, ParallelCluster added a Python management API that you can use in a Lambda Layer to &lt;a href="https://aws.amazon.com/blogs/hpc/automate-your-clusters-by-creating-self-documenting-hpc-with-aws-parallelcluster/"&gt;completely automate the deployment and updating of your cluster&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Putting this all together&lt;/h3&gt; 
&lt;p&gt;The &lt;a href="https://github.com/aws-samples/aws-eda-slurm-cluster"&gt;aws-eda-slurm-cluster&lt;/a&gt; repository on GitHub uses all these new ParallelCluster features to quickly and easily create an EDA-specific ParallelCluster in the VPC of your choosing. The cluster supports CentOS 7, RHEL 7 and 8, x86_64 and arm64 architectures, On Demand and Spot Instances, heterogeneous queues, and up to 50 instance types. You can easily configure EDA license counts and fair share allocations using simple configuration files. By default, it selects the instance types that are best for EDA workloads. The cluster sets compute node weights based on the cost of instance types so that Slurm chooses the lowest cost node that meets job requirements.&lt;/p&gt; 
&lt;p&gt;In addition to the normal ParallelCluster partitions, it defines a batch and an interactive partition that each contain all the compute nodes. The batch partition is the default and the interactive partition is identical except that it has a much higher weight. If you need a job to run quickly, for example to debug a simulation failure, and the batch partition is full, &lt;strong&gt;you can jump to the head of the line&lt;/strong&gt; by submitting your job to the interactive queue and Slurm will schedule it using the next available license and compute node.&lt;/p&gt; 
&lt;p&gt;ParallelCluster and Slurm typically expect you to &lt;code&gt;ssh&lt;/code&gt; to a login node or the head node to use the cluster, but semiconductor engineers expect to be able to use the cluster from a shell on their virtual desktops. With &lt;em&gt;aws-eda-slurm-cluster&lt;/em&gt;, you can configure submitter hosts so that they can directly access one or more clusters. The cluster configures submitter hosts as Slurm login nodes and creates modulefiles you load to set up the shell environment to use the cluster. It also supports submitters that have a different OS or CPU architecture than the cluster.&lt;/p&gt; 
&lt;h2&gt;Deployment&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://aws-samples.github.io/aws-eda-slurm-cluster/"&gt;aws-eda-slurm-cluster GitHub page&lt;/a&gt; documents the simple deployment process. The cluster uses the &lt;a href="https://aws.amazon.com/cdk/"&gt;AWS Cloud Development Kit&lt;/a&gt; (CDK) to create a &lt;a href="https://github.com/aws-samples/aws-eda-slurm-cluster/blob/main/source/cdk/cdk_slurm_stack.py"&gt;custom application&lt;/a&gt; that reads a configuration file and creates an AWS CloudFormation stack that creates and configures the cluster in less than 30 minutes.&lt;/p&gt; 
&lt;p&gt;The CloudFormation stack creates and configures your customized ParallelCluster. When you no longer need the cluster, you simply delete the CloudFormation &lt;em&gt;stack&lt;/em&gt; and CloudFormation deletes the cluster for you. If you need to update the cluster, then update the configuration file and rerun the CDK application.&lt;/p&gt; 
&lt;p&gt;The cluster uses a YAML configuration file with a different &lt;a href="https://github.com/aws-samples/aws-eda-slurm-cluster/blob/main/source/cdk/config_schema.py#L241-L454"&gt;schema&lt;/a&gt; than ParallelCluster’s configuration file. The following basic configuration will use ParallelCluster to create a RHEL 8 Slurm cluster configured for EDA workloads. Note the prerequisite items highlighted in red. It requires an AWS VPC, subnet, and EC2 key pair. If you don’t already have these, &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/tutorials_07_slurm-accounting-v3.html#slurm-accounting-vpc-v3"&gt;this tutorial&lt;/a&gt; shows how to create them. &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/tutorials_07_slurm-accounting-v3.html#slurm-accounting-db-stack-v3"&gt;This tutorial&lt;/a&gt; shows how to create a Slurm accounting database stack. Include the stack name in the configuration. The licenses section allows the configuration of 1 or more software licenses that Slurm will track to make sure that jobs don’t use more than the number of configured licenses.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;StackName: eda-pc-3-7-2-rhel8-x86-config
Region: &amp;lt;region&amp;gt;
SshKeyPair: &amp;lt;ec2-key-pair&amp;gt;
VpcId: vpc-xxxx
SubnetId: subnet-xxxx
slurm:
  ClusterName: eda-pc-3-7-2-rhel8-x86
  MungeKeySecret: /slurm/munge_key
  ParallelClusterConfig:
    Version: '3.7.2'
    Image:
      Os: 'rhel8'
    Architecture: 'x86_64'
    Database:
      DatabaseStackName: &amp;lt;parallel-cluster-database-stack&amp;gt;
  SlurmCtl: {}
  InstanceConfig:
    NodeCounts:
      DefaultMaxCount: 10
Licenses:
  &amp;lt;license-name&amp;gt;:
    Count: 10
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Deployment is as simple as executing the following commands from the root of the git repository.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ source setup.sh
$ ./install.sh --config-file &amp;lt;config-filename&amp;gt; --cdk-cmd create&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will create a CloudFormation stack named &lt;strong&gt;eda-pc-3-7-2-rhel8-x86-config&lt;/strong&gt; that will create and configure a ParallelCluster called &lt;strong&gt;eda-pc-3-7-2-rhel8-x86&lt;/strong&gt;. If you need to update the configuration, for example to add a custom compute node AMI, then you just edit the configuration file and run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ source setup.sh
$ ./install.sh --config-file &amp;lt;config-filename&amp;gt; --cdk-cmd update&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use they cluster by connecting to the head node or a login node. If you would like the convenience of accessing the cluster directly from a submitter host, &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-view-stack-data-resources.html"&gt;the output&lt;/a&gt; of the &lt;strong&gt;eda-pc-3-7-2-rhel8-x86-config&lt;/strong&gt; stack has commands for mounting the cluster’s NFS export on submitter hosts and configuring them to use the stack. After you configure the submitter host, you can easily use the cluster. Simply load the provided modulefile and run Slurm commands like in the following example which will open an interactive bash shell on a compute node with 1 GB of memory and 1 CPU core for, at most, an hour.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ module load eda-pc-3-7-2-rhel8-x86
$ srun -p interactive --mem 1G -c 1 --time 1:00:00 --pty /bin/bash&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The module file sets up the environment for the Slurm cluster and configures Slurm defaults for the path, number of cores, default amount of memory, job timeout, and more. This makes it so that users must override the defaults to get more than minimal cluster resources. By default, their jobs will only get 1 core, 100 MB of memory, and a time limit of 1 hour.&lt;/p&gt; 
&lt;p&gt;Another nice feature of the cluster is that it creates custom ParallelCluster AMI build configuration files. You can use them to create AMIs with all the packages typically required by EDA tools. You can find the build configuration files in the repo at &lt;em&gt;source/resources/parallel-cluster/config/&amp;lt;parallel-cluster-version&amp;gt;/&amp;lt;ClusterName&amp;gt;&lt;/em&gt; or on the head node or submitter host at &lt;em&gt;/opt/slurm/&amp;lt;ClusterName&amp;gt;/config/build-files&lt;/em&gt;. The GitHub page documents the &lt;a href="https://aws-samples.github.io/aws-eda-slurm-cluster/custom-amis/"&gt;process for building custom EDA AMIs&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;You can easily configure and deploy a Slurm cluster on AWS using AWS ParallelCluster and &lt;a href="https://github.com/aws-samples/aws-eda-slurm-cluster"&gt;aws-eda-slurm-cluster&lt;/a&gt; that can run your most demanding EDA workloads and take advantage of the performance and scalability of AWS to meet your project needs.&lt;/p&gt; 
&lt;p&gt;Contact your AWS account team and schedule a meeting with our semiconductor industry specialists for more information and help getting your EDA workloads running on AWS.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Using a Level 4 Digital Twin for scenario analysis and risk assessment of manufacturing production on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/using-a-level-4-digital-twin-for-scenario-analysis-and-risk-assessment-of-manufacturing-production-on-aws/</link>
		
		<dc:creator><![CDATA[Ross Pivovar]]></dc:creator>
		<pubDate>Tue, 12 Dec 2023 15:09:36 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">f0833eb293524843a9cbd91da39f3e12e62cf6bb</guid>

					<description>This post was contributed by Orang Vahid (Dir of Engineering Services) and Kayla Rossi (Application Engineer) at Maplesoft, and Ross Pivovar (Solution Architect) and Adam Rasheed (Snr Manager) from Autonomous Computing at AWS One of the most common objectives for our Digital Twin (DT) customers is to use DTs for scenario analysis to assess risk […]</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by &lt;/em&gt;&lt;em&gt;Orang Vahid (Dir of Engineering Services) &lt;/em&gt;&lt;em&gt;and Kayla Rossi (Application Engineer) &lt;/em&gt;&lt;em&gt;at Maplesoft, and &lt;/em&gt;&lt;em&gt;Ross Pivovar (Solution Architect) and &lt;/em&gt;&lt;em&gt;Adam Rasheed (Snr Manager) from Autonomous Computing at AWS&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;One of the most common objectives for our Digital Twin (DT) customers is to use DTs for scenario analysis to assess risk and drive operational decisions. Customers want Digital Twins of their industrial facilities, production processes, and equipment to simulate different scenarios to optimize their operations and predictive maintenance strategies.&lt;/p&gt; 
&lt;p&gt;In a prior post, we described &lt;a href="https://aws.amazon.com/blogs/iot/digital-twins-on-aws-unlocking-business-value-and-outcomes/"&gt;a four-level digital twin framework&lt;/a&gt; to help you understand these use cases and the technologies required to build them.&lt;/p&gt; 
&lt;p&gt;In this post today, we’re going to show you how to use an L4 &lt;em&gt;Living Digital Twin&lt;/em&gt; for scenario analysis and operational decision support for a manufacturing line. We’ll use &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow&lt;/a&gt; to combine a physics-based model created using &lt;a href="https://www.maplesoft.com/products/maplesim/"&gt;MapleSim&lt;/a&gt; with probabilistic Bayesian methods to calibrate the L4 DT so it can adapt to the changing real-world conditions as the equipment degrades over time.&lt;/p&gt; 
&lt;p&gt;Then we’ll show you how to use the calibrated L4 DT to perform scenario analysis and risk assessment to simulate possible outcomes and make informed decisions. MapleSim provides the tools to create engineering simulation models of machine equipment and TwinFlow is an AWS open-source framework for building and deploying predictive models at scale.&lt;/p&gt; 
&lt;h2&gt;L4 Living Digital Twin of roll-to-roll manufacturing&lt;/h2&gt; 
&lt;p&gt;For our use case, we considered the web-handling process in roll-to-roll manufacturing for continuous materials like paper, film, and textiles. The web-handling process involves unwinding the material from a spool, guiding it through various treatments like printing or coating, and then winding it onto individual rolls. It’s essential that we precisely control the tension, alignment, and speed to ensure smooth processing and maintain product quality.&lt;/p&gt; 
&lt;p&gt;Figure 1 shows a schematic diagram of the web-handling equipment consisting of 9 rollers (labeled with “R”) and 12 material spans (labeled with “S”).&lt;/p&gt; 
&lt;div id="attachment_3136" style="width: 635px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3136" loading="lazy" class="size-full wp-image-3136" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.03.46.png" alt="Figure 1 Schematic diagram of the web-handling equipment in which each span is labeled as S1 through S12 and rollers are label R1 through R10." width="625" height="199"&gt;
 &lt;p id="caption-attachment-3136" class="wp-caption-text"&gt;Figure 1 Schematic diagram of the web-handling equipment in which each span is labeled as S1 through S12 and rollers are label R1 through R10.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In our &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/"&gt;previous post&lt;/a&gt;, we showed you how to build and deploy L4 Digital Twin &lt;em&gt;self-calibrating&lt;/em&gt; virtual sensors for predicting the tension in each of the spans and the slip velocity at each of the rollers throughout the web-handling process.&lt;/p&gt; 
&lt;p&gt;An L4 Living Digital Twin focuses on modeling the behavior of the physical system by updating the model parameters using real world observations. The capability to update the model is what makes it a “living” digital twin that’s synchronized with the physical system and continuously adapting as the physical system evolves. These situations are common in industrial facilities or manufacturing plants as equipment degrades over time. Examples of real-world observations include continuous data (time-series data from physical sensors), discrete measurements (discrete sensor measurements), or discrete observations (visual inspection data).&lt;/p&gt; 
&lt;p&gt;In this post, we’re extending our earlier example of L4 Digital Twin self-calibrating virtual sensors to conduct &lt;em&gt;what-if&lt;/em&gt; scenario analysis predictions. Conceptually, an L4 Digital Twin forecast looks like Figure 2 which plots the predicted value over time. For our example, we focused on predicting the span tension within the material being manufactured. This is operationally important because tension failures occur when the tension within the material exceeds a threshold (175 Newtons in this example) resulting in product quality issues (e.g., scratches, breakage, wrinkling, or troughing).&lt;/p&gt; 
&lt;div id="attachment_3137" style="width: 645px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3137" loading="lazy" class="size-full wp-image-3137" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.04.28.png" alt="Figure 2 Conceptual plot showing historical measured data (observations) and L4 Digital Twin future forecast with uncertainty bounds showing when the predicted value will cross the failure threshold." width="635" height="380"&gt;
 &lt;p id="caption-attachment-3137" class="wp-caption-text"&gt;Figure 2 Conceptual plot showing historical measured data (observations) and L4 Digital Twin future forecast with uncertainty bounds showing when the predicted value will cross the failure threshold.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The vertical line (marked “&lt;em&gt;Today&lt;/em&gt;”) shows the delineation between historical data collected in the past (the blue dots) and the future prediction with uncertainty bounds. In our case, the historical observations are values determined from the past inferred viscous damping coefficients because physical sensor measurements weren’t available. The L4 Digital Twin makes a prediction of the tension value today, and at all points in the future, including when the tension value will cross the damage threshold. The delta between &lt;em&gt;Today&lt;/em&gt; and the time at which the prediction crosses the threshold represents the amount of time the operator has available to take some corrective action.&lt;/p&gt; 
&lt;p&gt;Of course, we can’t predict the future with perfect certainty so it’s important that all predictions be quantified with uncertainty bounds. This allows the operator to plan their corrective actions based on their risk tolerance. If the operator is risk averse, then they’d take corrective actions before the earliest uncertainty band crosses the threshold. In this way, they have a high probability of applying corrective action before failure happens. If they’re risk neutral, they would use the mean value. If they are willing to accept high risk, they could delay scheduling corrective actions to the later uncertainty band, recognizing that there’s a high probability of failure before they apply corrective actions. This last option may be logical when part replacement is more costly than lost production downtime.&lt;/p&gt; 
&lt;p&gt;In our example, we’re focusing on tension failure that results in a product quality issue, but the exact same approach can be used to predict equipment failure and remaining useful life (RUL) to proactively develop preventive maintenance plans.&lt;/p&gt; 
&lt;h2&gt;Example scenario analysis&lt;/h2&gt; 
&lt;p&gt;A potential scenario requiring risk assessment involves the detection of gradual dirt build-up on the roller bearings. Using the time series of the component degradation, we can predict when a failure could occur in the future. This knowledge lets us estimate when maintenance should be done.&lt;/p&gt; 
&lt;p&gt;The end result is that we can 1) maximize throughput of the manufacturing line to increase revenue and; 2) reduce costs by reducing defects and unnecessary downtime. A &lt;em&gt;third&lt;/em&gt; benefit that’s rather application specific is a more targeted maintenance scheme. If we regularly schedule maintenance on all components, we run the risk of performing maintenance on components that don’t need servicing, increasing waste. Using a calibrated digital twin, we can specifically target the components that are degrading instead of everything at once.&lt;/p&gt; 
&lt;p&gt;To solidify these ideas, let’s think about the web-handling roll-to-roll manufacturing line again. Using the L4 Digital Twin we described in our last post lets us simulate a synthetic degradation scenario. In that scenario, we simulated bearing degradation by manually increasing the viscous damping coefficient for roller 9 from 0 on Day 15 to 0.2 on Day 19.&lt;/p&gt; 
&lt;p&gt;In Figure 3, we see the L4 Digital Twin is capturing that roller 9’s viscous damping coefficient is increasing from Day 15 to Day 19. We can tell that viscous damping is increasing, causing a change in tension in multiple spans, and we need to decide our course of action. The immediate risk to assess is whether we need to shut down the line to perform maintenance – or not.&lt;/p&gt; 
&lt;p&gt;Imagine that the dirt build up is detected on a Friday morning and the maintenance crew will be out on Monday because it’s a 3-day holiday weekend. Can we continue to generate product over the weekend and run the risk of increasing defects, or is it more cost effective to shut down the line for repairs on Friday before the maintenance crew heads home? Making this decision requires a &lt;em&gt;forecast&lt;/em&gt; of the viscous damping coefficient and the span tension, &lt;em&gt;and&lt;/em&gt; the uncertainty around these predictions. For example, predicting the damage threshold will be exceeded in 4 days ± 1 day means the maintenance can be deferred to Tuesday. If the prediction is 4 days ± 3 days, then it could very well be an issue tomorrow when the maintenance crew is out for the weekend.&lt;/p&gt; 
&lt;div id="attachment_3138" style="width: 878px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3138" loading="lazy" class="size-full wp-image-3138" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.05.06.png" alt="Figure 3 The inferred viscous damping coefficients calculated via TwinFlow UKF with MapleSim digital twin. Viscous damping coefficient of roller 9 is predicted to be increasing over the last several days" width="868" height="784"&gt;
 &lt;p id="caption-attachment-3138" class="wp-caption-text"&gt;Figure 3 The inferred viscous damping coefficients calculated via TwinFlow UKF with MapleSim digital twin. Viscous damping coefficient of roller 9 is predicted to be increasing over the last several days&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Using TwinFlow to perform scenario analysis&lt;/h2&gt; 
&lt;p&gt;Like in our previous post, we first used MapleSim to create the physics-based model of the web-handling process and exported the model as a Modelica Functional Mockup Unit (FMU) which is an industry standard file format for simulation models.&lt;/p&gt; 
&lt;p&gt;We then used &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow&lt;/a&gt; to combine the MapleSim model with an &lt;a href="https://en.wikipedia.org/wiki/Kalman_filter"&gt;Unscented Kalman Filter (UKF)&lt;/a&gt; to infer the viscous damping coefficients of the rollers in the manufacturing line. This calibration process tunes the model based on the available physical sensor data (rotation speeds of the rollers). In that previous post, we showed how the self-calibrated virtual sensors then used the incoming sensor measurements (roller angular velocity and rotation speeds) to predict tension and slip velocity at any moment in time. Here, we’ll show how to use the same L4 Digital Twin to probabilistically forecast when the span tension will exceed the threshold. Readers can find this example described in an &lt;a href="https://aws.amazon.com/solutions/guidance/self-calibrating-level-4-digital-twins-on-aws/"&gt;AWS Solution&lt;/a&gt;, and a full CDK deployment of the code used for this example up on &lt;a href="https://github.com/aws-solutions-library-samples/guidance-for-self-calibrating-level-4-digital-twins-on-aws"&gt;Github&lt;/a&gt; with instructions about how to customize for your application and deploy it.&lt;/p&gt; 
&lt;p&gt;Using TwinFlow we combine the calibrated L4 Digital Twin with forecasting models to both predict when we’ll exceed the failure threshold – and the uncertainty associated with that prediction. There are a large variety of forecasting models that range from parametric models that work well on small data sets, to non-parametric deep-learning models that often require larger data sets and hyperparameter tuning.&lt;/p&gt; 
&lt;p&gt;For our application, we specifically want an estimate of the uncertainty in the forecast and a non-parametric method so that we don’t need to manually develop the model. A &lt;a href="https://en.wikipedia.org/wiki/Gaussian_process"&gt;Gaussian Process&lt;/a&gt; (GP) is a natural fit for these requirements and a scalable low-code representation is available in TwinFlow. Using the historical time series data of the viscous damping coefficient, we can fit a GP to the data and forecast 4 days into the future along with the 95% uncertainty – as we’ve done in Figure 4.&lt;/p&gt; 
&lt;div id="attachment_3139" style="width: 838px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3139" loading="lazy" class="size-full wp-image-3139" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.05.35.png" alt="Figure 4 Forecast of viscous damping coefficient over next 4 days." width="828" height="372"&gt;
 &lt;p id="caption-attachment-3139" class="wp-caption-text"&gt;Figure 4 Forecast of viscous damping coefficient over next 4 days.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The uncertainty represents the prediction of possible future states for the viscous damping coefficients which we can then use with our L4 Digital Twin to forecast the span tension like in Figure 5. This figure corresponds to the conceptual diagram we drew in Figure 2 and you can tell that the tension crosses the threshold at 21.7 days with a lower uncertainty bound of 20.9 days. Given that today is Day 19, we can restate this as &lt;em&gt;failure is expected in 2.7 days with an uncertainty lower bound of 1.9 days from today&lt;/em&gt;. We now have enough information to decide whether to shut down the manufacturing line for maintenance or not.&lt;/p&gt; 
&lt;p&gt;In our example, the operator notices a potential issue on Friday morning before the Monday long weekend. Our prediction is that failure will most likely occur after 2.7 days (Mon), but it could be as soon as 1.9 days (Sun) and the operator should preemptively perform the repair to avoid risking losing the weekend production. This type of analysis is very challenging for an operator to perform and the decision is often based on best judgement and operator experience. An additional benefit is that the L4 Digital Twin is modeling each component within the web-handling line – enabling the operator to identify &lt;em&gt;which specific rollers&lt;/em&gt; are likely to require maintenance instead of spending time and money on all the rollers in the line.&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt;
&lt;div id="attachment_3140" style="width: 874px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3140" loading="lazy" class="size-full wp-image-3140" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.06.07.png" alt="Figure 5 Plot showing historical measured data (observations) and L4 Digital Twin future forecast with uncertainty bounds showing when the tension will cross the failure threshold [175 N]." width="864" height="387"&gt;
 &lt;p id="caption-attachment-3140" class="wp-caption-text"&gt;Figure 5 Plot showing historical measured data (observations) and L4 Digital Twin future forecast with uncertainty bounds showing when the tension will cross the failure threshold [175 N].&lt;/p&gt;
&lt;/div&gt;The value of using an L4 Digital Twin is further understood by examining the probability distributions of the forecasts. While the GP uncertainty of the viscous damping coefficient is (by definition) a Gaussian distribution, the resulting uncertainty of the tension (the shaded region) in Figure 5 isn’t symmetric around the mean value – indicating that the probability distribution at each time slice is 
&lt;em&gt;not&lt;/em&gt; Gaussian (without this information, it would likely be assumed to be Gaussian).
&lt;p&gt;&lt;/p&gt; 
&lt;p&gt;Figure 6 shows the forecast result at different time slices after being inserted into the digital twin. We can see that behavior is very non-linear with bimodal probability distributions, unlike the presumed normal (Gaussian) distributions shown in the conceptual diagram in Figure 2. We see the value of the L4 Digital Twin here because it’s impossible to know before-hand what the probability distributions would have been – and using the common assumption of Gaussian distributions would have resulted in inaccurate predictions.&lt;/p&gt; 
&lt;p&gt;The physical cause of this non-Gaussian distribution is the nature of the physics. The changing viscous damping coefficients can exhibit discontinuous changes in material slip, resulting in abrupt tension changes. This wouldn’t have been detected if we just used a data-only approach.&lt;/p&gt; 
&lt;div id="attachment_3141" style="width: 631px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3141" loading="lazy" class="size-full wp-image-3141" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.06.40.png" alt="Figure 6 Probability density distributions for max span tension at different forecasted time slices. " width="621" height="440"&gt;
 &lt;p id="caption-attachment-3141" class="wp-caption-text"&gt;Figure 6 Probability density distributions for max span tension at different forecasted time slices.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;AWS Architecture&lt;/h2&gt; 
&lt;p&gt;The AWS architecture used for the L4 Digital Twin risk assessment is depicted in Figure 7. This architecture assumes the user has already enabled an &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/"&gt;L4 self-calibrating digital twin&lt;/a&gt; where the output of the previous architecture is being pushed into an &lt;a href="https://aws.amazon.com/iot-sitewise/"&gt;AWS IoT SiteWise&lt;/a&gt; database. You can find a full CDK deployment of the code used for the example in this post on &lt;a href="https://github.com/aws-solutions-library-samples/guidance-for-self-calibrating-level-4-digital-twins-on-aws"&gt;Github&lt;/a&gt; with instructions on how to customize for your application and deployment. In Figure 7, steps 1 – 3 are the same as the Level 4 digital twin.&lt;/p&gt; 
&lt;p&gt;With a small amount of code, we can use TwinFlow to pull down the IoT SiteWise data to an &lt;a href="https://aws.amazon.com/ec2/"&gt;Amazon EC2&lt;/a&gt; instance. We then fit a Gaussian Process (from the TwinStat module in TwinFlow) to the AWS IoT SiteWise data, forecast potential future outcomes, and then sampled the potential outcomes to obtain a data set to simulate with our digital twin.&lt;/p&gt; 
&lt;p&gt;Once we have a dataset, we can submit the different inputs to the cloud (step 4) like an on-premises HPC cluster. The main point of divergence from a standard on-premises HPC cluster is the requirement to containerize the application-specific code. &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; is our cloud-native HPC scheduler that includes backend options for &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon ECS&lt;/a&gt;, which is an AWS-specific container orchestrations service, &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt;, which is a serverless execution option, and &lt;a href="https://aws.amazon.com/eks/"&gt;Amazon EKS&lt;/a&gt; which is our Kubernetes option. We used Amazon ECS because we wanted EC2 instances with large numbers of CPUs than those available for Fargate. Also, ECS enables fully-automated deployment unlike EKS.&lt;/p&gt; 
&lt;p&gt;TwinFlow reads a task list, or loads from memory, the various scenarios to be simulated. A container that includes the MapleSim digital twin and any application-specific automation is stored in a container within ECR for cloud access. The specific EC2 instance type is automatically selected by AWS Batch auto-scaling based on the user-defined CPU/GPU and memory requirements.&lt;/p&gt; 
&lt;p&gt;At step 5, the output predictions of the L4 DT simulations are generated with textual explanations and stored in an &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service&lt;/a&gt; (Amazon S3) bucket which can then be made available to users via an interface. We can also store the prediction results in a database such as &lt;a href="https://aws.amazon.com/rds/"&gt;Amazon RDS&lt;/a&gt; (step 6), which can be pulled back into &lt;a href="https://aws.amazon.com/iot-twinmaker/"&gt;AWS IoT TwinMaker&lt;/a&gt; to compare with other data streams or review past forecasts for accuracy assessment.&lt;/p&gt; 
&lt;div id="attachment_3142" style="width: 865px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3142" loading="lazy" class="size-full wp-image-3142" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-13.07.13.png" alt="Figure 7 AWS cloud architecture needed to achieve digital twin periodic-calibration" width="855" height="412"&gt;
 &lt;p id="caption-attachment-3142" class="wp-caption-text"&gt;Figure 7 AWS cloud architecture needed to achieve digital twin periodic-calibration&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;In this post, we showed how to use a L4 Digital Twin to perform risk assessment and scenario analysis using an FMU model created by MapleSim. MapleSim provides the physics-based model in the form of an FMU and TwinFlow allows us to run scalable number of scenarios on AWS, providing efficient and elastic computing resources. In this L4 Digital Twin example the versatility of the hybrid modeling-based approach is demonstrated as the means of predicting fault scenarios that may have been missed if solely relying on physics-only or data-only based methods. Using the information gained from the scenario analysis enables you to do risk assessment and informed decision making.&lt;/p&gt; 
&lt;p&gt;If you want to request a proof of concept or if you have feedback on the AWS tools, please reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-21.24.55.png" alt="Kayla Rossi" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Kayla Rossi&lt;/h3&gt; 
  &lt;p&gt;Kayla is an Application Engineer at Maplesoft providing customers with advanced engineering support for the digital modeling of their complex production machines. Her project experience includes the simulation and analysis of web handling and converting systems across various industrial applications, including printing and battery production.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/06/CleanShot-2023-12-06-at-21.25.00.png" alt="Orang Vahid" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Orang Vahid&lt;/h3&gt; 
  &lt;p&gt;Orang has over 20 years of experience in system-level modeling, advanced dynamic systems, frictional vibration and control, automotive noise and vibration, and mechanical engineering design. He is a frequent invited speaker and has published numerous papers on various topics in engineering design. At Maplesoft he is Director of Engineering Services, with a focus on the use and development of MapleSim solutions.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Slurm REST API in AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/slurm-rest-api-in-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Sean Smith]]></dc:creator>
		<pubDate>Wed, 06 Dec 2023 15:30:55 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">c6ddf535719513e79c43687d50d07ddd004a82a6</guid>

					<description>Looking to integrate AWS ParallelCluster into an automated workflow? This post shows how to submit and monitor jobs programmatically with Slurm REST API (code examples included).</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Sean Smith, Sr HPC Solution Architect, and Ryan Kilpadi, SDE Intern, HPC&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;AWS ParallelCluster offers powerful compute capabilities for problems ranging from discovering new drugs, to designing F1 race cars, to predicting the weather. In all these cases there’s a need for a human to sit in the loop – maybe an engineer running a simulation or perhaps a scientist submitting their lab results for analysis.&lt;/p&gt; 
&lt;p&gt;In this post we’ll show you how to programmatically submit and monitor jobs using the open-source Slurm REST API. This allows ParallelCluster to be integrated into an automated system via API calls. For example, this could mean that whenever a genome sample is read from a sequencer, it’s automatically fed through a secondary analysis pipeline to align the individual reads, or when new satellite data lands in an Amazon S3 bucket, it triggers a job to create the latest weather forecast.&lt;/p&gt; 
&lt;p&gt;Today, we’ll show how to set this up with AWS ParallelCluster. We’ll also link to a GitHub repository with code you can use and show examples of how to call the API using both curl and Python.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;This diagram shows an example cluster architecture with the Slurm REST API. The REST API runs on the HeadNode and submits jobs to the compute queues. The credentials used to authenticate with the API are stored in AWS Secrets Manager. The compute queues shown are examples only: the cluster can be configured with any instance configuration you desire.&lt;/p&gt; 
&lt;div id="attachment_3120" style="width: 849px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3120" loading="lazy" class="size-full wp-image-3120" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/27/CleanShot-2023-11-27-at-13.37.39.png" alt="Figure 1 – The REST API runs on the HeadNode and submits jobs to the compute queues. The credentials used to authenticate with the API are stored in AWS Secrets Manager. The compute queues shown are examples only, the cluster can be configured with any instance configuration you desire." width="839" height="385"&gt;
 &lt;p id="caption-attachment-3120" class="wp-caption-text"&gt;Figure 1 – The REST API runs on the HeadNode and submits jobs to the compute queues. The credentials used to authenticate with the API are stored in AWS Secrets Manager. The compute queues shown are examples only, the cluster can be configured with any instance configuration you desire.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For this tutorial, we’ll be using ParallelCluster UI to set up our cluster with the Slurm REST API enabled. To set up ParallelCluster UI, &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-pcui-v3.html"&gt;refer to our online documentation&lt;/a&gt;. If you’d rather use the ParallelCluster CLI, see the example YAML configuration step 5.&lt;/p&gt; 
&lt;h2&gt;Step 1 – Create a Security Group to allow inbound API requests&lt;/h2&gt; 
&lt;p&gt;By default, your cluster will not be able to accept incoming HTTPS requests to the REST API. You will need to create a security group to allow traffic from outside the cluster to call the API.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to the &lt;a href="http://console.aws.amazon.com/ec2/#SecurityGroups"&gt;EC2 Security Group console&lt;/a&gt; and choose &lt;strong&gt;Create security group&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Under Security group name, enter &lt;code&gt;Slurm REST API&lt;/code&gt;&amp;nbsp;(or another name of your choosing)&lt;/li&gt; 
 &lt;li&gt;Ensure the VPC matches your cluster’s VPC&lt;/li&gt; 
 &lt;li&gt;Add an Inbound rule and select &lt;code&gt;HTTPS&lt;/code&gt; under Type, then change the Source to only the CIDR range that you want to have access. For example, you can use the CIDR associated with your VPC to restrict access to within your VPC.&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;Create security group&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_3121" style="width: 931px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3121" loading="lazy" class="size-full wp-image-3121" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/27/CleanShot-2023-11-27-at-13.39.07.png" alt="Figure 2 – Create your security group, adding a VPC and an inbound rule to allow only HTTPS connections from a specific CID" width="921" height="453"&gt;
 &lt;p id="caption-attachment-3121" class="wp-caption-text"&gt;Figure 2 – Create your security group, adding a VPC and an inbound rule to allow only HTTPS connections from a specific CID&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Step 2 – Add Additional IAM Permissions&lt;/h2&gt; 
&lt;p&gt;If you’re using the AWS ParallelCluster UI, please follow the instructions under the ParallelCluster UI Tutorial &lt;strong&gt;section ‘G’&lt;/strong&gt; – &lt;a href="https://pcluster.cloud/02-tutorials/07-setup-iam.html"&gt;Setup IAM Permissions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Step 3 – Configure your Cluster&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;In your cluster configuration, return to the &lt;strong&gt;HeadNode section &amp;gt; Advanced options &amp;gt; Additional Security Groups &amp;gt;&lt;/strong&gt; add the &lt;em&gt;Slurm REST API&lt;/em&gt; Security Group you created in Step 1. Under &lt;strong&gt;Scripts &amp;gt; on node configured &amp;gt;&lt;/strong&gt; add the following script:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/rest-api/postinstall.sh&lt;/code&gt;&lt;/pre&gt; 
&lt;div id="attachment_3122" style="width: 639px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3122" loading="lazy" class="size-full wp-image-3122" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/27/CleanShot-2023-11-27-at-13.41.18.png" alt="Figure 4 – Add the script to be run after node configuration" width="629" height="288"&gt;
 &lt;p id="caption-attachment-3122" class="wp-caption-text"&gt;Figure 4 – Add the script to be run after node configuration&lt;/p&gt;
&lt;/div&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Under &lt;strong&gt;Additional IAM permissions&lt;/strong&gt;, add the policy:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;arn:aws:iam::aws:policy/SecretsManagerReadWrite&lt;/code&gt;&lt;/pre&gt; 
&lt;div id="attachment_3123" style="width: 838px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3123" loading="lazy" class="size-full wp-image-3123" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/27/CleanShot-2023-11-27-at-13.42.08.png" alt="Figure 5 – Add IAM policy to allow updates to AWS SecretsManager. This is needed to automatically refresh the JSON Web Token (JWT)." width="828" height="231"&gt;
 &lt;p id="caption-attachment-3123" class="wp-caption-text"&gt;Figure 5 – Add IAM policy to allow updates to AWS SecretsManager. This is needed to automatically refresh the JSON Web Token (JWT).&lt;/p&gt;
&lt;/div&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Create your cluster.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Step 4 – Validate the configuration&lt;/h2&gt; 
&lt;p&gt;Your configuration file should look something like the text that follows. If you opted to use the CLI instead of the UI, you will need to replace:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;AdditionalSecurityGroups&lt;/code&gt; – this should contain an additional security group that allows connections to the Slurm REST API (Step 1).&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;OnNodeConfigured&lt;/code&gt;: thish should reference the post-install script: &lt;code&gt;&lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/rest-api/postinstall.sh"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/rest-api/postinstall.sh&lt;/a&gt;&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Imds:
  ImdsSupport: v1.0
HeadNode:
  InstanceType: c5.xlarge
  Imds:
    Secured: true
  Ssh:
    KeyName: amzn2
  LocalStorage:
    RootVolume:
      VolumeType: gp3
  Networking:
    SubnetId: subnet-xxxxxxxxxxxxxx
    AdditionalSecurityGroups:
      - sg-slurmrestapixxxxxxxxxx
  Iam:
    AdditionalIamPolicies:
      - Policy: arn:aws:iam::aws:policy/SecretsManagerReadWrite
  CustomActions:
    OnNodeConfigured:
      Script: &amp;gt;-
        &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/rest-api/postinstall.sh" rel="noopener noreferrer"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/rest-api/postinstall.sh&lt;/a&gt;
Scheduling:
  Scheduler: slurm
  SlurmQueues:
    - Name: queue-1
      ComputeResources:
        - Name: queue-1-cr-1
          Instances:
            - InstanceType: c5.xlarge
          MinCount: 0
          MaxCount: 4
      ComputeSettings:
        LocalStorage:
          RootVolume:
            VolumeType: gp3
      Networking:
        SubnetIds:
          - subnet-xxxxxxxxxxxxxxxxxx
Region: us-east-2
Image:
	Os: alinux2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Step 5 – Call the API&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Log in to a machine on the same network that you allowed via the Security Group in Step 1. Make sure this machine is able to talk to the HeadNode.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;ssh username@ip&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Set the following environment variable:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;export CLUSTER_NAME=[name of cluster]&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Find the information needed to call the API and construct an API request. To do this, we’ll need a few pieces of information. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;JWT token&lt;/strong&gt;: The post install script will have created a secret in &lt;a href="http://console.aws.amazon.com/secretsmanager"&gt;AWS SecretsManager&lt;/a&gt; under the name &lt;code&gt;slurm_token_$CLUSTER_NAME&lt;/code&gt; . Either use the AWS console or the AWS CLI to find your secret based on the cluster name:&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;export JWT=$(aws secretsmanager get-secret-value --secret-id slurm_token_$CLUSTER_NAME | jq -r '.SecretString')&lt;/code&gt;NOTE: Since the Slurm REST API script is not integrated into ParallelCluster, this secret will not be automatically deleted along with the cluster. You may want to remove it manually on cluster deletion.&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li style="list-style-type: none"&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Head node public IP:&lt;/strong&gt; This can be found in your Amazon EC2 dashboard or by using the ParallelCluster CLI:&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;export HEADNODE_IP=$(pcluster describe-cluster-instances -n $CLUSTER_NAME | jq -r '.instances[0].publicIpAddress')&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li style="list-style-type: none"&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Cluster user&lt;/strong&gt;: This depends on your AMI, but it will usually be either ec2-user , ubuntu , or centos.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;export CLUSTER_USER=ec2-user&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Call the API using curl:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;curl -H "X-SLURM-USER-NAME: $CLUSTER_USER" -H "X-SLURM-USER-TOKEN: $JWT" https://$HEADNODE_IP/slurm/v0.0.39/ping -k&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You’ll get a response back like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;{
    "meta": {
        "plugin": {
            "type": "openapi\/v0.0.39",
            "name": "REST v0.0.39"
        },
        "Slurm": {
            "version": {
            "major": 23,
            "micro": 2,
            "minor": 2
        },
         "release": "23.02.2"
    }...
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Submit a job using the API. Specify the job parameters using JSON. You may need to modify the standard directories depending on the cluster user.&lt;/li&gt; 
 &lt;li&gt;Post a job to the API:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;curl -H "X-SLURM-USER-TOKEN: $CLUSTER_USER" -H "X-SLURM-USER-TOKEN: $JWT" -X POST https://$IP/slurm/v0.0.39/job/submit -H "Content-Type: application/json" -d @testjob.json -k&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;c. Verify that the job is running:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;curl -H "X-SLURM-USER-NAME: $CLUSTER_USER" -H "X-SLURM-USER-TOKEN: $JWT" https://$IP/slurm/v0.0.39/jobs -k&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Calling the API using the Python requests library&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a script called &lt;code&gt;slurmapi.py&lt;/code&gt; with the following contents:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;#!/usr/bin/env python3
import argparse
import boto3
import requests
import json

# Create argument parser
parser = argparse.ArgumentParser()
parser.add_argument('-n', '--cluster-name', type=str, required=True)
parser.add_argument('-u', '--cluster-user', type=str, required=False)
subparsers = parser.add_subparsers(dest='command', required=True)

diag_parser = subparsers.add_parser('diag', help="Get diagnostics")
ping_parser = subparsers.add_parser('ping', help="Ping test")

submit_job_parser = subparsers.add_parser('submit-job', help="Submit a job")
submit_job_parser.add_argument('-j', '--job', type=str, required=True)

list_jobs_parser = subparsers.add_parser('list-jobs', help="List active jobs")

describe_job_parser = subparsers.add_parser('describe-job', help="Describe a job by id")
describe_job_parser.add_argument('-j', '--job-id', type=int, required=True)

cancel_parser = subparsers.add_parser('cancel-job', help="Cancel a job")
cancel_parser.add_argument('-j', '--job-id', type=int, required=True)

args = parser.parse_args()

# Get JWT token
client = boto3.client('secretsmanager')
boto_response = client.get_secret_value(SecretId=f'slurm_token_{args.cluster_name}')
jwt_token = boto_response['SecretString']

# Get cluster headnode IP
client = boto3.client('ec2')
filters = [{'Name': 'tag:parallelcluster:cluster-name', 'Values': [args.cluster_name]}]
boto_response = client.describe_instances(Filters=filters)
headnode_ip = boto_response['Reservations'][0]['Instances'][0]['PublicIpAddress']

url = f'https://{headnode_ip}/slurm/v0.0.39'
headers = {'X-SLURM-USER-TOKEN': jwt_token}
if args.cluster_user:
    headers['X-SLURM-USER-NAME'] = args.cluster_user

# Make request
if args.command == 'ping':
    r = requests.get(f'{url}/ping', headers=headers, verify=False)
elif args.command == 'diag':
    r = requests.get(f'{url}/diag', headers=headers, verify=False)
elif args.command == 'submit-job':
    with open(args.job) as job_file:
        job_json = json.load(job_file)
    r = requests.post(f'{url}/job/submit', headers=headers, json=job_json, verify=False)
elif args.command == 'list-jobs':
    r = requests.get(f'{url}/jobs', headers=headers, verify=False)
elif args.command == 'describe-job':
    r = requests.get(f'{url}/job/{args.job_id}', headers=headers, verify=False)
elif args.command == 'cancel-job':
    r = requests.delete(f'{url}/job/{args.job_id}', headers=headers, verify=False)

print(r.text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Submitt a job&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;./slurmapi.py -n [cluster_name] submit-job -u [cluster_user] -j testjob.json&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Getting more information&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;./slurmapi.py -h&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Setting up the Slurm REST API allows you to programmatically control the cluster, this makes it possible to build the cluster into an automated workflow. This enables new use cases such as automated secondary analysis of genomics data, risk analysis in financial markets, weather prediction, among a myriad of other use cases. We’re excited to see what you build, drop us a line on twitter to showcase what you come up with.&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/27/CleanShot-2023-11-27-at-14.07.01.png" alt="Ryan Kilpadi " width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Ryan Kilpadi&lt;/h3&gt; 
  &lt;p&gt;Ryan Kilpadi is a returning SDE intern on the HPC team working on AWS ParallelCluster. He worked on implementing the Slurm REST API on ParallelCluster as a summer internship project in 2022.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>New: Research and Engineering Studio on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/new-research-and-engineering-studio-on-aws/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Mon, 13 Nov 2023 22:14:01 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[End User Computing]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[DCV]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[FEA]]></category>
		<category><![CDATA[Finite Element Analysis]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[VDI]]></category>
		<category><![CDATA[visualization]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">e085244ba2853fcab2e7687e1f7ed8d1e80e089c</guid>

					<description>Today we’re announcing Research and Engineering Studio on AWS, a self-service portal to help scientists and engineers access and manage virtual desktops to see their data and run their interactive applications in the cloud.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="size-full wp-image-3095 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/08/Ingenio-icon_2pt-line-version-illustration.png" alt="" width="380" height="235"&gt;Your brain is extravagantly equipped for visualization. More than 50% of your cerebral cortex is dedicated to processing sensory input from your eyes, making it the information super highway that helps complex ideas form in your head incredibly quickly. Our output devices – our limbs and our voices – had to develop unique articulation skills, just to match the speed at which we can &lt;em&gt;learn from seeing&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;So, it’s fitting that today we’re announcing Research and Engineering Studio on AWS (RES), a self-service portal to help scientists and engineers access and manage their research and design workspaces, including virtual desktops to &lt;em&gt;see&lt;/em&gt; their data and run their interactive applications in the cloud.&lt;/p&gt; 
&lt;p&gt;Setting up – and then managing – virtual desktops manually takes a lot of work, taking time away from the core mission of research and development groups: solving hard scientific problems. We’ve also noticed that when processes to access resources become complicated, users often take shortcuts (like sharing passwords or setting file permissions to be globally read/write).&lt;/p&gt; 
&lt;p&gt;With RES, we’re putting all the launch and monitoring tools behind a single pane of glass, so administrators can deploy new desktop environments quickly, from their own pre-prepared library of software images and trusted applications. This gives their users the freedom to come and go when needed – incredibly important when chasing new lines of inquiry – without impacting their budgets or the security posture of their organization.&lt;/p&gt; 
&lt;p&gt;In today’s post, we’ll explain what RES is and how it works, and we’ll explain how to deploy it in your own AWS account, and get it ready for your users.&lt;/p&gt; 
&lt;h2&gt;Visualization without the headaches&lt;/h2&gt; 
&lt;p&gt;Research and Engineering Studio on AWS has been designed to make it easier for site admins to provide a suite of applications, data, and large-scale compute resources through familiar interfaces – starting today with interactive desktops.&lt;/p&gt; 
&lt;p&gt;When you login to RES through your browser, you’ll have the ability to see the virtual desktops that are available to you, based on permissions that draw from your membership of user &lt;strong&gt;Groups&lt;/strong&gt; that are associated with &lt;strong&gt;Projects&lt;/strong&gt;. We’ll explain this a little further on.&lt;/p&gt; 
&lt;p&gt;On the main Virtual Desktops screen, you’ll have the ability to see thumbnails of sessions that are live and running, and other sessions that are suspended. All have sets of controls to spin them up, or shut them down. You can even schedule sessions start and stop during specific time windows (like 9-5 Mon-Fri, if that’s your thing). This is also where you can create new sessions, based on software stacks your admin has provided for you – this only take a couple of minutes, typically.&lt;/p&gt; 
&lt;p&gt;By clicking &lt;strong&gt;Connect&lt;/strong&gt; in the RES interface for a running session, you can open a new browser tab containing an encrypted shared desktop session with the remote system using NICE DCV – our own &lt;a href="https://aws.amazon.com/blogs/hpc/pushing-pixels-with-nice-dcv/"&gt;high performance streaming protocol&lt;/a&gt;. DCV uses a lot of tricks with network protocols designed to make remote desktops feel like they’re local. The techniques it uses to do this are remarkable enough that Netflix uses it to provide their editors and FX teams access to powerful Amazon EC2 systems so they can produce the great content you’re probably watching this week on TV. This is great for scientists and engineers, since our interactive applications often need precision screen controls and only work well with access to truly large memory models.&lt;/p&gt; 
&lt;div id="attachment_3096" style="width: 853px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3096" loading="lazy" class="size-full wp-image-3096" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/08/CleanShot-2023-11-08-at-10.57.51.png" alt="Figure 1 - the RES Virtual Desktops screen lists all the sessions a user has previously created, with controls to spin up, shutdown, or schedule uptime." width="843" height="481"&gt;
 &lt;p id="caption-attachment-3096" class="wp-caption-text"&gt;Figure 1 – the RES Virtual Desktops screen lists all the sessions a user has previously created, with controls to spin up, shutdown, or schedule uptime.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;RES allows users to share their desktops, too. While the sessions they start in RES will always belong to them, they can share a session too, like the way you can do that in a conferencing system like Zoom. This can be incredibly useful for working sessions where a group of engineers in a team need draw design insights by poring over visualizations of massive datasets – without needing to move the datasets themselves. A desktop share can have an expiration date attached to it, too.&lt;/p&gt; 
&lt;div id="attachment_3104" style="width: 1767px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3104" loading="lazy" class="wp-image-3104 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/09/share-session.png" alt="Figure 2 - RES permits users to share their virtual desktop sessions with others in a controlled fashion." width="1757" height="988"&gt;
 &lt;p id="caption-attachment-3104" class="wp-caption-text"&gt;Figure 2 – RES permits users to share their virtual desktop sessions with others in a controlled fashion.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Projects, users, and groups&lt;/h2&gt; 
&lt;p&gt;To manage all of this, RES works from a few important concepts, all of which are likely familiar to you already.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;First is the &lt;strong&gt;Project&lt;/strong&gt;. In most organizations – public and private – everything revolves around &lt;em&gt;project teams&lt;/em&gt;. Resources are typically created for – and consumed by – project teams, with access, billing, and even software stacks being driven by project needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Users and Groups: &lt;/strong&gt;Users can be members of multiple Groups, and in RES a Group can be associated with one or more Projects.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Like most institutions, you probably have an investment in a user identity and authentication system, complete with existing project groups that you’ve defined to define access for shared files and other resources. We wanted to align with that, so RES – out of the box – is built to connect with your existing directory service, starting with Active Directory (AD), helping you avoid doing this work twice.&lt;/p&gt; 
&lt;p&gt;At launch, RES can connect to your existing AD server &lt;em&gt;on-premises&lt;/em&gt; or &lt;em&gt;in the cloud&lt;/em&gt;. If you want to separate your RES environment from your regular user space – to create an engineering sandbox, for instance – you can spin up a managed AD using AWS Directory Service to privately manage a set of users.&lt;/p&gt; 
&lt;div id="attachment_3133" style="width: 3526px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3133" loading="lazy" class="wp-image-3133 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/12/01/users-shot.png" alt="Figure 3 - Identity functions, like defining users and groups, are done centrally in an Active Directory environment, which you specify when installing RES in your AWS environment. The AD can be on-premises or in the cloud and can - if you wish - be separate from your corporate directory service." width="3516" height="1976"&gt;
 &lt;p id="caption-attachment-3133" class="wp-caption-text"&gt;Figure 3 – Identity functions, like defining users and groups, are done centrally in an Active Directory environment, which you specify when installing RES in your AWS environment. The AD can be on-premises or in the cloud and can – if you wish – be separate from your corporate directory service.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;RES enables access to &lt;strong&gt;Project&lt;/strong&gt; resources, by specifying which &lt;strong&gt;Groups&lt;/strong&gt; are assigned to the &lt;strong&gt;Project&lt;/strong&gt;. You define this at Project creation, but of course you can change it afterwards, too (see Figure 4).&lt;/p&gt; 
&lt;div id="attachment_3107" style="width: 1768px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3107" loading="lazy" class="wp-image-3107 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/09/create-project.png" alt="Figure 4 - when you create a Project in RES, you can specify which user groups are permitted to access its resources. The Groups and Users are inherited from your Active Directory service." width="1758" height="988"&gt;
 &lt;p id="caption-attachment-3107" class="wp-caption-text"&gt;Figure 4 – when you create a Project in RES, you can specify which user groups are permitted to access its resources. The Groups and Users are inherited from your Active Directory service.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;RES is designed to bind to your AD and draw its knowledge of your teams from there, which allows RES to reflect changes you make to Group memberships right away.&lt;/p&gt; 
&lt;h2&gt;Storage&lt;/h2&gt; 
&lt;p&gt;At launch, RES supports two file systems which customers have told us are most useful for managing users’ files. Amazon Elastic File System (EFS) and Amazon FSx for NetApp ONTAP. While EFS is great for customers with large Linux deployments using NFSv4, FSx for NetApp ONTAP can offer the same file system using either NFS or SMB – which is helpful if you’re running a hybrid Windows and Linux environment and your users regularly flip back and forth, depending on the applications they’re using.&lt;/p&gt; 
&lt;div id="attachment_3100" style="width: 851px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3100" loading="lazy" class="size-full wp-image-3100" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/08/CleanShot-2023-11-08-at-10.59.48.png" alt="Figure 5 - You can manage file systems in RES, and associate them with Projects. At launch, RES supports Amazon EFS, and Amazon FSx for NetApp ONTAP." width="841" height="229"&gt;
 &lt;p id="caption-attachment-3100" class="wp-caption-text"&gt;Figure 5 – You can manage file systems in RES, and associate them with Projects. At launch, RES supports Amazon EFS, and Amazon FSx for NetApp ONTAP.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;RES allows an administrator to spin up new filesystems or to &lt;em&gt;onboard existing filesystems&lt;/em&gt; which you might have previously created for your project. You need to associate a file system resource with a &lt;strong&gt;Project&lt;/strong&gt; to make it visible to the users who are member of the Groups associated with that project. Individual file and folder access is still controlled in the usual way at an operating system level with permissions, which again will usually map back to your Users and Groups definitions in the AD you’re using for your directory service.&lt;/p&gt; 
&lt;h2&gt;Software stacks and machine images&lt;/h2&gt; 
&lt;p&gt;RES uses Amazon Machine Images (AMIs) as the basis for customizing the environments you deploy for your users. If this is new to you, an AMI is an image that provides the information required to launch an instance. You need to specify an AMI whenever you launch an instance. You can launch multiple instances from a single AMI when you need lots of machines with the same configuration. And you can use different AMIs to launch instances when you need add some local flavor to your users’ environment.&lt;/p&gt; 
&lt;p&gt;RES extends this concept a little further by narrowing the set of instances and GPUs it will offer based on an AMI. And – as you’d expect – the resulting stack needs to be associated with a Project before a user will know about it. This is helpful not just for customizing what your users can &lt;em&gt;do&lt;/em&gt;, but it’s great when you have commercially licensed software, too. For example – all members of Project &lt;em&gt;Beeblebrox&lt;/em&gt; can access an AMI with the &lt;em&gt;Marvin&lt;/em&gt; license embedded in it.&lt;/p&gt; 
&lt;p&gt;Creating an AMI is almost as easy as launching an existing one, making your changes, and then &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html#creating-an-ami"&gt;asking the Amazon EC2 console&lt;/a&gt; to create a new AMI from it. After that, you just need to take the ID of your AMI to RES admin screen and register it.&lt;/p&gt; 
&lt;p&gt;Starting today, RES is designed to support Windows and Linux operating systems, specifically: Windows Server 2019 (Datacenter Version 1809), Amazon Linux 2, CentOS 7, Red Hat Enterprise Linux 7, Red Hat Enterprise Linux 8, and Red Hat Enterprise Linux 9. We’ll keep the list of supported operating systems up to date in the online documentation for RES.&lt;/p&gt; 
&lt;h2&gt;One click deployment – or two&lt;/h2&gt; 
&lt;p&gt;Research and Engineering Studio is a self-managed, open-source product which you install in your own AWS account.&lt;/p&gt; 
&lt;p&gt;There are several ways of getting up and running with RES. Which one you choose depends on your experience with AWS, and how heavily you want to customize it to suit your local needs. This includes a “&lt;strong&gt;batteries included”&lt;/strong&gt; method which is a full, one-click launchable AWS CloudFormation stack, that can be used to build an entire environment, including a managed AD server.&lt;/p&gt; 
&lt;p&gt;RES is a supported open-source project, available via its public &lt;a href="https://github.com/aws/res"&gt;GitHub repository&lt;/a&gt;, which is where you can find the RES installation recipes (including several components, supplied by the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;HPC Recipe Library&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;RES can be used in the following Regions: US East (Ohio), US East (N. Virginia), US West (N. California), US West (Oregon), Asia Pacific (Singapore), Asia Pacific (Sydney), Europe (Ireland), Europe (Frankfurt), Europe (London), Europe (Paris), Asia Pacific (Mumbai) and AWS GovCloud (US-West).&lt;/p&gt; 
&lt;p&gt;Links to documentation, and more are available on the &lt;a href="https://aws.amazon.com/hpc/res"&gt;Research and Engineering Studio homepage&lt;/a&gt;. Try it out and let us know what you think at&amp;nbsp;&lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Deep-dive into Ansys Fluent performance on Ansys Gateway powered by AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/deep-dive-into-ansys-fluent-performance-on-ansys-gateway-powered-by-aws/</link>
		
		<dc:creator><![CDATA[Dnyanesh Digraskar]]></dc:creator>
		<pubDate>Thu, 09 Nov 2023 15:38:16 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">62218975828b3a44d88617555b4bff2d5132a214</guid>

					<description>In this post, we’ll show you the performance and price curves when Ansys Gateway, powered by AWS runs on different HPC instances - this should help you make the right hardware choices for running Fluent simulations in the cloud.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Dnyanesh Digraskar, Principal HPC Partner Solutions Architect, AWS, Ashwini Kumar, Senior Principal Engineer, Ansys, Nicole Diana, Director, Fluids Business Unit, Ansys.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Today, we’re going to deep-dive into the performance and associated cost of running computational fluid dynamics (CFD) simulations on AWS using Ansys Fluent through the &lt;a href="https://www.ansys.com/products/cloud/ansys-gateway"&gt;Ansys Gateway powered by AWS&lt;/a&gt; (or just “Ansys Gateway” for the rest of this post).&lt;/p&gt; 
&lt;p&gt;Ansys Gateway is an &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-ppiyshk2oin3i"&gt;AWS Marketplace&lt;/a&gt; hosted solution for users to manage their complete Ansys virtual desktop and HPC simulation workflows with more than fifty Ansys products in their AWS cloud environment. &lt;a href="https://www.ansys.com/resource-center/case-study/ansys-emirates-team-new-zealand"&gt;Emirates Team New Zealand&lt;/a&gt; and &lt;a href="https://www.ansys.com/resource-center/case-study/ansys-turntide-technologies"&gt;Turntide Technologies&lt;/a&gt; use Ansys Gateway to accelerate their design and engineering simulation cycles.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://www.ansys.com/products/fluids/ansys-fluent"&gt;Ansys Fluent&lt;/a&gt;, an advanced physics modeling simulation software is used by engineers and scientists across industries like automotive, aerospace, manufacturing, and energy to innovate and optimize product development.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll evaluate the performance and price characteristics for three test cases on various configurations of Amazon Elastic Compute Cloud (Amazon EC2) instance types. Using the results in this post, you’ll be able to make appropriate hardware choices for running Ansys Fluent simulations.&lt;/p&gt; 
&lt;h2&gt;Ansys Gateway recap&lt;/h2&gt; 
&lt;p&gt;Ansys Gateway is a secure, online platform that enables users to create, manage, and execute complete computer-aided engineering (CAE) workflows in their own AWS accounts. Earlier this year, we published a &lt;a href="https://aws.amazon.com/blogs/apn/accelerate-your-ansys-simulations-with-ansys-gateway-powered-by-aws/"&gt;blog post&lt;/a&gt; that described solution architecture components, security implementation, and typical end-user workflows for using Ansys Gateway.&lt;/p&gt; 
&lt;p&gt;The Ansys applications (solvers) available through Ansys Gateway are pre-configured, validated, and extensively benchmarked on various Amazon EC2 hardware for performance and price. Ansys users can refer to the &lt;a href="https://ansyshelp.ansys.com/account/secured?returnurl=/Views/Secured/gateway/v000/en/gateway_ru/bk_gateway_ru.html"&gt;recommended usage&lt;/a&gt; guidelines on the Ansys Gateway documentation in &lt;a href="https://www.ansys.com/support"&gt;Ansys Help&lt;/a&gt; to setup their virtual desktop infrastructure (VDI) or HPC environment with the recommended Amazon EC2 instance types. Users can then carry out simulations with solvers of their choice straight out-of-the-box without the need to manually setup and tune the solvers, simulation environment, and hardware parameters.&lt;/p&gt; 
&lt;h2&gt;Benchmark information and simulation environment&lt;/h2&gt; 
&lt;h3&gt;Benchmarks&lt;/h3&gt; 
&lt;p&gt;For the benchmarking purposes of this post, we’ve used the test cases from the standard &lt;a href="https://www.ansys.com/it-solutions/benchmarks-overview"&gt;Ansys Fluent Benchmarks&lt;/a&gt; suite. The model description of each test case, including the mesh size represented in terms of number of cells, turbulence model used, and the fluid-flow condition are shown in Table 1. These benchmarks, shown in Figures 1a – 1c for visual reference, represent the typical size and physics modeled by users. We used Ansys Fluent version 2023 R1 to run the simulations.&lt;/p&gt; 
&lt;div id="attachment_3037" style="width: 899px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3037" loading="lazy" class="size-full wp-image-3037" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.06.26.png" alt="Table 1: Description of Ansys Fluent benchmarking test cases used for this post." width="889" height="198"&gt;
 &lt;p id="caption-attachment-3037" class="wp-caption-text"&gt;Table 1: Description of Ansys Fluent benchmarking test cases used for this post.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3038" style="width: 1107px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3038" loading="lazy" class="size-full wp-image-3038" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.10.32.png" alt="Figure 1a: Visual representation of Ansys Fluent benchmarking test cases used for this post – Steady-state simulation of flow through a vehicle exhaust system with 33 million cells. Figure 1b: Visual representation of Ansys Fluent benchmarking test cases used for this post – Transient simulation of flow through a combustor with 71 million cells. Figure 1c: Visual representation of Ansys Fluent benchmarking test cases used for this post – Steady-state simulation of external aerodynamics of Formula 1 race car with 140 million cells." width="1097" height="241"&gt;
 &lt;p id="caption-attachment-3038" class="wp-caption-text"&gt;Figure 1 – &lt;strong&gt;a&lt;/strong&gt;: Visual representation of Ansys Fluent benchmarking test cases used for this post – Steady-state simulation of flow through a vehicle exhaust system with 33 million cells. &lt;strong&gt;b&lt;/strong&gt;: Visual representation of Ansys Fluent benchmarking test cases used for this post – Transient simulation of flow through a combustor with 71 million cells. &lt;strong&gt;c&lt;/strong&gt;: Visual representation of Ansys Fluent benchmarking test cases used for this post – Steady-state simulation of external aerodynamics of Formula 1 race car with 140 million cells.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Simulation environment&lt;/h3&gt; 
&lt;p&gt;AWS recently announced the &lt;a href="https://aws.amazon.com/ec2/instance-types/hpc7a/"&gt;Amazon EC2 Hpc7a&lt;/a&gt; instance type, powered by 4&lt;sup&gt;th&lt;/sup&gt; generation AMD EPYC&lt;sup&gt;TM&lt;/sup&gt; (Genoa) processors with up to 192 physical cores and 300 Gbps Elastic Fabric Adapter (EFA) network bandwidth. We compared the performance of these benchmarks on Hpc7a and the previous generation &lt;a href="https://aws.amazon.com/ec2/instance-types/hpc6a/"&gt;Hpc6a&lt;/a&gt; instances.&lt;/p&gt; 
&lt;p&gt;The following table (Table 2) summarizes Amazon EC2 instance configurations used:&lt;/p&gt; 
&lt;div id="attachment_3039" style="width: 936px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3039" loading="lazy" class="size-full wp-image-3039" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.12.16.png" alt="Table 2: Amazon EC2 instances used for running the benchmarking test cases" width="926" height="362"&gt;
 &lt;p id="caption-attachment-3039" class="wp-caption-text"&gt;Table 2: Amazon EC2 instances used for running the benchmarking test cases&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Analysis methodology&lt;/h2&gt; 
&lt;p&gt;Our objective for running these benchmarks was to quantify Ansys Fluent performance, associated hardware, platform, and license costs, and thus be able to recommend the appropriate Amazon EC2 instance type to use on Ansys Gateway. With that in mind, we performed the analysis by measuring the following:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Solver Rating to represent the solver performance&lt;/li&gt; 
 &lt;li&gt;Ratio of performance to hardware configuration cost&lt;/li&gt; 
 &lt;li&gt;Ratio of performance to total job cost&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Solver Rating:&lt;/strong&gt; the &lt;em&gt;Solver Rating&lt;/em&gt; is defined as the number of times the benchmark can be run on a given machine in 24 hours. It’s computed by dividing the number of seconds in a day by the number of seconds required to run the benchmark. A higher &lt;em&gt;Solver Rating&lt;/em&gt; indicates better performance. &lt;em&gt;Solver Rating&lt;/em&gt; is the primary metric we used in this post to report the performance. We ran our simulations for 1000 iterations for steady-state flow or 1000 timesteps for transient flow.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Job cost:&lt;/strong&gt; The total job (or simulation) cost is comprised of three main components: the Amazon EC2 cost, Ansys Gateway charge of $0.25 per running Amazon EC2 instance per hour, and the Ansys Licensing&amp;nbsp;cost.&lt;/p&gt; 
&lt;p&gt;These cost representations can guide you to select the right HPC configuration to&amp;nbsp;meet any one of these three simulation goals:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;minimize job cost&lt;/li&gt; 
 &lt;li&gt;maximize performance&lt;/li&gt; 
 &lt;li&gt;achieve the best performance/cost ratio&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Note that for the purpose of this post, we used the Ansys Elastic Licensing to account for the licensing costs which might not be fully representative if you’re using leased or perpetual licenses. Also, we haven’t accounted for storage and networking charges in our calculations, simply because compute constitutes most (or nearly all) of the infrastructure costs for these simulations. We used Amazon EC2 on-demand costs from the us-east-2 (Ohio) region. You can take advantage of flexible Amazon EC2 pricing options like &lt;a href="https://aws.amazon.com/savingsplans/"&gt;Compute Savings Plan&lt;/a&gt; or &lt;a href="https://aws.amazon.com/ec2/pricing/reserved-instances/"&gt;Reserved Instances&lt;/a&gt; (RI) which provide a significant discount (up to 72%) compared to On-Demand pricing.&lt;/p&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;p&gt;To understand Ansys Fluent performance, we plotted the variation of the Solver Rating against the number of instance cores for each of our three test cases. These plots are in Figures 2 (a, b, c). Higher Solver Rating signifies better performance. The &lt;em&gt;Vehicle Exhaust&lt;/em&gt; model with 33 million cells was scaled to 1536 cores, while the &lt;em&gt;Combustor&lt;/em&gt; and &lt;em&gt;F1 Race Car&lt;/em&gt; models were scaled to over 6000 cores, respectively.&lt;/p&gt; 
&lt;div id="attachment_3040" style="width: 708px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3040" loading="lazy" class="size-full wp-image-3040" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.13.17.png" alt="Figure 2a: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Vehicle exhaust benchmark." width="698" height="451"&gt;
 &lt;p id="caption-attachment-3040" class="wp-caption-text"&gt;Figure 2a: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Vehicle exhaust benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3041" style="width: 697px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3041" loading="lazy" class="size-full wp-image-3041" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.13.53.png" alt="Figure 2b: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Combustor benchmark." width="687" height="436"&gt;
 &lt;p id="caption-attachment-3041" class="wp-caption-text"&gt;Figure 2b: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Combustor benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3042" style="width: 703px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3042" loading="lazy" class="size-full wp-image-3042" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.14.32.png" alt="Figure 2c: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Formula 1 car benchmark." width="693" height="442"&gt;
 &lt;p id="caption-attachment-3042" class="wp-caption-text"&gt;Figure 2c: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instance cores for the Formula 1 car benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Since the number of physical cores on Hpc6a and Hpc7a instance types are different, the relative performance between the two instance types on cores and node level differ, too. In Figures 3 (a, b, c) we plotted the variation of the Solver Rating with the number of instances (nodes).&lt;/p&gt; 
&lt;div id="attachment_3043" style="width: 706px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3043" loading="lazy" class="size-full wp-image-3043" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.15.25.png" alt="Figure 3a: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Vehicle Exhaust benchmark." width="696" height="398"&gt;
 &lt;p id="caption-attachment-3043" class="wp-caption-text"&gt;Figure 3a: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Vehicle Exhaust benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3044" style="width: 708px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3044" loading="lazy" class="size-full wp-image-3044" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.15.53.png" alt="Figure 3b: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Combustor benchmark." width="698" height="444"&gt;
 &lt;p id="caption-attachment-3044" class="wp-caption-text"&gt;Figure 3b: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Combustor benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3045" style="width: 748px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3045" loading="lazy" class="size-full wp-image-3045" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.16.19.png" alt="Figure 3c: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Formula 1 race car benchmark." width="738" height="470"&gt;
 &lt;p id="caption-attachment-3045" class="wp-caption-text"&gt;Figure 3c: Comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a and Hpc7a instances (nodes) for the Formula 1 race car benchmark.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For the Vehicle Exhaust model with 33 million cells, the performance improvement with Hpc7a instances was 1.2x at 1536 cores on a &lt;em&gt;per-core &lt;/em&gt;basis, and 1.78x at 32 instances on a &lt;em&gt;per-instance&lt;/em&gt; basis. For the Combustor benchmark model with 71 million cells, the performance improvement with Hpc7a instances is 1.3x at 3072 cores on a &lt;em&gt;per-core&lt;/em&gt; basis, and was 1.83x at 32 instances on &lt;em&gt;per-instance&lt;/em&gt; basis.&lt;/p&gt; 
&lt;p&gt;As the benchmark model size increased, the instance topology and higher EFA networking bandwidth of Hpc7a instances helps to achieve better scaling. For the Formula 1 Race Car model with 140 million cells, the performance improvement &lt;em&gt;per-core&lt;/em&gt; with Hpc7a instances was 1.3x at 6144 cores, and was 2.1x at 32 instances on a &lt;em&gt;per-instance&lt;/em&gt; basis.&lt;/p&gt; 
&lt;h3&gt;Full cores vs partial cores&lt;/h3&gt; 
&lt;p&gt;These plots show performance results when running Ansys Fluent on the full available set of physical cores: 192 for hpc7a.96xlarge, and 96 for hpc6a.48xlarge. But it’s possible to manually disable certain numbers of physical cores – or use process pinning on each instance – to achieve better &lt;em&gt;per-core&lt;/em&gt; performance because of increased memory bandwidth per core. We call this under-subscribing.&lt;/p&gt; 
&lt;p&gt;When running simulations on the Hpc6a instance type, which is available in only one size, under-subscribing is implemented in Ansys Gateway via job setup scripts. Under-subscribing on Hpc7a isn’t required because it’s a feature of the different instance sizes.&lt;/p&gt; 
&lt;p&gt;In this section we’ll take a detailed look at the impact this under-subscribing technique has on Ansys Fluent performance. Figures 4a, 4b, 4c show variations of Solver Rating with the number of instance cores for 100%, 50%, and 25% cores enabled per-instance for Hpc6a and corresponding sizes of Hpc7a instance type.&lt;/p&gt; 
&lt;div id="attachment_3046" style="width: 694px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3046" loading="lazy" class="size-full wp-image-3046" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.17.08.png" alt="Figure 4a: Vehicle exhaust benchmark - comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance." width="684" height="413"&gt;
 &lt;p id="caption-attachment-3046" class="wp-caption-text"&gt;Figure 4a: Vehicle exhaust benchmark – comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3047" style="width: 689px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3047" loading="lazy" class="size-full wp-image-3047" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.17.34.png" alt="Figure 4a: Vehicle exhaust benchmark - comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance." width="679" height="463"&gt;
 &lt;p id="caption-attachment-3047" class="wp-caption-text"&gt;Figure 4a: Vehicle exhaust benchmark – comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3048" style="width: 718px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3048" loading="lazy" class="size-full wp-image-3048" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.17.57.png" alt="Figure 4c: F1 Race Car benchmark - comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. " width="708" height="454"&gt;
 &lt;p id="caption-attachment-3048" class="wp-caption-text"&gt;Figure 4c: F1 Race Car benchmark – comparison of Ansys Fluent solver rating with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;From Figures 4a – 4c, it’s clear that as we undersubscribe an instance (thus increasing the available memory bandwidth per core), the simulation performance improves. For the vehicle exhaust benchmark, the performance improvement with 50% and 25% cores enabled compared to 100% cores for both Hpc6a and Hpc7a instances was between 1.3x and 1.8x.&lt;/p&gt; 
&lt;p&gt;As the size of the simulation model increased, the performance improvement due to the under-subscribing became even more pronounced. For the Combustor model, the performance improvement with 50% and 25% cores enabled compared to 100% cores on both Hpc6a and Hpc7a was between 1.7x and 2.6x.&lt;/p&gt; 
&lt;p&gt;Finally, for the F1 Race Car model, the performance improvement with 50% and 25% cores enabled compared to 100% cores for both Hpc6a and Hpc7a instances was between 1.8x and 2.9x.&lt;/p&gt; 
&lt;h2&gt;Simulation costs&lt;/h2&gt; 
&lt;p&gt;Now let’s look at the cost impacts (based on cloud infrastructure and licensing) for running these benchmarks with our various instance configurations.&lt;/p&gt; 
&lt;p&gt;Figures 5a – 5b show variations of &lt;em&gt;performance to cloud-infrastructure-cost&lt;/em&gt; with across a range of &lt;em&gt;cores-enabled per-instance&lt;/em&gt; for Hpc6a and Hpc7a, for the smallest and the largest benchmark case.&lt;/p&gt; 
&lt;p&gt;The cloud infrastructure costs plotted here represent the total hardware cost based on Amazon EC2, and the Ansys Gateway hardware flat charge of $0.25 per-instance per-hour. This is a good metric to follow when you want to evaluate the instances for best performance while considering the cloud infrastructure costs only.&lt;/p&gt; 
&lt;div id="attachment_3049" style="width: 711px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3049" loading="lazy" class="size-full wp-image-3049" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.18.51.png" alt="Figure 5a: Vehicle exhaust benchmark – performance to cloud infrastructure cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Higher is better. Fully subscribed Hpc6a and Hpc7a instances provide the best performance to cost ratio. " width="701" height="416"&gt;
 &lt;p id="caption-attachment-3049" class="wp-caption-text"&gt;Figure 5a: Vehicle exhaust benchmark – performance to cloud infrastructure cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Higher is better. Fully subscribed Hpc6a and Hpc7a instances provide the best performance to cost ratio.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3051" style="width: 706px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3051" loading="lazy" class="size-full wp-image-3051" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.19.56.png" alt="Figure 5b: F1 race car benchmark – performance to cloud infrastructure cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Higher is better. Fully subscribed Hpc6a and Hpc7a instances provide the best performance to cost ratio. " width="696" height="433"&gt;
 &lt;p id="caption-attachment-3051" class="wp-caption-text"&gt;Figure 5b: F1 race car benchmark – performance to cloud infrastructure cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Higher is better. Fully subscribed Hpc6a and Hpc7a instances provide the best performance to cost ratio.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;From Figures 5a – 5b, you can tell that simulations run on the fully-subscribed Hpc6a and Hpc7a instances have the best performance &lt;em&gt;per hardware-configuration&lt;/em&gt; cost. At higher core counts, the cloud infrastructure costs start to increase, resulting in a drop of the &lt;em&gt;performance-to-cost&lt;/em&gt; ratio.&lt;/p&gt; 
&lt;p&gt;In Figure 6, we plotted the &lt;em&gt;performance to total-job-cost&lt;/em&gt; ratio, where the total job cost is the sum of cloud infrastructure &lt;em&gt;and&lt;/em&gt; the Ansys licensing cost. As the Ansys licensing costs can vary for each customer depending on their licensing agreement, for the purpose of this post we used the Ansys Elastic Currency (AEC) to represent the licensing cost, to keep it simple.&lt;/p&gt; 
&lt;div id="attachment_3052" style="width: 752px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3052" loading="lazy" class="size-full wp-image-3052" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.20.40.png" alt="Figure 6: F1 race car benchmark – performance to total cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Total cost includes Ansys licensing. Higher is better. Hpc6a with 25% subscribed and hpc7a.24xlarge instances provide the best performance to total cost ratio. " width="742" height="459"&gt;
 &lt;p id="caption-attachment-3052" class="wp-caption-text"&gt;Figure 6: F1 race car benchmark – performance to total cost ratio with Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Total cost includes Ansys licensing. Higher is better. Hpc6a with 25% subscribed and hpc7a.24xlarge instances provide the best performance to total cost ratio.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;From Figure 6, you can see that simulations run on Hpc6a instances with 25% cores enabled, and hpc7a.24xlarge show the best &lt;em&gt;performance to total-cost&lt;/em&gt; ratio. Running the simulations with 50% cores enabled on Hpc6a or on Amazon EC2 hpc7a.48xlarge is a good idea for customers with pre-existing Ansys licensing who are looking to balance performance and cloud infrastructure costs. All instance types provide better value at higher core counts because per-core license costs decrease when you use more cores.&lt;/p&gt; 
&lt;p&gt;In our plots, we focused on performance and cost comparisons, but customers are also interested in making the right Amazon EC2 choices based on &lt;strong&gt;simulation runtime and costs&lt;/strong&gt;. In Figures 7 and 8, we plotted the variation of cloud infrastructure and total costs with simulation runtime. By referring to these plots, you can choose the right instance for your preferred simulation runtime.&lt;/p&gt; 
&lt;div id="attachment_3053" style="width: 685px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3053" loading="lazy" class="size-full wp-image-3053" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.21.18.png" alt="Figure 7: F1 race car benchmark – variation of cloud infrastructure cost with simulation runtime on Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Lower cost and runtime are better. Fully subscribed Hpc6a and Hpc7a instances offer the lowest costs for a given simulation runtime. " width="675" height="441"&gt;
 &lt;p id="caption-attachment-3053" class="wp-caption-text"&gt;Figure 7: F1 race car benchmark – variation of cloud infrastructure cost with simulation runtime on Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Lower cost and runtime are better. Fully subscribed Hpc6a and Hpc7a instances offer the lowest costs for a given simulation runtime.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3054" style="width: 678px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3054" loading="lazy" class="size-full wp-image-3054" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.21.43.png" alt="Figure 8: F1 race car benchmark – variation of total cost i.e., cloud infrastructure + licensing cost with simulation runtime on Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Lower cost and runtime are better. Hpc6a with 25% subscribed and hpc7a.24xlarge instances offer the best total cost for a given simulation runtime. " width="668" height="435"&gt;
 &lt;p id="caption-attachment-3054" class="wp-caption-text"&gt;Figure 8: F1 race car benchmark – variation of total cost i.e., cloud infrastructure + licensing cost with simulation runtime on Amazon EC2 Hpc6a instance with 100%, 50%, and 25% cores enabled, and different sizes of Amazon EC2 Hpc7a instance. Lower cost and runtime are better. Hpc6a with 25% subscribed and hpc7a.24xlarge instances offer the best total cost for a given simulation runtime.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;From Figure 7, you can see that when prioritizing cloud infrastructure cost for a desired simulation runtime, the fully-subscribed instances offer the lowest cost because of utilizing maximum available cores on the instances.&lt;/p&gt; 
&lt;p&gt;When we add the software licensing costs – which can be high for larger simulations – the benefit of running the simulations on under-subscribed instances for performance gain helps to drive the total cost down. It’s clear from Figure 8 that under-subscribed instances offer the best total cost for a given simulation runtime.&lt;/p&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;Today we described the price-performance characteristics of Ansys Fluent CFD simulations on Ansys Gateway powered by AWS. We showed you some of our best practices for running simulations on different &lt;a href="https://aws.amazon.com/ec2/"&gt;Amazon EC2&lt;/a&gt; compute-optimized instances. And we described how total job costs comprise of infrastructure costs and Ansys licenses for the Ansys Fluent simulations. With these results you should be able to select the right instance types for running your simulations, whether your goal is to maximize performance or minimize costs.&lt;/p&gt; 
&lt;p&gt;You can get started with Ansys Gateway powered by AWS by subscribing through the &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-ppiyshk2oin3i"&gt;AWS Marketplace&lt;/a&gt;. Follow the &lt;a href="https://www.youtube.com/playlist?list=PL0lZXwHtV6Omw8rjdXmr4UiDj2WZksygA"&gt;Ansys Gateway YouTube channel&lt;/a&gt; for in-depth step-by-step tutorials and video guidelines to get started with setup and running Ansys applications. Finally, you can refer to the &lt;a href="https://forum.ansys.com/forums/forum/installation-and-licensing/ansys-gateway/"&gt;Ansys innovation Space learning forums&lt;/a&gt; for Ansys Gateway specific help.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.28.29.png" alt="Ashwini Kumar " width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Ashwini Kumar&lt;/h3&gt; 
  &lt;p&gt;Ashwini Kumar is a senior principal application engineer at Ansys, specializing in Ansys Cloud solutions, where he plays a pivotal role in assisting Ansys clients with optimizing their cloud investments. With over three decades of expertise in customer relationship management, cloud computing, technical support, and services related to Fluid Flow and Heat Transfer, he brings a wealth of knowledge to his role. His educational background includes a Ph.D. from the University of Minnesota in Minneapolis-St. Paul, Minnesota, a master’s degree from the University of Manitoba in Winnipeg, Canada, and a B.Tech from G.B. Pant University of Agriculture and Technology in Pantnagar, India.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-10.28.34.png" alt="Nicole Diana " width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Nicole Diana&lt;/h3&gt; 
  &lt;p&gt;Nicole Diana is Director of research and development Verification for the computational fluids dynamics (CFD) product line at Ansys. She has over twenty-five years of experience in the development, verification, and application of Ansys CFD products.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Lattice Boltzmann simulation with Palabos on AWS using Graviton-based Amazon EC2 Hpc7g instances</title>
		<link>https://aws.amazon.com/blogs/hpc/lattice-boltzmann-simulation-with-palabos-on-aws-using-graviton-based-amazon-ec2-hpc7g-instances/</link>
		
		<dc:creator><![CDATA[Jun Tang]]></dc:creator>
		<pubDate>Wed, 08 Nov 2023 13:47:14 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<guid isPermaLink="false">ecb740a48f70c44fb346183b25ed64dbe70baea3</guid>

					<description>In this post we’ll show you the performance when running the Parallel Lattice Boltzmann Solver (Palabos) on the latest generation of AWS Graviton CPUs in Hpc7g instances on AWS.</description>
										<content:encoded>&lt;p&gt;Computational fluid dynamics (CFD) has grown over several decades to become a widely used tool to study a broad range of important industrial and academic problems, ranging from automotive and aircraft design to the study of blood flow inside the human body.&lt;/p&gt; 
&lt;p&gt;Whilst traditional Navier-Stokes (NS) based codes using the finite-volume approach are still the most widely used, an alternative approach – the Lattice Boltzmann method (LBM) – has emerged in the last two decades. LBM methods are particularly attractive to both industry and academia.&lt;/p&gt; 
&lt;p&gt;In this post we’ll show you the performance and cost benefits of running the Parallel Lattice Boltzmann Solver (&lt;em&gt;Palabos&lt;/em&gt;) [1] – a specific LBM solver – on the latest generation of Amazon Elastic Compute Cloud instances based on the AWS Graviton.&lt;/p&gt; 
&lt;p&gt;Palabos is an open-source, C++ solver developed by the University of Geneva. It’s designed to run and scale efficiently on HPC clusters. Palabos is a highly versatile computational tool and has been widely used in the LBM community and is often a reference implementation for many Lattice Boltzmann models.&lt;/p&gt; 
&lt;h2&gt;Lattice Boltzmann models (LBMs)&lt;/h2&gt; 
&lt;p&gt;We mentioned LBMs are popular with both industry and academia. There are several reasons for this.&lt;/p&gt; 
&lt;p&gt;First, they’re flexible. You can model complex geometries whether they’re stationary or moving. Second, they suffer from minimal numerical dissipation, which makes them ideal for high-fidelity scale-resolving methods and aeroacoustics.&lt;/p&gt; 
&lt;p&gt;They also have superior computational performance, which stems from two key characteristics: data locality, and the fact that they’re easier to vectorize and set up for multithreading (on both CPUs and GPUs) compared to NS-based methods.&lt;/p&gt; 
&lt;p&gt;To illustrate these points, we’re going to show you how Palabos simulates blood flows [2][3] that can help researchers better understand vascular diseases and cancer cell movement through blood vessels. This can also provide a controlled environment to evaluate different treatment options.&lt;/p&gt; 
&lt;p&gt;There’s another reason LBMs are interesting. Given they’re highly scalable, researchers can run them on thousands or tens of thousands of cores to either get answers very quickly, or drive up the fidelity of the simulations themselves – or often both. But the availability of HPC resources can potentially become a bottleneck for many. As you can imagine, that causes a lot of researchers to look for large scale resources beyond their usual environments: enter AWS which can offer all the additional capacity they’re asking for. It’s also robust, secure by design, low cost and incredibly flexible.&lt;/p&gt; 
&lt;h2&gt;Introducing AWS Graviton3E&lt;/h2&gt; 
&lt;p&gt;Amazon EC2 offers hundreds of instance types optimized to fit different use cases. Instances vary based on CPU, memory, storage, and networking bandwidth giving customers the flexibility to choose the right mix of resources for their applications.&lt;/p&gt; 
&lt;p&gt;Amazon EC2 Hpc7g instances are the &lt;a href="https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/"&gt;latest addition&lt;/a&gt; to the extended family of Graviton-based EC2 instances. They carry DDR5 memory with an AWS Graviton3E processor, and 200Gbps low-latency networking delivered by Elastic Fabric Adapter – especially important when you’re scaling to a large number of instances.&lt;a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc7g-instances-powered-by-aws-graviton3e-processors-optimized-for-high-performance-computing-workloads/"&gt; They also consume up to 60% less energy&lt;/a&gt; for the same workloads than other comparable x86-based instances tailored for HPC applications – this is great for the planet.&lt;/p&gt; 
&lt;p&gt;In &lt;a href="https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/"&gt;an earlier&amp;nbsp;post&lt;/a&gt;, we published some performance results for real world workloads from CFD, finite-element analysis (FEA), molecular dynamics, and numerical weather prediction (NWP).&lt;/p&gt; 
&lt;p&gt;For this post, we compared Palabos performance across two HPC instance types offered by AWS and&amp;nbsp;we found that Hpc7g has competitive performance. In our tests, Hpc7g delivered up to&amp;nbsp;75% better performance and up to 3x better &lt;em&gt;price-performance&lt;/em&gt; compared to the previous generation Graviton instances (C6gn).&lt;/p&gt; 
&lt;h2&gt;Benchmark simulation result and performance&lt;/h2&gt; 
&lt;p&gt;For our tests, we used two instance types: Hpc7g.16xlarge (the latest processor in Graviton family customized for HPC applications) and C6gn.16xlarge (the previous generation processor in Graviton family).&amp;nbsp;We used AWS ParallelCluster to launch our environment, manage these fleets, and provide an Amazon FSx for Lustre (a popular parallel file system). ParallelCluster uses Slurm for its workload manager, making it familiar and easier to use. There’s an example ParallelCluster configuration file in &lt;a href="https://github.com/aws/aws-graviton-getting-started/blob/main/HPC/scripts-setup/hpc7g-ubuntu2004-useast1.yaml"&gt;the Graviton HPC best practices guide&lt;/a&gt;. You can also find a one-click launchable recipe in the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;HPC Recipes Library&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Cavity 3D benchmark&lt;/h3&gt; 
&lt;p&gt;In our first test, we used the “&lt;a href="https://gitlab.unige.ch/hpc/benchmark-aimp-2022/-/blob/main/Palabos/README.md"&gt;lid-driven cavity problem in a cuboid&lt;/a&gt;”, a three-dimension benchmark with 1 billion cells (1001x1001x1001). The top wall moves to the right with a constant velocity, while the other walls are stationary. Figure 1 is a snapshot of the simulated velocity field at 10.1 sec. We used Palabos v2.3.0, compiled with GNU Compiler v11.1.0 and Open MPI v4.1.4.&lt;/p&gt; 
&lt;div id="attachment_3081" style="width: 767px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3081" loading="lazy" class="size-full wp-image-3081" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.24.40.png" alt="Figure 1. A display of Palabos output velocity field at 10.1 sec for the three-dimension lid driven cavity problem. Arrow shows flow direction and speed." width="757" height="495"&gt;
 &lt;p id="caption-attachment-3081" class="wp-caption-text"&gt;Figure 1. A display of Palabos output velocity field at 10.1 sec for the three-dimension lid driven cavity problem. Arrow shows flow direction and speed.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We ran the same benchmark on both Hpc7g and C6gn scaling from 2 to 128 instances (8192cores) in both cases. We calculated throughput in &lt;em&gt;million site updates per second&lt;/em&gt; (MLUPS)&amp;nbsp;which we’ve shown in Table 1.&lt;/p&gt; 
&lt;p&gt;At 8192 cores, the solver maintains a strong scaling efficiency of 60% on Hpc7g (Figure 2), which also had 75% higher throughput than the previous generation Graviton 2 (C6gn) instance. The simulation cost on Hpc7g is close to 1/3 of the cost of doing it on C6gn (Figure 3).&lt;/p&gt; 
&lt;div id="attachment_3082" style="width: 564px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3082" loading="lazy" class="size-full wp-image-3082" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.25.27.png" alt="Table 1. Cavity 3D benchmark performance (higher is better)" width="554" height="306"&gt;
 &lt;p id="caption-attachment-3082" class="wp-caption-text"&gt;Table 1. Cavity 3D benchmark performance (higher is better)&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3083" style="width: 758px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3083" loading="lazy" class="size-full wp-image-3083" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.26.07.png" alt="Figure 2. Cavity 3D benchmark throughput (higher is better) on our two instance types; Hpc7g shows good efficiency at 8192 cores for this strong scaling test." width="748" height="561"&gt;
 &lt;p id="caption-attachment-3083" class="wp-caption-text"&gt;Figure 2. Cavity 3D benchmark throughput (higher is better) on our two instance types; Hpc7g shows good efficiency at 8192 cores for this strong scaling test.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3084" style="width: 760px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3084" loading="lazy" class="size-full wp-image-3084" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.26.33.png" alt="Figure 3. Cavity 3D benchmark cost for 10000 iterations on our two instance types, lower is better." width="750" height="564"&gt;
 &lt;p id="caption-attachment-3084" class="wp-caption-text"&gt;Figure 3. Cavity 3D benchmark cost for 10000 iterations on our two instance types, lower is better.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Cellular blood flow simulation benchmark&lt;/h3&gt; 
&lt;p&gt;Next, we studied the &lt;em&gt;cellular blood computation&lt;/em&gt;, which has three components: the &lt;em&gt;fluid solver&lt;/em&gt;, the &lt;em&gt;solid solver,&lt;/em&gt; and the &lt;em&gt;fluid-solid interaction&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;The fluid solver solves the weakly-compressible Navier-Stokes equations. The solid solver, based on nodal projective finite elements method (npFEM) resolves the trajectories and deformations of the blood cells. And the fluid-solid interaction is loosely coupled through an immersed boundary condition.&lt;/p&gt; 
&lt;p&gt;The result of this hybrid simulation is shown in Figure 4.&amp;nbsp;For &lt;a href="https://gitlab.com/unigespc/palabos/-/blob/master/examples/showCases/bloodFlowDefoBodies/README.md?ref_type=heads"&gt;this second benchmark&lt;/a&gt;, we simulated the blood flow in a 50x50x50um cube with 476 red blood cells (RBCs) and 95 platelets (PLTs).&lt;/p&gt; 
&lt;div id="attachment_3085" style="width: 767px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3085" loading="lazy" class="size-full wp-image-3085" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.27.17.png" alt="Figure 4. Hybrid simulation of blood plasma (Palabos) and deformable RBCs/PLTs (npFEM)." width="757" height="533"&gt;
 &lt;p id="caption-attachment-3085" class="wp-caption-text"&gt;Figure 4. Hybrid simulation of blood plasma (Palabos) and deformable RBCs/PLTs (npFEM).&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Table 2 and Figure 5 show the performance in the number of iterations we can simulate &lt;em&gt;per minute&lt;/em&gt; (so higher is better).&lt;/p&gt; 
&lt;p&gt;Hpc7g delivered up to 50% better performance and 2.45x better price-performance over C6gn for these tests.&lt;/p&gt; 
&lt;div id="attachment_3086" style="width: 678px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3086" loading="lazy" class="size-full wp-image-3086" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.27.55.png" alt="Table 2. Cellular bloodFlow benchmark throughput (higher is better)" width="668" height="211"&gt;
 &lt;p id="caption-attachment-3086" class="wp-caption-text"&gt;Table 2. Cellular bloodFlow benchmark throughput (higher is better)&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3087" style="width: 717px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3087" loading="lazy" class="size-full wp-image-3087" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/07/CleanShot-2023-11-07-at-16.30.23.png" alt="Figure 5. Cellular blood flow benchmark throughput (higher is better) on our two instance types." width="707" height="552"&gt;
 &lt;p id="caption-attachment-3087" class="wp-caption-text"&gt;Figure 5. Cellular blood flow benchmark throughput (higher is better) on our two instance types.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusions&lt;/h2&gt; 
&lt;p&gt;In this post we showed you that a popular LBM code can be easily run on Graviton-based Amazon EC2 instances with up to 8192 cores. The Amazon EC2 Hpc7g instances showed up to 70% better performance and up to 3x better price-performance over the previous generation Graviton instances for Palabos.&lt;/p&gt; 
&lt;p&gt;The 3D Cavity benchmark scaled efficiently on Hpc7g instances up to 8192&amp;nbsp;cores and we think this illustrates how the combination of a computationally efficiency code, and large-scale cloud HPC facility can enable you to run larger cases than you could likely complete using limited on-prem HPC resources.&lt;/p&gt; 
&lt;p&gt;You can find the instructions to set up and run HPC applications on Graviton in our &lt;a href="https://github.com/aws/aws-graviton-getting-started/tree/main/HPC"&gt;best practice guide&lt;/a&gt;. And you can find easy to consume recipes for building clusters to suite your taste in the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;HPC Recipe Library&lt;/a&gt;. These are open source projects on GitHub so you can &lt;a href="https://github.com/aws/aws-graviton-getting-started/issues"&gt;let us know&lt;/a&gt; directly if you run into any technical issues. Let us know how you get on.&lt;/p&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;p&gt;[1] Latt, J., Malaspinas, O., Kontaxakis, D., Parmigiani, A., Lagrava, D., Brogi, F., Belgacem, M. B., Thorimbert, Y., Leclaire, S., Li, S., Marson, F., Lemus, J., Kotsalos, C., Conradin, R., Coreixas, C., Petkantchin, R., Raynaud, F., Beny, J., &amp;amp; Chopard, B. (2021). &lt;a href="https://www.sciencedirect.com/science/article/pii/S0898122120301267?via%3Dihub"&gt;Palabos: Parallel lattice boltzmann solver&lt;/a&gt;. Computers and Mathematics with Applications, 81, 334–350. https://doi.org/10.1016/j.camwa.2020.03.022&lt;/p&gt; 
&lt;p&gt;[2] Kotsalos, C., Latt, J., Beny, J., &amp;amp; Chopard, B. (2020). &lt;a href="https://arxiv.org/abs/1911.03062"&gt;Digital Blood in massively parallel CPU/GPU systems for the study of platelet transport.&lt;/a&gt; Interface Focus, 11(1), 20190116. https://doi.org/10.1098/rsfs.2019.0116&lt;/p&gt; 
&lt;p&gt;[3] Zavodszky, G., van Rooij, B., Azizi, V., Alowayyed, S., &amp;amp; Hoekstra, A. (2017). &lt;a href="https://www.sciencedirect.com/science/article/pii/S1877050917306245"&gt;Hemocell: A high-performance microscopic cellular library&lt;/a&gt;. &lt;em&gt;Procedia Computer Science&lt;/em&gt;, &lt;em&gt;108&lt;/em&gt;, 159–165. https://doi.org/10.1016/j.procs.2017.05.084&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Using Fleet Training to Improve Level 3 Digital Twin Virtual Sensors with Ansys on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/using-fleet-training-to-improve-l3-digital-twin-virtual-sensors-with-ansys-on-aws/</link>
		
		<dc:creator><![CDATA[Ross Pivovar]]></dc:creator>
		<pubDate>Tue, 07 Nov 2023 16:05:15 +0000</pubDate>
				<category><![CDATA[Amazon Neptune]]></category>
		<category><![CDATA[Amazon Timestream]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS IoT TwinMaker]]></category>
		<category><![CDATA[AWS Lambda]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Internet of Things]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">b2742a64559bd3c77020ec79c0838f5932a71ca8</guid>

					<description>AWS is developing new tools that enable easier and faster deployment of level 3/4 digital twins. This post discusses how a fleet calibrated level 3 digital twin can be cost effectively deployed on AWS Cloud.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post contrbuted by Ross Pivovar, Solution Architect, Autonomous Computing, and&amp;nbsp;&lt;/em&gt;&lt;em&gt;Adam Rasheed, Head of Autonomous Computing at AWS, and &lt;/em&gt;&lt;em&gt;Matt Adams, Lead Product Specialist at Ansys.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/blogs/hpc/deploying-a-level-3-digital-twin-virtual-sensor-with-ansys-on-aws/"&gt;In a previous post&lt;/a&gt;, we shared how to build and deploy a &lt;a href="https://aws.amazon.com/blogs/iot/l3-predictive-digital-twins/"&gt;Level 3 Digital Twin&lt;/a&gt; virtual sensor using an Ansys model on AWS. Virtual sensors are desirable when it is difficult to physically measure a quantity, expensive to measure, or difficult to maintain the sensor. In this post, we’ll discuss challenges with Level 3 (L3) virtual sensors and describe an improved approach using a fleet-trained L3 virtual sensor.&lt;/p&gt; 
&lt;p&gt;We used &lt;a href="https://www.ansys.com/products/digital-twin/ansys-twin-builder"&gt;Ansys Twin Builder and Twin Deployer&lt;/a&gt; to build the physics model and then &lt;a href="https://aws.amazon.com/iot-twinmaker/"&gt;AWS IoT TwinMaker&lt;/a&gt; and the &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow framework&lt;/a&gt; to deploy on AWS. To provide context, we use our &lt;a href="https://aws.amazon.com/blogs/iot/digital-twins-on-aws-unlocking-business-value-and-outcomes/"&gt;four-level Digital Twin leveling index&lt;/a&gt; to help customers understand their use cases and the technologies required to achieve their desired business value.&lt;/p&gt; 
&lt;h2&gt;Looking at challenges with L3 virtual sensor&lt;/h2&gt; 
&lt;p&gt;L3 virtual sensors are typically based on either machine-learning (ML) models or physics-based models. ML models require large training sets to cover the full domain of possible outcomes and extrapolation can be a safety concern. Comparatively, physics-based models only require small validation sets to confirm accuracy. However, these models do not reflect environmental changes like equipment degradation or deviation from the initial assumptions used to develop the physics models.&lt;/p&gt; 
&lt;p&gt;For example, Figure 1 compares the predicted value for pressure from the L3 Digital Twin virtual sensor (orange line, labeled Level 3 DT) from the prior post to the measured pressure sensor value (blue line, labeled Measured Value). The blue shaded regions around each line represent the uncertainty in the predicted and measured values. The virtual sensor is relying on the thermodynamic model of the ideal (new) compressor and initially (&amp;lt; 200 days) correctly predicts the discharge pressure of the fluid post-compression.&lt;/p&gt; 
&lt;div id="attachment_3007" style="width: 790px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3007" loading="lazy" class="size-full wp-image-3007" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure1.jpg" alt="Figure 1 Demonstration of L3 performance degradation due to lack of continual calibration" width="780" height="189"&gt;
 &lt;p id="caption-attachment-3007" class="wp-caption-text"&gt;Figure 1: Graph comparing L3 DT virtual sensor prediction (without periodic calibration) versus sensor data.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As the natural gas compressor train operates in a harsh environment, the overall compressor efficiency degrades over time. The virtual sensor does not know that the compressor efficiency has decreased, and falsely predicts a higher pressure. The L3 digital twin virtual sensor (orange line) has no way to update its model parameters to improve the predictions to account for the decrease in efficiency. As a result, as shown in the graph, after 400 days, the L3 digital twin virtual sensor prediction for pressure deviated well beyond the measured sensor values (even beyond the shaded region showing the uncertainty). An operator relying on the L3 digital twin virtual sensor would mistakenly think there is a safety issue and shutdown the compressor&lt;/p&gt; 
&lt;p&gt;To alleviate this issue, operators could perform maintenance at regularly scheduled intervals, for example, every 100 days. Regular maintenance keeps the compressors running approximately at the same efficiency as new compressor trains. However, frequent maintenance is costly requiring shut-down of the facility and needs to be compared to the cost of running a compressor train in a less efficient state. Additionally, the compressor degradation depends on the environmental and operating conditions, so in some cases, doing maintenance at 100 days is too frequent, resulting in unnecessary maintenance costs. In other cases, like after a major sandstorm, maintenance is required more frequently to maintain production efficiency.&lt;/p&gt; 
&lt;h2&gt;Using a fleet-trained L3 virtual sensor&lt;/h2&gt; 
&lt;p&gt;An alternative is to train the model using data that includes compressor degradation, like a drop in compressor efficiency. For equipment operators running fleets of compressors, such data can be obtained from a “fleet leader” compressor that is fully instrumented. This is a common situation where the operator identifies the “fleet leader” compressor which is the oldest compressor which they intentionally fully instrument and for which they collect the most data.&lt;/p&gt; 
&lt;p&gt;In practice, simply identifying the oldest compressor is insufficient, as age is often not an ideal predictor of equipment degradation. For example, a compressor train operating in a cold, dry arctic environment degrades very differently than a compressor train operating in a hot sandy desert despite being the same age. Therefore, we need an approach that combines a physics-based understanding of the efficiency degradation with an ML model that can adjust the predictions based on the environmental and operational history of the compressor&lt;/p&gt; 
&lt;p&gt;The exact features to use for this type of adaptation depends on the physical process. In our approach, we recognize from the thermodynamics that the compressor exit pressure (P2) is a function of the temperature ratio across the compressor (T2/T1). The temperature ratio increases as the compressor degrades (for a specific pressure ratio) from physics. By incorporating this crucial piece of physics as a feature into our predictor-corrector approach (described in detail in the prior post), we’re able to improve the prediction to incorporate the compressor efficiency degradation.&lt;/p&gt; 
&lt;p&gt;We also must perform an additional step to ensure that our ML corrector model is not extrapolating beyond its training data set. Figure 2 displays the workflow for this approach for a single stage compressor. As before, the predictor model is the physics-based thermodynamic model for an ideal compressor and this time the ML corrector model is trained including the temperature ratio (T2/T1) as a feature.&lt;/p&gt; 
&lt;div id="attachment_3008" style="width: 790px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3008" loading="lazy" class="size-full wp-image-3008" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure2.jpg" alt="Figure 2: Workflow of a self-calibrating virtual sensor based on fleet data." width="780" height="260"&gt;
 &lt;p id="caption-attachment-3008" class="wp-caption-text"&gt;Figure 2: Workflow of a self-calibrating virtual sensor based on fleet data.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For a production model, we first perform a statistical check to calculate the probability that the model is not extrapolating beyond its training data set (Input Data Deviation Detector Model). We use a predictor-corrector model, as described in &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-a-level-3-digital-twin-virtual-sensor-with-ansys-on-aws/"&gt;the prior post&lt;/a&gt;, if the model is not extrapolating. However, this time the ML corrector model includes the critical feature of temperature ratio. If we find the model is extrapolating, we trigger a model re-training by looking to the fleet for more data that covers the range of input data our compressor experiences.&lt;/p&gt; 
&lt;p&gt;As a first pass, we use data from the fully instrumented fleet leader, but with multiple fully instrumented compressors, we employ the same statistical algorithm to identify the fleet compressor with the highest probability of overlapping data. If there is no other available compressor with sufficient overlapping data, we employ a different approach of using an L4 self-calibrating virtual sensor, which we will describe in a future post.&lt;/p&gt; 
&lt;p&gt;Figure 3 shows the P2 prediction from the L3 virtual sensor model trained using the first 200 days of operation of the fully instrumented fleet leader compressor (green line). Note that this model is plotted against temperature ratio as we know it to be a better proxy for compressor performance compared to using age. For validation purposes, the orange circles show the measured P2 from our compressor during its first 200 days of operation. Normally, the orange circle data would not be available, as we would be relying on the green curve as the P2 virtual sensor prediction. As expected, we see good agreement between the measured values (orange circles) and the L3 virtual sensor.&lt;/p&gt; 
&lt;div id="attachment_3009" style="width: 634px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3009" loading="lazy" class="wp-image-3009 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure3.jpg" alt="Figure 3: Graph comparing L3 DT Fleet-Trained virtual sensor prediction versus sensor data for first 200 days of operation." width="624" height="164"&gt;
 &lt;p id="caption-attachment-3009" class="wp-caption-text"&gt;Figure 3: Graph comparing L3 DT Fleet-Trained virtual sensor prediction versus sensor data for first 200 days of operation.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;However, as the compressor degrades, a higher temperature ratio doesn’t necessarily indicate higher pressure. Figure 4 shows the complete data set (0 to 1500 days of operation) from Figure 3 as our compressor efficiency degrades.&lt;/p&gt; 
&lt;div id="attachment_3010" style="width: 790px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3010" loading="lazy" class="wp-image-3010 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure4.jpg" alt="Figure 4: Graph comparing L3 DT Fleet-Trained virtual sensor predictions versus sensor data for first 200 days and 500 days of operation." width="780" height="202"&gt;
 &lt;p id="caption-attachment-3010" class="wp-caption-text"&gt;Figure 4: Graph comparing L3 DT Fleet-Trained virtual sensor predictions versus sensor data for first 200 days and 500 days of operation.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The new compressor data (&amp;lt;200 days of operation, red circles) is separated from the rest of the data (0 days to 1500 days of operation, black circles). In this case, we trained the ML corrector model using data from the existing fleet leader compressor that has been operating for several years (up to a temperature ratio of ~1.26). We can visually see how the improved predictor-corrector model is able to predict the compressor exit pressure up to a compressor temperature ratio of ~1.26. Beyond that, we see the prediction again deviates from the measured values indicating the ML corrector model is now extrapolating and should be retrained with a different data set. By this time, the fleet leader compressor will have likely aged and degraded further, so its dataset would presumably be suitable for training for our virtual sensor.&lt;/p&gt; 
&lt;h2&gt;Identifying when the ML corrector model is extrapolating&lt;/h2&gt; 
&lt;p&gt;The workflow in Figure 2 describes a crucial step of determining when the fleet-trained L3 Digital Twin model is extrapolating and needs to be retrained. For this step, we developed a function that combines &lt;a href="https://en.wikipedia.org/wiki/Mixture_model"&gt;Gaussian Mixture Models&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Monte_Carlo_integration"&gt;Monte Carlo Integration&lt;/a&gt; to determine the probability that new sensor data collected by our sensors is supported by the training data set. With applying this method, we generate a training validity plot shown in Figure 5a, which shows the probability the sensor data is within the range of the training data (the first 200 days of the fleet-leader compressor). As expected, for less than 200 days, the measured sensor data exhibits a high probability of falling within the valid calibration range but then decreases over time. At approximately 200 days, the probability drops to near 0, indicating that the incoming sensor data deviated from the training set. We see this visually in Figure 5b where there is a strong overlap between the training data and the incoming sensor data for less than 100 days of operation. Conversely, Figure 5c shows poor overlap between the training data and the incoming sensor data for up to 500 days of operation.&lt;/p&gt; 
&lt;div id="attachment_3011" style="width: 1429px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3011" loading="lazy" class="wp-image-3011 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure5.jpg" alt="Figure 5: a) Probability new sensor data is supported by the training data using the first 200 days of data. b) Incoming sensor data overlaps the calibration data at 100 days. c) incoming sensor data does not overlap the calibration data at 500 days." width="1419" height="406"&gt;
 &lt;p id="caption-attachment-3011" class="wp-caption-text"&gt;Figure 5: a) Probability new sensor data is supported by the training data using the first 200 days of data. b) Incoming sensor data overlaps the calibration data at 100 days. c) incoming sensor data does not overlap the calibration data at 500 days.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;This behavior corresponds to Figure 1, where the predicted P2 values begin to deviate from the measured P2 values. Specifically, the uncertainty bands around the predicted and measured P2 no longer overlap. The Input Data Deviation model is not attempting to predict P2 or any other variable, rather it identifies when the incoming sensor data is different from the original training data.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deploying fleet trained L3 virtual sensor on AWS&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;To build and deploy our L3 digital twins, we use our &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;AWS TwinFlow framework&lt;/a&gt;. TwinFlow deploys and orchestrates predictive models at scale across a distributed computing architecture. It also probabilistically updates model parameters using real-world data and calculates prediction uncertainties while maintaining a fully auditable history.&lt;/p&gt; 
&lt;p&gt;In our prior post, we used TwinFlow to deploy an L3 virtual sensor when new data is added to an &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service (Amazon S3)&lt;/a&gt; bucket specifying the existence of a new virtual sensor. The workflow for the prior L3 virtual sensor is shown in the “Auto-Deployment Fleet Size” and Virtual Sensors for each component” boxes in Figure 6.&lt;/p&gt; 
&lt;div id="attachment_3012" style="width: 985px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3012" loading="lazy" class="wp-image-3012 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure6.jpg" alt="Figure 6: TwinFlow workflow enabling digital twins that auto-deploy, self-calibrate, and perform IoT sensor data deviation checks." width="975" height="879"&gt;
 &lt;p id="caption-attachment-3012" class="wp-caption-text"&gt;Figure 6: TwinFlow workflow enabling digital twins that auto-deploy, self-calibrate, and perform IoT sensor data deviation checks.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The Auto-Deployment box represents the workflow for auto-deployment, and the Virtual Sensors box for the initial model training. One key difference between the prior L3 virtual sensor and the present L3 fleet-trained virtual sensor is the inclusion of temperature ratio in the training feature set.&lt;/p&gt; 
&lt;p&gt;For the present implementation of the L3 fleet-trained virtual sensor, we use TwinFlow to add the additional workflows for model validity check (IoT Deviation Check box) and ML corrector model retraining (Self-Calibration box) layers as depicted in Figure 6. The model validity check runs the input data deviation detection model and displays an alarm in the AWS IoT TwinMaker dashboard when we are extrapolating beyond the training data set of the ML corrector model. If the model is found to be extrapolating, additional data is obtained from the fleet leader compressor, and the ML corrector model is retrained. If the newly trained model is still extrapolating, we adopt a more advanced approach discussed in our next post.&lt;/p&gt; 
&lt;p&gt;Figure 7 shows the AWS architecture supporting this workflow. In step 1, you download and install TwinFlow from &lt;a href="https://github.com/aws-samples/twinflow"&gt;GitHub&lt;/a&gt;. From here (steps 2 and 3), you can customize the virtual sensor TwinFlow pipelines and embed their digital twins in containers that are stored in &lt;a href="https://aws.amazon.com/ecr/"&gt;Amazon Elastic Container Registry (Amazon ECR)&lt;/a&gt;. At step 4, calibration data can be cost effectively archived in an S3 Bucket. Live sensor data is streamed into &lt;a href="https://aws.amazon.com/timestream/"&gt;Amazon Timestream&lt;/a&gt;, which is a serverless database that supports real-time data. At step 5, an &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; compute environment scales to meet your virtual sensors needs and optimally selects &lt;a href="https://aws.amazon.com/pm/ec2/"&gt;Amazon Elastic Compute Cloud (Amazon EC2)&lt;/a&gt; instances based on your requirements. &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; supports both CPU and GPU requirements, and it elastically scales up or down based on load. In addition, all container output logs are automatically streamed to &lt;a href="https://aws.amazon.com/cloudwatch/"&gt;Amazon CloudWatch&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The TwinFlow graph orchestrator resides in an Amazon EC2 instance and offloads tasks to either &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; functions (which are serverless and perform short specific tasks) or directly to EC2 instances in AWS Batch. At step 6, a training task is run on Batch that will check for new calibration data in an Amazon S3 data lake and initiate ML training in AWS Batch if new data exists. A Lambda function (step 7) automatically deploys new virtual sensors or removes unused virtual sensors in AWS Batch after checking the IoT sensor database.&lt;/p&gt; 
&lt;p&gt;AWS IoT TwinMaker (step 8) shows the digital twin predictions, incoming sensor data, and 3D assets. For operational excellence, at step 9 and 10 additional diagnostic logging and archiving is performed. Since we could potentially use a large number of virtual sensors, we recommend using a Lambda function to archive logs from CloudWatch to an S3 bucket. Lastly, &lt;a href="https://aws.amazon.com/neptune/"&gt;Amazon Neptune&lt;/a&gt; is used with TwinFlow to log each task execution on a graph, which also records the relationship between parent and children tasks enabling root cause analysis if an error occurs in the future.&lt;/p&gt; 
&lt;div id="attachment_3013" style="width: 634px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3013" loading="lazy" class="size-full wp-image-3013" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/L3_DigitalTwin_FleetTrainedVirtualSensor_Figure7.jpg" alt="Figure 7: AWS architecture for virtual sensors." width="624" height="364"&gt;
 &lt;p id="caption-attachment-3013" class="wp-caption-text"&gt;Figure 7: AWS architecture for virtual sensors.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we took an L3 digital twin virtual sensor workflow and added a fleet-calibration branch. We used data from the other fleet compressors to avoid having our virtual sensor extrapolate beyond its training data set. This L3 fleet-trained virtual sensor application is practical for operators running equipment fleets, like wind farms, compressor trains, oil wells, and other industrial applications. If you found this post interesting, then you should also read about &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/"&gt;a more advanced approach on deploying Level 4 digital twin self-calibrating virtual sensors&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to request a proof of concept or if you have feedback on the AWS tools, please reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Some of the content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/27/matt-adams-ansys-profile.png" alt="Matt Adams" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Matt Adams&lt;/h3&gt; 
  &lt;p&gt;Matt has 11 years of experience in developing machine learning methods to enhance physics-based simulations. Matt is a Lead Product Specialist at Ansys where he leads the development of software tools for building hybrid digital twins.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Accelerating green-urban planning simulations with AWS Batch</title>
		<link>https://aws.amazon.com/blogs/hpc/accelerating-green-urban-planning-simulations-with-aws-batch/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Wed, 01 Nov 2023 17:44:48 +0000</pubDate>
				<category><![CDATA[Amazon EC2 Container Registry]]></category>
		<category><![CDATA[Amazon EC2 Container Service]]></category>
		<category><![CDATA[Amazon Elastic Container Service]]></category>
		<category><![CDATA[AWS Fargate]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[HPC]]></category>
		<guid isPermaLink="false">1df35da35e864e989a1ccd4ad78a766bee85fee9</guid>

					<description>In this blog post, we’ll explore how Green Urban Scenarios simulator (GUS) helps urban planners explore the impact of green infrastructure on the urban environment using digital twins and simulations scaled using AWS Batch.</description>
										<content:encoded>&lt;p&gt;Despite the increasing risk of environmental threats to cities across the world, and the critical role the natural world plays in protecting cities and their residents from climate extremes, effective green urban planning receives little representation in city-planning and infrastructure decisions.&lt;/p&gt; 
&lt;p&gt;We know that urban forests [1][2] have a &lt;a href="https://www.researchgate.net/profile/David-Nowak-6/publication/312470680_Understanding_the_benefits_and_costs_of_urban_forest_ecosystems/links/5bf543bf299bf1124fe26670/Understanding-the-benefits-and-costs-of-urban-forest-ecosystems.pdf"&gt;wide range&lt;/a&gt; of economic and social benefits, and are one of the most effective ways to reduce climate risk in cities.&lt;/p&gt; 
&lt;p&gt;In this blog post, we’ll explore how the Netherlands-based organization &lt;em&gt;Lucidminds AI&lt;/em&gt; is addressing this challenge through their Green Urban Scenarios simulator (GUS). The team initially built GUS to power the TreesAI project, a collaboration between Lucidminds and Dark Matter Labs. It’s a tool built on AWS so that urban planners, researchers, green portfolio managers, and others can explore the impact of green infrastructure on the urban environment through the power of digital twins and numerical simulations scaled using AWS Batch.&lt;/p&gt; 
&lt;h2&gt;Trees – a natural city defender&lt;/h2&gt; 
&lt;p&gt;Trees are natural carbon sinks. As trees grow, they absorb the greenhouse gas CO2 and sequester carbon in their fibers until an event like combustion or decomposition releases the carbon back into the atmosphere as CO2. Even if a tree is harvested for wood, the carbon remains locked away and out of the atmosphere. Trees provide essential habitat for wildlife like birds and their canopies provide shade and shelter which reduces the amount of energy needed to cool buildings. Their roots help prevent soil erosion, and they improve air quality by removing and breaking down pollutants such as sulfur dioxide.&lt;/p&gt; 
&lt;p&gt;In short, planting trees is one of the most effective ways to combat climate change and promote biodiversity in cities. But urban planners and decision makers need tools to understand what the best strategies are for building or strengthening their urban forests, such as where trees should be placed, what types of trees to plant, and the optimal level of tree maintenance required. Critically, it’s also important to understand the impacts of proposed urban forest plans, in particular the estimated amount of carbon sequestered under different scenarios.&lt;/p&gt; 
&lt;p&gt;Nature-based solutions that use digital technologies, like Green Urban Scenarios (GUS), are making it easier for cities to understand how trees can benefit their communities. With a better understanding of how trees reduce emissions, governments and businesses can make more informed decisions about which trees to plant where, and how-to best care for them.&lt;/p&gt; 
&lt;h2&gt;How the “Green Urban Simulator” works&lt;/h2&gt; 
&lt;p&gt;To accurately simulate hypothetical green urban planning projects for a city, the simulations must reflect the unique characteristics of the city in question. Every city has its own built environment and geography that defines its associated urban forest.&lt;/p&gt; 
&lt;div id="attachment_3024" style="width: 825px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3024" loading="lazy" class="size-full wp-image-3024" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.21.58.png" alt="Figure 1 - Green Urban Scenarios simulator (GUS) is a tool built on AWS so that urban planners, researchers, green portfolio managers, and others can explore the impact of green infrastructure on the urban environment." width="815" height="495"&gt;
 &lt;p id="caption-attachment-3024" class="wp-caption-text"&gt;Figure 1 – Green Urban Scenarios simulator (GUS) is a tool built on AWS so that urban planners, researchers, green portfolio managers, and others can explore the impact of green infrastructure on the urban environment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For simulations to be useful, they must be initialized with data that reflects the real environment. Therefore, the first step in the process of green urban simulation is to capture and digitize the unique aspects of a city’s urban forest, namely the spatial coordinates, species, diameter, height, and current condition of individual trees across a city.&lt;/p&gt; 
&lt;p&gt;Lucidmind’s team uses a combination of available datasets including satellite imagery, street view images, and field surveys, to build digital urban forests. This data is collected into a CSV file where each line represents a single tree and columns that correspond to the tree characteristics we described. The CSV file is then fed into the simulation modules to initialize the computational domain of a green urban simulation.&lt;/p&gt; 
&lt;div id="attachment_3025" style="width: 827px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3025" loading="lazy" class="size-full wp-image-3025" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.22.39.png" alt="Figure 2 - Characteristics of Urban Forests Complex Systems consider specificity, heterogeneity and dynamic interaction between trees and their environment" width="817" height="487"&gt;
 &lt;p id="caption-attachment-3025" class="wp-caption-text"&gt;Figure 2 – Characteristics of Urban Forests Complex Systems consider specificity, heterogeneity and dynamic interaction between trees and their environment&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To illustrate the capabilities of GUS, the Lucidminds team performed simulations of the city of Amsterdam, the Dutch capital, that incorporated over 250,000 individual trees. That is just the trees currently growing in Amsterdam. One of the powerful features of simulations is the ability to examine hypothetical scenarios – such as the city of Amsterdam planting another 100,000 trees &amp;nbsp;– to understand how their urban forest might evolve and sequester carbon over the next 30 years.&lt;/p&gt; 
&lt;p&gt;There’s an endless number of possible arrangements for trees that can be simulated for any city in the world. Of course, as the number of trees in any simulation grows, so can the computational expense required to complete the simulation. To ensure that their simulations could scale along with the simulation ambitions, the Lucidminds looked to the power of AWS’s massive computing capacity and HPC services like AWS Batch.&lt;/p&gt; 
&lt;h2&gt;Running GUS on AWS&lt;/h2&gt; 
&lt;p&gt;The GUS engine is written in Python and is available as an &lt;a href="https://pypi.org/project/pygus/2.0.4/"&gt;open source Python package called ‘pygus’&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Like many simulations, there are phenomena represented in the simulations that behave probabilistically, such as the chance on any individual simulated day that a storm materializes resulting in precipitation that impacts some or all the trees in the city.&lt;/p&gt; 
&lt;p&gt;If you’re not familiar with simulations with probabilistic mechanisms, the important thing to understand is that if probabilities are embedded in a simulation engine, a user can run the exact same simulation code with the exact same simulation inputs (like tree data) and the two simulations will almost certainly produce different results. This is a perfectly acceptable outcome, one that comes with the territory of simulating scenarios where we can’t know for sure every single sub-event of a simulation, but we can assign a probability distribution to the occurrence of those events and draw from that distribution during the simulation.&lt;/p&gt; 
&lt;p&gt;The drawback to incorporating probabilities in simulations, however, is that it becomes necessary to run the simulations many times over to understand all the different possible outcomes. This mathematical technique is called &lt;a href="https://aws.amazon.com/what-is/monte-carlo-simulation/"&gt;Monte Carlo simulation&lt;/a&gt; and is widely used in simulations across the fields of physics, finance, biology, and engineering, to name a few. Depending on the complexity of the simulation and the number of probabilistic variables/events, hundreds or thousands of simulations might be necessary to understand the emerging behavior. And that’s just for one scenario or set of input parameters. If you want to simulate a different scenario, say where you replace a few dozen trees near the city center with trees of a different species, then you might need to run those thousands of simulations again.&lt;/p&gt; 
&lt;p&gt;If a single simulation iteration (a term used in this context to describe just one run of a simulation) takes a fraction of a second, then it’s not a big deal to run a thousand simulations one after the other to get an ensemble. That would only take a few minutes and you’d have a robust set of results to examine. Many simulations, however, take much longer to run. For instance, a single GUS iteration for Amsterdam’s 250k trees takes a little over an hour (~75 minutes) on a typical laptop or equivalent EC2 instance and the number of iterations needed to confidently simulate a single scenario is around a thousand.&lt;/p&gt; 
&lt;p&gt;Running each iteration back-to-back on a single laptop or a similar EC2 instance would take about 2 months, which is obviously not a feasible time frame to make these simulations useful for decision makers, especially considering that there are likely dozens or hundreds of scenarios that should be simulated when developing an urban planning project.&lt;/p&gt; 
&lt;p&gt;The solution to this problem is to embrace the highly parallel nature of Monte Carlo-based simulation. For the 1,000-simulation set described above, each iteration is independent of the other 999, meaning that we can split up the 1,000 simulations, run them separately and even at the same time, and then combine the results at the end. So, in theory we could spin up 1,000 EC2 instances, run our simulation code with the same inputs on each instance, dump the results to a shared storage location (say, an Amazon S3 bucket), and complete all of that in roughly the same 75 minutes that it would take to run a single simulation iteration.&lt;/p&gt; 
&lt;p&gt;But how do we go about orchestrating all those compute instances and getting our simulation code on each one? Enter AWS Batch.&lt;/p&gt; 
&lt;p&gt;AWS Batch is a fully managed batch computing service that plans, schedules, and runs containerized batches or machine learning (ML) workloads across the full range of AWS compute offerings, such as&amp;nbsp;Amazon ECS,&amp;nbsp;Amazon EKS,&amp;nbsp;AWS Fargate, using Spot or On-Demand Amazon EC2 instances.&lt;/p&gt; 
&lt;p&gt;Batch is a great solution for performing Monte Carlo solutions because it handles a lot of the undifferentiated heavy lifting of setting up the compute resources needed to run simulations leaving you more time to focus more on developing the &lt;em&gt;actual simulation&lt;/em&gt; code and analyzing it’s results.&lt;/p&gt; 
&lt;p&gt;To run a GUS of Amsterdam, Lucidminds took simplicity a step further by using AWS Fargate in combination with AWS Batch. AWS Fargate is a serverless, pay-as-you-go compute engine that lets you focus on building applications without managing servers or choosing instance types. With Fargate you simply package your application in containers, specify the CPU and memory requirements, define networking and IAM policies, and launch the application. Fargate then launches and scales the compute to closely match the resource requirements that you specify for the container.&lt;/p&gt; 
&lt;p&gt;To illustrate what running Monte Carlo simulations using AWS Batch looks like in practice, let’s look at the steps that Lucidminds took to get their Python-based simulation application running at scale using AWS resources. We’ll break down the steps into two distinct phases: 1) containerizing and testing the simulation application and; 2) scaling the simulation application using AWS Batch.&lt;/p&gt; 
&lt;p&gt;As a helpful guide to understand how the various AWS services used in this post (simulation application and scaling) fit together, refer to the reference architecture diagram in Figure 3.&lt;/p&gt; 
&lt;div id="attachment_3026" style="width: 857px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3026" loading="lazy" class="size-full wp-image-3026" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.24.02.png" alt="Figure 3 - Reference architecture diagram for running Monte Carlo simulations." width="847" height="632"&gt;
 &lt;p id="caption-attachment-3026" class="wp-caption-text"&gt;Figure 3 – Reference architecture diagram for running Monte Carlo simulations.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Phase 1: containerize, set up, and test simulation application on Amazon Elastic Container Service (Amazon ECS)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Containerize the simulation application&lt;/strong&gt; – AWS Batch runs jobs using Docker container images. In this post we’re not going to cover how to containerize applications, just mention that is a part of the process. If you’re new to containers, check out this quick primer on the subject: &lt;a href="https://aws.amazon.com/what-is/containerization/"&gt;AWS: What is Containerization?&lt;/a&gt; You can containerize applications using Docker technology on your local machine or using AWS services like AWS Cloud9, depending on where you are developing your applications. Lucidminds built and tested their containers on their local machines.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Push container image to Amazon Elastic Container Registry (Amazon ECR) – &lt;/strong&gt;Once you have a containerized application, we need to get it to an image repository. Lucidminds used ECR, which is a fully managed container registry offering high-performance hosting, so you can reliably deploy application images and artifacts anywhere. The first step is to &lt;a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/repository-create.html"&gt;create an image repository&lt;/a&gt;, which can be done via the AWS Command Line Interface (CLI) or using the AWS console. If you’re unsure about what commands are needed to tag and push images to an ECR repository, simply create a repository through the AWS console, navigate to the repository and click &lt;strong&gt;View push commands&lt;/strong&gt; to view the steps to push an image to your new repository.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_3027" style="width: 854px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3027" loading="lazy" class="size-full wp-image-3027" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.25.58.png" alt="Figure 4 - Creating an Image repository for your containerized application on Amazon Elastic Container Registry" width="844" height="277"&gt;
 &lt;p id="caption-attachment-3027" class="wp-caption-text"&gt;Figure 4 – Creating an Image repository for your containerized application on Amazon Elastic Container Registry&lt;/p&gt;
&lt;/div&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;&lt;strong&gt;Set up databases that connect to the simulation container&lt;/strong&gt; – The GUS application connects to two databases to store data needed during the simulations (Amazon MemoryDB for Redis) and outputs of simulation runs for caching purposes (Amazon RDS for PostgreSQL). Caching simulation outputs allows Lucidminds to reduce their need for compute when users enter a set of simulation parameters that have already been computed in the past. Not all simulations require a database connection and it is not a requirement for running Monte Carlo simulations using AWS Batch, that’s just how the Green Urban Simulator application works. GUS also writes simulation results to an S3 bucket. It’s strongly recommended that container instances are launched within a Virtual Private Cloud (VPC), and required when using managed compute instances (discussed later), so for the simulation containers to output results to S3 buckets, you’ll need a VPC endpoint. For more information, see &lt;a href="https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"&gt;Gateway endpoints for Amazon S3&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Test application on Amazon Elastic Container Service (ECS) cluster&lt;/strong&gt; – Make sure to test that the application is working as anticipated before trying to scale. Lucidminds tested their simulation application using an ECS cluster, to ensure that the container could be loaded and run, and that the application can successfully connect to and read/write results to the associated data stores. Once GUS was performing sas expected, it was time to get running with AWS Batch.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Phase 2: set up and run simulations using AWS Batch&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Create a compute environment (CE) &lt;/strong&gt;– A compute environment is a collection of compute resources used to run batch jobs. Before you can run jobs in AWS Batch, you need a CE. You can set up a managed CE which is managed by AWS, or an unmanaged CE that you manage yourself. You can configure managed CEs to use Fargate or On-demand Amazon EC2 instances. For each of those options you can choose to use Spot Instances at a deep discount, however they can be stopped suddenly and restarted at any time with a 2-minute warning (potentially from a checkpoint). If you’re not familiar with Spot Instances, check out the &lt;a href="https://aws.amazon.com/ec2/spot/"&gt;Amazon EC2 Spot Instances product page&lt;/a&gt;. For the simulations of Amsterdam, Lucidminds chose to use a managed CE, configured with AWS Fargate. During setup of a Fargate-configured CE, you choose a name for the environment, select whether you want to run with Spot Instances, set the maximum number of vCPUs the environment can use, and set up the network configuration by choosing (or creating) a VPC, subnets into which resources are launched, and the security groups to be associated with the launched instances.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create a job queue, associate to CE &lt;/strong&gt;– Jobs queues are the place that submitted jobs sit until they are scheduled to run and are associated with CEs. You can set up multiple job queues and set a priority order to determine what jobs get run first in an environment where multiple types of jobs are being submitted. For example, you can create a queue that uses Amazon EC2 On-Demand instances for high priority jobs and another queue that uses Amazon EC2 Spot Instances for low-priority jobs. For additional details on setting up job queues, see the &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/job_queues.html"&gt;AWS Batch job queues&lt;/a&gt;&lt;u&gt; documentation&lt;/u&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Create and register a job definition&lt;/strong&gt; – Job definitions describe the job to be executed, including parameters, environment variables, and compute requirements. Job definitions are how you tell AWS Batch the location of the container image you’re running, the number of vCPUs and amount of memory the container should use with the container, IAM roles the job might need, and any commands the container should run when starting. Once you create a job definition, it can be reused or shared by multiple jobs. In the Lucidminds case for Amsterdam, the GUS container used 1 vCPU (it’s a single-threaded application) and 2 GB per container.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Submit an AWS Batch array job&lt;/strong&gt; – Now that all AWS Batch resources are set up, we’re ready to run simulations. You can submit jobs using the AWS Console or the AWS CLI. When submitting a job, you select the CE, job queue, and job definition. If needed, you can override many of the parameters specified in the job definition at runtime. One additional parameter we can set that is particularly useful in the case of the GUS (and Monte Carlo simulations in general) is: &lt;strong&gt;Array Size&lt;/strong&gt;. This parameter can be set between 2 and 10,000 and is used to run &lt;em&gt;array jobs&lt;/em&gt;: jobs that share common parameters, like the job definition, vCPUs, and memory. These jobs run as a collection of related, yet separate, basic jobs that might be distributed across multiple hosts and might run concurrently. Array jobs are the most efficient way to run extremely-parallel jobs like Monte Carlo simulations or parametric sweeps.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;As we mentioned earlier, running an ensemble of simulations for a green urban scenario does not require any simulation input parameters to be different, because the inherent probabilistic nature of the simulations provides different results for the same set of inputs. If you have an application or simulation to which you’d like to pass a sequence of different input parameters, such as for parameter sweeps or grid search cases, you can use the AWS_BATCH_JOB_ARRAY_INDEX environment variable to differentiate the child jobs. For a quick, simple, and yet highly-illustrative tutorial that demonstrates this concept, see &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/array_index_example.html"&gt;Tutorial: Using the array job index to control job differentiation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Once you submit your job, you can track it on the AWS Batch dashboard. In Figure 5, we’ve shown a screenshot of an example array job with 100 child jobs submitted and mid-run. As the jobs get scheduled and processed, they move across the dashboard from &lt;strong&gt;Submitted&lt;/strong&gt; to either &lt;strong&gt;Succeeded&lt;/strong&gt; or &lt;strong&gt;Failed&lt;/strong&gt;.&lt;/p&gt; 
&lt;div id="attachment_3028" style="width: 854px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3028" loading="lazy" class="size-full wp-image-3028" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.27.20.png" alt="Figure 5 - Reviewing results from induvidual jobs on Amazon CloudWatch" width="844" height="363"&gt;
 &lt;p id="caption-attachment-3028" class="wp-caption-text"&gt;Figure 5 – Reviewing results from induvidual jobs on Amazon CloudWatch&lt;/p&gt;
&lt;/div&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;&lt;strong&gt;Consolidate and view results&lt;/strong&gt; – Once the job completes, it’s time to review the results. You can navigate into individual jobs and view the logs via CloudWatch. In the array job screenshot shown in Figure 4, you’d get there by clicking one of the hyperlinked numbers in the column &lt;strong&gt;Job Index&lt;/strong&gt; and by clicking the link under &lt;strong&gt;Log stream name&lt;/strong&gt;. If your application writes results to an Amazon S3 bucket as the GUS does, you might need to include an extra step where the results are consolidated to make for easier analysis.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;GUS in action&lt;/h2&gt; 
&lt;p&gt;Lucidmind’s pilot project with the City of Glasgow, aimed at supporting the city’s climate targets by growing canopy cover in deprived neighborhoods, has yielded promising results. Soon the city of Stuttgart (in Germany) will adopt the GUS Framework. With GUS, these cities can run city-scale simulations and explore multiple scenarios to identify optimal projects that align with their net-zero climate targets. By leveraging the GUS framework, they can harness the numerous benefits of trees, including carbon sequestration and storage, storm-water retention, and mitigating heatwave effects.&lt;/p&gt; 
&lt;p&gt;The GUS framework, consisting of a set of meticulously designed microservices, serves as a powerful tool for various organizations worldwide. VAIV, a South Korean company specializing in AI and big data solutions, will utilize the GUS API to create digital twins of their projects, specifically leveraging the heatwave effects module to address their unique use-case. This demonstrates the versatility and adaptability of the GUS framework, catering to diverse requirements across different regions and sectors. Stuttgart is another example where GUS is used for long term urban deforestation planning and decision making.&lt;/p&gt; 
&lt;p&gt;A GUS demo is now available for anyone who’d like to run city-scale simulations and explore multiple scenarios, gaining valuable insights into the impact of your decisions on urban environments. You can visit &lt;a href="http://run.greenurbanscenarios.com/"&gt;run.greenurbanscenarios.com/&lt;/a&gt;, set up an example scenario, and click “run on AWS” to run a simulation and view the results. The science behind GUS is explained in a peer-reviewed journal article [3].&lt;/p&gt; 
&lt;div id="attachment_3029" style="width: 767px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3029" loading="lazy" class="size-full wp-image-3029" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.27.55.png" alt="Figure 6 - GUS is available to run online" width="757" height="628"&gt;
 &lt;p id="caption-attachment-3029" class="wp-caption-text"&gt;Figure 6 – GUS is available to run online&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3030" style="width: 851px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3030" loading="lazy" class="size-full wp-image-3030" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.29.25.png" alt="Figure 7 - Insights generated by GUS Simulations ran on AWS HPC. Impact Analysis Dashboard for Carbon, Water retention, Air Quality and Canopy Cover" width="841" height="522"&gt;
 &lt;p id="caption-attachment-3030" class="wp-caption-text"&gt;Figure 7 – Insights generated by GUS Simulations ran on AWS HPC. Impact Analysis Dashboard for Carbon, Water retention, Air Quality and Canopy Cover&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The GUS dashboard’s standardized outputs are generated through Monte Carlo experiments, considering the probabilistic nature of climate and tree growth dynamics influenced by their surroundings.&lt;/p&gt; 
&lt;p&gt;GUS leverages AWS computing services to perform Monte Carlo experiments. In Figure 8, we illustrate a simulation ensemble of fifty different simulations for three different maintenance scenarios (high, medium, and low maintenance). The solid line represents the mean carbon sequestration of the fifty forest growth trajectories. The high-maintenance scenario assumes state-of-the-art pruning, replanting, and disease treatment, while lower maintenance scenarios involve reduced or nonexistent care provisions.&lt;/p&gt; 
&lt;p&gt;These simulation results would tell a decision maker the amount of carbon a proposed green urban project could be expected to sequester over time, and the decision maker could use that information in combination with cost estimates for each maintenance scenario specific to their area to better understand the cost-benefit tradeoffs associated with green urban planning.&lt;/p&gt; 
&lt;div id="attachment_3031" style="width: 854px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3031" loading="lazy" class="size-full wp-image-3031" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/31/CleanShot-2023-10-31-at-17.30.16.png" alt="Figure 8 - Monte Carlo simulations of sequestration showing that the higher the tree maintenance, the more carbon will be captured over time." width="844" height="493"&gt;
 &lt;p id="caption-attachment-3031" class="wp-caption-text"&gt;Figure 8 – Monte Carlo simulations of sequestration showing that the higher the tree maintenance, the more carbon will be captured over time.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Urban forests are an important part of the fight against climate change. Digital technologies such as scenario analysis, digital twins, agent-based modeling, and high performance computing on AWS using services like AWS Batch can help us simulate urban forests and examine how different potential green urban projects can have an impact on our cities.&lt;/p&gt; 
&lt;p&gt;In this post, we’ve not only examined why simulation techniques like Lucidmind’s Green Urban Simulator (GUS) are important tools for the future or urban planning and green cities, but also how AWS Batch can be used to scale simulations and drastically reduce the amount of time to get results.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;References&lt;/h3&gt; 
&lt;p&gt;[1] Journal of Arboriculture 18(5): September 1992 227 ASSESSING THE BENEFITS AND COSTS OF THE URBAN FOREST by John F. Dwyer, E. Gregory McPherson, Herbert W. Schroeder, and Rowan A. Rowntree&lt;/p&gt; 
&lt;p&gt;[2] David J. Nowak and John F. Dwyer, Understanding the Benefits and Costs of Urban Forest Ecosystems. &amp;nbsp;&lt;a href="https://www.researchgate.net/publication/321619207_Urban_and_Community_Forestry_in_the_Northeast"&gt;Urban and Community Forestry in the Northeast.&amp;nbsp;&lt;/a&gt;Doi: 10.1007/978-1-4020-4289-8_2.&lt;/p&gt; 
&lt;p&gt;[3] Bulent Ozel and Marko Petrovic. 2023. Green Urban Scenarios: A framework for digital twin representation and simulation for urban forests and their impact analysis, &lt;a href="https://joa.isa-arbor.com/index.asp"&gt;Journal of Arboriculture &amp;amp; Urban Forestry &lt;/a&gt;(Forthcoming).&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-17.40.16.png" alt="Oguzhan Yayla" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Oguzhan Yayla&lt;/h3&gt; 
  &lt;p&gt;Oguzhan Yayla is Co-Founder of Lucidminds, a driving force behind the transition towards regenerative economies. With a strong background as a Complex Systems Engineer and Digital Infrastructure Architect, Oguzhan is dedicated to building the necessary infrastructure and tools that will shape the transition toward a more sustainable, prosperous future and break out of the carbon tunnel vision.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-17.40.20.png" alt="Roni Bulent Ozel" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Roni Bulent Ozel&lt;/h3&gt; 
  &lt;p&gt;Roni Bulent Ozel is the co-founder and CEO of Lucidminds AI, with a double PhD in complex systems, economics, and computer science. He excels in bridging business, technology, science, and policy-making, leading innovations published in peer-reviewed articles, books, patents, and software packages. His mission is to drive systemic change for an equitable planet, benefiting humans, non-humans, and future generations.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/11/01/CleanShot-2023-11-01-at-17.40.27.png" alt="Jake Doran" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Jake Doran&lt;/h3&gt; 
  &lt;p&gt;Jake Doran originally studied theoretical physics before transitioning to a career in software. Most recently he works as a researcher and engineer at LucidMinds, hoping to build and promote systems of societal good through technology.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Deploying Level 4 Digital Twin Self-Calibrating Virtual Sensors on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/deploying-level-4-digital-twin-self-calibrating-virtual-sensors-on-aws/</link>
		
		<dc:creator><![CDATA[Ross Pivovar]]></dc:creator>
		<pubDate>Mon, 30 Oct 2023 15:04:39 +0000</pubDate>
				<category><![CDATA[AWS IoT TwinMaker]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Internet of Things]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<guid isPermaLink="false">13884cdb21d9f5d41adb23e1c678c023812d8ed6</guid>

					<description>Digital twins can be hard if they deviate from real-world behavior as real systems degrade and change over time. Today we’ll show digital twins that calibrate on operational data, using TwinFlow on AWS.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Ross Pivovar, Solution Architect, Autonomous Computing, and Adam Rasheed, Head of Autonomous Computing at AWS; and Kayla Rossi, Application Engineer, and Orang Vahid, Director of Engineering Services at Maplesoft.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;In a &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-a-level-3-digital-twin-virtual-sensor-with-ansys-on-aws/"&gt;previous post&lt;/a&gt; we shared the common customer use case of deploying &lt;a href="https://aws.amazon.com/blogs/hpc/deploying-a-level-3-digital-twin-virtual-sensor-with-ansys-on-aws/"&gt;Level 3 Digital Twin (L3 DT) virtual sensors&lt;/a&gt; to help operators make more informed decisions in situations where using physical sensors is difficult, expensive, or impractical. One of the challenges is that L3 DT virtual sensors use pre-trained models that deviate from real-world behavior as the physical system degrades and changes over time. Operators then become hesitant to base operational decisions solely on the L3 DT virtual sensor predictions.&lt;/p&gt; 
&lt;p&gt;Today we’ll describe how to build and deploy L4 DT self-calibrating virtual sensors where operational data is used to automatically calibrate the virtual sensor. This automated self-calibration allows the virtual sensor predictions to adapt and more closely match the real-world, allowing the operators to take proactive corrective actions to prevent failures and optimize performance. For more discussion on the different levels for digital twins, check out &lt;a href="https://aws.amazon.com/blogs/iot/digital-twins-on-aws-unlocking-business-value-and-outcomes/"&gt;our previous post describing the AWS L1-L4 Digital Twin framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In this post, we use a&lt;a href="https://en.wikipedia.org/wiki/Modelica"&gt; Modelica&lt;/a&gt;-based model created using &lt;a href="https://www.maplesoft.com/products/maplesim/"&gt;Maplesoft’s MapleSim simulation and engineering design software&lt;/a&gt;. MapleSim provides the tools to create engineering simulation models of machine equipment and exports them as Functional Mockup Units (FMUs) which is an industry standard file format for simulation models. We then use &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow&lt;/a&gt; to deploy the model on AWS and calibrate the FMU using probabilistic Bayesian estimation techniques to statistically infer the unmeasured model coefficients – using the incoming sensor data to calibrate against. TwinFlow is an AWS open-source framework for building and deploying predictive models at scale.&lt;/p&gt; 
&lt;h2&gt;Roll-to-roll manufacturing&lt;/h2&gt; 
&lt;p&gt;For our use case, we’ll consider the web-handling process in roll-to-roll manufacturing (depicted in Figure 1) used for continuous materials such as paper, film, and textiles. The web-handling process involves unwinding the material from a spool, guiding it through various treatments such as printing or coating, and then winding it onto individual rolls. Precise control of tension, alignment, and speed is essential to ensure smooth processing and maintain product quality.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Web Line Perspective 2023 07 20" width="500" height="281" src="https://www.youtube-nocookie.com/embed/gouyAMuY6fs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;div class="wp-caption aligncenter" style="width: 945px"&gt; 
 &lt;p id="caption-attachment-2964" class="wp-caption-text"&gt;Figure 1: A short movie showing the dynamics of the web material handling process in roll to roll manufacturing. The web material is the sheet passing through the rollers which control the tension and speed of the manufacturing process. The driven rollers set the web speed and tension.&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;There are many different failure mechanisms, however for this post, we’ll focus on two failure modes: &lt;strong&gt;high tension failures&lt;/strong&gt; and &lt;strong&gt;slip failures&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Figure 2 shows a schematic diagram of the web-handling equipment with the material spans between the rollers labeled from S1 through S12. Figure 2b shows a screenshot of the MapleSim web-handling simulation model of the roll-to-roll manufacturing line.&lt;/p&gt; 
&lt;div id="attachment_2964" style="width: 945px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2964" loading="lazy" class="size-full wp-image-2964" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure2.jpg" alt="Figure 2: a) Schematic diagram of the web handling equipment in which each span is labeled, and b) Screenshot of the MapleSim simulation model of the web handling equipment." width="935" height="560"&gt;
 &lt;p id="caption-attachment-2964" class="wp-caption-text"&gt;Figure 2: a) Schematic diagram of the web handling equipment in which each span is labeled, and b) Screenshot of the MapleSim simulation model of the web handling equipment.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Referring to Figure 2, tension failures occur when the tension within the material at a particular span exceeds a threshold (175 Newtons in this example) resulting in web deformities like wrinkling and troughing. Slip failures are more subtle and occur when the relative movement between the web material and rollers isn’t in sync, causing the web to be dragged across the roller. Slip failures are quantified by measuring the slip velocity which is the difference between the linear velocity of the web material and the tangential velocity of the roller. We consider it a slip failure when the slip velocity exceeds a threshold (0.0001 m/s in this case). It can result in difficult-to-detect defects, misalignment, and reduced product quality if not controlled.&lt;/p&gt; 
&lt;p&gt;Tension and slip are measurable variables, but it’s expensive and intrusive to add these sensors at every location along the manufacturing line. It’s more common to have a limited number of these sensors at key locations while measuring the angular velocity (rotation speed, often colloquially referred to as RPM) of each of the rollers and relying on best practices to control the manufacturing process. This results in sub-optimal process control and relies heavily on operator experience. A digital twin of this process can be used to calculate the tension and slip velocity when combined with the proper IoT data.&lt;/p&gt; 
&lt;p&gt;Both failure mechanisms are the direct result of the rollers becoming more difficult to turn because of dirt and grime fouling the bearings, increasing the viscous friction. In our example, the viscous friction of the rollers is accounted for in the MapleSim model. The model solves the equations of motion, but requires estimates of the viscous friction coefficients to use for the rollers in the model. Viscous friction (damping) is not a directly measurable variable but can be inferred by calibrating the model using the measured angular velocities.&lt;/p&gt; 
&lt;p&gt;In essence, we’re seeking to infer the viscous friction coefficient by leveraging statistics, the FMU model, and our probabilistic Bayesian estimation methods.&lt;/p&gt; 
&lt;p&gt;We use the calibrated L4 Digital Twin to create self-calibrating virtual sensors of tension and slip velocity. These L4 DT self-calibrating virtual sensors can then be used by operators to support operational decisions to control the manufacturing process.&lt;/p&gt; 
&lt;h2&gt;Example failure scenario&lt;/h2&gt; 
&lt;p&gt;To understand the process of combining IoT data streams with the FMU model to create an L4 Digital Twin, we’ll examine a failure scenario that we simulated. In our synthetic failure scenario, we introduced defects for rollers 3 and 9 in the form of contamination – leading to increased friction. Here, we linearly increased the bearing viscous damping coefficients from 0 on Day 15 to 0.2 on Day 23 (which we’ve shown in Figure 3).&lt;/p&gt; 
&lt;div id="attachment_2965" style="width: 896px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2965" loading="lazy" class="size-full wp-image-2965" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure3.jpg" alt="Figure 3: Plot showing the artificially induced increase in viscous damping for rollers 3 and 9 starting from 0 on Day 15 to 0.2 on Day 23." width="886" height="480"&gt;
 &lt;p id="caption-attachment-2965" class="wp-caption-text"&gt;Figure 3: Plot showing the artificially induced increase in viscous damping for rollers 3 and 9 starting from 0 on Day 15 to 0.2 on Day 23.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 4 shows the angular velocity measurements (both plots show the same data with different y-axis scale) for each of the rollers. Figure 4b shows that – as expected – at Day 15, the angular velocity for rollers 3 and 9 begin to change because of the increasing viscous damping we introduced to simulate dirt build-up. Since the entire web line is linked via the material, a single roller increasing in resistance impacts the angular velocities of the surrounding rollers. In our case, we see that rollers 1,2,3,7,8,9, and 10 all experience changes in angular velocity.&lt;/p&gt; 
&lt;p&gt;By Day 21, the dirt build-up is sufficient to cause a very large change in the angular velocity of roller 9 as we’ve shown in Figure 4.&lt;/p&gt; 
&lt;div id="attachment_2966" style="width: 1384px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2966" loading="lazy" class="size-full wp-image-2966" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure4.jpg" alt="Figure 4: Incoming IoT data streams for angular velocity. Left and right figures are the same data with the scaling of the y-axis changed to enable observing tiny differences." width="1374" height="442"&gt;
 &lt;p id="caption-attachment-2966" class="wp-caption-text"&gt;Figure 4: Incoming IoT data streams for angular velocity. Left and right figures are the same data with the scaling of the y-axis changed to enable observing tiny differences.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 5 shows the corresponding span tensions and slip velocities (that we would measure with IoT sensors if they were installed) leading to tension failures in spans 4, 5, and 6 and slip failure at roller 9. Figure 5a shows spans 4, 5, and 6 exceeding the 175N threshold and Figure 5b shows roller 9 with non-zero slip velocity.&lt;/p&gt; 
&lt;div id="attachment_2967" style="width: 1351px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2967" loading="lazy" class="size-full wp-image-2967" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure5.jpg" alt="Figure 5: a) Measured span tensions during the dirt build up scenario in which spans 4,5, and 6 exceed the failure threshold for the webbing material, and b) measured roller slip velocities during the dirt build up scenario in which roller 9 eventually exhibits significant slippage around day 21." width="1341" height="399"&gt;
 &lt;p id="caption-attachment-2967" class="wp-caption-text"&gt;Figure 5: a) Measured span tensions during the dirt build up scenario in which spans 4,5, and 6 exceed the failure threshold for the webbing material, and b) measured roller slip velocities during the dirt build up scenario in which roller 9 eventually exhibits significant slippage around day 21.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As mentioned already, in practice, operators typically only measure the angular velocity of the rollers – and span tension might be measured using a load cell for only one span across the entire production line.&lt;/p&gt; 
&lt;p&gt;This example shows the importance of calibrating the system model to use the correct viscous friction coefficients. Without regular calibrations, the model predictions for span tension and roller slip velocity will be incorrect, resulting in the operator being unaware of potential, impending failures.&lt;/p&gt; 
&lt;h2&gt;Using TwinFlow to calibrate the L4 Digital Twin&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow&lt;/a&gt; is the AWS open-source framework for building and deploying millions of predictive models at scale on a distributed, heterogeneous architecture. TwinFlow incorporates the &lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt; module to orchestrate the model deployment and the &lt;a href="https://github.com/aws-samples/twinstat"&gt;TwinStat&lt;/a&gt; module which includes statistical methods for building and deploying L4 Digital Twins. These methods include techniques to build quick-execution (seconds or less) response surface models (RSMs) from complex simulation models that could take hours to run and would be too costly and too slow to support operational decisions.&lt;/p&gt; 
&lt;p&gt;The TwinStat module also includes methods to probabilistically update and calibrate L4 Digital Twins. In our case, the MapleSim model is exported as an FMU which already has a very quick execution time on the order of a few seconds. Since the degradation effect due to roller dirt accumulation is gradual (hours to days), our FMU execution time is more than sufficient – and cost-efficient.&lt;/p&gt; 
&lt;p&gt;The model calibration techniques in TwinStat include probabilistic Bayesian estimation methods like &lt;a href="https://en.wikipedia.org/wiki/Kalman_filter"&gt;Unscented Kalman Filters (UKF)&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Particle_filter"&gt;Particle Filters (PF)&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Gaussian_process"&gt;Gaussian Processes&lt;/a&gt;. Each of the methods have pros and cons and preferred scenarios, which we’ll discuss in a future post. For our present use case, UKF provides the necessary accuracy while minimizing the compute time. We used UKF in TwinFlow with the MapleSim FMU model to calibrate the viscous coefficients using the measured angular velocity IoT data.&lt;/p&gt; 
&lt;p&gt;To use TwinFlow, we needed an appropriately-sized Amazon EC2 instance. For our specific scenario (fast FMU with low memory requirements) we wanted to minimize the network overhead, hardware procurement time, and container download and activation time needed to run UKF. We know that UKF scales with the number of FMU executions by 2*D+1, where D is the number of variables included in the UKF. For our example, we have 9 measured angular velocities and 9 unmeasured viscous damping coefficients for a total of 18 variables. For each incoming IoT data point, UKF will run the FMU 37 times. Thus, we select an EC2 instance with around 37 cores for optimal runtime performance. In this scenario, the FMU is single-threaded (you’d need to increase your instance size if you used a multi-threaded FMU with UKF). TwinFlow parallelizes the UKF execution with multi-processing, thus the more CPUs, the shorter the runtime.&lt;/p&gt; 
&lt;p&gt;In our calibration example, we used the synthetic dataset for the angular velocity plotted in Figure 4 to represent the real-world measured sensor data. We used UKF to iterate and determine the correct viscous damping coefficients in the FMU model to match the measured angular velocities. Figure 6a shows the values determined by UKF for the viscous damping coefficients, as well as the estimates of uncertainty (shaded regions around the lines).&lt;/p&gt; 
&lt;p&gt;We see that UKF correctly estimates the coefficients to be approximately zero for all rollers &lt;em&gt;except 3 and 9&lt;/em&gt;, which increased to a value of approximately 0.20, giving us confidence that UKF is successfully calibrating the L4 Digital Twin to model the evolving behavior of the web-handling system.&lt;/p&gt; 
&lt;p&gt;Figure 6b shows how the UKF estimates for angular velocity lies directly on top of the IoT sensor data (open circle data points). The shaded region represents uncertainty.&lt;/p&gt; 
&lt;div id="attachment_2968" style="width: 634px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2968" loading="lazy" class="size-full wp-image-2968" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure6.jpg" alt="Figure 6: a) UKF estimates for a) viscous damping coefficients (commonly denoted with letter ”b” ) and b) angular velocity of each roller with uncertainty estimations (shaded region). We see that the viscous damping coefficients for rollers 3 and 9 increase from 0 to 0.2, corresponding to the failure scenario we manually introduced when creating the synthetic data, and we see the UKF angular velocity estimates matching the IoT synthetic data." width="624" height="244"&gt;
 &lt;p id="caption-attachment-2968" class="wp-caption-text"&gt;Figure 6: a) UKF estimates for a) viscous damping coefficients (commonly denoted with letter ”b” ) and b) angular velocity of each roller with uncertainty estimations (shaded region). We see that the viscous damping coefficients for rollers 3 and 9 increase from 0 to 0.2, corresponding to the failure scenario we manually introduced when creating the synthetic data, and we see the UKF angular velocity estimates matching the IoT synthetic data.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We can now examine the performance of the L4 Digital Twin self-calibrated virtual sensor for tension and slip by checking the residuals are close to zero. The &lt;em&gt;residual&lt;/em&gt; is the difference between the virtual sensor predicted value and the actual IoT measured data value (in this case our synthetic dataset).&lt;/p&gt; 
&lt;p&gt;Figure 7 shows the residuals for tension and slip velocity are near zero, meaning that the virtual sensor predicted values closely match the true values, and we can eliminate the costly physical sensors for measuring tension and slip velocity.&lt;/p&gt; 
&lt;div id="attachment_2969" style="width: 768px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2969" loading="lazy" class="size-full wp-image-2969" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure7.jpg" alt="Figure 7: Residuals of the digital twin output comparing tension on the left and slip velocity on the right. Due to the example scenario using synthetic data that is noiseless, the residuals are almost perfectly zero." width="758" height="312"&gt;
 &lt;p id="caption-attachment-2969" class="wp-caption-text"&gt;Figure 7: Residuals of the digital twin output comparing tension on the left and slip velocity on the right. Due to the example scenario using synthetic data that is noiseless, the residuals are almost perfectly zero.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Now, we compare the results of using an uncalibrated L3 virtual sensor model with the L4 self-calibrating virtual sensor.&lt;/p&gt; 
&lt;p&gt;Figure 8a shows that the L3 digital twin initially matches the measured angular velocity, but then misses the changes as the system performance degrades due to dirt accumulation, whereas the L4 self-calibrated digital twin virtual sensor evolves with the real physical system and matches the measured data closely. Similarly, Figure 8b and Figure 8c show that the L3 (uncalibrated) virtual sensors initially match the measured data, but then underpredict the roller 9 slip velocity and the span 6 tension as the system degrades and the operator is completely unaware of the impending failure.&lt;/p&gt; 
&lt;p&gt;The L4 (self-calibrating) virtual sensor predictions, however, closely match the measured data, allowing the operator to take corrective actions &lt;em&gt;prior to failure&lt;/em&gt;. The slip failure is particularly noteworthy since it induces difficult-to-detect material imperfections that can make their way into the final product. We see that the L3 uncalibrated virtual sensor initially makes reasonable predictions, but as the web-handling system behavior evolves over time (due to real-world degradation), a self-calibrating L4 virtual sensor is required to adapt the model to the changing real-world conditions.&lt;/p&gt; 
&lt;div id="attachment_2970" style="width: 634px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2970" loading="lazy" class="size-full wp-image-2970" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/AWS_DeployingL4DTSelfCalibratingVirtualSensor_Figure8.jpg" alt="Figure 8: Comparison of a digital twins that are either calibrated or uncalibrated relative to measured data." width="624" height="204"&gt;
 &lt;p id="caption-attachment-2970" class="wp-caption-text"&gt;Figure 8: Comparison of a digital twins that are either calibrated or uncalibrated relative to measured data.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;AWS Architecture&lt;/h2&gt; 
&lt;p&gt;These are great results, so it’s worth taking a moment to explain what architectural decisions we made that allowed us to get here.&lt;/p&gt; 
&lt;p&gt;The AWS architecture used for the L4 Digital Twin virtual sensor calibration is shown in Figure 9. In our scenario, we periodically collected sensor data every 10-20 mins – a sufficient time resolution to capture the failure phenomena of interest.&lt;/p&gt; 
&lt;p&gt;It’s important to balance the trade-off of increased time resolution versus what is needed for the use case, as gathering a lot of data at high frequency results in unnecessary data storage and additional compute costs. We used an &lt;a href="https://aws.amazon.com/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt; scheduler to enable periodic calibration. We could have alternatively added logic to the container code to first calculate the error of the digital twin and decided to only calibrate if an error threshold is violated. And since Amazon EventBridge can only handle a maximum of 100 scheduling rules, we’d need to modify the architecture to use a Lambda function between Amazon EventBridge and &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, if we needed to scale to millions of tasks.&lt;/p&gt; 
&lt;p&gt;In Step 1 in Figure 9, we’ve shown how you can download &lt;a href="https://github.com/aws-samples/twinflow"&gt;TwinFlow&lt;/a&gt; to a temporary EC2 instance where they can customize, build, and push containers to cloud repositories, and then deploy your infrastructure as code (IaC).&lt;/p&gt; 
&lt;p&gt;Next, you can modify the example container and insert your own model into the container. The container gets pushed up to Amazon Elastic Container Registry (Amazon ECR) where it’s now available to all AWS services.&lt;/p&gt; 
&lt;p&gt;At step 4, you connect your IoT data from an edge location into a cloud database like &lt;a href="https://aws.amazon.com/iot-sitewise/"&gt;AWS IoT SiteWise&lt;/a&gt;, which is a serverless database designed to handle sensors with user defined attributes.&lt;/p&gt; 
&lt;p&gt;At this point, the Amazon EventBridge scheduler calls tasks in an &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; compute environment which loads the customized container, pulls data from &lt;a href="https://aws.amazon.com/iot-sitewise/"&gt;AWS IoT SiteWise&lt;/a&gt;, calibrates the L4 Digital Twin, saves the calibration to an S3 bucket, makes physics predictions, and saves them back in AWS IoT SiteWise.&lt;/p&gt; 
&lt;p&gt;AWS Batch selects the optimal EC2 instance type, auto-scales up or out, and logs all task output in Amazon CloudWatch. Finally, we used &lt;a href="https://aws.amazon.com/iot-twinmaker/"&gt;AWS IoT TwinMaker&lt;/a&gt; to create dashboards in Grafana so operators can review the measured and predicted data in near real time – and make operational decisions.&lt;/p&gt; 
&lt;div id="attachment_2971" style="width: 987px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2971" loading="lazy" class="size-full wp-image-2971" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/HPCBlog-244-fig9.png" alt="Figure 9: AWS Cloud architecture needed to achieve digital twin periodic-calibration" width="977" height="550"&gt;
 &lt;p id="caption-attachment-2971" class="wp-caption-text"&gt;Figure 9: AWS Cloud architecture needed to achieve digital twin periodic-calibration.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we showed how to build an L4 Digital Twin self-calibrating virtual sensor on AWS using an FMU model created by MapleSim.&lt;/p&gt; 
&lt;p&gt;MapleSim provides the physics model in the form of an FMU, and TwinFlow allows us to use incoming IoT data to probabilistically calibrate the L4 Digital Twin virtual sensor for &lt;em&gt;span tension&lt;/em&gt; and &lt;em&gt;roller slip velocity&lt;/em&gt;. In future posts, we’ll discuss how to use the calibrated L4 Digital Twin to perform scenario analysis, risk assessment, and process optimization.&lt;/p&gt; 
&lt;p&gt;If you want to request a proof of concept or if you have feedback on the AWS tools, reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Some of the content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/kayla-rossi-profile.png" alt="Kayla Rossi" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Kayla Rossi&lt;/h3&gt; 
  &lt;p&gt;Kayla is an Application Engineer at Maplesoft providing customers with advanced engineering support for the digital modeling of their complex production machines. Her project experience includes the simulation and analysis of web handling and converting systems across various industrial applications, including printing and battery production.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/18/orang-vahid-profile.png" alt="Dr. Orang Vahid" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Dr. Orang Vahid&lt;/h3&gt; 
  &lt;p&gt;Orang has over 20 years of experience in system-level modeling, advanced dynamic systems, frictional vibration and control, automotive noise and vibration, and mechanical engineering design. He is a frequent invited speaker and has published numerous papers on various topics in engineering design. At Maplesoft he is Director of Engineering Services, with a focus on the use and development of MapleSim solutions.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>EFA: how fixing one thing, led to an improvement for … everyone</title>
		<link>https://aws.amazon.com/blogs/hpc/efa-how-fixing-one-thing-lead-to-an-improvement-for-everyone/</link>
		
		<dc:creator><![CDATA[Shi Jin]]></dc:creator>
		<pubDate>Thu, 26 Oct 2023 14:30:25 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[Finite Element Analysis]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[MPI]]></category>
		<category><![CDATA[Protein Folding]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">ba7892a9031f1d2a300dccb5eeca53894380222e</guid>

					<description>Today, we're diving deep into the open-source frameworks that move MPI messages around, and showing you how work we did in the Open MPI and libfabrics community lead to an improvement for EFA users - and everyone else, too.</description>
										<content:encoded>&lt;p&gt;We routinely test a lot of HPC applications to make sure they work well on AWS, because we think that’s the only benchmark that matters. Hardware improvements from generation to generation are always important, but software is more frequently the place we find gains for customers.&lt;/p&gt; 
&lt;p&gt;Last summer, our performance teams alerted us to a discrepancy in runtimes when a single application ran with several different MPIs, while on the same hardware. At a high level, codes running with Open MPI were coming in a little slower than when using other commercial or open-source MPIs. This concerned us, because Open MPI is the most popular MPI option for customers using AWS Graviton (our home-grown Arm64 CPU) and we don’t want customers working on that processor to be disadvantaged.&lt;/p&gt; 
&lt;p&gt;In today’s post, we’ll explain how deep our engineers had to go to solve this puzzle (spoiler: it didn’t end with Open MPI). We’ll show you the outcomes from that work with some benchmark results including a real workload.&lt;/p&gt; 
&lt;p&gt;Along the way, we’ll give you a peek at how our engineers work in the open-source community to improve performance for everyone, and you’ll get a sense of the valuable contributions from the people in those communities who support these MPIs.&lt;/p&gt; 
&lt;p&gt;And – of course – we’ll show you how to get your hands on this so your code just runs faster – on every CPU (not just Graviton).&lt;/p&gt; 
&lt;h2&gt;First, some architecture&lt;/h2&gt; 
&lt;p&gt;If you’ve been paying &lt;em&gt;close attention&lt;/em&gt; to the posts on this channel&lt;em&gt;,&lt;/em&gt; you’ll know that the Elastic Fabric Adapter (EFA) is key to much of the performance you see in our posts. EFA works underneath your MPI and is often (we hope) just plain invisible to you and your application. EFA’s responsibility is to move messages securely – and rapidly – across our fabric between the instances that form parts of your cluster job. Doing this in a cloud environment adds an element or two to the task, because the cloud is both always under construction, and truly &lt;a href="https://youtu.be/JIQETrFC_SQ?t=1484"&gt;massive in scale&lt;/a&gt;. Scale isn’t the enemy however, and &lt;a href="https://aws.amazon.com/blogs/hpc/in-the-search-for-performance-theres-more-than-one-way-to-build-a-network/"&gt;we’ve exploited that fact&lt;/a&gt; to pull off &lt;a href="https://aws.amazon.com/blogs/hpc/second-generation-efa-improving-hpc-and-ml-application-performance-in-the-cloud/"&gt;great performance for HPC and ML codes&lt;/a&gt; with an interconnect built on the existing AWS network from existing technologies like Ethernet.&lt;/p&gt; 
&lt;p&gt;EFA remains largely invisible, because we chose to expose it through libfabric, which is also known as Open Fabrics Interfaces (OFI). This is a communications API for lots of parallel and distributed computing applications. By design, it’s lower-level than, say, MPI, and so allows lots of hardware makers (like AWS) to abstract their networking fabrics, so application programmers don’t need to pay attention to too many details of how they’re built.&lt;/p&gt; 
&lt;div id="attachment_2860" style="width: 976px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2860" loading="lazy" class="size-full wp-image-2860" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/13/CleanShot-2023-09-13-at-09.04.03.png" alt="Figure 1 - Open Fabrics Interfaces (OFI), or libfabric is the abstraction we've chosen to expose EFA to MPIs and NCCL. This allows application programmers to be productive without needing to know too much about how the fabric underneath does its work." width="966" height="466"&gt;
 &lt;p id="caption-attachment-2860" class="wp-caption-text"&gt;Figure 1 – Open Fabrics Interfaces (OFI), or libfabric is the abstraction we’ve chosen to expose EFA to MPIs and NCCL. This allows application programmers to be productive without needing to know too much about how the fabric underneath does its work.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As Figure 1 shows, there are several components and APIs in libfabric that MPI interacts with. While it’s impossible to explain them all in this post, let’s take a minute to work through a simple API flow for MPI that sends and receives a message between two ranks – using libfabric.&lt;/p&gt; 
&lt;h3&gt;An aside: a simple data flow example&lt;/h3&gt; 
&lt;p&gt;There are 3 groups of libfabric APIs involved: a &lt;em&gt;data sending&lt;/em&gt; API, a &lt;em&gt;data receiving&lt;/em&gt; API, and a &lt;em&gt;completion polling&lt;/em&gt; API. We’ve illustrated their connections in Figure 2.&lt;/p&gt; 
&lt;div id="attachment_2861" style="width: 553px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2861" loading="lazy" class="size-full wp-image-2861" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/13/CleanShot-2023-09-13-at-09.05.04.png" alt="Figure 2 – API flow for MPI to send and receive a message between two ranks using libfabric." width="543" height="474"&gt;
 &lt;p id="caption-attachment-2861" class="wp-caption-text"&gt;Figure 2 – API flow for MPI to send and receive a message between two ranks using libfabric.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;On the sender side, MPI initiates a message sending via the &lt;code&gt;fi_tsend&lt;/code&gt; function (&lt;a href="https://ofiwg.github.io/libfabric/main/man/fi_tagged.3.html"&gt;or its variants&lt;/a&gt;). This allows MPI to post a sending request that includes the address, length, and tag of the &lt;em&gt;sending&lt;/em&gt; buffer. On the receiver side, MPI posts a receiving request via the &lt;code&gt;fi_trecv&lt;/code&gt; call. This includes the address, length, and tag of the &lt;em&gt;receiving&lt;/em&gt; buffer. The tags are used for the receiver to filter what messages should be received from one or more senders.&lt;/p&gt; 
&lt;p&gt;Both sending and receiving APIs return &lt;em&gt;asynchronously&lt;/em&gt;, which means the sending and receiving operations don’t necessarily complete when the calls return. Except for some special cases, MPI needs to poll a completion queue (CQ) to get the completion events for sending and receiving operations, through &lt;code&gt;fi_cq_read&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Hopefully this gives you a picture of what goes on under the hood when MPI ranks want to communicate with each other. Anyhow, back to our main story now …&lt;/p&gt; 
&lt;h2&gt;Intra-node comms&lt;/h2&gt; 
&lt;p&gt;Digging deeper, the other MPIs seemed to have better &lt;em&gt;intra-node performance&lt;/em&gt; than Open MPI. This might sound a little surprising – you could be forgiven for presuming that &lt;em&gt;intra&lt;/em&gt;-node performance would be … more of a &lt;em&gt;solved problem&lt;/em&gt;. It’s literally cores sending messages to other cores &lt;em&gt;inside the same compute node&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Open MPI is designed to use a layer called the &lt;strong&gt;matching transport layer&lt;/strong&gt; (MTL) whenever libfabric is used to manage the two-sided tagged message sending and receiving operations (there &lt;em&gt;is&lt;/em&gt; another layer called the &lt;em&gt;byte transfer layer&lt;/em&gt; that uses libfabric for one-sided operations, like write and read, but we’re not going to discuss it in this post). This layer offloads &lt;em&gt;all communications&lt;/em&gt;, whether they’re intra-node or inter-node, to the libfabric layer. This is different from how the others do it, because they use their own shared-memory implementations for intra-node communications, &lt;em&gt;by default&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Other MPIs allow you to define which underlying libfabric provider they should use for shared memory operations (usually by tweaking some environment variables). Open MPI &lt;em&gt;also&lt;/em&gt; has its own shared-memory implementation in its BTL (Byte Transfer Layer) component (BTL/vader in Open MPI 4, and BTL/sm in Open MPI 5). This allowed us to explore several pathways to narrow down the specific portions of code where trouble was occurring.&lt;/p&gt; 
&lt;p&gt;We noticed that when running on a single instance, Open MPI with BTL/SM had similar performance to the others when they were using their native routines. As we noted above, due to low-level implementation details in Open MPI, when libfabric is being used, BTL/SM &lt;em&gt;cannot&lt;/em&gt; be used.&lt;/p&gt; 
&lt;p&gt;This is where some subtle effects can have quite an impact on outcomes. When Open MPI sends an intra-node message through the libfabric EFA provider, the EFA provider actually hands over the message to the libfabric SHM provider, which is yet another provider in libfabric – &lt;em&gt;that supports pure intra-node communications&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;When we put this to the test with &lt;a href="http://mvapich.cse.ohio-state.edu/benchmarks/"&gt;OSU’s benchmarking suite&lt;/a&gt; and some instrumented code, the high level performance gap we noticed between Open MPI with libfabric EFA and the other MPI’s native implementations could be decomposed into – mainly – two parts:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The gap between libfabric’s SHM provider itself and the others’ shared memory implementation.&lt;/li&gt; 
 &lt;li&gt;The gap between libfabric with (EFA + SHM)&amp;nbsp;and libfabric SHM on its own.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For a sense of scale, for small message sizes under 64 bytes (using a c5n.18xlarge instance) these gaps were adding around 0.4-0.5 µsec extra ping-pong latency. Given these are messages travelling &lt;em&gt;inside&lt;/em&gt; a machine and never making it to a PCI bus (let alone a network cable), this was a big penalty.&lt;/p&gt; 
&lt;h2&gt;Community efforts&lt;/h2&gt; 
&lt;p&gt;To address these two gaps, we needed to pursue directions of effort, in parallel.&lt;/p&gt; 
&lt;p&gt;Addressing the first gap involving the libfabric SHM provider performance became an effort with the help of the libfabric maintainers to optimize the locking mechanisms in the queues, and some other details – which we’ll dive into in a future post.&lt;/p&gt; 
&lt;p&gt;For the second gap involving the handoff between the EFA provider and the SHM provider, the AWS team worked to onboard the EFA provider using a newly-developed libfabric interface called&amp;nbsp;the &lt;a href="https://ofiwg.github.io/libfabric/main/man/fi_peer.3.html"&gt;Peer API&lt;/a&gt; so we could use the SHM provider as a &lt;em&gt;peer provider&lt;/em&gt;.&amp;nbsp;Peer providers are a way for independently developed providers to be used together in a tight fashion, which helps everyone (a) avoid layering overhead; and (b) resist the urge to duplicate each other’s’ provider functionality.&lt;/p&gt; 
&lt;div id="attachment_2862" style="width: 1178px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2862" loading="lazy" class="size-full wp-image-2862" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/13/CleanShot-2023-09-13-at-09.06.37.png" alt="Figure 3 - The base case before our improvement work began, characterized by a long pathway through all the layers of the stack, even though intra-node traffic should be able to short circuit much of it." width="1168" height="489"&gt;
 &lt;p id="caption-attachment-2862" class="wp-caption-text"&gt;Figure 3 – The base case before our improvement work began, characterized by a long pathway through all the layers of the stack, even though intra-node traffic should be able to short circuit much of it.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We’ve illustrated the contrasting pathways that small messages traverse when using the Peer API. Figures 3 and 4 depict these two paradigms – before and after we introduced peering through this new API. We’ll summarize it from two angles: the data movement and the completion-events population.&lt;/p&gt; 
&lt;div id="attachment_2863" style="width: 1151px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2863" loading="lazy" class="size-full wp-image-2863" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/13/CleanShot-2023-09-13-at-09.08.18.png" alt="Figure 4 - After we introduced the Peer API, which effectively short-circuits the pathways an intra-node message needs to travel." width="1141" height="594"&gt;
 &lt;p id="caption-attachment-2863" class="wp-caption-text"&gt;Figure 4 – After we introduced the Peer API, which effectively short-circuits the pathways an intra-node message needs to travel.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Data movement&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Before using the Peer API&lt;/strong&gt;, when MPI sends a message using the libfabric EFA provider, it cside, MPI posts a receiving requestalls &lt;code&gt;fi_tsend()&lt;/code&gt;(&lt;a href="https://ofiwg.github.io/libfabric/main/man/fi_tagged.3.html"&gt;or its variants&lt;/a&gt;) through the EFA endpoint. The EFA provider performs protocol selection logic and adds EFA headers to the message body before posting the message again though the SHM endpoint. After getting the send request from EFA, SHM does its &lt;em&gt;own &lt;/em&gt;protocol selection and message packaging &lt;em&gt;again,&lt;/em&gt; before copying the message into a ‘bounce’ buffer, which is shared in both the sender’s and receiver’s memory spaces. Over on the receiver’s side, SHM will copy the received message to a bounce buffer in the EFA provider, before delivering the message to the application’s buffer via another memory copy. That’s a lot of memory copies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;After using the Peer API&lt;/strong&gt;, on the sender’s side, EFA will check to see if SHM is enabled, and if the destination is on the same instance. If that’s true, EFA will immediately forward the &lt;code&gt;fi_tsend()&lt;/code&gt; request from MPI to SHM. On the receiver side, SHM copies the received message directly to the application buffer without the extra forwarding through the bounce buffer in the EFA layer.&lt;/p&gt; 
&lt;h3&gt;Completion events population&lt;/h3&gt; 
&lt;p&gt;The improvement on the completion events population is almost as dramatic. MPI gets a notification for the send/receive completion events by polling the completion queues.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Before using the Peer API&lt;/strong&gt;, the completion events generated by SHM when the actual send/receive operation completed needed another forwarding &lt;em&gt;inside&lt;/em&gt; the EFA provider before populating to the completion queues that the MPI polls from.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;After using the Peer API&lt;/strong&gt;, EFA and SHM just share the same completion queue allocated by MPI, and SHM can write the completion events directly to the queue.&lt;/p&gt; 
&lt;h2&gt;Performance improvements&lt;/h2&gt; 
&lt;p&gt;Let’s show you the performance improvements we saw when we ran both micro-benchmarks and real applications – the ultimate test.&lt;/p&gt; 
&lt;p&gt;For micro benchmarks, we use the &lt;a href="https://mvapich.cse.ohio-state.edu/benchmarks/"&gt;OSU Latency benchmark&lt;/a&gt; to measure the latency of point-to-point comms for two ranks, and OSU’s &lt;code&gt;MPI_Alltoallw&lt;/code&gt; test to measure the latency for collective comms.&lt;/p&gt; 
&lt;p&gt;For a real-world application, we ran &lt;a href="https://openfoam.org/"&gt;OpenFOAM&lt;/a&gt; with a 4M cell motorbike model.&lt;/p&gt; 
&lt;p&gt;In our testing, we used Open MPI 4.1.5 with two different versions of libfabric: 1.18.1 (&lt;em&gt;before&lt;/em&gt; the Peer API), and 1.19.0 (&lt;em&gt;after&lt;/em&gt; implementing Peer API).&lt;/p&gt; 
&lt;h3&gt;OSU latency&lt;/h3&gt; 
&lt;p&gt;This is where the improvements from our combined efforts are most visible – by measuring the point-to-point (two-rank) MPI send and receive ping-pong latency.&lt;/p&gt; 
&lt;p&gt;Figure 5 shows the OSU latency benchmark when running Open MPI with libfabric 1.18.1 (before using Peer API), and again with the newer libfabric 1.19.0 (after implementing the Peer API). We ran all these tests on two ranks on two different instance types: An hpc6a.48xlarge built around an x86 architecture, and the hpc7g.32xlarge, which is built using the AWS Graviton 3E – an Arm64 architecture.&lt;/p&gt; 
&lt;div id="attachment_2988" style="width: 1090px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2988" loading="lazy" class="size-full wp-image-2988" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/23/CleanShot-2023-10-23-at-18.41.29.png" alt="Figure 5 - Peer to peer intra-node MPI ping-pong latency measurements, before and after we implemented the Peering API in our libfabric providers. Latency approximately halved, which is a dramatic improvement." width="1080" height="463"&gt;
 &lt;p id="caption-attachment-2988" class="wp-caption-text"&gt;Figure 5 – Peer to peer intra-node MPI ping-pong latency measurements, before and after we implemented the Peering API in our libfabric providers. Latency approximately halved, which is a dramatic improvement.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The latency we measured approximately halved – a dramatic result, but in line with what we should expect when we removed all the unnecessary memory copies and method calls by using the Peer API, and the improvements in SHM itself.&lt;/p&gt; 
&lt;p&gt;The work continues in this area: the gap between libfabric 1.19.0 and the other private implementations will likely be reduced further – our teams are working together with the libfabric community to optimize the SHM provider further, with a goal of making it comparable to MPICH. This is a space to watch.&lt;/p&gt; 
&lt;h3&gt;OSU all-to-all communication&lt;/h3&gt; 
&lt;p&gt;To see the impact on MPI &lt;em&gt;collective operations&lt;/em&gt;, we benchmarked &lt;code&gt;MPI_Alltoallw&lt;/code&gt; – well known for stressing MPI communications.&lt;/p&gt; 
&lt;div id="attachment_2989" style="width: 1057px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2989" loading="lazy" class="size-full wp-image-2989" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/23/CleanShot-2023-10-23-at-18.42.06.png" alt="Figure 6 - The impact on collective operations. Latency dropped by between a third (in the case of the Hpc6a nodes) and a half (for Hpc7g)." width="1047" height="464"&gt;
 &lt;p id="caption-attachment-2989" class="wp-caption-text"&gt;Figure 6 – The impact on collective operations. Latency dropped by between a third (in the case of the Hpc6a nodes) and a half (for Hpc7g).&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 6 shows the results again for running Open MPI with libfabric 1.18.1 (before Peer API), libfabric 1.19.0 (after Peer API). This time we used 64 ranks and conducted the tests as before: using Hpc6a and Hpc7g instances.&lt;/p&gt; 
&lt;p&gt;The impact was significant. On Hpc6a, the latency fell generally by about a third. For Hpc7g, it fell by half.&lt;/p&gt; 
&lt;h3&gt;OpenFOAM motorbike 4M case&lt;/h3&gt; 
&lt;p&gt;The real test of our work is always an actual application that a customer runs: micro-benchmarks can only tell you so much. We ran the same &lt;em&gt;OpenFOAM motorBike 4M cell&lt;/em&gt; case on 4 x hpc6a.48xlarge (96 ranks per node) and 6 x hpc7g.16xlarge (64 ranks per node) – 386 total ranks in both cases. And in this case, we only tested with Open MPI, but with both our newer and older libfabric versions.&lt;/p&gt; 
&lt;p&gt;We measured &lt;em&gt;runs per day&lt;/em&gt; to focus on what a customer will care about – the pace at which they can results from different CFD models. The runs showed around a 10% improvement from libfabric 1.18.1 to libfabric 1.19.0 across both instance types. We’ve graphed this in Figure 7.&lt;/p&gt; 
&lt;div id="attachment_2990" style="width: 642px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2990" loading="lazy" class="size-full wp-image-2990" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/23/CleanShot-2023-10-23-at-18.42.58.png" alt="Figure 7 - Runs per day for our OpenFOAM workload across two instance types, both before and after Peer API was used. Both show around 10% improvement from the new methods." width="632" height="483"&gt;
 &lt;p id="caption-attachment-2990" class="wp-caption-text"&gt;Figure 7 – Runs per day for our OpenFOAM workload across two instance types, both before and after Peer API was used. Both show around 10% improvement from the new methods.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;How to get this new code for your workloads&lt;/h2&gt; 
&lt;p&gt;The improvements we’ve shown here are already in the main branch of the &lt;a href="https://github.com/ofiwg/libfabric"&gt;OFIWG/libfabric GitHub repo&lt;/a&gt; and they’re part of libfabric 1.19, which was released at the end of August. We ingested that into EFA installer 1.27.0, which also shipped recently. You can always &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start.html"&gt;check our documentation&lt;/a&gt; to find out how to get the latest version.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;The enhancements we’ve described in this post should make everyone’s lives easier. In most cases, the intra-node latency between ranks using Open MPI will drop by around a half – without any code changes in the end user application. In our real-world test using a CFD benchmark, we saw a 10% performance gain.&lt;/p&gt; 
&lt;p&gt;We think there are two significant takeaways from this for the HPC community.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The first&lt;/strong&gt; is that our engineering teams are always looking for ways to boost performance. Sometimes that happens in hardware, very often firmware, and – like today’s post – software, too. When we find those ways, we’ll roll them out as quick as we can, and they’ll form part of the pattern that cloud HPC users have come to expect: things just get faster (and easier) over time, without compromising security.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The second&lt;/strong&gt; is that those same engineers work collaboratively with their counterparts at partners (and competitors, too) to make things better, and faster – for everyone. This collaborative angle, underscores a truth that innovation thrives in shared effort rather than isolated silos. We’re thankful to have been able to work with so many dedicated engineers from around the world to bring these results to you today.&lt;/p&gt; 
&lt;p&gt;Reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt; if you have any questions – or if you want to find a way to contribute, too.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Unpacking the power of agent-based models for environmental and socio-economic impact</title>
		<link>https://aws.amazon.com/blogs/hpc/unpacking-the-power-of-agent-based-models-for-environmental-and-socio-economic-impact/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Tue, 24 Oct 2023 13:16:52 +0000</pubDate>
				<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">7d5dc1a9167d60992b36e694dc5e534206fd493f</guid>

					<description>Today we’ll help you understand agent-based models and their potential to impact the environment and socio-economic systems. We'll explore a few use cases and show how ABMs can improve our comprehension of environmental and social decision making.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="size-full wp-image-2848 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/08/boofla88_conways_game_of_life_as_a_concept_3D_close_zoom_photo__61aa6916-7a48-466e-93c7-00b102db121d.png" alt="" width="380" height="212"&gt;&lt;em&gt;This post was contributed by Namid Stillman, Quantitative Researcher at Simudyne, and Sam Bydlon and Ilan Gleiser from the Global Impact Computing team at AWS.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Agent-based modeling (ABM) is a powerful computational modeling approach that centers on the simulation of interactions between autonomous agents to understand the behavior of the broader system and its governing principles. Agent-based simulation can be traced back all the way to the mid-20&lt;sup&gt;th&lt;/sup&gt; century and John von Neumann’s universal constructor, but due to the large computational requirements of running agent-based simulations widespread development and application did not come until the 1990s.&lt;/p&gt; 
&lt;p&gt;Since then, ABMs have evolved and gained prominence in helping us model and understand complex systems by representing the behaviors and interactions between agents. With the emergence of cloud technology, in particular high-performance computing services on AWS, agent-based simulation is poised to enter a new era of scale and accessibility. By developing simulations that mimic systems such as economies and scaling the number of agents to be comparable to reality, we can explore hypothetical scenarios that would not be possible, or might be too risky, to test in the real world.&lt;/p&gt; 
&lt;p&gt;With the drop in computing costs and the democratization of HPC on AWS, ABMs have become a significant tool for analyzing the complexities of the environmental and socio-economic impacts of our decisions. By simulating interactions between individual ‘agents’, like households, governments, firms, trees, and vehicles, for example, ABMs can grant a deeper insight into effects of economic policy decisions, changes to supply chains, urban plastics flow, and climate risks, among others.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll help you understand agent-based models and their potential to impact the environment and socio-economic systems. We’ll explore a few use cases of ABMs and show how they can improve our comprehension of environmental and social decision making.&lt;/p&gt; 
&lt;div id="attachment_2837" style="width: 989px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2837" loading="lazy" class="size-full wp-image-2837" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/08/CleanShot-2023-09-08-at-11.29.07.png" alt="Fig 1: Examples of Agent-Based Model simulations available from Simudyne SDK on AWS HPC " width="979" height="332"&gt;
 &lt;p id="caption-attachment-2837" class="wp-caption-text"&gt;Fig 1: Examples of Agent-Based Model simulations available from Simudyne SDK on AWS HPC&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Economic Policy Simulation&lt;/h2&gt; 
&lt;p&gt;Economic policy simulation is a crucial application of ABMs for understanding the complex dynamics of social and economic systems. ABMs provide a unique approach to modeling the interactions between individuals and institutions within an economy, allowing for a more realistic representation of economic behaviors and policy outcomes.&lt;/p&gt; 
&lt;p&gt;In stark contrast to traditional methods of economic modeling, ABMs revolutionize the way we analyze and model economic scenarios. Unlike classical approaches that analyze macroeconomic trends and assume the homogeneity of individuals, ABMs are built as bottom-up models that capture the intricate heterogeneity and complexity of human decision-making processes. This approach enables businesses, policymakers, and economists to simulate a broad range of economic models and analyze their potential impacts, giving them an edge over classical economic methods. ABMs are also useful for evaluating the outcomes of different policy interventions and projecting probable future scenarios, thereby improving policymaking in many domains.&lt;/p&gt; 
&lt;p&gt;One area where ABMs have been particularly valuable is in the study of macroeconomic policy, which we described in a previous &lt;a href="https://aws.amazon.com/blogs/hpc/rigor-and-flexibility-the-benefits-of-agent-based-computational-economics/"&gt;post&lt;/a&gt;. ABMs can simulate the effects of fiscal and monetary policy – like changes in tax rates, government spending, or interest rates – on key economic variables like income distribution, inflation, and GDP growth. This allows policymakers to simulate the potential impacts of policy changes before implementing them in the real world, making for more informed decisions. For instance, policy makers can study the impact of introducing a universal basic income on the distribution of wealth among agents in an economy.&lt;/p&gt; 
&lt;p&gt;ABMs can also capture the dynamics of financial markets and banking systems, providing insights into the effects of regulatory policies on financial stability and systemic risk. By simulating the interactions between banks, investors, and borrowers, ABMs can shed light on the transmission channels through which shocks (like climate events) propagate within the financial system, helping policymakers design more robust regulatory frameworks.&lt;/p&gt; 
&lt;p&gt;As the science of economic agent-based modeling advances, incorporating reinforcement learning (RL) agents into simulations is becoming increasingly popular because the technique allows agents to learn from experience and make better decisions. In an RL framework, agents initially operate by trial and error but are rewarded or punished based on their actions. This means the agents learn to make optimal decisions under uncertain conditions, mimicking the way humans learn. Incorporating reinforcement learning into agent-based simulations can enhance the predictive power of the simulations through these feedback loops, where agents can learn and adjust their behavior based on past outcomes and their own &amp;nbsp;&amp;nbsp;mistakes. With each iteration, agents become more adept at predicting future outcomes and choosing the best course of action to achieve their goals.&lt;/p&gt; 
&lt;p&gt;The scalability and computational power of cloud computing platforms like AWS have made it possible to build ABM simulations for economic policy research. AWS HPC services – AWS Batch and AWS ParallelCluster – allow researchers to run complex and computationally-intensive ABMs. This means researchers can simulate populations with millions of individuals and firms, where individuals can have differences in preferences, access to information and learning rates. They can also leverage these services to more efficiently explore a broader range of policy scenarios, conduct sensitivity analysis, and perform parameter calibration. Since they can distribute simulations across multiple virtual machines, researchers can reduce the time required for running simulations, accelerating the research process and enabling faster policy evaluation.&lt;/p&gt; 
&lt;h2&gt;Supply-Chain Decarbonization&lt;/h2&gt; 
&lt;p&gt;Supply-chain decarbonization is a critical aspect of sustainability efforts of businesses and policymakers who are aiming to reduce the greenhouse gas emissions associated with the production, transportation, and distribution of goods and services. The transition to a low-carbon economy requires significant changes in supply-chain operations and practices. In fact, many firms and governments are aiming for net-zero carbon emissions by 2040, so ABMs can play a crucial role supporting these firms to accurately model and optimize their supply chains and perhaps even achieving those targets sooner.&lt;/p&gt; 
&lt;p&gt;ABMs enable businesses and policymakers to simulate the complex interactions between the various actors within a supply chain: manufacturers, suppliers, logistics providers, and consumers. By modeling the behavior and decision-making processes of these actors, ABMs can help identify opportunities and strategies for decarbonizing supply chains while minimizing disruptions and maximizing economic efficiency.&lt;/p&gt; 
&lt;p&gt;One of the key benefits of ABMs for supply-chain decarbonization is their ability to capture the heterogeneity and dynamics of &lt;em&gt;real-world&lt;/em&gt; supply chains. Each agent in a supply chain has different characteristics, capabilities, and goals, and ABMs can represent this diversity. This lets companies analyze the impact of different agents’ behaviors and decision-making processes on the overall carbon footprint of the supply chain. For example, &amp;nbsp;Amazon can model their entire supply chain so that every facility (like inbound cross-docking facilities, or fulfillment centers) are modelled as agents. Each facility agent will have its own unique parameters – just like in the real world. Millions, or billions of products are then tracked moving through the supply chain. Various scenarios and policy changes that are aimed at reducing costs or carbon footprint can be simulated safely to identify the policies that meet the desired targets without sacrificing key business metrics, like on time delivery.&lt;/p&gt; 
&lt;p&gt;For example, ABMs can simulate the effects of adopting more sustainable manufacturing processes or using renewable energy sources in transportation. By modeling the interactions between manufacturers, suppliers, and logistics providers, supply chain optimization teams can use their supply chain simulator to identify potential bottlenecks, trade-offs, and synergies that arise from different decarbonization strategies.&lt;/p&gt; 
&lt;p&gt;ABMs can also capture the influence of external factors on supply-chain decarbonization, like government tax policies, consumer preferences, and technological advancements. They can simulate the impact of policy interventions, too – such as carbon pricing, or subsidies for renewable energy – on supply-chain emissions. By incorporating these factors into the simulation, ABMs can provide valuable insights into the potential effectiveness and unintended consequences of different policy options.&lt;/p&gt; 
&lt;p&gt;To effectively run large scale ABM simulations for supply-chain decarbonization, HPC capabilities, such as those provided by AWS, are essential. The vast scale and intricacy of supply chains necessitate the use of large-scale simulations, which enable researchers to analyze data at various levels of granularity, ranging from individual warehouses up to the national and even international distribution networks. This allows for a more comprehensive evaluation of different decarbonization strategies, identifying those that are most effective and practical in different contexts. At the national and international level, the use of HPC on AWS is essential for providing the necessary scale of compute power to integrate these granular details and achieve accurate and actionable insights. These technologies are empowering supply chain optimization teams to make critical advances towards more sustainable and resilient supply chains that benefit society, and the planet, too.&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;strong&gt;&lt;em&gt;“Overall, ABMs along with HPC is a novel approach to solving supply chain related problems and to decarbonize the supply chain by optimizing it.”&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;strong&gt;&lt;em&gt;Siva Veluchamy, &lt;/em&gt;&lt;/strong&gt;Sr. Simulation Scientist, Amazon World-Wide Design Engineering&lt;/p&gt; 
&lt;h2&gt;Plastic Flow Simulations&lt;/h2&gt; 
&lt;p&gt;Plastic flow simulations are an important tool in understanding the movement and distribution of plastic waste in the environment. According to the UN, “&lt;em&gt;every year 19-23 million tonnes of plastic waste leaks into aquatic ecosystems, &lt;/em&gt;&lt;a href="mailto:https://www.unep.org/plastic-pollution"&gt;&lt;em&gt;polluting lakes, rivers and seas&lt;/em&gt;&lt;/a&gt;“. There is increasing concern about plastic pollution with 175 countries agreeing to implement legally binding guidelines by 2024. Given this, and plastic’s impact on ecosystems and human health, it’s crucial to develop strategies and policies that effectively manage and reduce plastic waste.&lt;/p&gt; 
&lt;p&gt;ABMs offer a unique approach to simulating plastic flow by modeling the behavior and interactions of various agents involved in the plastic lifecycle. These agents can include manufacturers, consumers, waste management facilities, and natural processes like weather patterns and ocean currents.&lt;/p&gt; 
&lt;p&gt;According to researchers at &lt;a href="mailto:mailto:https://www.iaria.org/conferences2021/filesSIMUL21/50033_SIMUL.pdf"&gt;Purdue University&lt;/a&gt; and the &lt;a href="mailto:https://doi.org/10.1016/j.procir.2020.01.083"&gt;National University of Singapore&lt;/a&gt;, one of the key benefits of ABMs for plastic flow simulations is their ability to capture the complex and dynamic nature of plastic waste movement. Plastic waste can travel through multiple pathways, like rivers, wind, and ocean currents, and can accumulate in place like beaches, riversides, or ocean gyres. ABMs can simulate these movement patterns and help identify areas of high plastic concentration, letting policymakers target their efforts for maximum impact.&lt;/p&gt; 
&lt;p&gt;As Purdue researchers have shown, using data from the American Chemistry Council and the National Association for PET Container Resources, ABMs can also simulate the behavior of different actors within the plastic supply chain and waste management system. This includes modeling consumer behavior, like plastic consumption patterns and recycling habits, and the operations of waste management facilities and their capacity to handle plastic waste. By incorporating these factors into the simulation, ABMs can provide insights into the effectiveness of different interventions, such as implementing recycling programs or reducing single-use plastic consumption.&lt;/p&gt; 
&lt;h2&gt;Green Urban Planning Simulation&lt;/h2&gt; 
&lt;p&gt;The Green Urban Scenarios simulator (&lt;a href="https://run.greenurbanscenarios.com/"&gt;GUS&lt;/a&gt;), powered by AWS HPC, is a digital twin of a city that can be used to run environmental scenarios. The GUS simulator was launched by Lucid Minds, a start-up company from Germany, and uses agent-based modeling on top of a digital twin of proposed green urban projects, such as tree planting and maintenance in a city, to understand the effects of a project before it is implemented. The simulators allows urban planners to explore potential impacts of proposed projects, including the amount of carbon sequestered, or the quantity of pollution that might be removed from the local environment, or the ability of tree planting to retain water and mitigate the impacts of storms.&lt;/p&gt; 
&lt;h2&gt;Circular Economy Simulations&lt;/h2&gt; 
&lt;p&gt;A circular economy is a system where the goal is to minimize waste and pollution by keeping materials in use for as long as possible. This involves a shift away from the traditional linear economic model of “take, make, use, and dispose” and towards a more circular approach. As outlined by the University of Cambridge Judge Business School, the goal of a circular economy “is to put back into the system everything relating to production, distribution and consumption, in order to extract as much value as possible from the resources and materials we utilize.”&lt;/p&gt; 
&lt;p&gt;Circular economy simulations are a critical tool in understanding the potential impact of circular economy strategies on social and environmental systems. Using ABMs, businesses and policymakers can analyze the effects of different circular economy strategies on the overall sustainability of a system.&lt;/p&gt; 
&lt;p&gt;ABMs let organizations model the interactions between various actors and factors involved in circular economy initiatives, like businesses, consumers, waste management systems, and policymakers. By simulating the behavior and decision-making processes of these actors, ABMs can help evaluate the effectiveness of different circular economy strategies in reducing resource consumption, minimizing waste generation, and promoting sustainable economic growth. This allows researchers to explore a wider range of circular economy scenarios, evaluate different strategies, and identify the most effective interventions for transitioning to a sustainable and regenerative economic system.&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;strong&gt;&lt;em&gt;“Agent Based Modeling (ABM) has been instrumental in several circular economy analysis projects at NREL. It has helped us to identify the potential barriers and enablers to &lt;/em&gt;&lt;/strong&gt;&lt;a href="https://www.nature.com/articles/s41560-021-00888-5"&gt;&lt;strong&gt;&lt;em&gt;increasing the circularity of renewable energy technologies&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;em&gt;, such as &lt;/em&gt;&lt;/strong&gt;&lt;a href="mailto:https://www.nrel.gov/docs/fy23osti/84141.pdf"&gt;&lt;strong&gt;&lt;em&gt;wind turbines and photovoltaic panels&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;em&gt;.”&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;strong&gt;Julien Walzberg&lt;/strong&gt;, Ph.D., Researcher, National Renewable Energy Laboratory&lt;/p&gt; 
&lt;h2&gt;Simulating climate risk scenarios&lt;/h2&gt; 
&lt;p&gt;As the world experiences increasingly severe and frequent climate events, all businesses will be affected to some degree. Simulations provide a powerful tool for decision makers to evaluate the range of possible scenarios they may face and prepare accordingly. More importantly, simulations give insights into the cost of inaction, like failing to invest in supply chain resilience or promoting recycling schemes. &amp;nbsp;Agent-based models extend the impact of simulations to human-dependent systems like logistic networks, waste management facilities, or even entire economies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Simudyne&lt;/strong&gt;, a provider of simulation technologies, is a company leading the way in this space. They have developed a platform and software development kit (SDK) that uses patented graph-based computing to allow businesses to simulate different aspects of their operations using ABMs. These models range from national supply chains to firm-specific exposure to national climate shocks. Simulations of the supply chain are designed to account for various factors like different suppliers, transportation routes, and inventory levels. This granular view provides a comprehensive understanding of how different sections of the network are affected by extreme weather events and can be used to identify steps towards decarbonisation in line with net zero goals.&lt;/p&gt; 
&lt;p&gt;Alternatively, Simudyne can model shocks at the scale of an entire economy, the kind caused by events like flooding or heatwaves. They can also account for government incentive schemes that reward early adopters in green innovation to develop new production processes and technologies with the explicit aim of reducing environmental risks. These models give insights into key vulnerabilities at the scale of the sector or an individual firm and options for offsetting these risks. This provides a valuable tool for business leaders and policy makers alike.&lt;/p&gt; 
&lt;p&gt;By simulating climate risk events with ABMs, companies like Deloitte can make better-informed decisions on every aspect of their business, quickly assess the financial consequences of a risk scenario, and respond in a more coordinated and efficient manner. This means that companies can develop contingency plans for managing unexpected climate events rather than be caught off guard.&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;strong&gt;&lt;em&gt;“Agent-based modelling gives us a clear view of the interplay between climate change and our economies. It’s a tool that sharpens our understanding, helping us pinpoint inefficiencies in supply chains, reduce costs, and cut carbon emissions. With it, we can make better decisions for a safer world.”&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p style="text-align: right"&gt;&lt;strong&gt;Justin Lyon&lt;/strong&gt;, CEO, Simudyne&lt;/p&gt; 
&lt;h2&gt;Leveraging the Simudyne SDK with AWS HPC services&lt;/h2&gt; 
&lt;p&gt;Developing agent-based models can be difficult both in terms of model construction and deployment. The Simudyne SDK is a Java-based simulation engine that makes model development easy and accessible, meaning businesses can spend more time understanding the results rather than developing them. Importantly, the Simudyne SDK leverages patented graph-computation to efficiently scale ABMs from tens to millions of agents. This allows for huge and complex systems to be modelled and forecast, such as simulating three weeks of nationwide supply chains with hundreds of millions of products flowing through the supply chain every week at the resolution of a single item in less than an hour.&lt;/p&gt; 
&lt;p&gt;One of the central benefits of the Simudyne SDK is its flexibility. The SDK can be used to simulate supply chains, customer networks, fast moving consumer goods, or even entire economies. However, not all simulations are alike and identifying the right computational architecture depends on understanding&amp;nbsp;the specifics to each model. The number of agents in the simulation, the type of data that is passed between agents, the size or amount of data being passed, how the simulation is used by other processes such as ML workflows, and how both input and output data is accessed, are all factors that impact compute requirements. For now, let’s go through some of these considerations in turn.&lt;/p&gt; 
&lt;p&gt;One of biggest considerations when building agent-based models is understanding how many agents will be in the simulation and how they interact. This relates to the agent interaction network, the communication framework for messages to be passed between agents. Some interaction networks can be very simple, such as financial exchanges which have a `star-like’ interaction network (all agents link to the market and not to each other) or they can be very complex, like a social network, where links between agents are dynamic, changing over time. The Simudyne SDK’s graph-based computational approach efficiently simulates millions of agents, allowing users to easily simulate real-world systems.&lt;/p&gt; 
&lt;p&gt;Just as critical as the number of agents in the simulations is the type of data that passes between them. The Simudyne SDK uses a message passing framework to distribute information across the simulations. The size of the messages drives the amount of memory needed to run the simulations. Whether there are several million agents passing a small amount of data or tens of agents passing large volumes, memory consideration is key to enabling efficient scaling of the simulation. AWS supports high-performance high-memory computing resources. When deciding on which EC2 instances to use, it is critical that there is sufficient memory for each instance. Simulation profiling and benchmarking can help determine the amount of memory required.&lt;/p&gt; 
&lt;p&gt;A simulation is only as useful as how it is used. Some simulations, like models of an entire economy, are standalone. They can be used for economic forecasting, highlighting the impact of shocks to the economy such as rising inflation. Other simulations, like supply chain models, can be used to chart the best course of action. By running multiple simulations with different supply chain structures (for example, choosing many small distributors or a few large ones), strategies can be compared, and an optimal structure can be identified. Optimization modules can find optimal strategies in an efficient way.&lt;/p&gt; 
&lt;p&gt;When deciding on how to structure a simulation pipeline, it’s imperative that the data fed into the simulation is well described and understood. Related considerations include the format, frequency and mechanism by which data is passed to the simulation. For example, is the data a static source that is kept in an Amazon S3 bucket? Or is it continually updated, using real-time data streaming from &lt;a href="https://aws.amazon.com/kinesis/"&gt;Amazon Kinesis? &lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Similarly, the output of the data must be properly structured to fit into other downstream tasks. If the data is required for reports, then a simple CSV or text file might suffice. More complex data pipelines might require parquet or H5 files which can be easily read by other code sources, such as in Python or C/C++ scripts.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;With the rise of HPC on AWS, agent-based modeling has become an even more powerful tool for understanding complex systems and their impact on our environment and society. The ability to simulate the behaviors and interactions of individual agents has led to deeper insights into the effects of economic policy decisions, urban plastics flow, and climate risks. ABMs have democratized the process of analyzing environmental and socio-economic impacts, making it easier and more affordable to understand the complex interactions that drive our world. AWS’ HPC services and wide selection of instance types make it an excellent tool to harness the power of ABMs and help us make better decisions for the future.&lt;/p&gt; 
&lt;p&gt;Looking to learn more about how to use Simudyne’s SDK and AWS HPC resources to solve your current and future business problems? Simudyne has a team of expert simulation engineers ready to help. Reach out today to find out more. For an evaluation of your ABM simulation workloads, please &lt;a href="mailto:ask-hpc@amazon.com"&gt;reach out to&lt;/a&gt; the AWS Global Impact Computing Team and we will schedule a discovery session with you.&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/08/CleanShot-2023-09-08-at-11.52.13.png" alt="Namid Stillman" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Namid Stillman&lt;/h3&gt; 
  &lt;p&gt;Dr. Namid Stillman is a Quantitative Researcher at Simudyne. He has a background in multi-disciplinary research, including in fields of material science, cell migration and nanotechnology, with a focus on developing interpretable AI method for scientific research. He is the author of the GNNs in Action textbook from Manning Press and co-organizer of the Simulation-based Science interest group at the Alan Turing Institute.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Why you should use Fargate with AWS Batch for your serverless batch architectures</title>
		<link>https://aws.amazon.com/blogs/hpc/why-use-fargate-with-aws-batch-for-serverless-batch-compute/</link>
		
		<dc:creator><![CDATA[Angel Pizarro]]></dc:creator>
		<pubDate>Thu, 19 Oct 2023 16:35:41 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS Fargate]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Serverless]]></category>
		<guid isPermaLink="false">ed6ef3c81c5346f412fd19dd442308e952baeb55</guid>

					<description>AWS Batch recently added support for Graviton and Windows containers on Fargate. Read about how these and other features like large task sizes and configurable local storage make AWS Batch on Fargate a fantastic serverless solution for your batch workloads.</description>
										<content:encoded>&lt;p&gt;The AWS Batch team recently &lt;a href="https://aws.amazon.com/about-aws/whats-new/2023/08/aws-batch-fargate-linux-arm64-windows-x86-containers-console/"&gt;launched&lt;/a&gt; support for Graviton and Windows containers running on AWS Fargate resources. Combine that announcement with other recent additions such as larger task sizes and configurable local storage and Fargate becomes an &lt;em&gt;even better&lt;/em&gt; serverless solution for your batch and asynchronous workloads.&lt;/p&gt; 
&lt;p&gt;Let’s take a look at the features that make Fargate a great choice for your small to medium scale AWS Batch environments, starting with the most recent announcements.&lt;/p&gt; 
&lt;h2&gt;Graviton resources&lt;/h2&gt; 
&lt;p&gt;Previously, Batch on Fargate only supported running containers that leverage an x86 CPU architecture, even though Fargate itself was able to run tasks using Linux containers built for the Graviton2 Arm architecture. We closed that gap with this release and you can now leverage Graviton2 which delivers 2-3.5 times better CPU performance per watt than any other processors of the same generation for your Batch jobs, making them a sustainable choice. They also have very capable price/performance characteristics and so are idea for analyzing data, logs, or other batch processing. Fargate does not yet support Graviton3.&lt;/p&gt; 
&lt;p&gt;The best way to find out whether x86 or aarch64 resources give you better price/performance is to try them out on your own application. You can leverage &lt;a href="https://aws.amazon.com/codepipeline/"&gt;AWS CodePipeline&lt;/a&gt; to build multi-architecture container images and store them in &lt;a href="https://aws.amazon.com/blogs/containers/introducing-multi-architecture-container-images-for-amazon-ecr/"&gt;Amazon Elastic Container Registry (Amazon ECR)&lt;/a&gt;. If you need some guidance for building multi-architecture container images, this AWS samples &lt;a href="https://github.com/aws-samples/aws-multiarch-container-build-pipeline"&gt;GitHub repository&lt;/a&gt; is a good place to start.&lt;/p&gt; 
&lt;p&gt;To leverage Fargate Graviton resources, you set the job definition’s runtimePlatform.cpuArchitecture parameter to ARM64. Then submit a job to a job queue that is configured to use Fargate resources. Graviton with Fargate is available in most AWS Regions, but there are some limitations for you should consider such as being limited to Linux containers. The full set of considerations are outlined in the documentation in &lt;a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-arm64.html"&gt;&lt;em&gt;Working with 64-bit ARM workloads on Amazon ECS&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Windows containers&lt;/h2&gt; 
&lt;p&gt;Since its release, AWS Batch has only supported running containers on top of the Linux operating system. Customers could build against the .NET Core for Linux, but not native Windows containers. Those containers are required to run Windows-only applications. Windows Batch jobs can also automate tasks that would benefit from a Windows environment, for example when you want to integrate with Microsoft Active Directory. Now customers can leverage the runtimePlatform.operatingSystemFamily parameter to designate that the container should run on a variety of &lt;a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform-windows-fargate.html"&gt;Windows Server releases supported by Fargate&lt;/a&gt;. With Fargate taking care of the licensing, running batch processing jobs on Windows is simpler than managing your own licensing for the ephemeral resources.&lt;/p&gt; 
&lt;p&gt;One thing to keep in mind when working with Windows containers is the larger size of the windows base layer compared to most Linux versions. Even with the recent Server Core image size reduction introduced with Windows Core 2022, at the time of writing it’s still 3.91GB. Additionally, you need to ensure &lt;a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform-windows-fargate.html"&gt;compatibility&lt;/a&gt; between the Windows version of the job and the Windows version used on the Batch compute environment. For a full set of considerations, refer to the documentation on using &lt;a href="https://docs.aws.amazon.com/AmazonECS/latest/userguide/what-is-fargate.html#windows-considerations"&gt;Windows containers on AWS Fargate&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Lastly, we recommend that you map Windows and Linux compute environments to platform-specific job queues to avoid trying to run jobs using the wrong operating system.&lt;/p&gt; 
&lt;h2&gt;Larger task sizes&lt;/h2&gt; 
&lt;p&gt;Last October &lt;a href="https://aws.amazon.com/about-aws/whats-new/2022/10/aws-batch-increases-compute-memory-resource-configurations-fargate-type-jobs-4x/"&gt;we announced&lt;/a&gt; the ability to launch larger Fargate type jobs that use up to 16 vCPUs and up to 120 GiB of memory. This is approximately a 4x increase from previous limits.&lt;/p&gt; 
&lt;p&gt;These larger task sizes enable you to run more compute-heavy and/or memory-intensive applications like machine learning inference, scientific modeling, and distributed analytics on Fargate. Larger vCPU and memory options may also make migration to serverless container compute simpler for jobs that need more compute resources and cannot be easily re-architected into smaller sized containers.&lt;/p&gt; 
&lt;p&gt;To take advantage of these larger task sizes, set the appropriate values for the resourceRequirements parameter of your containerProperties. You’ll want to pay attention to the &lt;a href="https://docs.aws.amazon.com/batch/latest/APIReference/API_ResourceRequirement.html"&gt;API documentation&lt;/a&gt; about the valid combination of values for Fargate for VCPU and MEMORY. For example, if your value for VCPU is &lt;em&gt;1&lt;/em&gt;, then the valid values for MEMORY (in MiB) are &lt;em&gt;2048, 3072, 4096, 5120, 6144, 7168&lt;/em&gt;, or &lt;em&gt;8192&lt;/em&gt;.&lt;/p&gt; 
&lt;h2&gt;Configurable local storage volumes&lt;/h2&gt; 
&lt;p&gt;More recently, the AWS Batch team &lt;a href="https://aws.amazon.com/about-aws/whats-new/2023/03/aws-batch-configurable-ephemeral-storage-fargate/"&gt;released a feature&lt;/a&gt; that allows you to configure ephemeral storage up to 200 GiB in size on Fargate job definitions. The previous default was 20 GiB, which was not nearly enough for data-heavy processes like genomics or video processing. If you need even more storage space, you still have the option to configure a shared Amazon Elastic File System (Amazon EFS) mountpoint as part of the job definition.&lt;/p&gt; 
&lt;p&gt;To define the size of an ephemeral volume for a job, use the ephemeralStorage &lt;a href="https://docs.aws.amazon.com/batch/latest/APIReference/API_EphemeralStorage.html"&gt;parameter&lt;/a&gt; in the job definition’s containerProperties. You can set a value from 21 up to 200 GiB. Not specifying this parameter results in the default value of 20 GiB for local storage. This parameter is available only when you’re using Fargate.&lt;/p&gt; 
&lt;h2&gt;Other great Fargate features for batch workloads&lt;/h2&gt; 
&lt;p&gt;Even before adding support for larger tasks, larger local storage, and Graviton and Windows containers, Fargate had some nice advantages for running batch workloads.&lt;/p&gt; 
&lt;h3&gt;Job level cost allocation&lt;/h3&gt; 
&lt;p&gt;When running Batch jobs on EC2 resources, it can take some effort to accurately determine the compute cost of any given job that ran on the shared instance. Since Fargate jobs are metered individually, you have an easier time determining the cost of jobs as opposed to joining the metered usage against &lt;a href="https://docs.aws.amazon.com/AmazonECS/latest/userguide/usage-reports.html#task-cur"&gt;split cost allocation data for tasks&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Fargate with Amazon EC2 Spot capacity provider&lt;/h3&gt; 
&lt;p&gt;Fargate resources can leverage EC2 Spot capacity to run jobs at a discounted rate compared to the on-demand price. The usual Spot criteria apply — meaning that your jobs could be interrupted but you’ll get two-minute warning. For Fargate, the warning is sent as a task state change event to Amazon EventBridge and as a SIGTERM signal to the running task.&lt;/p&gt; 
&lt;p&gt;You can take advantage of the SIGTERM signal in your application code to gracefully exit the process, possibly checkpointing data to shared storage so that the task can start at a later time when capacity becomes available. The SIGTERM signal must be received from within the container to perform any cleanup actions. Failure to process this signal results in the task receiving a SIGKILL signal and that, in turn, may result in data loss or corruption.&lt;/p&gt; 
&lt;p&gt;Please note that Fargate Spot is not supported for&amp;nbsp;ARM64&amp;nbsp;and Windows-based containers on Fargate, but we still think it is a great option for your other Batch on Fargate workloads.&lt;/p&gt; 
&lt;h3&gt;Shared storage across jobs using Amazon Elastic File System&lt;/h3&gt; 
&lt;p&gt;Expanded local storage is great, but sometimes you need to send the output of one task as the input to another, and copying all that data around can take time (and hence money). Sometimes it is easier and faster to share data across tasks directly using a shared file system. A shared filesystem would also be advantageous for checkpointing processes for later restart, as in the case of Spot interruptions.&lt;/p&gt; 
&lt;p&gt;You can define mountpoints Amazon Elastic File System (EFS) at the job definition level. If you define two job definitions with the same mount point, they effectively have a common place to read and write data. You can read more about how to do that &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-support-for-per-job-amazon-efs-volumes-in-aws-batch/"&gt;in our “how to use EFS with AWS Batch” blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Finally, whenever you leverage shared storage across jobs and applications, you should always pay attention to how they may interact with each other. Specifically you should identify and address any security boundaries or data overwriting scenarios that may occur when separate job definitions mount the same volume.&lt;/p&gt; 
&lt;h2&gt;“Is Fargate right for me?”&lt;/h2&gt; 
&lt;p&gt;If you’re thinking about leveraging Fargate with AWS Batch, it’s worth taking a moment to consider your overall scale and throughput needs.&lt;/p&gt; 
&lt;p&gt;Batch maps a single job request to a single Fargate resource. This means that your maximum jobs-per-minute dispatch rate is limited at 500 task launches per minute. Alternatively, if you use Batch with EC2, multiple tasks are placed on running instances resulting in faster job placement.&lt;/p&gt; 
&lt;p&gt;Fargate also has a service limit defining the total number of concurrent Fargate vCPUs you can launch in a Region. Depending on the size of your jobs, it defines the number of concurrent jobs you can run. You can find your Fargate service limits using the &lt;a href="https://console.aws.amazon.com/servicequotas/home/services/fargate/quotas"&gt;AWS Service Quotas management console&lt;/a&gt;. You’ll also find a mechanism there for requesting an increase, should you need it.&lt;/p&gt; 
&lt;p&gt;Depending on your workload (e.g. the size of the tasks, duration of each job, and frequency of the jobs), it’s recommended to reach out to AWS Support in advance if you’re planning on running very large workloads using&amp;nbsp;Batch&amp;nbsp;and Fargate.&amp;nbsp;For more information on how to choose the underlying compute model, see our best practices documentation on &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/best-practices.html#bestpractice4"&gt;how to choose the right compute environment resource&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;With the release of Graviton and Windows container support, AWS Batch with Fargate has become a very capable serverless batch computing solution. It’s a great fit if you’re using AWS Batch for background, asynchronous tasks or for data processing.&lt;/p&gt; 
&lt;p&gt;If you want to try out the features mentioned in this post, log into the &lt;a href="https://console.aws.amazon.com/batch/home?"&gt;AWS Batch management console&lt;/a&gt; and try out the features yourself!&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Introducing login nodes in AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/introducing-login-nodes-in-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Austin Cherian]]></dc:creator>
		<pubDate>Wed, 18 Oct 2023 11:17:52 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[Slurm]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">b47b51c49022a980aec35361efa3990537c5c3a1</guid>

					<description>AWS ParallelCluster 3.7 now supports adding login nodes to your cluster, out of the box. Here, we'll show you how to set this up, and highlight some important tunable options for tweaking the experience.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright wp-image-2940 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/10/12/AdobeStock_593057386-expanded.png" alt="Introducing login nodes in AWS ParallelCluster" width="380" height="212"&gt;If you’re a user of a Slurm-based HPC cluster, it’s likely you interact with your cluster using a &lt;em&gt;login node&lt;/em&gt;. It’s the portal through which you access your cluster’s vast computational resources. You’ve probably used one to browse your files, submit jobs (and check on them) and compile your code.&lt;/p&gt; 
&lt;p&gt;You can do all these things using the headnode, too, but when a cluster is shared among multiple users in an enterprise or lab, someone compiling their code on the headnode can hamper other users trying to submit jobs, or just doing their own work. Some AWS ParallelCluster customers have worked around this limitation by manually creating login nodes for their users, but this involved a lot of undocumented steps and forced their admins to know about ParallelCluster’s internals.&lt;/p&gt; 
&lt;p&gt;So we’re happy to announce that AWS ParallelCluster 3.7 now supports adding login nodes to your cluster, out of the box. In this post we’ll show you an example of setting this up for a cluster, and highlight some of the more important tunable options for tweaking the experience.&lt;/p&gt; 
&lt;h2&gt;Getting started with AWS ParallelCluster Login Nodes&lt;/h2&gt; 
&lt;p&gt;Login nodes are specified in a similar way to compute nodes: as a ‘pool of nodes’ which in this case have single purpose. You can specify one pool of login nodes with as many instances as you would like to configure for your cluster.&lt;/p&gt; 
&lt;p&gt;If you want to try this feature out and explore how it works &lt;em&gt;without having to first design a cluster&lt;/em&gt;, you can use our &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/login_nodes"&gt;one-click launchable stack&lt;/a&gt; from the HPC Recipes Library (a &lt;em&gt;super useful&lt;/em&gt; resource which we described in a &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;recent post on this channel&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Or, you can follow the steps here to augment an existing configuration file you have on hand, to enable login nodes on a new cluster, or to make an update to an existing one.&lt;/p&gt; 
&lt;h3&gt;Step 1: Configure ParallelCluster with login nodes&lt;/h3&gt; 
&lt;p&gt;First, ensure that you’re using ParallelCluster 3.7.0. &amp;nbsp;(or later) which introduces this new feature. You can then create a new cluster or update an existing one with login nodes.&lt;/p&gt; 
&lt;p&gt;Enabling login nodes starts by configuring them in the YAML configuration file that describes your ParallelCluster. You can always retrieve the YAML config for a running cluster using the &lt;code&gt;pcluster-describe&lt;/code&gt; command (there’s an example in &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcluster.describe-cluster-v3.html"&gt;our documentation&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;When you defined your ComputeResources as part of &lt;code&gt;SlurmQueues&lt;/code&gt;, you specified the instance type, security groups, and several other details.&lt;/p&gt; 
&lt;p&gt;It’s similar for login nodes where you now define these parameters in a new section called &lt;code&gt;LoginNodes&lt;/code&gt;. There are a few more settings unique to login nodes like: &lt;code&gt;Count&lt;/code&gt;, which specifies the number of nodes; and the &lt;code&gt;GraceTimePeriod&lt;/code&gt;, which lets you set a &lt;em&gt;countdown timer&lt;/em&gt; for logged in users on a node when ParallelCluster is planning to stop it. The full spread of options is in our &lt;code&gt;LoginNodes&lt;/code&gt; &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/LoginNodes-v3.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The following snippet (which you can add to the end of your config file) shows how to setup a cluster with three login nodes, as part of a pool named &lt;code&gt;CFDCluster&lt;/code&gt; that act as a front door to users to login and submit fluid dynamics jobs on your cluster.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;LoginNodes:
   Pools:
    - Name: CFDCluster
      Count: 3  # Specify the number of login nodes you need
      InstanceType: t2.micro  # Choose an appropriate instance type
      Ssh:
         KeyName: CFSClusterKey # The key pair setup in your AWS account
      Networking: 
        SubnetId: 
          - subnet-XXXXXXXXX  # Specify the subnet for your login nodes
        SecurityGroups:
          - sg-XXXXXXXXX  # security groups your EC2 Login nodes will be within
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can customize the settings according to your requirements, including the instance type, subnet, and security groups. There are some additional settings you can specify like IAM roles and policies, and additional security groups. You can also define custom AMIs if you want to be more opinionated about the experience for your users when they login (for instance, by giving them access to compilers or licensed debuggers). You’ll find all these details in the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/LoginNodes-v3.html#LoginNodes-v3.properties"&gt;ParallelCluster documentation on LoginNodes properties&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 2: Update your cluster&lt;/h3&gt; 
&lt;p&gt;After you’ve finished editing your ParallelCluster config file, you’re ready to update your running cluster, or create a new one.&lt;/p&gt; 
&lt;p&gt;If you’re setting up a &lt;em&gt;new cluster&lt;/em&gt; with login nodes, use this command along with the configuration file that contains your settings:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster —cluster-configuration your_config_file_name —cluster-name your_cluster_name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you’re updating an &lt;em&gt;existing cluster&lt;/em&gt; then it’s just:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-cluster --cluster-configuration your_config_file_name --cluster-name your_cluster_name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Of course, replace &lt;code&gt;your_config_file_name&lt;/code&gt; with the name of your configuration file and &lt;code&gt;your_cluster_name&lt;/code&gt; with the name of your cluster.&lt;/p&gt; 
&lt;h3&gt;Step 3: Access your login nodes&lt;/h3&gt; 
&lt;p&gt;Once your cluster is ready, users with appropriate permissions can access the login nodes using SSH. But first they’ll need to find the address of the nodes they want to connect to.&lt;/p&gt; 
&lt;p&gt;Login nodes are provisioned with a single connection address to a Network Load Balancer (which is a feature of Elastic Load Balancer), specifically configured for the pool of login nodes. The exact address depends on the type of subnet you specified in the &lt;code&gt;LoginNodes&lt;/code&gt; pool configuration. All connection requests are managed by the Network Load Balancer using &lt;em&gt;round-robin routing&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;To retrieve the address of the single connection provisioned to access the login nodes, you can run the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcluster.describe-cluster-v3.html"&gt;pcluster describe-cluster&lt;/a&gt; command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster describe-cluster --cluster-name your_cluster_name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command will also provide more information about the &lt;em&gt;status&lt;/em&gt; of the login nodes. Here’s an example what it returns for our login nodes:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;"loginNodes": 
{ "status": "active", 
  "address": "8af2145440569xyz.us-east-1.amazonaws.com", 
  "scheme": "internet-facing|internal", 
  "healthyNodes": 3, 
  "unhealthyNodes": 0 },
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now it’s just a simple matter of SSH’ing to that address:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;ssh username@8af2145440569xyz.us-east-1.amazonaws.com&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Users can submit and manage jobs, and they’ll have access to any shared storage the cluster uses, too, so they can manage their files and see the output from their jobs.&lt;/p&gt; 
&lt;p&gt;You’ll notice that the &lt;code&gt;describe-cluster&lt;/code&gt; output also gave you some information on the status and health of your login nodes. You can get more granular information on the state, IP address, and launch time for individual login nodes in the pool, by using the &lt;code&gt;pcluster describe-cluster-instances&lt;/code&gt; command, specifying the node-type as &lt;code&gt;LoginNodes&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster describe-cluster-instances --node-type LoginNode --cluster-name your_cluster_name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Check our &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcluster.describe-cluster-instances-v3.html"&gt;documentation&lt;/a&gt; for more about this.&lt;/p&gt; 
&lt;h2&gt;Controlling the population of login nodes&lt;/h2&gt; 
&lt;p&gt;Once configured, your login nodes will keep running until you remove them from the pool.&lt;/p&gt; 
&lt;h3&gt;Adding and removing login nodes&lt;/h3&gt; 
&lt;p&gt;Adding and removing login nodes from a pool is straight forward. You set the Count parameter of the &lt;code&gt;LoginNodes&lt;/code&gt; configuration in the ParallelCluster YAML file to your desired number. Then, update your running cluster configuration using the &lt;code&gt;pcluster update-cluster command&lt;/code&gt;, as before:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-cluster --cluster-configuration your_config_file_name --cluster-name your_cluster_name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To remove all login nodes you can just set the &lt;code&gt;Count&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt; and update the cluster.&lt;/p&gt; 
&lt;h3&gt;Setting a grace-time message for users when terminating Login Nodes&lt;/h3&gt; 
&lt;p&gt;When you remove login nodes from the pool using the &lt;code&gt;Count&lt;/code&gt; parameter we just described, ParallelCluster will terminate the Amazon EC2 instances powering them. During a node’s termination, logged in users will receive terminal notifications in their SSH windows alerting them about the impending shutdown. The message will specify a grace-time period during which no new connections will be allowed, except for those from the cluster’s default user. The message is customizable by the cluster administrator from the headnode or from a login node by editing the file &lt;code&gt;/opt/parallelcluster/shared_login_nodes/loginmgtd_config.json&lt;/code&gt; . Here’s what that looks like, by default:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;{
  "termination_script_path": "/opt/parallelcluster/shared_login_nodes/loginmgtd_on_termination.sh",
  "termination_message": "The system will be terminated within 10 minutes.",
  "gracetime_period": "10"
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;AWS ParallelCluster 3.7 introduced login nodes – a powerful and flexible way to manage access and more carefully define the user experience for your HPC cluster users.&lt;/p&gt; 
&lt;p&gt;By distributing interactive user sessions across a pool of dedicated login nodes, you can improve your cluster’s performance, expand the tools available to your end users, and streamline everyone’s data access. With careful configuration and management, login nodes can become an integral part of your cloud-based HPC infrastructure, enabling efficient and scalable computing for your organization.&lt;/p&gt; 
&lt;p&gt;If you want to get started right away, you can use our &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/login_nodes"&gt;one-click launchable stack&lt;/a&gt; from the HPC Recipes Library to get a complete test environment launched quickly, which you can customize (or delete) later. This comes from the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/"&gt;HPC Recipe Library&lt;/a&gt; which can help you quickly achieve feature-rich,&amp;nbsp;&lt;em&gt;reliable&lt;/em&gt;&amp;nbsp;HPC deployments that are ready to run a diverse range of workloads – regardless of where you’re starting from.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Financial services industry HPC migrations using AWS ParallelCluster with Slurm</title>
		<link>https://aws.amazon.com/blogs/hpc/financial-services-industry-hpc-migrations-using-aws-parallelcluster-with-slurm/</link>
		
		<dc:creator><![CDATA[Vinay Arora]]></dc:creator>
		<pubDate>Tue, 10 Oct 2023 13:14:45 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[FSI]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Slurm]]></category>
		<guid isPermaLink="false">38335ed4c953ad6c7ef375c45681146e51ae92b4</guid>

					<description>In this post, we’ll walk you through how banks and other financial services firms migrate or burst their grid workloads onto AWS using AWS ParallelCluster and the Slurm scheduler.</description>
										<content:encoded>&lt;p&gt;If you’re reading this post, you’ll know that HPC is a way of solving hard problems by slicing the problem space up amongst a lot of computers. You might have also heard the term ‘&lt;em&gt;grid computing’&lt;/em&gt; in the context of financial services. This refers to centralized systems which are typically used to provide ‘utility computing’ to support HPC workloads.&lt;/p&gt; 
&lt;p&gt;In financial services these types of jobs use complex algorithms to calculate as many risk or trading scenarios in parallel as possible, bound only by the amount of compute available. Financial services firms are always keen to reduce their operational costs and look to the cloud as a way to provide the elasticity and cost benefits to do this, but also: the ability to respond rapidly to changing economic conditions and market volatility.&lt;/p&gt; 
&lt;p&gt;Customers from all the major Financial Services Industry (FSI) verticals (including banking, capital markets, and insurance) are moving their on-premises grid workloads partially, or entirely, to the cloud. These workloads typically include high volumes of short-running tasks.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk through how these firms can migrate or burst their grid workloads onto AWS using AWS ParallelCluster and the Slurm scheduler – and we’ll give you an introduction to those two packages, if you’ve not met them before.&lt;/p&gt; 
&lt;h2&gt;Loosely-coupled Grid scenarios&lt;/h2&gt; 
&lt;p&gt;Grid workloads often require hundreds of thousands, or millions, of parallel processes to complete a calculation or simulation. Generally, these jobs run on a single node, consuming one process or multiple processes with shared memory parallelization (SMP) for parallelization &lt;em&gt;within&lt;/em&gt; the node. The parallel processes, or the iterations in the simulation, are post-processed to create one solution or discovery from the simulation.&lt;/p&gt; 
&lt;p&gt;During the simulation, the operations can take place in any order, and the loss of any one node or job in a loosely coupled workload usually doesn’t delay the entire calculation. The lost work can be picked up later or omitted altogether. The nodes involved in the calculation can vary in specification and power. This gives us some hints about the types of compute that we need to run these loosely-connected simulations.&lt;/p&gt; 
&lt;h2&gt;Architectural considerations&lt;/h2&gt; 
&lt;h3&gt;Networking&lt;/h3&gt; 
&lt;p&gt;Grid processes run in parallel and don’t communicate with each other (at all) to exchange information. This means the performance of jobs isn’t impacted by latency between the nodes. This is great because it means that instead of optimizing for latency, we can instead optimize for &lt;em&gt;instance availability&lt;/em&gt;: we can spread the job out over several Availability Zones (AZs), each of which have their own capacity pools. This is especially useful when using the Amazon Elastic Compute Cloud (Amazon EC2) Spot purchasing model because the availability of Spot instances in any given AZ can be limited.&lt;/p&gt; 
&lt;p&gt;By providing AWS ParallelCluster with options about where to place the compute, we can achieve an architecture that can scale up to tens or hundreds of thousands of cores.&lt;/p&gt; 
&lt;h3&gt;Storage&lt;/h3&gt; 
&lt;p&gt;Loosely coupled jobs also have a great characteristic when working with shared storage. Typically, they need to read data in, compute on it, and then write it out all without interfering with any other job. This is the perfect use case for &lt;em&gt;object storage&lt;/em&gt;. In this post, we’ll focus on Amazon Simple Storage Service (Amazon S3) as the storage layer because it provides internet-scale storage, it’s low cost, and it scales up to hundreds of thousands of tasks working simultaneously.&lt;/p&gt; 
&lt;h3&gt;Compute&lt;/h3&gt; 
&lt;p&gt;Choosing instance types means looking at the job’s requirement for memory:vCPU ratio, availability, and architecture support for the workload. (ie x86 or aarch64). In this post, we’ll stick with x86, to provide the greatest instance availability &lt;em&gt;and compatibility&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;ParallelCluster can request instances based on what’s available and will &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/slurm-short-capacity-fail-mode-v3.html"&gt;quickly “fail-over”&lt;/a&gt; if the instances can’t be launched. For the purposes of testing we chose instances from both x86 manufacturers’ families because they share the same instruction set and by choosing both we have a larger pool to draw from.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll use &lt;em&gt;vCPUs&lt;/em&gt; instead of &lt;em&gt;cores&lt;/em&gt; because most x86 instances onAWS have hyperthreading enabled. Hyperthreading allows two virtual cores to share the same arithmetic/logic unit (ALU). This increases overall throughput by doubling the core count but deceases the individual core’s &lt;em&gt;solve-time&lt;/em&gt;. This is sometimes a controversial topic in HPC, but for grid workloads in FSI, we’ve seen that hyperthreading is a net benefit, so we’re leaving it enabled.&lt;/p&gt; 
&lt;h3&gt;Scheduling&lt;/h3&gt; 
&lt;p&gt;Running a few tasks on a single instance is easy to manage by hand, but once you scale up the number of tasks, you’ll need a scheduler to manage their lifecycle. &lt;strong&gt;Slurm&lt;/strong&gt; is an open-source job scheduler that’s optimized for scheduling both tightly-coupled &lt;em&gt;and&lt;/em&gt; loosely-coupled tasks across multiple instances. It handles queuing, execution, retries, and accounting. ParallelCluster sets up Slurm using the &lt;a href="https://slurm.schedmd.com/elastic_computing.html"&gt;Slurm cloud bursting plugin&lt;/a&gt; and manages the scaling of Amazon EC2 instances.&lt;/p&gt; 
&lt;p&gt;We recommend enabling Slurm’s &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/slurm-mem-based-scheduling-v3.html"&gt;memory based scheduling&lt;/a&gt; feature. This provides an easy way to request specific memory amounts and compute directly in the job submission, i.e. 1 vCPU and 4 GB of memory could be set in the Slurm &lt;code&gt;sbatch&lt;/code&gt; file. It also helps Slurm maximize the CPU utilization of all the cores across the fleet.&lt;/p&gt; 
&lt;h2&gt;AWS ParallelCluster&lt;/h2&gt; 
&lt;p&gt;AWS ParallelCluster is an open-source cluster management tool that makes it straightforward for you to deploy, manage, and scale Slurm-based clusters on AWS. ParallelCluster allows you to leverage the elasticity of AWS by providing an easy way to expand and contract compute queues that you define. You can use multiple instance types, in multiple Availability Zones, and create job submission queues with a lot of creative freedom.&lt;/p&gt; 
&lt;p&gt;You can also quickly spin up clusters for experimenting and prototyping. Since ParallelCluster uses a simple YAML file to define all the resources you need, the process of standing up additional clusters – for production, dev, or testing – is an automated (and secure) process.&lt;/p&gt; 
&lt;p&gt;Let’s look at a reference architecture and a tutorial that you can use to deploy this in your own AWS account.&lt;/p&gt; 
&lt;div id="attachment_2873" style="width: 1077px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2873" loading="lazy" class="size-full wp-image-2873" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/18/CleanShot-2023-09-18-at-13.16.09.png" alt="Figure 1 –The reference architecture for FSI grid-style computing on AWS using AWS ParallelCluster. The queues span over all the Availability Zones in the region and read and write data from an Amazon S3 bucket in that region." width="1067" height="490"&gt;
 &lt;p id="caption-attachment-2873" class="wp-caption-text"&gt;Figure 1 –The reference architecture for FSI grid-style computing on AWS using AWS ParallelCluster. The queues span over all the Availability Zones in the region and read and write data from an Amazon S3 bucket in that region.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Implementation details&lt;/h2&gt; 
&lt;p&gt;In the following steps, we’ll show you how to setup a cluster with a &lt;strong&gt;headnode&lt;/strong&gt; and two &lt;strong&gt;compute queues&lt;/strong&gt;. The queues span over all the Availability Zones in the region and read and write data from an Amazon S3 bucket in that region. The Slurm scheduler process (&lt;code&gt;slurmctld&lt;/code&gt;) runs on the &lt;code&gt;HeadNode&lt;/code&gt; and users can login there to submit jobs via AWS Systems Manager (SSM), using SSH, or with a remote desktop connection using DCV as shown in the reference architecture in Figure 1.&lt;/p&gt; 
&lt;p&gt;You can follow along with these steps in the &lt;a href="https://catalog.workshops.aws/fsi-slurm-aws-parallelcluster"&gt;FSI Tutorial&lt;/a&gt; hands-on lab.&lt;/p&gt; 
&lt;p&gt;First, install AWS ParallelCluster CLI or UI. We’ll assume you’re using the CLI for now. In ParallelCluster, you initiate the creation of a cluster through the CLI by specifying the location of your configuration file, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster -c cluster.yaml -n cluster-name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Your config file might look something like the this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;HeadNode:
  InstanceType: c5a.xlarge
  Networking:
    SubnetId: subnet-846f1aff
  LocalStorage:
    RootVolume:
      VolumeType: gp3
  Iam:
    AdditionalIamPolicies:
      - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
  Dcv:
    Enabled: true
  CustomActions:
    OnNodeConfigured:
      Sequence:
        - Script: &amp;gt;-
            &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh" rel="noopener noreferrer"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh&lt;/a&gt;
        - Script: &amp;gt;-
            &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/pyxis/postinstall.sh" rel="noopener noreferrer"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/pyxis/postinstall.sh&lt;/a&gt;
          Args:
            - /fsx
  Imds:
    Secured: true
Scheduling:
  Scheduler: slurm
  SlurmQueues:
    - Name: c6i
      AllocationStrategy: capacity-optimized
      ComputeResources:
        - Name: spot
          Instances:
            - InstanceType: c6i.32xlarge
            - InstanceType: c6a.32xlarge
            - InstanceType: m6i.32xlarge
            - InstanceType: m6a.32xlarge
            - InstanceType: r6i.32xlarge
            - InstanceType: r6a.32xlarge
          MinCount: 0
          MaxCount: 100
      CustomActions:
        OnNodeConfigured:
          Sequence:
            - Script: &amp;gt;-
                &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/pyxis/postinstall.sh" rel="noopener noreferrer"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/pyxis/postinstall.sh&lt;/a&gt;
              Args:
                - /fsx
      ComputeSettings:
        LocalStorage:
          RootVolume:
            VolumeType: gp3
      Networking:
        SubnetIds:
          - subnet-8b15a7c6
          - subnet-04f44e9dc1c8ee425
          - subnet-91ab80f8
        PlacementGroup:
          Enabled: false
      CapacityType: SPOT
      Iam:
        AdditionalIamPolicies:
          - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
Region: us-east-2
Imds:
  ImdsSupport: v2.0
Image:
  Os: alinux2
SharedStorage:
  - Name: Ebs0
    StorageType: Ebs
    MountDir: /shared
    EbsSettings:
      VolumeType: gp3
      DeletionPolicy: Retain
      Size: '100'
Tags:
  - Key: parallelcluster-ui
    Value: 'true'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This config has a few key sections:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;HeadNode&lt;/code&gt; – this is the Amazon EC2 instance that runs the Slurm scheduler processes (&lt;code&gt;slurmctld&lt;/code&gt;, for example). Because this instance is responsible for scaling up the cluster, scheduling jobs, and serving the config files, we recommend going with an instance like the c6i.2xlarge that has sufficient resources (8 vCPUs and 16 GB memory) to run Slurm comfortably.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;SlurmQueues&lt;/code&gt; – these will depend on your resource requirements. For example, if you have jobs that requires 1 vCPUs and 2 GB memory, set up a queue with c6i instances. These instances all meet the memory to core ratio and specifying multiples different sizes of them will ensure you’re most likely to get your desired capacity. If you require more memory, the m6i and r6i instances offer 8 GB per vCPU and 16 GB per vCPU, respectively. We recommend you offer your users a choice by putting each instance family in their own &lt;code&gt;SlurmQueue&lt;/code&gt;. In addition to diversifying the instance types by providing multiple from each family, we also recommend you provide multiple Availability Zones, because this expands the number of pools you’re drawing from and is especially important for using Amazon EC2 Spot.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Storage&lt;/code&gt; – for grid workloads there’s rarely a need for a parallel filesystem – more important is a filesystem that can serve traffic in multiple Availability Zones and scale up to the thousands of jobs that can execute concurrently. We recommend using Amazon S3 or if there’s a need for a POSIX compliant filesystem, then Amazon FSx for OpenZFS. The creation of the filesystem is out of scope for this blog, but you can learn more by reading about &lt;a href="https://aws.amazon.com/blogs/hpc/expanded-filesystems-support-in-aws-parallelcluster-3-2/"&gt;Filesystem Support in AWS ParallelCluster&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When ParallelCluster builds your cluster, it’ll attach any existing filesystems you specified in the config file. Those filesystems are typically used for applications, libraries, and users’ data. It’s important to have a separation between the filesystem and cluster configuration because it allows for easy upgrades later when ParallelCluster release a new version which you want to take advantage of. In the config file we showed, it calls for ParallelCluster itself to create an EBS-based shared file system, mounted as &lt;code&gt;/shared&lt;/code&gt;. This will be NFS-exported to the compute nodes in the cluster when they’re created.&lt;/p&gt; 
&lt;p&gt;Once your cluster is up and running, you have several ways to interact with it:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;You can login to the head node: you can use normal SSH, or connect through SSM using the attached &lt;code&gt;SSMInstanceCore&lt;/code&gt; SSM allows access to instances that are not routable through SSH, like those in a private subnet.&lt;/li&gt; 
 &lt;li&gt;You can submit jobs using the default Slurm commands like &lt;code&gt;sbatch&lt;/code&gt;, &lt;code&gt;srun&lt;/code&gt;, and then monitor with &lt;code&gt;squeue&lt;/code&gt; and &lt;code&gt;sinfo&lt;/code&gt;. See Slurm &lt;a href="https://slurm.schedmd.com/sbatch.html"&gt;sbatch documentation&lt;/a&gt; for more details on options for submitting jobs, and the syntax for the run scripts.&lt;/li&gt; 
 &lt;li&gt;When Slurm detects new jobs in the queue, it uses the &lt;a href="https://slurm.schedmd.com/elastic_computing.html"&gt;Cloud Scheduling&lt;/a&gt; plugin, which calls a &lt;code&gt;ResumeProgram&lt;/code&gt; script that ParallelCluster manages. This script, in turn, calls the Amazon EC2 Fleet API to allocate instances to the queue. The jobs run soon after the instances bootstrap (typically 2-3 minutes).&lt;/li&gt; 
 &lt;li&gt;Once all jobs are complete, the instances are left running for a short ScaleDownIdleTime, in case more jobs arrive. This defaults to 10 minutes, but you can configure it yourself by specifying a &lt;code&gt;ScaleDownIdleTime&lt;/code&gt; &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/multiple-queue-mode-slurm-user-guide-v3.html"&gt;parameter in the ParallelCluster config file&lt;/a&gt;. After this time expires, ParallelCluster will terminate the instances, and wait again for new jobs to arrive in the queues.&lt;/li&gt; 
 &lt;li&gt;If the cluster is deleted, all instances &lt;em&gt;including the head node&lt;/em&gt; will be terminated.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There’s an &lt;a href="https://catalog.workshops.aws/fsi-slurm-aws-parallelcluster/en-US"&gt;AWS Workshop available online&lt;/a&gt; to show you how to calculate the price of a financial &lt;em&gt;auto-callable option&lt;/em&gt; based on this ParallelCluster architecture. The workshop provides step-by-step guidance to create and configure ParallelCluster and to deploy the containerized workload.&lt;/p&gt; 
&lt;h2&gt;Best Practices when using ParallelCluster with Slurm&lt;/h2&gt; 
&lt;h3&gt;Smaller and heterogeneous job duration&lt;/h3&gt; 
&lt;p&gt;If your job durations are small, for example in seconds and somewhat heterogeneous, we suggest following &lt;a href="https://slurm.schedmd.com/high_throughput.html"&gt;the recommendations from SchedMD&lt;/a&gt; (the makers of Slurm) to tune the cluster for the best job throughput – measured in tasks per second. It’s possible for Slurm to scale to 500 jobs/sec – under the right conditions and with the right architecture.&lt;/p&gt; 
&lt;h3&gt;Large Scale Clusters&lt;/h3&gt; 
&lt;p&gt;If your cluster exceeds 1024 instances, we recommend that following the &lt;a href="https://slurm.schedmd.com/big_sys.html"&gt;Slurm Large Cluster Administration guide.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You’ll also want to consider your &lt;strong&gt;HeadNode&lt;/strong&gt; architecture. Using a c6i.2xlarge won’t be sufficient for the network bandwidth demanded by slurmctld when it’s communicating with all the child nodes. We recommend using a network optimized instance with &lt;em&gt;at least&lt;/em&gt; 50 Gbps of dedicated bandwidth, like the c6in.8xlarge. Remember, the HeadNode instance communicates with the compute instances &lt;em&gt;and&lt;/em&gt; serves the Slurm config file, and application binaries via NFS.&lt;/p&gt; 
&lt;p&gt;You’ll also need to review your AWS account limits, (which, we remind you, are set on a per-region basis):&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Amazon EC2 Limits&lt;/strong&gt; – make sure you increase the Amazon EC2 limits for the compute nodes you plan to use. You can review these, and request limit increases via the AWS Service Quotas console.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;EBS limits&lt;/strong&gt; – each compute node mounts a 35 GB root volume (or larger) of gp3 EBS storage. If you plan to launch more instances than your quota (which defaults to 50 TiB), you should &lt;a href="https://console.aws.amazon.com/servicequotas/home/services/ebs/quotas"&gt;increase this limit&lt;/a&gt;, too.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Route53 limit&lt;/strong&gt; – each compute node is added to a Route53 Private Hosted Zone. This has a default limit of 10,000 records. If you plan to exceed this limit, make sure to &lt;a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/DNSLimitations.html"&gt;request a limit increase&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Acadian did this – so you can you&lt;/h3&gt; 
&lt;p&gt;Acadian chose to use Slurm in AWS ParallelCluster to execute thousands of these heterogeneous jobs. This cloud-native grid solution enables them to benefit from faster and seamless compute capacity provisioning &lt;em&gt;and&lt;/em&gt; on-demand auto scaling features, resulting in optimal results.&lt;/p&gt; 
&lt;p&gt;According to Jian Pan, Head of Quantitative Systems at Acadian, “&lt;em&gt;we are able to reap immediate benefit such as maintaining consistent 4hr model runtime in AWS, comparing to 20~40hr runtime on-premise when resource is under stress&lt;/em&gt;”.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Slurm can handle a cluster that grows and shrinks – driven by demand. It’s able to this dynamically by scaling compute resources from Amazon EC2. Compute fleets can be spun up to complete the workload in the queue. When the jobs are complete, ParallelCluster scales those same fleets back down to the minimum level you set (usually zero).&lt;/p&gt; 
&lt;p&gt;If you want to try this yourself, you can follow the steps in the &lt;a href="https://catalog.workshops.aws/fsi-slurm-aws-parallelcluster"&gt;FSI Tutorial&lt;/a&gt; hands-on lab, and when you’ve done that, you can try a real application to &lt;a href="https://catalog.workshops.aws/fsi-slurm-aws-parallelcluster/en-US/02-application/03-job-submit"&gt;calculate the price&lt;/a&gt; of a financial &lt;em&gt;auto-callable option&lt;/em&gt;. Let us know how to get on – you can reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Conceptual design using generative AI and CFD simulations on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/conceptual-design-using-generative-ai-and-cfd-simulations-on-aws/</link>
		
		<dc:creator><![CDATA[Dr. Vidyasagar Ananthan]]></dc:creator>
		<pubDate>Mon, 02 Oct 2023 14:19:33 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">ebf53bce2be2ea1f1b340e195b5361045a4f1f7f</guid>

					<description>In this post we’ll show how generative AI, combined with conventional physics-based CFD can create a rapid design process to explore new design concepts in automotive and aerospace from just a single image.</description>
										<content:encoded>&lt;p&gt;In this post we’ll demonstrate how &lt;a href="https://aws.amazon.com/generative-ai/"&gt;generative AI&lt;/a&gt; techniques can be combined with conventional physics-based&amp;nbsp;&lt;a href="https://aws.amazon.com/hpc/cfd/"&gt;computational fluid dynamics&lt;/a&gt;&amp;nbsp;(CFD) simulations to create a rapid conceptual design process that can be used to explore new design concepts in the automotive, motorsport, and aerospace sectors from just a single image.&lt;/p&gt; 
&lt;p&gt;Thanks to AWS services like &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, and the open-source&amp;nbsp;&lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt;, this can all be combined into an event-driven workflow on AWS that could scale to explore millions of possible scenarios. TwinGraph is the model orchestration module within the open-source&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow framework&lt;/a&gt;&amp;nbsp;that enables deploying predictive modeling, simulation, and&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;Level 4 Digital Twin&lt;/a&gt;&amp;nbsp;workflows at scale.&lt;/p&gt; 
&lt;p&gt;The generative capability of machine learning (ML) algorithms holds significant promise across diverse industries. Generative AI techniques are powered by large machine learning models, pre-trained on very large amounts of data. We’re seeing the impact of these models in several areas, including transformer models for natural language processing, text-to-image models like &lt;a href="https://en.wikipedia.org/wiki/Stable_Diffusion"&gt;Stable Diffusion&lt;/a&gt; for image manipulation, and generative adversarial networks for zero-shot classifiers.&lt;/p&gt; 
&lt;h2&gt;Why is this important?&lt;/h2&gt; 
&lt;p&gt;Today, an AI image generator like Stable Diffusion can be employed to generate conceptual designs for cars, planes, and other vehicles. However, these methods lack a foundation in understanding performance factors like aerodynamic drag because they don’t consider the underlying physical laws and the design constraints like noise levels and the extra energy usage they drive. In an era of increased emphasis on energy efficiency and sustainability, conceptual designs must extend beyond just adhering to style guidelines.&lt;/p&gt; 
&lt;p&gt;Over past decades, CFD-driven design optimization grew significantly across a number of industries. A general workflow typically involves simulating complex geometries using conventional physics-based solvers for different sets of individual parameters (e.g. &lt;em&gt;wing chord length&lt;/em&gt;, or &lt;em&gt;rear window angle&lt;/em&gt;) and finding the optimal setting across an entire parameter space.&lt;/p&gt; 
&lt;p&gt;While these solvers offer a lot of accuracy, they’re computationally intensive and time-consuming, which slows down the pace of engineering design. There’s been a growing interest in combining conventional CFD models with ML approaches to try to overcome these computational challenges. Using generative AI in the design process allows efficient sweeping of the parameter space in a non-parametric manner, based on physically meaningful design configurations of the system being examined.&lt;/p&gt; 
&lt;p&gt;We want to show you how generative AI can be applied to design optimization. In this post we’ll focus on its effectiveness in finding better solutions for drag reduction. Our approach combines generative AI for the initial design phase with &lt;a href="https://www.openfoam.com/"&gt;OpenFOAM&lt;/a&gt; CFD simulations for the evaluation of vehicle aerodynamics.&lt;/p&gt; 
&lt;p&gt;Through this process, we’ve developed a workflow that empowers users to define a non-parametric design optimization problem as an algorithm suitable for execution on AWS – at scale with robust infrastructure. This is underpinned by&amp;nbsp; &lt;a href="https://github.com/aws-samples/twingraph"&gt;Twingraph&lt;/a&gt; which does the undifferentiated heavy lifting of dynamic task orchestration, gathering provenance information, and scaling.&lt;/p&gt; 
&lt;div id="attachment_2896" style="width: 1860px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-2896" loading="lazy" class="wp-image-2896 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.25.21@2x.png" alt="Figure 1: Overall workflow for iterative design optimization of car aerodynamics by combining Generative AI techniques and Computational Fluid Dynamics simulations" width="1850" height="576"&gt;
 &lt;p id="caption-attachment-2896" class="wp-caption-text"&gt;Figure 1: Overall workflow for iterative design optimization of car aerodynamics by combining Generative AI techniques and Computational Fluid Dynamics simulations&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Design iterations through Stable Diffusion&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/CompVis/stable-diffusion"&gt;Stable Diffusion&amp;nbsp;&lt;/a&gt;is a generative AI model which enables image generation through text-driven manipulation. The underlying architecture of Stable Diffusion comprises of three key phases:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;obtaining a latent representation of the image, which captures the meaning of objects/people depicted in the image&lt;/li&gt; 
 &lt;li&gt;progressively adding Gaussian noise to this representation&lt;/li&gt; 
 &lt;li&gt;reconstructing the image through removing the noise, resulting in a modified variant of the original image which reflects the semantics of the text prompt.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As an example, in Figure 2 we show the results from using Stable Diffusion to modify an automotive design, starting from a stock image of a sedan to convert it into a sporty aerodynamic design. This resulted in a series of transformations from the original to a modified image. For this exercise, the pre-trained Stable Diffusion model was used for the image generation, but this can be fine-tuned to match the design philosophy of the individual car manufacturer.&lt;/p&gt; 
&lt;div id="attachment_2906" style="width: 490px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2906" loading="lazy" class="wp-image-2906 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/Figure2-converted.gif" alt="Figure 2: Sequential transformation of a car design, given an appropriate prompt to improve aerodynamics and make the car sportier, using an image-to-image pipeline with Stable Diffusion 2.1" width="480" height="360"&gt;
 &lt;p id="caption-attachment-2906" class="wp-caption-text"&gt;Figure 2: Sequential transformation of a car design, given an appropriate prompt to improve aerodynamics and make the car sportier, using an image-to-image pipeline with Stable Diffusion 2.1&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;But this transformation &lt;em&gt;isn’t based on the underlying physics&lt;/em&gt; – it’s an interpretation of the prompts alongside the latent space embedding, which is a condensed mathematical representation of the image, within the Stable Diffusion algorithm. In turn, this interpretation is really driven by training data that exposed the model to sports cars, leading to a predisposition for similar looking designs. To accurately evaluate if the transformation path is an improvement in the aerodynamics of the vehicle, the natural step would be to convert the image into a 3D representation that can be used &amp;nbsp;for high-fidelity CFD simulations.&lt;/p&gt; 
&lt;h2&gt;Generation of point-cloud using neural radiance fields&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2012.02190"&gt;Neural radiance fields&lt;/a&gt;&amp;nbsp;(NeRF) are algorithms showing great promise for converting one or more images into full 3D representations. Combining&amp;nbsp;&lt;a href="https://arxiv.org/abs/2211.11674"&gt;bootstrapped NeRF&amp;nbsp;&lt;/a&gt;with &lt;em&gt;generative adversarial networks&lt;/em&gt; (GANs), we can reconstruct multiple poses of objects to augment and improve the predictions.&lt;/p&gt; 
&lt;p&gt;To make this work, we feed images of the car into NeRFs to obtain &lt;em&gt;signed-distance functions&lt;/em&gt; (SDFs) and construct point-cloud representations like we’ve shown in Figure 3. We fine-tuned the NeRF model using the &lt;a href="https://cvgl.stanford.edu/projects/pascal3d.html"&gt;Pascal3D&lt;/a&gt; data set for 3D object reconstruction.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 3: Point-cloud of car (bottom) obtained using NeRF." width="500" height="375" src="https://www.youtube-nocookie.com/embed/3NtY7eTLVTs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 3: Point-cloud of car (bottom) obtained using NeRF by using the base image (top), transforming the image into a 3D structure representing the car.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Reconstruction of surface topology from point-cloud&lt;/h2&gt; 
&lt;p&gt;The point-cloud representation lacks the crucial connectivity or surface topology information that’s required for understanding the behavior of air flow around the vehicle.&lt;/p&gt; 
&lt;p&gt;To reconstruct the surface from the point-cloud, we first generated an &lt;em&gt;unstructured polygonal Alpha shape mesh&lt;/em&gt; (for a non-convex hull). We achieved this through a coarse Delaunay triangulation using the &lt;a href="http://www.open3d.org/"&gt;Open3D&lt;/a&gt; library. This computational geometric technique identifies the encompassing surface including the points presented in the point-cloud, generated from NeRF.&lt;/p&gt; 
&lt;p&gt;To further refine the mesh, we extracted the points generated on the surface (nodes of the initial triangulation) together with estimated normals from the Alpha shapes. This surface point-cloud is ingested into a&amp;nbsp;&lt;a href="https://research.nvidia.com/labs/toronto-ai/NKSR/"&gt;Neural Kernel Surface Reconstruction&lt;/a&gt;&amp;nbsp;(NKSR) algorithm, which is a machine learning technique that can perform fast surface reconstructions from sparse data. The final result is displayed in Figure 4. While this technique doesn’t capture finer surface details, the overall shape of the car is approximately modeled by the resulting mesh.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 4: Surface mesh generated using Neural Kernel Surface Reconstruction" width="500" height="375" src="https://www.youtube-nocookie.com/embed/sgHBGi5kMN0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 4: Surface mesh generated using Neural Kernel Surface Reconstruction, with the original point cloud (top) and the triangulated mesh (bottom) showing a good match in general topographic features.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Running CFD simulations on OpenFOAM&lt;/h2&gt; 
&lt;p&gt;We used &lt;a href="https://www.openfoam.com/"&gt;OpenFoam&lt;/a&gt; to compute the flow field around the vehicle. We build an unstructured hex-dominate mesh with prismatic boundary layer cells using blockMesh and SnappyHexMesh from the .obj file we generated in the previous step.&lt;/p&gt; 
&lt;p&gt;For this post, we intentionally opted for much lower refinement levels than what is typically employed in the industry (we can increase these levels as required). Our mesh count was approximately one million cells, on average – this changes slightly depending on the geometry itself. To accelerate the CFD part of the process we restricted ourselves to steady-state RANS simulations using the k-omega SST model (for industrial applications you could extend this to use hybrid RANS-LES or WMLES methods, which have a higher fidelity).&lt;/p&gt; 
&lt;p&gt;Finally, in our setup we used the &lt;em&gt;simpleFoam&lt;/em&gt; solver based upon the semi-implicit method for pressure-linked equations (SIMPLE) algorithm. Table 1 shows the parameters.&lt;/p&gt; 
&lt;div id="attachment_2924" style="width: 410px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2924" loading="lazy" class="wp-image-2924 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.37.31@2x-1.png" alt="Table 1: Constants used for computational fluid dynamics simulations" width="400" height="155"&gt;
 &lt;p id="caption-attachment-2924" class="wp-caption-text"&gt;Table 1: Constants used for computational fluid dynamics simulations&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The image in Figure 5 displays the streamlines as well as surface pressure over the initial car design. For this illustrative simulation, we used 425,045 cells for the mesh.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 5: Streamlines visualizing flow field around a generated car mesh." width="500" height="281" src="https://www.youtube-nocookie.com/embed/EUzvYkivaZo?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 5: Streamlines visualizing flow field around a generated car mesh – these are indicative of the fluid velocities, while the colors on the car surface represent the surface pressure magnitude.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;To compute the drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;values during post-processing, we derived a reference wheelbase length and frontal surface areas based on the initial designs – these reference values remain relatively constant across all observations. We used the final drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;values in the pipeline to evaluate and rank the generated design options to find the best intermediate choices.&lt;/p&gt; 
&lt;h2&gt;Integration into Simulation-Guided Design Workflow on AWS&lt;/h2&gt; 
&lt;p&gt;The overall workflow has five key components: image generation, Stable Diffusion, point-cloud with NeRF, mesh generation with Open3d and NKSR, and finally the OpenFoam CFD simulation. Each of these are containerized and orchestrated by the&amp;nbsp;&lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt; orchestration module within the &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We deployed this workflow by using&amp;nbsp;&lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;&amp;nbsp;and scaled it as needed to find the optimal designs. We achieved the necessary scale by leveraging both CPU &lt;em&gt;and&lt;/em&gt; GPU architectures depending on the specific requirements of each workflow component.&lt;/p&gt; 
&lt;div id="attachment_2899" style="width: 1720px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-2899" loading="lazy" class="size-full wp-image-2899" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.32.19@2x.png" alt="Figure 6: AWS Architecture diagram for a running workflow from connecting with a remote client, to launching the algorithmic pipeline on TwinGraph and executing jobs on AWS Batch, and visualizing results." width="1710" height="974"&gt;
 &lt;p id="caption-attachment-2899" class="wp-caption-text"&gt;Figure 6: AWS Architecture diagram for a running workflow from connecting with a remote client, to launching the algorithmic pipeline on TwinGraph and executing jobs on AWS Batch, and visualizing results.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We repeated the experiment for a number of generated images across multiple cycles, and uploaded the results from each experiment automatically to&amp;nbsp;&lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service (Amazon S3)&lt;/a&gt;. This ensured persistent storage of our results. The necessary meta-data provenance information from each experiment was automatically uploaded to an&amp;nbsp;&lt;a href="https://aws.amazon.com/neptune/"&gt;Amazon Neptune&lt;/a&gt;&amp;nbsp;graph database for the subsequent analysis.&lt;/p&gt; 
&lt;p&gt;Once the results were generated, we could retrieve the specific outcomes we were interested in from Amazon S3 using a GPU instance – we ran our visualization interface thought a high-performance remote desktop protocol called &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;NICE DCV&lt;/a&gt; (an AWS product).&lt;/p&gt; 
&lt;p&gt;Overall,&amp;nbsp;&lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt;&amp;nbsp;orchestrates tasks in an asynchronous manner, which means we can execute multiple experiments concurrently, at scale using AWS Batch.&lt;/p&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;p&gt;As part of the numerical experiments, we ran 10 different instances (10 &lt;em&gt;variants&lt;/em&gt;) of the Stable Diffusion image-to-image generation, surface reconstruction, &lt;em&gt;and&lt;/em&gt; CFD simulations with the same initialization as in Figure 1 with a generic sedan.&lt;/p&gt; 
&lt;p&gt;We used the variant image corresponding to the lowest drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;value, to seed the &lt;em&gt;next&lt;/em&gt; image generation sequence cycle. To guide the design iteration process at a fine level, we reduced the strength of image-to-image changes significantly, compared to Figure 1.&lt;br&gt; We repeated this cycle 20 times (or 20 &lt;em&gt;generations&lt;/em&gt;) and the results are plotted in Figure 7 as the minimum drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;per generation and in Figure 8 as a heat map of the drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;&amp;nbsp;values for each of the generated designs.&lt;/p&gt; 
&lt;div id="attachment_2900" style="width: 1608px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-2900" loading="lazy" class="size-full wp-image-2900" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.33.34@2x.png" alt="Figure 7: Minimum drag coefficients per generation, corresponding to the image variants used to create the subsequent generation of variants. The general downward trend, though non-monotonic due to intermediate car configurations, is indicative that the pipeline improves the drag performance of the best variant through the generations." width="1598" height="968"&gt;
 &lt;p id="caption-attachment-2900" class="wp-caption-text"&gt;Figure 7: Minimum drag coefficients per generation, corresponding to the image variants used to create the subsequent generation of variants. The general downward trend, though non-monotonic due to intermediate car configurations, is indicative that the pipeline improves the drag performance of the best variant through the generations.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 7 shows that the averaged drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) decreases gradually from an initial value of 0.46 to approximately 0.4. During the sequential generations, the decrease is non-monotonic. This is due to the non-parametric and non-linear nature of the optimization procedure – allowing the image generation process to arbitrarily morph the car design with a final goal of reducing drag.&lt;/p&gt; 
&lt;p&gt;Moreover, the intermediate design configurations have incomplete components which result in increased drag for several generations until the image generation process resolves the features. We investigated this further through drag coefficients corresponding to &lt;em&gt;each variant&lt;/em&gt; in the 20 generations we show in Figure 8. The average drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) slightly &lt;em&gt;increases&lt;/em&gt; in the intermediate generations but then gradually &lt;em&gt;decreases&lt;/em&gt; towards the end of the 20 generations.&lt;/p&gt; 
&lt;div id="attachment_2901" style="width: 1632px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-2901" loading="lazy" class="size-full wp-image-2901" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.34.33@2x.png" alt="Figure 8: Illustration of drag coefficients associated with each of 10 variants (vertical axes) during each of generations (horizontal axes, sequences consisting of image generation, mesh reconstruction and simulation). The blue regions indicate lower drag, and we can observe a trend of increased blue regions going across the generations." width="1622" height="904"&gt;
 &lt;p id="caption-attachment-2901" class="wp-caption-text"&gt;Figure 8: Illustration of drag coefficients associated with each of 10 variants (vertical axes) during each of generations (horizontal axes, sequences consisting of image generation, mesh reconstruction and simulation). The blue regions indicate lower drag, and we can observe a trend of increased blue regions going across the generations.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2918" style="width: 482px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2918" loading="lazy" class="size-full wp-image-2918" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/Figure9-converted.gif" alt="Figure 9: Sequence of transitions through the generations for the optimal variant in drag coefficient, demonstrating a smoothing of the hood of the car, a reduced angle of windshield." width="472" height="346"&gt;
 &lt;p id="caption-attachment-2918" class="wp-caption-text"&gt;Figure 9: Sequence of transitions through the generations for the optimal variant in drag coefficient, demonstrating a smoothing of the hood of the car, a reduced angle of windshield.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 9 provides insights into the evolution of car design across generations. The hood of the car adapts to a curved shape with considerable removal of material. Also, the angle of the windshield to the horizon reduces and there are slight changes in the curvature of the car’s rear section. Overall these are subtle, yet significant, changes driven by a generative AI process demonstrating the potential for guiding and making informed design choices through an automated &lt;em&gt;physics-informed&lt;/em&gt; pipeline.&lt;/p&gt; 
&lt;h2&gt;Limitations and future work&lt;/h2&gt; 
&lt;p&gt;The approaches presented here hold promise for accelerating aesthetically and sustainability-focused non-parametric design optimization. But there are limits at each stage that will need to be overcome.&lt;/p&gt; 
&lt;p&gt;As pre-trained Stable Diffusion network weights change and evolve (due to further training), the predictions will change in an unpredictable manner – making repeatability an issue. Also, capturing surface topology and roughness accurately using this method is complex due to the lossy reconstruction from point-clouds compared to full resolution computer-aided design (CAD) meshes. This is important for accurate drag calculations.&lt;/p&gt; 
&lt;p&gt;However, with improvements in generative AI algorithms, we can expect workflows that couple machine learning to classic physics-based simulations to provide practical benefits. We can integrate multiple components into a single algorithmic pipeline that can scale agnostically with respect to the underlying infrastructure, computer architecture and choice of programming models. This provides a template to deploy &lt;em&gt;future&lt;/em&gt; design optimization concepts across multiple domains more easily and reliably.&lt;/p&gt; 
&lt;h2&gt;Conclusions&lt;/h2&gt; 
&lt;p&gt;In this post, we discussed the potential for integrating generative AI techniques with physics-based CFD simulations. We demonstrated a methodology that has the capability to guide the image generation process with physics-informed drag coefficients (C&lt;sub&gt;d&lt;/sub&gt;) using CFD simulations.&lt;/p&gt; 
&lt;p&gt;We also showcased how to turn these images into 3D meshes. These meshes were used for the CFD simulations, but can &lt;em&gt;also &lt;/em&gt;be imported into CAD programs so they can be used in real design processes.&lt;/p&gt; 
&lt;p&gt;Best of all, we combined this into a single event-driven workflow thanks to &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; and &lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt; – which allows for scaling out machine learning and simulation tasks.&lt;/p&gt; 
&lt;p&gt;This work has been focused on running inference using generative AI models, but we could use &lt;a href="https://aws.amazon.com/bedrock/"&gt;Amazon Bedrock&lt;/a&gt; and &lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html"&gt;Amazon SageMaker JumpStart&lt;/a&gt; to improve the developer experience when fine-tuning the &amp;nbsp;models. Amazon Bedrock is a fully managed service that provides foundation models from a collection of leading AI startups (and from Amazon itself) via an API. Bedrock allows you to speed up your development of generative AI applications that you can privately customize and scale using secure and reliable infrastructure in AWS. SageMaker Jumpstart offers you the capability to train and fine tune those foundation models in a managed environment.&lt;/p&gt; 
&lt;p&gt;This approach still requires further development to be applicable to industry, but it demonstrates the potential of integrating generative AI techniques with physics-based simulations. This potential extends beyond automotive CFD design and holds promise to a lot of other scientific and engineering fields.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Implementing AWS ParallelCluster in a Shared VPC</title>
		<link>https://aws.amazon.com/blogs/hpc/implementing-aws-parallelcluster-in-a-shared-vpc/</link>
		
		<dc:creator><![CDATA[Pedro Gil]]></dc:creator>
		<pubDate>Tue, 26 Sep 2023 15:11:47 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">dbe82324fb2c3a9277531ded24917622b6e0ec25</guid>

					<description>In this post we’ll show you how to deploy ParallelCluster in a shared VPC environment so you can separate infrastructure management, cluster operations, and help segregate costs, too.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was controbuted by Pedro Gil, Solutions Architect, and Ryan Anderson, Software Engineer HPC Engineering&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;AWS Shared Virtual Private Cloud (VPC) is a feature that allows multiple AWS accounts to share a single VPC, enabling them to collaborate on resources within the same network. This helps in managing and sharing network resources within an organization, and allows teams to work independently without compromising the security of the VPC.&lt;/p&gt; 
&lt;p&gt;AWS ParallelCluster is an open source cluster management tool that makes it easy for you to deploy and manage high performance computing (HPC) clusters on AWS.&lt;/p&gt; 
&lt;p&gt;Installing ParallelCluster in a shared VPC – when using Slurm as the scheduler – is often a challenge because ParallelCluster assumes that the Amazon Route53 Hosted Zone and the VPC belongs to the same account where the cluster is being created.&lt;/p&gt; 
&lt;p&gt;In this post we’ll show you a solution that gets ParallelCluster up and running in a shared VPC environment where the VPC belongs to one account and it is shared to another account for resource deployment operations.&lt;/p&gt; 
&lt;h2&gt;Overview of our solution&lt;/h2&gt; 
&lt;p&gt;We’ll show you how to deploy ParallelCluster into Account B using a shared VPC from infrastructure in Account A.&lt;/p&gt; 
&lt;div id="attachment_2808" style="width: 1624px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2808" loading="lazy" class="size-full wp-image-2808" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-16.41.52@2x.png" alt="Figure 1 – VPC Resource Share created on Account A (VPC infrastructure account) and shared to Account B (ParallelCluster creation account) using AWS Resource Access Manager." width="1614" height="1400"&gt;
 &lt;p id="caption-attachment-2808" class="wp-caption-text"&gt;Figure 1 – VPC Resource Share created on Account A (VPC infrastructure account) and shared to Account B (ParallelCluster creation account) using AWS Resource Access Manager.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;These are the steps we’ll take for the solution:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Step 1- Create a Route 53 Private Hosted Zone and associate it with shared VPC&lt;/li&gt; 
 &lt;li&gt;Step 2- Create an additional AWS Identity and Access Management&lt;/li&gt; 
 &lt;li&gt;(IAM) Policy&lt;/li&gt; 
 &lt;li&gt;Step 3- Install AWS ParallelCluster and its configuration file&lt;/li&gt; 
 &lt;li&gt;Step 4- Modify the configuration file to add the additional policy and to include the &lt;code&gt;Hosted Zone ID&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Step 5- Create a cluster using this configuration file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;For this walkthrough, you should have the following prerequisites:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An AWS account B (cluster creation account) where the VPC was shared and an AWS account A (VPC account owner) where the VPC was created (like in Figure 1).&lt;/li&gt; 
 &lt;li&gt;A user with sufficient privileges to create the IAM Policy, Amazon Route53 Private Zone and to install ParallelCluster.&lt;/li&gt; 
 &lt;li&gt;AWS resources: AWS console access, AWS CLI using AWS Cloud9&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Step 1 – Create Amazon Route53 Private Hosted Zone&lt;/h2&gt; 
&lt;p&gt;ParallelCluster uses Amazon Route53 Private Hosted Zone to resolve cluster nodes and creating one across accounts requires the following specific procedure.&lt;/p&gt; 
&lt;p&gt;First, log in to Account B and create a Private Hosted Zone using the Route53 service console. Associate it with any existing VPC in Account B (you’ll remove this association at the end of this section). Take note of the Private Hosted Zone id you just created.&lt;/p&gt; 
&lt;p&gt;Next, using AWS Cloud9 in Account B, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws route53 create-vpc-association-authorization --hosted-zone-id &amp;lt;hosted-zone-id&amp;gt; --vpc VPCRegion=&amp;lt;region&amp;gt;,VPCId=&amp;lt;vpc-id&amp;gt; --region &amp;lt;region&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command authorizes the VPC association between the private hosted zone you just created and the VPC from Account A. Use the Hosted Zone ID that you obtained in previous step. Use the AWS region and ID of the shared VPC.&lt;/p&gt; 
&lt;p&gt;Now, using AWS Cloud9 instance in Account A, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws route53 associate-vpc-with-hosted-zone --hosted-zone-id &amp;lt;hosted-zone-id&amp;gt; --vpc VPCRegion=&amp;lt;region&amp;gt;,VPCId=&amp;lt;vpc-id&amp;gt; --region &amp;lt;region&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command creates the association between the private hosted zone in Account B and the VPC in Account A. Use the Hosted Zone ID from earlier, and the Region and ID of the VPC in Account A.&lt;/p&gt; 
&lt;p&gt;Finally, go back to the Route 53 service console on Account B and verify that the shared VPC association with the Private Hosted Zone is listed. Delete the association of the local VPC done in step A.&lt;/p&gt; 
&lt;h2&gt;Step 2 – Create the IAM policy&lt;/h2&gt; 
&lt;p&gt;We need to create an additional policy for the head node to have permissions to create cluster nodes in the shared VPC.&lt;/p&gt; 
&lt;p&gt;First, login to Account B and create a new IAM Policy using the following template. Use ManageHeadnodePermissions as the name of new policy. Use ID for Account A and the subnet ID from the Shared VPC where the compute nodes will be created.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;{
    "Version": "2012-10-17",
   "Statement": [
        {
            "Sid": "SharedSubnets",
            "Effect": "Allow",
            "Action": [
                "ec2:CreateTags",
                "ec2:RunInstances",
                "ec2:CreateFleet"
            ],
    "Resource": "arn:aws:ec2:&amp;lt;region&amp;gt;:&amp;lt;Account A ID&amp;gt;:subnet/&amp;lt;Subnet ID&amp;gt;"
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Step 3 – Install and configure ParallelCluster&lt;/h2&gt; 
&lt;p&gt;You should follow the steps to &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-configuring.html"&gt;install ParallelCluster&lt;/a&gt; on a suitable instance or laptop. After doing so, you’ll need to create a config file for ParallelCluster to use. The pcluster configure will step you through this process, asking you some questions, and creating a new config file at the end.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster configure –config config-file.yaml&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Choose the AWS region ID where your shared VPC is located.&lt;/li&gt; 
 &lt;li&gt;Choose your Amazon Elastic Compute Cloud (Amazon EC2) key pair – you’ll need to have one already.&lt;/li&gt; 
 &lt;li&gt;Choose Slurm as your scheduler&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;&amp;lt;n&amp;gt;&lt;/strong&gt; for VPC creation and select the existing shared VPC&lt;/li&gt; 
 &lt;li&gt;Choose an appropriate operating system&lt;/li&gt; 
 &lt;li&gt;Select appropriate instance types and queue configurations for your workload&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The next step would usually be to run pcluster create to build the cluster using the choices and parameters you just entered, however before we do that, we need to delve into the configuration file that this process produced and make some changes.&lt;/p&gt; 
&lt;h2&gt;Step 4 – Modify ParallelCluster configuration file&lt;/h2&gt; 
&lt;p&gt;Modify your ParallelCluster configuration file to include the following using your own Hosted Zone ID and the new Policy Name you created in the previous steps &lt;code&gt;ManageHeadnodePermissions&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;HeadNode:
  Iam:
    AdditionalIamPolicies:
      - Policy: arn:aws:iam::&amp;lt;Account B ID&amp;gt;:policy/ManageHeadnodePermissions
Scheduling:
  Scheduler: slurm
  SlurmSettings:
    Dns:
      HostedZoneId: &amp;lt;hosted-zone-id&amp;gt; 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 5 – Create your cluster&lt;/h4&gt; 
&lt;p&gt;It’s time to create your cluster. If you need to, you can find more details &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-configuring.html"&gt;in our documentation&lt;/a&gt;. But for now, you just need to run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster –cluster-name test-cluster –cluster-configuration cluster-config.yaml&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Congratulations, you have finished the creation of your cluster using AWS ParallelCluster in a shared VPC.&lt;/p&gt; 
&lt;h2&gt;Cleaning up&lt;/h2&gt; 
&lt;p&gt;It is a best practice to delete the association authorization after you create the association. This step prevents you from recreating the same association later and will not prevent you to create new ParallelCluster instances later. To delete the authorization, reconnect to Account A. Then, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws route53 delete-vpc-association-authorization --hosted-zone-id &amp;lt;hosted-zone-id&amp;gt; --vpc VPCRegion=&amp;lt;region&amp;gt;,VPCId=&amp;lt;vpc-id&amp;gt; --region &amp;lt;region&amp;gt;&amp;nbsp;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You might also want to delete cluster resources after you are done with your workload by running the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster delete-cluster –region &amp;lt;region&amp;gt; --cluster-name test-cluster&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;We’ve shown you how to install AWS ParallelCluster in a shared VPC environment, which means you can use a common VPC between AWS accounts inside an organization, while keeping billing and ownership separate for the users of the cluster.&lt;/p&gt; 
&lt;p&gt;When creating other clusters all you need to do is include the additional policy in the &lt;code&gt;headnode&lt;/code&gt; section of the configuration file and make sure you use the proper &lt;code&gt;Hosted Zone ID&lt;/code&gt;. Using AWS batch in ParallelCluster does &lt;em&gt;not&lt;/em&gt; require any changes to the cluster configuration or Route53 entry since it relies on its own internal mechanism to resolve hostnames.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
	</channel>
</rss>