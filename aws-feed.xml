<?xml version="1.0" encoding="UTF-8" standalone="no"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" version="2.0">

<channel>
	<title>AWS HPC Blog</title>
	<atom:link href="https://aws.amazon.com/blogs/hpc/feed/" rel="self" type="application/rss+xml"/>
	<link>https://aws.amazon.com/blogs/hpc/</link>
	<description>Just another Amazon Web Services site</description>
	<lastBuildDate>Tue, 10 Oct 2023 13:14:45 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>Financial services industry HPC migrations using AWS ParallelCluster with Slurm</title>
		<link>https://aws.amazon.com/blogs/hpc/financial-services-industry-hpc-migrations-using-aws-parallelcluster-with-slurm/</link>
		
		<dc:creator><![CDATA[Vinay Arora]]></dc:creator>
		<pubDate>Tue, 10 Oct 2023 13:14:45 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[FSI]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Slurm]]></category>
		<guid isPermaLink="false">38335ed4c953ad6c7ef375c45681146e51ae92b4</guid>

					<description>In this post, we’ll walk you through how banks and other financial services firms migrate or burst their grid workloads onto AWS using AWS ParallelCluster and the Slurm scheduler.</description>
										<content:encoded>&lt;p&gt;If you’re reading this post, you’ll know that HPC is a way of solving hard problems by slicing the problem space up amongst a lot of computers. You might have also heard the term ‘&lt;em&gt;grid computing’&lt;/em&gt; in the context of financial services. This refers to centralized systems which are typically used to provide ‘utility computing’ to support HPC workloads.&lt;/p&gt; 
&lt;p&gt;In financial services these types of jobs use complex algorithms to calculate as many risk or trading scenarios in parallel as possible, bound only by the amount of compute available. Financial services firms are always keen to reduce their operational costs and look to the cloud as a way to provide the elasticity and cost benefits to do this, but also: the ability to respond rapidly to changing economic conditions and market volatility.&lt;/p&gt; 
&lt;p&gt;Customers from all the major Financial Services Industry (FSI) verticals (including banking, capital markets, and insurance) are moving their on-premises grid workloads partially, or entirely, to the cloud. These workloads typically include high volumes of short-running tasks.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk through how these firms can migrate or burst their grid workloads onto AWS using AWS ParallelCluster and the Slurm scheduler – and we’ll give you an introduction to those two packages, if you’ve not met them before.&lt;/p&gt; 
&lt;h2&gt;Loosely-coupled Grid scenarios&lt;/h2&gt; 
&lt;p&gt;Grid workloads often require hundreds of thousands, or millions, of parallel processes to complete a calculation or simulation. Generally, these jobs run on a single node, consuming one process or multiple processes with shared memory parallelization (SMP) for parallelization &lt;em&gt;within&lt;/em&gt; the node. The parallel processes, or the iterations in the simulation, are post-processed to create one solution or discovery from the simulation.&lt;/p&gt; 
&lt;p&gt;During the simulation, the operations can take place in any order, and the loss of any one node or job in a loosely coupled workload usually doesn’t delay the entire calculation. The lost work can be picked up later or omitted altogether. The nodes involved in the calculation can vary in specification and power. This gives us some hints about the types of compute that we need to run these loosely-connected simulations.&lt;/p&gt; 
&lt;h2&gt;Architectural considerations&lt;/h2&gt; 
&lt;h3&gt;Networking&lt;/h3&gt; 
&lt;p&gt;Grid processes run in parallel and don’t communicate with each other (at all) to exchange information. This means the performance of jobs isn’t impacted by latency between the nodes. This is great because it means that instead of optimizing for latency, we can instead optimize for &lt;em&gt;instance availability&lt;/em&gt;: we can spread the job out over several Availability Zones (AZs), each of which have their own capacity pools. This is especially useful when using the Amazon Elastic Compute Cloud (Amazon EC2) Spot purchasing model because the availability of Spot instances in any given AZ can be limited.&lt;/p&gt; 
&lt;p&gt;By providing AWS ParallelCluster with options about where to place the compute, we can achieve an architecture that can scale up to tens or hundreds of thousands of cores.&lt;/p&gt; 
&lt;h3&gt;Storage&lt;/h3&gt; 
&lt;p&gt;Loosely coupled jobs also have a great characteristic when working with shared storage. Typically, they need to read data in, compute on it, and then write it out all without interfering with any other job. This is the perfect use case for &lt;em&gt;object storage&lt;/em&gt;. In this post, we’ll focus on Amazon Simple Storage Service (Amazon S3) as the storage layer because it provides internet-scale storage, it’s low cost, and it scales up to hundreds of thousands of tasks working simultaneously.&lt;/p&gt; 
&lt;h3&gt;Compute&lt;/h3&gt; 
&lt;p&gt;Choosing instance types means looking at the job’s requirement for memory:vCPU ratio, availability, and architecture support for the workload. (ie x86 or aarch64). In this post, we’ll stick with x86, to provide the greatest instance availability &lt;em&gt;and compatibility&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;ParallelCluster can request instances based on what’s available and will &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/slurm-short-capacity-fail-mode-v3.html"&gt;quickly “fail-over”&lt;/a&gt; if the instances can’t be launched. For the purposes of testing we chose instances from both x86 manufacturers’ families because they share the same instruction set and by choosing both we have a larger pool to draw from.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll use &lt;em&gt;vCPUs&lt;/em&gt; instead of &lt;em&gt;cores&lt;/em&gt; because most x86 instances onAWS have hyperthreading enabled. Hyperthreading allows two virtual cores to share the same arithmetic/logic unit (ALU). This increases overall throughput by doubling the core count but deceases the individual core’s &lt;em&gt;solve-time&lt;/em&gt;. This is sometimes a controversial topic in HPC, but for grid workloads in FSI, we’ve seen that hyperthreading is a net benefit, so we’re leaving it enabled.&lt;/p&gt; 
&lt;h3&gt;Scheduling&lt;/h3&gt; 
&lt;p&gt;Running a few tasks on a single instance is easy to manage by hand, but once you scale up the number of tasks, you’ll need a scheduler to manage their lifecycle. &lt;strong&gt;Slurm&lt;/strong&gt; is an open-source job scheduler that’s optimized for scheduling both tightly-coupled &lt;em&gt;and&lt;/em&gt; loosely-coupled tasks across multiple instances. It handles queuing, execution, retries, and accounting. ParallelCluster sets up Slurm using the &lt;a href="https://slurm.schedmd.com/elastic_computing.html"&gt;Slurm cloud bursting plugin&lt;/a&gt; and manages the scaling of Amazon EC2 instances.&lt;/p&gt; 
&lt;p&gt;We recommend enabling Slurm’s &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/slurm-mem-based-scheduling-v3.html"&gt;memory based scheduling&lt;/a&gt; feature. This provides an easy way to request specific memory amounts and compute directly in the job submission, i.e. 1 vCPU and 4 GB of memory could be set in the Slurm &lt;code&gt;sbatch&lt;/code&gt; file. It also helps Slurm maximize the CPU utilization of all the cores across the fleet.&lt;/p&gt; 
&lt;h2&gt;AWS ParallelCluster&lt;/h2&gt; 
&lt;p&gt;AWS ParallelCluster is an open-source cluster management tool that makes it straightforward for you to deploy, manage, and scale Slurm-based clusters on AWS. ParallelCluster allows you to leverage the elasticity of AWS by providing an easy way to expand and contract compute queues that you define. You can use multiple instance types, in multiple Availability Zones, and create job submission queues with a lot of creative freedom.&lt;/p&gt; 
&lt;p&gt;You can also quickly spin up clusters for experimenting and prototyping. Since ParallelCluster uses a simple YAML file to define all the resources you need, the process of standing up additional clusters – for production, dev, or testing – is an automated (and secure) process.&lt;/p&gt; 
&lt;p&gt;Let’s look at a reference architecture and a tutorial that you can use to deploy this in your own AWS account.&lt;/p&gt; 
&lt;div id="attachment_2873" style="width: 1077px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2873" class="size-full wp-image-2873" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/18/CleanShot-2023-09-18-at-13.16.09.png" alt="Figure 1 –The reference architecture for FSI grid-style computing on AWS using AWS ParallelCluster. The queues span over all the Availability Zones in the region and read and write data from an Amazon S3 bucket in that region." width="1067" height="490"&gt;
 &lt;p id="caption-attachment-2873" class="wp-caption-text"&gt;Figure 1 –The reference architecture for FSI grid-style computing on AWS using AWS ParallelCluster. The queues span over all the Availability Zones in the region and read and write data from an Amazon S3 bucket in that region.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Implementation details&lt;/h2&gt; 
&lt;p&gt;In the following steps, we’ll show you how to setup a cluster with a &lt;strong&gt;headnode&lt;/strong&gt; and two &lt;strong&gt;compute queues&lt;/strong&gt;. The queues span over all the Availability Zones in the region and read and write data from an Amazon S3 bucket in that region. The Slurm scheduler process (&lt;code&gt;slurmctld&lt;/code&gt;) runs on the &lt;code&gt;HeadNode&lt;/code&gt; and users can login there to submit jobs via AWS Systems Manager (SSM), using SSH, or with a remote desktop connection using DCV as shown in the reference architecture in Figure 1.&lt;/p&gt; 
&lt;p&gt;You can follow along with these steps in the &lt;a href="https://catalog.workshops.aws/fsi-slurm-aws-parallelcluster"&gt;FSI Tutorial&lt;/a&gt; hands-on lab.&lt;/p&gt; 
&lt;p&gt;First, install AWS ParallelCluster CLI or UI. We’ll assume you’re using the CLI for now. In ParallelCluster, you initiate the creation of a cluster through the CLI by specifying the location of your configuration file, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster -c cluster.yaml -n cluster-name&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Your config file might look something like the this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;HeadNode:
  InstanceType: c5a.xlarge
  Networking:
    SubnetId: subnet-846f1aff
  LocalStorage:
    RootVolume:
      VolumeType: gp3
  Iam:
    AdditionalIamPolicies:
      - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
  Dcv:
    Enabled: true
  CustomActions:
    OnNodeConfigured:
      Sequence:
        - Script: &amp;gt;-
            &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh" rel="noopener noreferrer"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh&lt;/a&gt;
        - Script: &amp;gt;-
            &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/pyxis/postinstall.sh" rel="noopener noreferrer"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/pyxis/postinstall.sh&lt;/a&gt;
          Args:
            - /fsx
  Imds:
    Secured: true
Scheduling:
  Scheduler: slurm
  SlurmQueues:
    - Name: c6i
      AllocationStrategy: capacity-optimized
      ComputeResources:
        - Name: spot
          Instances:
            - InstanceType: c6i.32xlarge
            - InstanceType: c6a.32xlarge
            - InstanceType: m6i.32xlarge
            - InstanceType: m6a.32xlarge
            - InstanceType: r6i.32xlarge
            - InstanceType: r6a.32xlarge
          MinCount: 0
          MaxCount: 100
      CustomActions:
        OnNodeConfigured:
          Sequence:
            - Script: &amp;gt;-
                &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/pyxis/postinstall.sh" rel="noopener noreferrer"&gt;https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/pyxis/postinstall.sh&lt;/a&gt;
              Args:
                - /fsx
      ComputeSettings:
        LocalStorage:
          RootVolume:
            VolumeType: gp3
      Networking:
        SubnetIds:
          - subnet-8b15a7c6
          - subnet-04f44e9dc1c8ee425
          - subnet-91ab80f8
        PlacementGroup:
          Enabled: false
      CapacityType: SPOT
      Iam:
        AdditionalIamPolicies:
          - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          - Policy: arn:aws:iam::aws:policy/AmazonS3FullAccess
Region: us-east-2
Imds:
  ImdsSupport: v2.0
Image:
  Os: alinux2
SharedStorage:
  - Name: Ebs0
    StorageType: Ebs
    MountDir: /shared
    EbsSettings:
      VolumeType: gp3
      DeletionPolicy: Retain
      Size: '100'
Tags:
  - Key: parallelcluster-ui
    Value: 'true'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This config has a few key sections:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;HeadNode&lt;/code&gt; – this is the Amazon EC2 instance that runs the Slurm scheduler processes (&lt;code&gt;slurmctld&lt;/code&gt;, for example). Because this instance is responsible for scaling up the cluster, scheduling jobs, and serving the config files, we recommend going with an instance like the c6i.2xlarge that has sufficient resources (8 vCPUs and 16 GB memory) to run Slurm comfortably.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;SlurmQueues&lt;/code&gt; – these will depend on your resource requirements. For example, if you have jobs that requires 1 vCPUs and 2 GB memory, set up a queue with c6i instances. These instances all meet the memory to core ratio and specifying multiples different sizes of them will ensure you’re most likely to get your desired capacity. If you require more memory, the m6i and r6i instances offer 8 GB per vCPU and 16 GB per vCPU, respectively. We recommend you offer your users a choice by putting each instance family in their own &lt;code&gt;SlurmQueue&lt;/code&gt;. In addition to diversifying the instance types by providing multiple from each family, we also recommend you provide multiple Availability Zones, because this expands the number of pools you’re drawing from and is especially important for using Amazon EC2 Spot.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Storage&lt;/code&gt; – for grid workloads there’s rarely a need for a parallel filesystem – more important is a filesystem that can serve traffic in multiple Availability Zones and scale up to the thousands of jobs that can execute concurrently. We recommend using Amazon S3 or if there’s a need for a POSIX compliant filesystem, then Amazon FSx for OpenZFS. The creation of the filesystem is out of scope for this blog, but you can learn more by reading about &lt;a href="https://aws.amazon.com/blogs/hpc/expanded-filesystems-support-in-aws-parallelcluster-3-2/"&gt;Filesystem Support in AWS ParallelCluster&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When ParallelCluster builds your cluster, it’ll attach any existing filesystems you specified in the config file. Those filesystems are typically used for applications, libraries, and users’ data. It’s important to have a separation between the filesystem and cluster configuration because it allows for easy upgrades later when ParallelCluster release a new version which you want to take advantage of. In the config file we showed, it calls for ParallelCluster itself to create an EBS-based shared file system, mounted as &lt;code&gt;/shared&lt;/code&gt;. This will be NFS-exported to the compute nodes in the cluster when they’re created.&lt;/p&gt; 
&lt;p&gt;Once your cluster is up and running, you have several ways to interact with it:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;You can login to the head node: you can use normal SSH, or connect through SSM using the attached &lt;code&gt;SSMInstanceCore&lt;/code&gt; SSM allows access to instances that are not routable through SSH, like those in a private subnet.&lt;/li&gt; 
 &lt;li&gt;You can submit jobs using the default Slurm commands like &lt;code&gt;sbatch&lt;/code&gt;, &lt;code&gt;srun&lt;/code&gt;, and then monitor with &lt;code&gt;squeue&lt;/code&gt; and &lt;code&gt;sinfo&lt;/code&gt;. See Slurm &lt;a href="https://slurm.schedmd.com/sbatch.html"&gt;sbatch documentation&lt;/a&gt; for more details on options for submitting jobs, and the syntax for the run scripts.&lt;/li&gt; 
 &lt;li&gt;When Slurm detects new jobs in the queue, it uses the &lt;a href="https://slurm.schedmd.com/elastic_computing.html"&gt;Cloud Scheduling&lt;/a&gt; plugin, which calls a &lt;code&gt;ResumeProgram&lt;/code&gt; script that ParallelCluster manages. This script, in turn, calls the Amazon EC2 Fleet API to allocate instances to the queue. The jobs run soon after the instances bootstrap (typically 2-3 minutes).&lt;/li&gt; 
 &lt;li&gt;Once all jobs are complete, the instances are left running for a short ScaleDownIdleTime, in case more jobs arrive. This defaults to 10 minutes, but you can configure it yourself by specifying a &lt;code&gt;ScaleDownIdleTime&lt;/code&gt; &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/multiple-queue-mode-slurm-user-guide-v3.html"&gt;parameter in the ParallelCluster config file&lt;/a&gt;. After this time expires, ParallelCluster will terminate the instances, and wait again for new jobs to arrive in the queues.&lt;/li&gt; 
 &lt;li&gt;If the cluster is deleted, all instances &lt;em&gt;including the head node&lt;/em&gt; will be terminated.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There’s an &lt;a href="https://studio.us-east-1.prod.workshops.aws/preview/5b6a7b59-b9e4-4a42-ac25-5795b9b66391/builds/1428e077-f472-4b3f-b1fc-feab5a3872f2/en-US"&gt;AWS Workshop available online&lt;/a&gt; to show you how to calculate the price of a financial &lt;em&gt;auto-callable option&lt;/em&gt; based on this ParallelCluster architecture. The workshop provides step-by-step guidance to create and configure ParallelCluster and to deploy the containerized workload.&lt;/p&gt; 
&lt;h2&gt;Best Practices when using ParallelCluster with Slurm&lt;/h2&gt; 
&lt;h3&gt;Smaller and heterogeneous job duration&lt;/h3&gt; 
&lt;p&gt;If your job durations are small, for example in seconds and somewhat heterogeneous, we suggest following &lt;a href="https://slurm.schedmd.com/high_throughput.html"&gt;the recommendations from SchedMD&lt;/a&gt; (the makers of Slurm) to tune the cluster for the best job throughput – measured in tasks per second. It’s possible for Slurm to scale to 500 jobs/sec – under the right conditions and with the right architecture.&lt;/p&gt; 
&lt;h3&gt;Large Scale Clusters&lt;/h3&gt; 
&lt;p&gt;If your cluster exceeds 1024 instances, we recommend that following the &lt;a href="https://slurm.schedmd.com/big_sys.html"&gt;Slurm Large Cluster Administration guide.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You’ll also want to consider your &lt;strong&gt;HeadNode&lt;/strong&gt; architecture. Using a c6i.2xlarge won’t be sufficient for the network bandwidth demanded by slurmctld when it’s communicating with all the child nodes. We recommend using a network optimized instance with &lt;em&gt;at least&lt;/em&gt; 50 Gbps of dedicated bandwidth, like the c6in.8xlarge. Remember, the HeadNode instance communicates with the compute instances &lt;em&gt;and&lt;/em&gt; serves the Slurm config file, and application binaries via NFS.&lt;/p&gt; 
&lt;p&gt;You’ll also need to review your AWS account limits, (which, we remind you, are set on a per-region basis):&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Amazon EC2 Limits&lt;/strong&gt; – make sure you increase the Amazon EC2 limits for the compute nodes you plan to use. You can review these, and request limit increases via the AWS Service Quotas console.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;EBS limits&lt;/strong&gt; – each compute node mounts a 35 GB root volume (or larger) of gp3 EBS storage. If you plan to launch more instances than your quota (which defaults to 50 TiB), you should &lt;a href="https://console.aws.amazon.com/servicequotas/home/services/ebs/quotas"&gt;increase this limit&lt;/a&gt;, too.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Route53 limit&lt;/strong&gt; – each compute node is added to a Route53 Private Hosted Zone. This has a default limit of 10,000 records. If you plan to exceed this limit, make sure to &lt;a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/DNSLimitations.html"&gt;request a limit increase&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Acadian did this – so you can you&lt;/h3&gt; 
&lt;p&gt;Acadian chose to use Slurm in AWS ParallelCluster to execute thousands of these heterogeneous jobs. This cloud-native grid solution enables them to benefit from faster and seamless compute capacity provisioning &lt;em&gt;and&lt;/em&gt; on-demand auto scaling features, resulting in optimal results.&lt;/p&gt; 
&lt;p&gt;According to Jian Pan, Head of Quantitative Systems at Acadian, “&lt;em&gt;we are able to reap immediate benefit such as maintaining consistent 4hr model runtime in AWS, comparing to 20~40hr runtime on-premise when resource is under stress&lt;/em&gt;”.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Slurm can handle a cluster that grows and shrinks – driven by demand. It’s able to this dynamically by scaling compute resources from Amazon EC2. Compute fleets can be spun up to complete the workload in the queue. When the jobs are complete, ParallelCluster scales those same fleets back down to the minimum level you set (usually zero).&lt;/p&gt; 
&lt;p&gt;If you want to try this yourself, you can follow the steps in the &lt;a href="https://catalog.workshops.aws/fsi-slurm-aws-parallelcluster"&gt;FSI Tutorial&lt;/a&gt; hands-on lab, and when you’ve done that, you can try a real application to &lt;a href="https://studio.us-east-1.prod.workshops.aws/preview/5b6a7b59-b9e4-4a42-ac25-5795b9b66391/builds/1428e077-f472-4b3f-b1fc-feab5a3872f2/en-US"&gt;calculate the price&lt;/a&gt; of a financial &lt;em&gt;auto-callable option&lt;/em&gt;. Let us know how to get on – you can reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Conceptual design using generative AI and CFD simulations on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/conceptual-design-using-generative-ai-and-cfd-simulations-on-aws/</link>
		
		<dc:creator><![CDATA[Dr. Vidyasagar Ananthan]]></dc:creator>
		<pubDate>Mon, 02 Oct 2023 14:19:33 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">ebf53bce2be2ea1f1b340e195b5361045a4f1f7f</guid>

					<description>In this post we’ll show how generative AI, combined with conventional physics-based CFD can create a rapid design process to explore new design concepts in automotive and aerospace from just a single image.</description>
										<content:encoded>&lt;p&gt;In this post we’ll demonstrate how &lt;a href="https://aws.amazon.com/generative-ai/"&gt;generative AI&lt;/a&gt; techniques can be combined with conventional physics-based&amp;nbsp;&lt;a href="https://aws.amazon.com/hpc/cfd/"&gt;computational fluid dynamics&lt;/a&gt;&amp;nbsp;(CFD) simulations to create a rapid conceptual design process that can be used to explore new design concepts in the automotive, motorsport, and aerospace sectors from just a single image.&lt;/p&gt; 
&lt;p&gt;Thanks to AWS services like &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, and the open-source&amp;nbsp;&lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt;, this can all be combined into an event-driven workflow on AWS that could scale to explore millions of possible scenarios. TwinGraph is the model orchestration module within the open-source&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow framework&lt;/a&gt;&amp;nbsp;that enables deploying predictive modeling, simulation, and&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;Level 4 Digital Twin&lt;/a&gt;&amp;nbsp;workflows at scale.&lt;/p&gt; 
&lt;p&gt;The generative capability of machine learning (ML) algorithms holds significant promise across diverse industries. Generative AI techniques are powered by large machine learning models, pre-trained on very large amounts of data. We’re seeing the impact of these models in several areas, including transformer models for natural language processing, text-to-image models like &lt;a href="https://en.wikipedia.org/wiki/Stable_Diffusion"&gt;Stable Diffusion&lt;/a&gt; for image manipulation, and generative adversarial networks for zero-shot classifiers.&lt;/p&gt; 
&lt;h2&gt;Why is this important?&lt;/h2&gt; 
&lt;p&gt;Today, an AI image generator like Stable Diffusion can be employed to generate conceptual designs for cars, planes, and other vehicles. However, these methods lack a foundation in understanding performance factors like aerodynamic drag because they don’t consider the underlying physical laws and the design constraints like noise levels and the extra energy usage they drive. In an era of increased emphasis on energy efficiency and sustainability, conceptual designs must extend beyond just adhering to style guidelines.&lt;/p&gt; 
&lt;p&gt;Over past decades, CFD-driven design optimization grew significantly across a number of industries. A general workflow typically involves simulating complex geometries using conventional physics-based solvers for different sets of individual parameters (e.g. &lt;em&gt;wing chord length&lt;/em&gt;, or &lt;em&gt;rear window angle&lt;/em&gt;) and finding the optimal setting across an entire parameter space.&lt;/p&gt; 
&lt;p&gt;While these solvers offer a lot of accuracy, they’re computationally intensive and time-consuming, which slows down the pace of engineering design. There’s been a growing interest in combining conventional CFD models with ML approaches to try to overcome these computational challenges. Using generative AI in the design process allows efficient sweeping of the parameter space in a non-parametric manner, based on physically meaningful design configurations of the system being examined.&lt;/p&gt; 
&lt;p&gt;We want to show you how generative AI can be applied to design optimization. In this post we’ll focus on its effectiveness in finding better solutions for drag reduction. Our approach combines generative AI for the initial design phase with &lt;a href="https://www.openfoam.com/"&gt;OpenFOAM&lt;/a&gt; CFD simulations for the evaluation of vehicle aerodynamics.&lt;/p&gt; 
&lt;p&gt;Through this process, we’ve developed a workflow that empowers users to define a non-parametric design optimization problem as an algorithm suitable for execution on AWS – at scale with robust infrastructure. This is underpinned by&amp;nbsp; &lt;a href="https://github.com/aws-samples/twingraph"&gt;Twingraph&lt;/a&gt; which does the undifferentiated heavy lifting of dynamic task orchestration, gathering provenance information, and scaling.&lt;/p&gt; 
&lt;div id="attachment_2896" style="width: 1860px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-2896" loading="lazy" class="wp-image-2896 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.25.21@2x.png" alt="Figure 1: Overall workflow for iterative design optimization of car aerodynamics by combining Generative AI techniques and Computational Fluid Dynamics simulations" width="1850" height="576"&gt;
 &lt;p id="caption-attachment-2896" class="wp-caption-text"&gt;Figure 1: Overall workflow for iterative design optimization of car aerodynamics by combining Generative AI techniques and Computational Fluid Dynamics simulations&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Design iterations through Stable Diffusion&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/CompVis/stable-diffusion"&gt;Stable Diffusion&amp;nbsp;&lt;/a&gt;is a generative AI model which enables image generation through text-driven manipulation. The underlying architecture of Stable Diffusion comprises of three key phases:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;obtaining a latent representation of the image, which captures the meaning of objects/people depicted in the image&lt;/li&gt; 
 &lt;li&gt;progressively adding Gaussian noise to this representation&lt;/li&gt; 
 &lt;li&gt;reconstructing the image through removing the noise, resulting in a modified variant of the original image which reflects the semantics of the text prompt.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As an example, in Figure 2 we show the results from using Stable Diffusion to modify an automotive design, starting from a stock image of a sedan to convert it into a sporty aerodynamic design. This resulted in a series of transformations from the original to a modified image. For this exercise, the pre-trained Stable Diffusion model was used for the image generation, but this can be fine-tuned to match the design philosophy of the individual car manufacturer.&lt;/p&gt; 
&lt;div id="attachment_2906" style="width: 490px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2906" loading="lazy" class="wp-image-2906 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/Figure2-converted.gif" alt="Figure 2: Sequential transformation of a car design, given an appropriate prompt to improve aerodynamics and make the car sportier, using an image-to-image pipeline with Stable Diffusion 2.1" width="480" height="360"&gt;
 &lt;p id="caption-attachment-2906" class="wp-caption-text"&gt;Figure 2: Sequential transformation of a car design, given an appropriate prompt to improve aerodynamics and make the car sportier, using an image-to-image pipeline with Stable Diffusion 2.1&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;But this transformation &lt;em&gt;isn’t based on the underlying physics&lt;/em&gt; – it’s an interpretation of the prompts alongside the latent space embedding, which is a condensed mathematical representation of the image, within the Stable Diffusion algorithm. In turn, this interpretation is really driven by training data that exposed the model to sports cars, leading to a predisposition for similar looking designs. To accurately evaluate if the transformation path is an improvement in the aerodynamics of the vehicle, the natural step would be to convert the image into a 3D representation that can be used &amp;nbsp;for high-fidelity CFD simulations.&lt;/p&gt; 
&lt;h2&gt;Generation of point-cloud using neural radiance fields&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://arxiv.org/abs/2012.02190"&gt;Neural radiance fields&lt;/a&gt;&amp;nbsp;(NeRF) are algorithms showing great promise for converting one or more images into full 3D representations. Combining&amp;nbsp;&lt;a href="https://arxiv.org/abs/2211.11674"&gt;bootstrapped NeRF&amp;nbsp;&lt;/a&gt;with &lt;em&gt;generative adversarial networks&lt;/em&gt; (GANs), we can reconstruct multiple poses of objects to augment and improve the predictions.&lt;/p&gt; 
&lt;p&gt;To make this work, we feed images of the car into NeRFs to obtain &lt;em&gt;signed-distance functions&lt;/em&gt; (SDFs) and construct point-cloud representations like we’ve shown in Figure 3. We fine-tuned the NeRF model using the &lt;a href="https://cvgl.stanford.edu/projects/pascal3d.html"&gt;Pascal3D&lt;/a&gt; data set for 3D object reconstruction.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 3: Point-cloud of car (bottom) obtained using NeRF." width="500" height="375" src="https://www.youtube-nocookie.com/embed/3NtY7eTLVTs?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 3: Point-cloud of car (bottom) obtained using NeRF by using the base image (top), transforming the image into a 3D structure representing the car.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Reconstruction of surface topology from point-cloud&lt;/h2&gt; 
&lt;p&gt;The point-cloud representation lacks the crucial connectivity or surface topology information that’s required for understanding the behavior of air flow around the vehicle.&lt;/p&gt; 
&lt;p&gt;To reconstruct the surface from the point-cloud, we first generated an &lt;em&gt;unstructured polygonal Alpha shape mesh&lt;/em&gt; (for a non-convex hull). We achieved this through a coarse Delaunay triangulation using the &lt;a href="http://www.open3d.org/"&gt;Open3D&lt;/a&gt; library. This computational geometric technique identifies the encompassing surface including the points presented in the point-cloud, generated from NeRF.&lt;/p&gt; 
&lt;p&gt;To further refine the mesh, we extracted the points generated on the surface (nodes of the initial triangulation) together with estimated normals from the Alpha shapes. This surface point-cloud is ingested into a&amp;nbsp;&lt;a href="https://research.nvidia.com/labs/toronto-ai/NKSR/"&gt;Neural Kernel Surface Reconstruction&lt;/a&gt;&amp;nbsp;(NKSR) algorithm, which is a machine learning technique that can perform fast surface reconstructions from sparse data. The final result is displayed in Figure 4. While this technique doesn’t capture finer surface details, the overall shape of the car is approximately modeled by the resulting mesh.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 4: Surface mesh generated using Neural Kernel Surface Reconstruction" width="500" height="375" src="https://www.youtube-nocookie.com/embed/sgHBGi5kMN0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 4: Surface mesh generated using Neural Kernel Surface Reconstruction, with the original point cloud (top) and the triangulated mesh (bottom) showing a good match in general topographic features.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Running CFD simulations on OpenFOAM&lt;/h2&gt; 
&lt;p&gt;We used &lt;a href="https://www.openfoam.com/"&gt;OpenFoam&lt;/a&gt; to compute the flow field around the vehicle. We build an unstructured hex-dominate mesh with prismatic boundary layer cells using blockMesh and SnappyHexMesh from the .obj file we generated in the previous step.&lt;/p&gt; 
&lt;p&gt;For this post, we intentionally opted for much lower refinement levels than what is typically employed in the industry (we can increase these levels as required). Our mesh count was approximately one million cells, on average – this changes slightly depending on the geometry itself. To accelerate the CFD part of the process we restricted ourselves to steady-state RANS simulations using the k-omega SST model (for industrial applications you could extend this to use hybrid RANS-LES or WMLES methods, which have a higher fidelity).&lt;/p&gt; 
&lt;p&gt;Finally, in our setup we used the &lt;em&gt;simpleFoam&lt;/em&gt; solver based upon the semi-implicit method for pressure-linked equations (SIMPLE) algorithm. Table 1 shows the parameters.&lt;/p&gt; 
&lt;div id="attachment_2924" style="width: 410px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2924" loading="lazy" class="wp-image-2924 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.37.31@2x-1.png" alt="Table 1: Constants used for computational fluid dynamics simulations" width="400" height="155"&gt;
 &lt;p id="caption-attachment-2924" class="wp-caption-text"&gt;Table 1: Constants used for computational fluid dynamics simulations&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The image in Figure 5 displays the streamlines as well as surface pressure over the initial car design. For this illustrative simulation, we used 425,045 cells for the mesh.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 5: Streamlines visualizing flow field around a generated car mesh." width="500" height="281" src="https://www.youtube-nocookie.com/embed/EUzvYkivaZo?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 5: Streamlines visualizing flow field around a generated car mesh – these are indicative of the fluid velocities, while the colors on the car surface represent the surface pressure magnitude.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;To compute the drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;values during post-processing, we derived a reference wheelbase length and frontal surface areas based on the initial designs – these reference values remain relatively constant across all observations. We used the final drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;values in the pipeline to evaluate and rank the generated design options to find the best intermediate choices.&lt;/p&gt; 
&lt;h2&gt;Integration into Simulation-Guided Design Workflow on AWS&lt;/h2&gt; 
&lt;p&gt;The overall workflow has five key components: image generation, Stable Diffusion, point-cloud with NeRF, mesh generation with Open3d and NKSR, and finally the OpenFoam CFD simulation. Each of these are containerized and orchestrated by the&amp;nbsp;&lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt; orchestration module within the &lt;a href="https://aws.amazon.com/blogs/hpc/predictive-models-and-simulations-with-twinflow-on-aws/"&gt;TwinFlow framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We deployed this workflow by using&amp;nbsp;&lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;&amp;nbsp;and scaled it as needed to find the optimal designs. We achieved the necessary scale by leveraging both CPU &lt;em&gt;and&lt;/em&gt; GPU architectures depending on the specific requirements of each workflow component.&lt;/p&gt; 
&lt;div id="attachment_2899" style="width: 1720px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-2899" loading="lazy" class="size-full wp-image-2899" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.32.19@2x.png" alt="Figure 6: AWS Architecture diagram for a running workflow from connecting with a remote client, to launching the algorithmic pipeline on TwinGraph and executing jobs on AWS Batch, and visualizing results." width="1710" height="974"&gt;
 &lt;p id="caption-attachment-2899" class="wp-caption-text"&gt;Figure 6: AWS Architecture diagram for a running workflow from connecting with a remote client, to launching the algorithmic pipeline on TwinGraph and executing jobs on AWS Batch, and visualizing results.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We repeated the experiment for a number of generated images across multiple cycles, and uploaded the results from each experiment automatically to&amp;nbsp;&lt;a href="https://aws.amazon.com/s3/"&gt;Amazon Simple Storage Service (Amazon S3)&lt;/a&gt;. This ensured persistent storage of our results. The necessary meta-data provenance information from each experiment was automatically uploaded to an&amp;nbsp;&lt;a href="https://aws.amazon.com/neptune/"&gt;Amazon Neptune&lt;/a&gt;&amp;nbsp;graph database for the subsequent analysis.&lt;/p&gt; 
&lt;p&gt;Once the results were generated, we could retrieve the specific outcomes we were interested in from Amazon S3 using a GPU instance – we ran our visualization interface thought a high-performance remote desktop protocol called &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;NICE DCV&lt;/a&gt; (an AWS product).&lt;/p&gt; 
&lt;p&gt;Overall,&amp;nbsp;&lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt;&amp;nbsp;orchestrates tasks in an asynchronous manner, which means we can execute multiple experiments concurrently, at scale using AWS Batch.&lt;/p&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;p&gt;As part of the numerical experiments, we ran 10 different instances (10 &lt;em&gt;variants&lt;/em&gt;) of the Stable Diffusion image-to-image generation, surface reconstruction, &lt;em&gt;and&lt;/em&gt; CFD simulations with the same initialization as in Figure 1 with a generic sedan.&lt;/p&gt; 
&lt;p&gt;We used the variant image corresponding to the lowest drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;value, to seed the &lt;em&gt;next&lt;/em&gt; image generation sequence cycle. To guide the design iteration process at a fine level, we reduced the strength of image-to-image changes significantly, compared to Figure 1.&lt;br&gt; We repeated this cycle 20 times (or 20 &lt;em&gt;generations&lt;/em&gt;) and the results are plotted in Figure 7 as the minimum drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;per generation and in Figure 8 as a heat map of the drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) &amp;nbsp;&amp;nbsp;values for each of the generated designs.&lt;/p&gt; 
&lt;div id="attachment_2900" style="width: 1608px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-2900" loading="lazy" class="size-full wp-image-2900" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.33.34@2x.png" alt="Figure 7: Minimum drag coefficients per generation, corresponding to the image variants used to create the subsequent generation of variants. The general downward trend, though non-monotonic due to intermediate car configurations, is indicative that the pipeline improves the drag performance of the best variant through the generations." width="1598" height="968"&gt;
 &lt;p id="caption-attachment-2900" class="wp-caption-text"&gt;Figure 7: Minimum drag coefficients per generation, corresponding to the image variants used to create the subsequent generation of variants. The general downward trend, though non-monotonic due to intermediate car configurations, is indicative that the pipeline improves the drag performance of the best variant through the generations.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 7 shows that the averaged drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) decreases gradually from an initial value of 0.46 to approximately 0.4. During the sequential generations, the decrease is non-monotonic. This is due to the non-parametric and non-linear nature of the optimization procedure – allowing the image generation process to arbitrarily morph the car design with a final goal of reducing drag.&lt;/p&gt; 
&lt;p&gt;Moreover, the intermediate design configurations have incomplete components which result in increased drag for several generations until the image generation process resolves the features. We investigated this further through drag coefficients corresponding to &lt;em&gt;each variant&lt;/em&gt; in the 20 generations we show in Figure 8. The average drag coefficient (C&lt;sub&gt;d&lt;/sub&gt;) slightly &lt;em&gt;increases&lt;/em&gt; in the intermediate generations but then gradually &lt;em&gt;decreases&lt;/em&gt; towards the end of the 20 generations.&lt;/p&gt; 
&lt;div id="attachment_2901" style="width: 1632px" class="wp-caption alignright"&gt;
 &lt;img aria-describedby="caption-attachment-2901" loading="lazy" class="size-full wp-image-2901" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/CleanShot-2023-09-26-at-11.34.33@2x.png" alt="Figure 8: Illustration of drag coefficients associated with each of 10 variants (vertical axes) during each of generations (horizontal axes, sequences consisting of image generation, mesh reconstruction and simulation). The blue regions indicate lower drag, and we can observe a trend of increased blue regions going across the generations." width="1622" height="904"&gt;
 &lt;p id="caption-attachment-2901" class="wp-caption-text"&gt;Figure 8: Illustration of drag coefficients associated with each of 10 variants (vertical axes) during each of generations (horizontal axes, sequences consisting of image generation, mesh reconstruction and simulation). The blue regions indicate lower drag, and we can observe a trend of increased blue regions going across the generations.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2918" style="width: 482px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2918" loading="lazy" class="size-full wp-image-2918" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/26/Figure9-converted.gif" alt="Figure 9: Sequence of transitions through the generations for the optimal variant in drag coefficient, demonstrating a smoothing of the hood of the car, a reduced angle of windshield." width="472" height="346"&gt;
 &lt;p id="caption-attachment-2918" class="wp-caption-text"&gt;Figure 9: Sequence of transitions through the generations for the optimal variant in drag coefficient, demonstrating a smoothing of the hood of the car, a reduced angle of windshield.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 9 provides insights into the evolution of car design across generations. The hood of the car adapts to a curved shape with considerable removal of material. Also, the angle of the windshield to the horizon reduces and there are slight changes in the curvature of the car’s rear section. Overall these are subtle, yet significant, changes driven by a generative AI process demonstrating the potential for guiding and making informed design choices through an automated &lt;em&gt;physics-informed&lt;/em&gt; pipeline.&lt;/p&gt; 
&lt;h2&gt;Limitations and future work&lt;/h2&gt; 
&lt;p&gt;The approaches presented here hold promise for accelerating aesthetically and sustainability-focused non-parametric design optimization. But there are limits at each stage that will need to be overcome.&lt;/p&gt; 
&lt;p&gt;As pre-trained Stable Diffusion network weights change and evolve (due to further training), the predictions will change in an unpredictable manner – making repeatability an issue. Also, capturing surface topology and roughness accurately using this method is complex due to the lossy reconstruction from point-clouds compared to full resolution computer-aided design (CAD) meshes. This is important for accurate drag calculations.&lt;/p&gt; 
&lt;p&gt;However, with improvements in generative AI algorithms, we can expect workflows that couple machine learning to classic physics-based simulations to provide practical benefits. We can integrate multiple components into a single algorithmic pipeline that can scale agnostically with respect to the underlying infrastructure, computer architecture and choice of programming models. This provides a template to deploy &lt;em&gt;future&lt;/em&gt; design optimization concepts across multiple domains more easily and reliably.&lt;/p&gt; 
&lt;h2&gt;Conclusions&lt;/h2&gt; 
&lt;p&gt;In this post, we discussed the potential for integrating generative AI techniques with physics-based CFD simulations. We demonstrated a methodology that has the capability to guide the image generation process with physics-informed drag coefficients (C&lt;sub&gt;d&lt;/sub&gt;) using CFD simulations.&lt;/p&gt; 
&lt;p&gt;We also showcased how to turn these images into 3D meshes. These meshes were used for the CFD simulations, but can &lt;em&gt;also &lt;/em&gt;be imported into CAD programs so they can be used in real design processes.&lt;/p&gt; 
&lt;p&gt;Best of all, we combined this into a single event-driven workflow thanks to &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; and &lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph&lt;/a&gt; – which allows for scaling out machine learning and simulation tasks.&lt;/p&gt; 
&lt;p&gt;This work has been focused on running inference using generative AI models, but we could use &lt;a href="https://aws.amazon.com/bedrock/"&gt;Amazon Bedrock&lt;/a&gt; and &lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html"&gt;Amazon SageMaker JumpStart&lt;/a&gt; to improve the developer experience when fine-tuning the &amp;nbsp;models. Amazon Bedrock is a fully managed service that provides foundation models from a collection of leading AI startups (and from Amazon itself) via an API. Bedrock allows you to speed up your development of generative AI applications that you can privately customize and scale using secure and reliable infrastructure in AWS. SageMaker Jumpstart offers you the capability to train and fine tune those foundation models in a managed environment.&lt;/p&gt; 
&lt;p&gt;This approach still requires further development to be applicable to industry, but it demonstrates the potential of integrating generative AI techniques with physics-based simulations. This potential extends beyond automotive CFD design and holds promise to a lot of other scientific and engineering fields.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Implementing AWS ParallelCluster in a Shared VPC</title>
		<link>https://aws.amazon.com/blogs/hpc/implementing-aws-parallelcluster-in-a-shared-vpc/</link>
		
		<dc:creator><![CDATA[Pedro Gil]]></dc:creator>
		<pubDate>Tue, 26 Sep 2023 15:11:47 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">dbe82324fb2c3a9277531ded24917622b6e0ec25</guid>

					<description>In this post we’ll show you how to deploy ParallelCluster in a shared VPC environment so you can separate infrastructure management, cluster operations, and help segregate costs, too.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was controbuted by Pedro Gil, Solutions Architect, and Ryan Anderson, Software Engineer HPC Engineering&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;AWS Shared Virtual Private Cloud (VPC) is a feature that allows multiple AWS accounts to share a single VPC, enabling them to collaborate on resources within the same network. This helps in managing and sharing network resources within an organization, and allows teams to work independently without compromising the security of the VPC.&lt;/p&gt; 
&lt;p&gt;AWS ParallelCluster is an open source cluster management tool that makes it easy for you to deploy and manage high performance computing (HPC) clusters on AWS.&lt;/p&gt; 
&lt;p&gt;Installing ParallelCluster in a shared VPC – when using Slurm as the scheduler – is often a challenge because ParallelCluster assumes that the Amazon Route53 Hosted Zone and the VPC belongs to the same account where the cluster is being created.&lt;/p&gt; 
&lt;p&gt;In this post we’ll show you a solution that gets ParallelCluster up and running in a shared VPC environment where the VPC belongs to one account and it is shared to another account for resource deployment operations.&lt;/p&gt; 
&lt;h2&gt;Overview of our solution&lt;/h2&gt; 
&lt;p&gt;We’ll show you how to deploy ParallelCluster into Account B using a shared VPC from infrastructure in Account A.&lt;/p&gt; 
&lt;div id="attachment_2808" style="width: 1624px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2808" loading="lazy" class="size-full wp-image-2808" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-16.41.52@2x.png" alt="Figure 1 – VPC Resource Share created on Account A (VPC infrastructure account) and shared to Account B (ParallelCluster creation account) using AWS Resource Access Manager." width="1614" height="1400"&gt;
 &lt;p id="caption-attachment-2808" class="wp-caption-text"&gt;Figure 1 – VPC Resource Share created on Account A (VPC infrastructure account) and shared to Account B (ParallelCluster creation account) using AWS Resource Access Manager.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;These are the steps we’ll take for the solution:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Step 1- Create a Route 53 Private Hosted Zone and associate it with shared VPC&lt;/li&gt; 
 &lt;li&gt;Step 2- Create an additional AWS Identity and Access Management&lt;/li&gt; 
 &lt;li&gt;(IAM) Policy&lt;/li&gt; 
 &lt;li&gt;Step 3- Install AWS ParallelCluster and its configuration file&lt;/li&gt; 
 &lt;li&gt;Step 4- Modify the configuration file to add the additional policy and to include the &lt;code&gt;Hosted Zone ID&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Step 5- Create a cluster using this configuration file&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;For this walkthrough, you should have the following prerequisites:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An AWS account B (cluster creation account) where the VPC was shared and an AWS account A (VPC account owner) where the VPC was created (like in Figure 1).&lt;/li&gt; 
 &lt;li&gt;A user with sufficient privileges to create the IAM Policy, Amazon Route53 Private Zone and to install ParallelCluster.&lt;/li&gt; 
 &lt;li&gt;AWS resources: AWS console access, AWS CLI using AWS Cloud9&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Step 1 – Create Amazon Route53 Private Hosted Zone&lt;/h2&gt; 
&lt;p&gt;ParallelCluster uses Amazon Route53 Private Hosted Zone to resolve cluster nodes and creating one across accounts requires the following specific procedure.&lt;/p&gt; 
&lt;p&gt;First, log in to Account B and create a Private Hosted Zone using the Route53 service console. Associate it with any existing VPC in Account B (you’ll remove this association at the end of this section). Take note of the Private Hosted Zone id you just created.&lt;/p&gt; 
&lt;p&gt;Next, using AWS Cloud9 in Account B, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws route53 create-vpc-association-authorization --hosted-zone-id &amp;lt;hosted-zone-id&amp;gt; --vpc VPCRegion=&amp;lt;region&amp;gt;,VPCId=&amp;lt;vpc-id&amp;gt; --region &amp;lt;region&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command authorizes the VPC association between the private hosted zone you just created and the VPC from Account A. Use the Hosted Zone ID that you obtained in previous step. Use the AWS region and ID of the shared VPC.&lt;/p&gt; 
&lt;p&gt;Now, using AWS Cloud9 instance in Account A, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws route53 associate-vpc-with-hosted-zone --hosted-zone-id &amp;lt;hosted-zone-id&amp;gt; --vpc VPCRegion=&amp;lt;region&amp;gt;,VPCId=&amp;lt;vpc-id&amp;gt; --region &amp;lt;region&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This command creates the association between the private hosted zone in Account B and the VPC in Account A. Use the Hosted Zone ID from earlier, and the Region and ID of the VPC in Account A.&lt;/p&gt; 
&lt;p&gt;Finally, go back to the Route 53 service console on Account B and verify that the shared VPC association with the Private Hosted Zone is listed. Delete the association of the local VPC done in step A.&lt;/p&gt; 
&lt;h2&gt;Step 2 – Create the IAM policy&lt;/h2&gt; 
&lt;p&gt;We need to create an additional policy for the head node to have permissions to create cluster nodes in the shared VPC.&lt;/p&gt; 
&lt;p&gt;First, login to Account B and create a new IAM Policy using the following template. Use ManageHeadnodePermissions as the name of new policy. Use ID for Account A and the subnet ID from the Shared VPC where the compute nodes will be created.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;{
    "Version": "2012-10-17",
   "Statement": [
        {
            "Sid": "SharedSubnets",
            "Effect": "Allow",
            "Action": [
                "ec2:CreateTags",
                "ec2:RunInstances",
                "ec2:CreateFleet"
            ],
    "Resource": "arn:aws:ec2:&amp;lt;region&amp;gt;:&amp;lt;Account A ID&amp;gt;:subnet/&amp;lt;Subnet ID&amp;gt;"
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Step 3 – Install and configure ParallelCluster&lt;/h2&gt; 
&lt;p&gt;You should follow the steps to &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-configuring.html"&gt;install ParallelCluster&lt;/a&gt; on a suitable instance or laptop. After doing so, you’ll need to create a config file for ParallelCluster to use. The pcluster configure will step you through this process, asking you some questions, and creating a new config file at the end.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster configure –config config-file.yaml&lt;/code&gt;&lt;/pre&gt; 
&lt;ol&gt; 
 &lt;li&gt;Choose the AWS region ID where your shared VPC is located.&lt;/li&gt; 
 &lt;li&gt;Choose your Amazon Elastic Compute Cloud (Amazon EC2) key pair – you’ll need to have one already.&lt;/li&gt; 
 &lt;li&gt;Choose Slurm as your scheduler&lt;/li&gt; 
 &lt;li&gt;Choose &lt;strong&gt;&amp;lt;n&amp;gt;&lt;/strong&gt; for VPC creation and select the existing shared VPC&lt;/li&gt; 
 &lt;li&gt;Choose an appropriate operating system&lt;/li&gt; 
 &lt;li&gt;Select appropriate instance types and queue configurations for your workload&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The next step would usually be to run pcluster create to build the cluster using the choices and parameters you just entered, however before we do that, we need to delve into the configuration file that this process produced and make some changes.&lt;/p&gt; 
&lt;h2&gt;Step 4 – Modify ParallelCluster configuration file&lt;/h2&gt; 
&lt;p&gt;Modify your ParallelCluster configuration file to include the following using your own Hosted Zone ID and the new Policy Name you created in the previous steps &lt;code&gt;ManageHeadnodePermissions&lt;/code&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;HeadNode:
  Iam:
    AdditionalIamPolicies:
      - Policy: arn:aws:iam::&amp;lt;Account B ID&amp;gt;:policy/ManageHeadnodePermissions
Scheduling:
  Scheduler: slurm
  SlurmSettings:
    Dns:
      HostedZoneId: &amp;lt;hosted-zone-id&amp;gt; 
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Step 5 – Create your cluster&lt;/h4&gt; 
&lt;p&gt;It’s time to create your cluster. If you need to, you can find more details &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-configuring.html"&gt;in our documentation&lt;/a&gt;. But for now, you just need to run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster –cluster-name test-cluster –cluster-configuration cluster-config.yaml&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Congratulations, you have finished the creation of your cluster using AWS ParallelCluster in a shared VPC.&lt;/p&gt; 
&lt;h2&gt;Cleaning up&lt;/h2&gt; 
&lt;p&gt;It is a best practice to delete the association authorization after you create the association. This step prevents you from recreating the same association later and will not prevent you to create new ParallelCluster instances later. To delete the authorization, reconnect to Account A. Then, run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws route53 delete-vpc-association-authorization --hosted-zone-id &amp;lt;hosted-zone-id&amp;gt; --vpc VPCRegion=&amp;lt;region&amp;gt;,VPCId=&amp;lt;vpc-id&amp;gt; --region &amp;lt;region&amp;gt;&amp;nbsp;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You might also want to delete cluster resources after you are done with your workload by running the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster delete-cluster –region &amp;lt;region&amp;gt; --cluster-name test-cluster&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;We’ve shown you how to install AWS ParallelCluster in a shared VPC environment, which means you can use a common VPC between AWS accounts inside an organization, while keeping billing and ownership separate for the users of the cluster.&lt;/p&gt; 
&lt;p&gt;When creating other clusters all you need to do is include the additional policy in the &lt;code&gt;headnode&lt;/code&gt; section of the configuration file and make sure you use the proper &lt;code&gt;Hosted Zone ID&lt;/code&gt;. Using AWS batch in ParallelCluster does &lt;em&gt;not&lt;/em&gt; require any changes to the cluster configuration or Route53 entry since it relies on its own internal mechanism to resolve hostnames.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Introducing a community recipe library for HPC infrastructure on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/introducing-a-community-recipe-library-for-hpc-infrastructure-on-aws/</link>
		
		<dc:creator><![CDATA[Matt Vaughn]]></dc:creator>
		<pubDate>Mon, 25 Sep 2023 15:57:26 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Amazon FSx]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">7fc9c3693f241025a5266e1ccaa748626ae48563</guid>

					<description>Today we’re showing you our community library of HPC Recipes for AWS. It's a public repo @github that will help you achieve feature-rich, reliable HPC deployments ready to run your workloads no matter where you're starting from.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-2887" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/20/boofla88_a_tree_of_human_knowledge_e32ef8b1-ed02-4bf2-8ba4-523ccfdb89f2-1.png" alt="" width="380" height="212"&gt;We want to make it easier for customers to extend and build on AWS using tools like AWS ParallelCluster, Amazon FSx for Lustre, and some of the hundreds of other AWS services that customers often use to make discoveries from their data or simulations.&lt;/p&gt; 
&lt;p&gt;Recently, we introduced you to new capabilities in ParallelCluster that provide a neat way to &lt;a href="https://aws.amazon.com/blogs/hpc/automate-your-clusters-by-creating-self-documenting-hpc-with-aws-parallelcluster/"&gt;create self-documenting infrastructure&lt;/a&gt; – mainly by first writing the specs into AWS CloudFormation templates, then letting CloudFormation build the infrastructure for you behind the scenes. This lets you version control your cluster by putting your templates under source control. You can even embed their management in continuous integration systems.&lt;/p&gt; 
&lt;p&gt;Today we’re making available a community library of patterns that build on that functionality. &lt;strong&gt;HPC Recipes for AWS &lt;/strong&gt;is a &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/"&gt;public repository at GitHub&lt;/a&gt; that hosts interoperable CloudFormation templates designed to work together to build complete HPC environments. We believe this library will help customers achieve feature-rich, &lt;em&gt;reliable&lt;/em&gt; HPC deployments that are ready to run diverse workloads – regardless of where they’re starting from.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll provide some background about this new project, and take you for a tour of some of its components. Finally, we’ll show you how to cook up a cluster from these recipes in just a few minutes.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;By design, ParallelCluster makes it straightforward to create and manage HPC clusters. It handles the undifferentiated heavy lifting to orchestrate the compute, networking, and storage resources you need (assembled from around a dozen AWS services) into a coherent whole. Customers tell us that most common paths through cluster creation and management are well-served by ParallelCluster through its declarative &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/cluster-configuration-file-v3.html"&gt;cluster template file&lt;/a&gt;, reliable resource management, and an optional &lt;a href="https://www.youtube.com/watch?v=FAfVbiTVk24"&gt;web user interface&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;However, customers &lt;em&gt;also&lt;/em&gt; told us about two papercuts that they needed help with.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;First:&lt;/strong&gt; it can be complicated to incorporate external resources, like user authentication resources, or existing filesystems, or relational databases for job accounting – into ParallelCluster. The “right way” usually includes custom scripts, home-grown CloudFormation templates, or documented steps and workarounds.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Second: &lt;/strong&gt;Most of these problems have been solved before, sometimes by AWS solution architects and sometimes by the community at large. Why aren’t these solutions discoverable and reusable? ParallelCluster has made the cluster part of HPC easier, but there can be a steep learning curve involved to stand up the related parts of the infrastructure.&lt;/p&gt; 
&lt;p&gt;We agreed.&lt;/p&gt; 
&lt;p&gt;So, in ParallelCluster 3.6, we &lt;a href="https://youtu.be/dj2ZDmNOJps"&gt;built support for CloudFormation&lt;/a&gt;, so anyone could write templates to create and manage ParallelCluster clusters. These templates could sit beside other templates that launch surrounding and supporting resources.&lt;/p&gt; 
&lt;p&gt;But this opened up a new challenge: There are many ways to set up these resources, all quite valid. We wanted builders to be comfortable leveraging each other’s work, but without continuously reinventing each other’s wheels. We needed some mechanisms for making CloudFormation templates modular, interoperable, and shareable. And that led us to the work we’re sharing today.&lt;/p&gt; 
&lt;h2&gt;Introducing… HPC Recipes for AWS&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;HPC Recipes for AWS&lt;/strong&gt; is a public GitHub repo with over 20 recipes, organized into themes like networking, storage, user environments, and vertical integrations. Each recipe features downloadable assets, along with documentation and metadata to assist with discovery and attribution. The repository is managed by the HPC engineering team, with contributions from AWS solutions architects and our customers.&lt;/p&gt; 
&lt;p&gt;In the repo, you’ll find two general kinds of recipe:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Modular templates&lt;/strong&gt; – these launch specific services or enable particular configurations, and can be to stitched together with other templates. To that end, they share a common naming convention for their &lt;em&gt;parameters&lt;/em&gt; (inputs) and &lt;strong&gt;&lt;em&gt;outputs&lt;/em&gt;. &lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;One-click launchable stacks&lt;/strong&gt; – these combine multiple building blocks from the modular templates catalog, and often launch complete working clusters that you can adopt, tweak, or make your own.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Modular templates are great for standing up pieces of functionality that might be used by several other services or clusters. A great example is the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/db/slurm_accounting_db/"&gt;serverless SQL database&lt;/a&gt; you can use to power Slurm’s accounting features. Or it could be larger: setting up an &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/fsx_lustre"&gt;Amazon FSx for Lustre filesystem&lt;/a&gt; that’s going to be shared between multiple clusters.&lt;/p&gt; 
&lt;p&gt;One-click launchable stacks are more &lt;em&gt;opinionated&lt;/em&gt; assemblies of the modular components. They can be launched quickly, usually after you make some minor customization choices. For instance, you can launch &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/pcui/"&gt;AWS ParallelCluster UI&lt;/a&gt; with an HPC cluster ready-to-go. Or, you can bring up a specifically-tuned HPC cluster to try out the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/try_hpc7g/"&gt;latest AWS Graviton3E CPUs in our Hpc7g instances&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We think these recipes will help you quickly find a pattern that most closely resembles your own to &amp;nbsp;get started with. Afterwards, you can take a stack and customize it to make it your own. In any case, you can make tactical changes as your needs evolve over time, because … it’s the cloud.&lt;/p&gt; 
&lt;h3&gt;Fairness is nice, and useful too.&lt;/h3&gt; 
&lt;p&gt;Our repo is designed to be &lt;a href="https://www.nature.com/articles/s41597-022-01710-x"&gt;F.A.I.R.&lt;/a&gt; software to help ensure it is broadly useful:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Findable&lt;/strong&gt;: the recipes are organized into categories and clearly named, and are tagged by their relevant technologies and attributions so they’re easier to discover.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accessible&lt;/strong&gt;: the collection is hosted in a public GitHub repository under a MIT-0 license, which is standard for many AWS open-source projects. We make it even more accessible by mirroring assets from its recipes to Amazon S3, which gives every file an HTTPS URL that can be used with CloudFormation and other services. This means they can be imported into other recipes or embedded in quick-create links.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interoperable&lt;/strong&gt;: recipes are written in AWS CloudFormation or AWS CDK. They follow standards (where available) and best practices for naming and design. There is a good-faith effort to use clear, standard names for parameters, outputs, and exports.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reusable&lt;/strong&gt;: there are a growing number of modular infrastructure recipes. We intend that these can be used directly, but also imported into other recipes (even outside this collection). Furthermore, each recipe is clearly documented towards being a teaching instrument to promote modification and adaptation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It turns out that being FAIR can also make a project quite useful, as we’ll see next, as we explore how some of its recipes (and the repo’s design) can align to simplify HPC on AWS.&lt;/p&gt; 
&lt;h2&gt;Off to the Kitchen&lt;/h2&gt; 
&lt;p&gt;Let’s head to a virtual test kitchen to compare three recipes for cooking up a cluster with a persistent Lustre filesystem backed by Amazon Simple Storage Service (Amazon S3).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The first&lt;/strong&gt; shows a common case, where you want to use resources provisioned separately from your cluster. Using this, you’ll need to look up several values and input them into a launch template. This is the most versatile option if you’re a frequent CloudFormation user, but can lead to some repetitive tasks. It’s useful to understand how this works – especially if you’re likely to borrow stacks from other repos from time to time.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;The second&lt;/strong&gt; shows what can be done with infrastructure stacks that are written to work together – like the ones in the recipe library.&lt;/p&gt; 
&lt;p&gt;Finally, &lt;strong&gt;the third&lt;/strong&gt; recipe demonstrates how to connect separate stacks into a single streamlined launch template. This is the fastest way to get started with a complete environment, but you’ll probably want to customize these to adapt to your needs before you get too serious.&lt;/p&gt; 
&lt;p&gt;As we go through the recipes, we’ll call out how key features of HPC Recipes on AWS and AWS CloudFormation enable these designs.&lt;/p&gt; 
&lt;h2&gt;Recipe 1: cluster by hand&lt;/h2&gt; 
&lt;p&gt;Our first recipe uses &lt;strong&gt;modular templates&lt;/strong&gt; that create supporting infrastructure, then has us configure the cluster by hand with their outputs. Found at &lt;strong&gt;training/try_recipes_1&lt;/strong&gt;, it involves several steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create an HPC-ready VPC using the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/net/hpc_basic/"&gt;&lt;strong&gt;net/hpc_basic&lt;/strong&gt;&lt;/a&gt; There are several fields in this template, but the only one you &lt;em&gt;need&lt;/em&gt; to set is &lt;strong&gt;Availability Zone&lt;/strong&gt;. When the stack has been created, look up the VPC and subnet IDs in its outputs. They will be named &lt;strong&gt;VPC&lt;/strong&gt;, &lt;strong&gt;DefaultPublicSubnet&lt;/strong&gt;, and &lt;strong&gt;DefaultPrivateSubnet&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Provision an Amazon S3 bucket to back the FSx for Lustre filesystem with &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/s3_demo/"&gt;&lt;strong&gt;storage/s3_demo&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Now, create a persistent Lustre filesystem with a data repository association using &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/fsx_lustre_s3_dra/"&gt;&lt;strong&gt;storage/fsx_lustre_s3_dra&lt;/strong&gt;.&lt;/a&gt; Put the Amazon S3 bucket name in &lt;strong&gt;DataRepositoryPath&lt;/strong&gt;, formatted as an S3 URL. Select the networking stack VPC in &lt;strong&gt;VpcId&lt;/strong&gt; and its private subnet for &lt;strong&gt;SubnetId&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Once the filesystem and other resources are created, go the cluster recipe, and choose &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/training/try_recipes_1/"&gt;&lt;strong&gt;Launch stack&lt;/strong&gt;&lt;/a&gt; to create a an HPC system using outputs from all the supporting stacks.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To accomplish this last step, you’ll need to do additional output-to-parameter mappings (Figure 1):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Select the public subnet from your network stack for This is where the head node will launch. The private subnet goes in &lt;strong&gt;ComputeNodeSubnetId&lt;/strong&gt;. This is for the compute nodes.&lt;/li&gt; 
 &lt;li&gt;Go to the Outputs tab in the filesystem stack. Find the value for &lt;strong&gt;FSxLustreFilesystemId&lt;/strong&gt; and use it for your cluster’s &lt;strong&gt;FilesystemId&lt;/strong&gt; Use the value for &lt;strong&gt;FSxLustreSecurityGroupId&lt;/strong&gt; for the &lt;strong&gt;FilesystemSecurityGroupId&lt;/strong&gt; setting.&lt;/li&gt; 
 &lt;li&gt;Finally, choose the operating system, architecture, number of compute instances, and Lustre filesystem size and finish launching the stack.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div id="attachment_2880" style="width: 767px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2880" loading="lazy" class="size-full wp-image-2880" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/20/CleanShot-2023-09-20-at-17.01.08.png" alt="Figure 1. Cluster launch template requests outputs from other CloudFormation stacks" width="757" height="643"&gt;
 &lt;p id="caption-attachment-2880" class="wp-caption-text"&gt;Figure 1. Cluster launch template requests outputs from other CloudFormation stacks&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In around 10-15 minutes, the cluster will be ready to use.&lt;/p&gt; 
&lt;p&gt;Go to stack outputs and choose &lt;strong&gt;SystemManagerUrl&lt;/strong&gt; to log into the cluster with &lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/what-is-systems-manager.html"&gt;Amazon SSM&lt;/a&gt;. Once you’re in, you can view queues and run jobs. If you upload some data to the S3 bucket, it’ll &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data-repo.html"&gt;show up in the Lustre shared filesystem&lt;/a&gt; (and vice versa).&lt;/p&gt; 
&lt;p&gt;To shut down the cluster and its resources, delete the stacks in reverse order from when they were created. Start with the cluster stack, then the FSx storage stack, followed by the Amazon S3 stack, and finally, the network stack.&lt;/p&gt; 
&lt;p&gt;You might also notice that each recipe has a quick-create link that launches the AWS CloudFormation console. These are enabled by the automatic mirror of recipe assets to the Amazon S3 bucket we mentioned earlier.&lt;/p&gt; 
&lt;p&gt;Here is a template for creating a quick-create link:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-html"&gt;https://console.aws.amazon.com/cloudformation/home?region=REGION#/stacks/create/review?stackName=OPTIONAL-STACK-NAME&amp;amp;templateURL=HPC-RECIPES-S3-URL&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;All recipes that include a CloudFormation template can be embedded this way. You can learn more about quick-create links in the &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-stacks-quick-create-links.html"&gt;CloudFormation User Guide&lt;/a&gt; and the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes#incorporating-recipe-assets"&gt;documentation for this repo&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_2881" style="width: 900px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2881" loading="lazy" class="size-full wp-image-2881" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/20/CleanShot-2023-09-20-at-17.01.35.png" alt="Figure 2. Complex outputs-to-parameter mappings between CloudFormation stacks." width="890" height="523"&gt;
 &lt;p id="caption-attachment-2881" class="wp-caption-text"&gt;Figure 2. Complex outputs-to-parameter mappings between CloudFormation stacks.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Turning to the actual cluster recipe, there is a challenge with its design – the configuration is pretty simple yet we &lt;em&gt;still&lt;/em&gt; had to specify VPC and subnet two times and consult multiple stack outputs to launch it (Figure 2).&lt;/p&gt; 
&lt;p&gt;What if we need to integrate multi-user support or Slurm accounting (each their own stack with more sophisticated networking needs)? What if we also have additional filesystems, security groups, or IAM policies? We’d probably need a pen and paper to keep all the parameter mappings straight! It might also be challenging to remember how resources from the various stacks are related to one another when we need to make updates or delete them.&lt;/p&gt; 
&lt;h2&gt;Recipe 2: Cluster using Imports&lt;/h2&gt; 
&lt;p&gt;The next approach, &lt;strong&gt;training/try_recipes_2,&lt;/strong&gt; improves on the first by using CloudFormation’s &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-importvalue.html"&gt;ImportValue intrinsic function&lt;/a&gt; to bring in information from existing stacks. With this design, you provide the names of the &lt;em&gt;stacks&lt;/em&gt; that provide networking and filesystem support (Figure 3). Then the cluster CloudWatch template imports values &lt;em&gt;from their outputs&lt;/em&gt;.&lt;/p&gt; 
&lt;div id="attachment_2882" style="width: 868px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2882" loading="lazy" class="size-full wp-image-2882" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/20/CleanShot-2023-09-20-at-17.02.10.png" alt="Figure 3. Simplified templates with CloudFormation imports" width="858" height="517"&gt;
 &lt;p id="caption-attachment-2882" class="wp-caption-text"&gt;Figure 3. Simplified templates with CloudFormation imports&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Let’s see how it works in practice:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a HPC network stack with &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/net/hpc_basic/"&gt;&lt;strong&gt;net/hpc_basic&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Next, create an Amazon S3 bucket using the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/s3_demo/"&gt;&lt;strong&gt;storage/s3_demo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Stand up an Amazon FSx for Lustre filesystem using the “&lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/storage/fsx_lustre_s3_dra#alternative-import-stack"&gt;&lt;strong&gt;alternative import stack&lt;/strong&gt;&lt;/a&gt;” in the &lt;strong&gt;storage/fsx_lustre_s3_dra&lt;/strong&gt; recipe&lt;strong&gt;. &lt;/strong&gt;Instead of picking a VPC and subnet, just input the name of the networking stack from step 1 into &lt;strong&gt;NetworkStackNameParameter&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;When the FSx for Lustre stack is ready, go to the cluster recipe and choose &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/training/try_recipes_2/"&gt;&lt;strong&gt;Launch stack&lt;/strong&gt;&lt;/a&gt;. It will ask for the names of your &lt;em&gt;networking &lt;/em&gt;and &lt;em&gt;storage&lt;/em&gt; Then, it’ll use them to automatically import key configuration details like the VPC, subnets, and filesystem ID. That’s a lot less typing!&lt;/li&gt; 
 &lt;li&gt;Last, choose your operating system, architecture, number of compute instances, and size of the Lustre filesystem and complete the stack launch.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;After a few minutes, the cluster will be accessible via AWS Systems Manager, just like in the first recipe. To shut down the system and delete its dependencies, delete the stacks in reverse order from when they were created, beginning with the cluster stack.&lt;/p&gt; 
&lt;p&gt;As you can see, this approach streamlines the cluster creation process because you don’t have to look up parameter values – instead they are simply imported when the stack launches.&lt;/p&gt; 
&lt;p&gt;Besides providing a simplified end-user experience, there are two other benefits to this design. First, you can change out implementations of the modular stacks with your own CloudFormation templates. They just have to follow the parameter and export naming conventions expected by the other stacks. Second, this design helps promote sustainable infrastructure practices by organizing your deployment by logical units – like we recommend in the AWS Cloud Development Kit (CDK) &lt;a href="https://docs.aws.amazon.com/cdk/v2/guide/best-practices.html"&gt;best practices guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To get started with CloudFormation imports, read over the &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-hpc-recipes/main/recipes/training/try_recipes_2/assets/import.yaml"&gt;template for this recipe&lt;/a&gt;, as well as that of dependencies like the &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-hpc-recipes/main/recipes/net/hpc_basic/assets/public-private.yaml"&gt;networking recipe&lt;/a&gt;. Notice how exports from the dependency stacks get imported by name into the cluster template using the &lt;code&gt;Sub&lt;/code&gt; and &lt;code&gt;ImportValue&lt;/code&gt; intrinsic functions.&lt;/p&gt; 
&lt;h2&gt;Recipe 3: Automatic Cluster&lt;/h2&gt; 
&lt;p&gt;In our final recipe, &lt;strong&gt;training/try_recipes_3,&lt;/strong&gt; we demonstrate a &lt;strong&gt;one-click launchable stack&lt;/strong&gt;. The only required input is the Availability Zone you want to use. Everything else is automatically configured (Figure 4).&lt;/p&gt; 
&lt;p&gt;Using it is much simpler:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Go to the recipe and choose &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/training/try_recipes_3/"&gt;&lt;strong&gt;Launch stack&lt;/strong&gt;&lt;/a&gt;. You will be asked to select an Availability Zone, then choose the operating system, architecture, number of compute instances, and Lustre filesystem size. In a few minutes, the cluster will be ready to use. Go to stack outputs and navigate to &lt;strong&gt;SystemManagerUrl &lt;/strong&gt;to log in using a web console.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Getting rid of the various HPC resources is just as straightforward. Delete the main stack and CloudFormation will take care of shutting everything down in the correct order. If you want to keep some of the provisioned resources, you can go to the CloudFormation console, find the relevant component stack, and &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html"&gt;enable termination protection&lt;/a&gt; before deleting the parent stack (s).&lt;/p&gt; 
&lt;p&gt;This approach relies on an important CloudFormation capability called “&lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html"&gt;nested stacks&lt;/a&gt;“. With these, you can create resources by loading CloudFormation code from an S3 URL. In the case of this recipe, code for those resources comes from &lt;em&gt;other&lt;/em&gt; HPC recipes. It is quite an opinionated way of doing things, but provides a direct path for anyone to offer reproducible deployments of complex infrastructure for demonstrations, proofs of concept, or training.&lt;/p&gt; 
&lt;div id="attachment_2883" style="width: 894px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2883" loading="lazy" class="size-full wp-image-2883" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/09/20/CleanShot-2023-09-20-at-17.03.02.png" alt="Figure 4. Nested stacks can enable 1-step deployments of complex infrastructure" width="884" height="296"&gt;
 &lt;p id="caption-attachment-2883" class="wp-caption-text"&gt;Figure 4. Nested stacks can enable 1-step deployments of complex infrastructure&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To learn more about nested stacks, have a look at this &lt;a href="https://raw.githubusercontent.com/aws-samples/aws-hpc-recipes/main/recipes/training/try_recipes_3/assets/nested.yaml"&gt;recipe’s template file&lt;/a&gt;. Pay special attention to how &lt;code&gt;TemplateURL&lt;/code&gt; is used to import other HPC Recipes for AWS, and how &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/dynamic-references.html"&gt;dynamic references&lt;/a&gt; are used to link stack outputs and parameters.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;AWS HPC systems often depend on other AWS resources, like filesystems, networking, databases, or directory services. It can be complicated to set up all the dependencies and ensure they work together.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;HPC Recipes for AWS &lt;/strong&gt;is growing collection of modular and all-in-one infrastructure recipes that helps you accomplish that. You can learn from them, directly use them to configure and launch infrastructure, and extend or remix them to your own ends.&lt;/p&gt; 
&lt;p&gt;We invite you to try &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/latest/"&gt;launching an HPC cluster&lt;/a&gt; with one of these recipes today, &lt;a href="https://github.com/aws-samples/aws-hpc-recipes"&gt;explore the repository&lt;/a&gt; in greater detail, and contribute new recipes or improvements. You might also consider &lt;a href="https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars"&gt;starring the repository&lt;/a&gt; on GitHub so you can be informed of updates and new recipes.&lt;/p&gt; 
&lt;p&gt;Finally, if this new resource improves your workflow or helps you solve harder problems than you were before, reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt; and let us know about it.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Real-time quant trading on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/real-time-quant-trading-on-aws/</link>
		
		<dc:creator><![CDATA[Sam Farber]]></dc:creator>
		<pubDate>Tue, 19 Sep 2023 12:37:14 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[Financial Services]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[FSI]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">619fceb5fee43814924c6ba69d59f90e0b03f32b</guid>

					<description>In this post, we’ll show you an open-source solution for a real-time quant trading system that you can deploy on AWS. We’ll go over the challenges brought on by monitoring portfolios, the solution, and its components. We’ll finish with the installation and configuration process and show you how to use it.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-2756" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/02/boofla88_quantitative_trading_as_a_concept._blue_sky_background_b26f2fe6-a229-4eff-8e93-6e306b0ded60.png" alt="Real-time quant trading on AWS" width="380" height="212"&gt;This post was contributed by Boris Litvin, Financial Services SA; Sam Farber, Startup SA; Ronny Rodriguez, Senior Financial Services TPM; Adeleke Coker, Global Solutions Architect&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;There are a variety of vendors already providing real-time quant trading systems, the question is: “do we need another one?” We believe the answer is: “yes.” Quant trading is a never-ending arms race to extract alpha – the excess return a trader is able to attain, relative to the market. This is fueled by a flywheel of new compute capabilities, data analytics, and AI/ML. These made short-term (intraday) and mid-term (intraday to one week) alphas not only easier to discover, but also more feasible to execute.&lt;/p&gt; 
&lt;p&gt;As a result, we see an increased demand for a real-time quant trading cloud-native system to research and execute short/mid-term alpha, a popular strategy due to the higher risk/adjusted returns when compared to traditional ones.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll show you an open-source solution for a real-time quant trading system that you can deploy on AWS. We’ll go over the challenges brought on by monitoring portfolios, the solution, and its components. We’ll finish with the installation and configuration process and show you how to use it.&lt;/p&gt; 
&lt;p&gt;The nature of these short to mid-term trading strategies is very elastic, causing the timing of trading signals to be unpredictable. Portfolios change often, frequently overlap, and often contain hundreds to thousands of different positions. This in turn generates unpredictable and uneven demands for the compute capacity needed to manage these short-term portfolios throughout their lifecycle in real-time. Moreover, cost optimization is an important factor, influencing the feasibility of specific opportunities – in other words, price/performance matters.&lt;/p&gt; 
&lt;p&gt;The solution to the challenges described above is an AWS-native trading system capable of scaling up or down with near-zero operational overhead.&lt;/p&gt; 
&lt;div id="attachment_2739" style="width: 981px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2739" loading="lazy" class="size-full wp-image-2739" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/01/CleanShot-2023-08-01-at-16.39.53.png" alt="Figure 1 – This architecture describes the one-time installation process of the solution. It first details the use of the AWS Cloud Development Kit (AWS CDK) to deploy the solution into your AWS account. It then looks at the method of uploading certain values like API keys in AWS Secrets Manager. The diagram also shows the different application stacks that are created through AWS CloudFormation, like AWS Batch, different databases, as well as AWS Lambda. Keep in mind that for any specific customizations, stacks will need to be redeployed for application coding changes." width="971" height="495"&gt;
 &lt;p id="caption-attachment-2739" class="wp-caption-text"&gt;Figure 1 – This architecture describes the one-time installation process of the solution. It first details the use of the AWS Cloud Development Kit (AWS CDK) to deploy the solution into your AWS account. It then looks at the method of uploading certain values like API keys in AWS Secrets Manager. The diagram also shows the different application stacks that are created through AWS CloudFormation, like AWS Batch, different databases, as well as AWS Lambda. Keep in mind that for any specific customizations, stacks will need to be redeployed for application coding changes.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2740" style="width: 997px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2740" loading="lazy" class="size-full wp-image-2740" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/01/CleanShot-2023-08-01-at-16.40.26.png" alt="Figure 2 – This architecture describes the operational aspect of the solution and first shows the process of inserting the portfolio into Amazon DynamoDB. It then looks at the AWS Batch job that is triggered through AWS Lambda whenever there is a change in the portfolio. Finally, we have AWS Batch writing market data to Amazon Timestream as well as the use of Amazon Managed Grafana to produce real-time visualizations. Amazon EventBridge is also utilized to automatically trigger events based on trading hours." width="987" height="503"&gt;
 &lt;p id="caption-attachment-2740" class="wp-caption-text"&gt;Figure 2 – This architecture describes the operational aspect of the solution and first shows the process of inserting the portfolio into Amazon DynamoDB. It then looks at the AWS Batch job that is triggered through AWS Lambda whenever there is a change in the portfolio. Finally, we have AWS Batch writing market data to Amazon Timestream as well as the use of Amazon Managed Grafana to produce real-time visualizations. Amazon EventBridge is also utilized to automatically trigger events based on trading hours.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;AWS Batch is our mechanism to achieve system elasticity. It allows the flexibility to run on any Amazon Elastic Compute Cloud (Amazon EC2), Amazon Elastic Container Service (Amazon ECS) or Amazon Elastic Kubernetes Service (Amazon EKS) compute fleet, including EC2 Spot instances without operational and DevOps overhead. The event-driven design addresses the unpredictable nature of information arrival (i.e., signal generation, portfolio creation, news).&lt;/p&gt; 
&lt;p&gt;Amazon Timestream enables developers to achieve better productivity by eliminating undifferentiated heavy lifting. Most notably: a schema-less design with automatic duplicate detection on inserts. In addition, the service comes with time series functionality such as interpolation and smoothing. Amazon Managed Grafana automatically connects to Amazon Timestream and other data sources for both real-time and historical visualization/dashboards. Finally, we did the work to assemble disparate components into a single, one-click, deployable stack to ensure smooth initial installation and ongoing SDLC.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deploying the solution&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;These instructions will guide you to set up a real-time market portfolio application on AWS through the &lt;a href="https://aws.amazon.com/cdk/"&gt;AWS CDK&lt;/a&gt;. The deployed CDK infrastructure comes with an example portfolio of the S&amp;amp;P 500 based on &lt;a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2440866"&gt;Market Intraday Momentum&lt;/a&gt;. The intraday momentum pattern says that the first half-hour returns on the market since the previous day’s market close will predict the last half-hour returns. This predictability is typically stronger on more volatile days, on higher volume days, on recession days, and on major macroeconomic news release days.&lt;/p&gt; 
&lt;p&gt;Note: You will need a subscription and an API key to a market data feed like B-PIPE or IEX for this solution to fully work.&lt;/p&gt; 
&lt;p&gt;Initial Setup&lt;br&gt; You will use &lt;a href="https://aws.amazon.com/cloud9/"&gt;AWS Cloud9&lt;/a&gt;&amp;nbsp;as the IDE to setup the code and deploy the CDK environment. You can also use a different IDE if you’d prefer.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to the &lt;strong&gt;AWS Cloud9 &lt;/strong&gt;&lt;a href="https://console.aws.amazon.com/cloud9/"&gt;console&lt;/a&gt; and press &lt;strong&gt;Create environment&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Enter a name –&amp;nbsp;&lt;strong&gt;MarketPortfolioEnv.&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Use a &lt;strong&gt;t2.micro&lt;/strong&gt; instance type.&lt;/li&gt; 
 &lt;li&gt;Leave all other settings as default and choose &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;After a few minutes, the environment should be created. Under &lt;strong&gt;Cloud9 IDE&lt;/strong&gt;, press&amp;nbsp;&lt;strong&gt;Open.&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;In the command line at the bottom, clone the &lt;a href="https://github.com/aws-samples/quant-trading"&gt;Git repository&lt;/a&gt; using the following command:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;git clone https://github.com/aws-samples/quant-trading.git&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;CDK Deployment&lt;/h3&gt; 
&lt;p&gt;Now that the environment is setup, let’s deploy the application. You’ll need to run a few commands to get everything set up and this will allow for the entire application to be spun up through the CDK.&lt;/p&gt; 
&lt;p&gt;In the &lt;strong&gt;Cloud9 CLI&lt;/strong&gt;, navigate to the repository by entering the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd quant-trading&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, type in the following command to install the necessary dependencies, bootstrap the environment, and deploy the application using the CDK code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;./deployment.sh&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you get an error saying the docker build failed and says no space left on device run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;chmod +x aws-quant-infra/src/utils/resize_root.sh &amp;amp;&amp;amp;
aws-quant-infra/src/utils/resize_root.sh 50
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you get an error from creating the DynamoDB replica instance in the DB stack, you’ll need to go to the DynamoDB console and delete the replica from the console and delete the DB stack, then redeploy the CDK stack.&lt;/p&gt; 
&lt;h3&gt;Adding an API key to Begin Data Flow&lt;/h3&gt; 
&lt;p&gt;You can have data come in from either &lt;a href="https://iexcloud.io/"&gt;IEX&lt;/a&gt; or &lt;a href="https://www.bloomberg.com/professional/product/market-data-cloud/"&gt;B-PIPE&lt;/a&gt; (Bloomberg Market Data Feed). In this section, you’ll enter the API key in AWS Secrets Manager and that will enable the Intraday Momentum application to start working.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to the &lt;strong&gt;AWS Secrets Manager&lt;/strong&gt; console.&lt;/li&gt; 
 &lt;li&gt;You should see two secrets created: &lt;code&gt;api_token_pk_sandbox&lt;/code&gt; and &lt;code&gt;api_token_pk&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_2742" style="width: 813px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2742" loading="lazy" class="size-full wp-image-2742" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/01/CleanShot-2023-08-01-at-16.48.09.png" alt="Figure 2 - Using AWS Secrets Manager to store your API keys." width="803" height="147"&gt;
 &lt;p id="caption-attachment-2742" class="wp-caption-text"&gt;Figure 2 – Using AWS Secrets Manager to store your API keys.&lt;/p&gt;
&lt;/div&gt; 
&lt;ol&gt; 
 &lt;li&gt;Select &lt;code&gt;api_token_pk&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Scroll down to the section that says &lt;strong&gt;Secret value&lt;/strong&gt;&amp;nbsp;and towards the right, select &lt;strong&gt;Retrieve secret value.&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;ol&gt; 
 &lt;li&gt;Then, choose &lt;strong&gt;Edit&lt;/strong&gt;&amp;nbsp;and paste in your IEX or B-PIPE API key.&lt;/li&gt; 
 &lt;li&gt;Press &lt;strong&gt;Save&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Looking at the Results&lt;/h3&gt; 
&lt;p&gt;You can view the results of the Intraday Momentum application after the day end by going to the DynamoDB table.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to the &lt;strong&gt;AWS DynamoDB&lt;/strong&gt; console.&lt;/li&gt; 
 &lt;li&gt;On the left, select &lt;strong&gt;Tables &lt;/strong&gt;and then choose the table called &lt;code&gt;MvpPortfolioMonitoringPortfolioTable&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Then, press the orange button in the top right that says &lt;strong&gt;Explore table items&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_2741" style="width: 818px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2741" loading="lazy" class="size-full wp-image-2741" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/01/CleanShot-2023-08-01-at-16.44.40.png" alt="Using the AWS DynamoDB console to look at your results." width="808" height="291"&gt;
 &lt;p id="caption-attachment-2741" class="wp-caption-text"&gt;Figure 3- Using the AWS DynamoDB console to look at your results.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;You should then see&amp;nbsp;data populated at the bottom under &lt;strong&gt;Items returned.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Note: If you don’t see any data, select the orange &lt;strong&gt;Run &lt;/strong&gt;button to scan the table and retrieve the data.&lt;/p&gt; 
&lt;p&gt;If you’d like to analyze this data further, you can download it in CSV format by selecting &lt;strong&gt;Actions&lt;/strong&gt;, then &lt;strong&gt;Download results to CSV. &lt;/strong&gt;You can also use Amazon Managed Grafana to visualize the results.&lt;/p&gt; 
&lt;h2&gt;Clean Up&lt;/h2&gt; 
&lt;p&gt;You can delete the entire stack using the CDK.&lt;/p&gt; 
&lt;p&gt;Using the CLI where you deployed the stack, enter the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cdk destroy --all&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Quant trading, given its scale, short-term portfolio lifetime, and unpredictable nature of information arrival, uniquely benefits from cloud elasticity. However, designing the system to harvest the elasticity requires expert level knowledge (both, trading and cloud) and is resource intensive.&lt;/p&gt; 
&lt;p&gt;In this article, we described the system we built to jump-start quant trading on AWS and how to get set up by deploying the stack from the &lt;a href="https://github.com/aws-samples/quant-trading"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>How Maxar builds short duration ‘bursty’ HPC workloads on AWS at scale</title>
		<link>https://aws.amazon.com/blogs/hpc/how-maxar-builds-short-duration-bursty-hpc-workloads-on-aws-at-scale/</link>
		
		<dc:creator><![CDATA[Scott Ma]]></dc:creator>
		<pubDate>Tue, 12 Sep 2023 17:27:44 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">bfe6723330cc5ed1c5a36e9edb71bce3e6688c0b</guid>

					<description>In this post, we hear from Maxar's WeatherDesk team on how they deploy their HPC workloads using a “fail fast” software development technique so they can be sure of meeting customer deadlines for their business.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed&amp;nbsp;by Christopher Cassidy, Maxar Principal DevOps Engineer, &lt;/em&gt;&lt;em&gt;Stefan Cecelski, PhD, Maxar Principal Data Scientist, &lt;/em&gt;&lt;em&gt;Travis Hartman, Maxar Director of Weather and Climate&lt;/em&gt;&lt;em&gt;, Scott Ma, AWS Sr Solutions Architect, &lt;/em&gt;&lt;em&gt;Luke Wells, AWS Sr Technical Account Manager&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;High performance computing (HPC) has been key to solving the most complex problems in every industry and has been steadily changing the way we work and live. From weather forecasting to genome mapping to the search for extraterrestrial intelligence, HPC is helping to push the boundaries of what’s possible with advanced computing technologies.&lt;/p&gt; 
&lt;p&gt;Maxar’s WeatherDesk&lt;sup&gt;SM &lt;/sup&gt;leverages these advanced computing technologies to deliver weather forecasts faster to customers, enabling them to make better informed business decisions. &lt;a href="https://www.maxar.com/products/weatherdesk"&gt;WeatherDesk&lt;/a&gt;&amp;nbsp;builds HPC solutions on AWS to provide access to global numerical weather forecasts to stay ahead of emerging conditions that affect agriculture production, commodity trading, and financial markets. These forecasts are also vital for protecting critical infrastructure like power grids around the world, energy exploration and production, and even transportation.&amp;nbsp;The WeatherDesk platform provides access to data services, web applications, and information reports to customers around the clock via a software-as-a-service (SaaS) suite of offerings designed for specific personas – data scientists and developers, researchers, and executives and operators, respectively.&lt;/p&gt; 
&lt;p&gt;Maxar uses a number of HPC services like &lt;a href="https://aws.amazon.com/hpc/efa/"&gt;Elastic Fabric Adapter&lt;/a&gt; (EFA), the &lt;a href="https://aws.amazon.com/ec2/nitro/"&gt;AWS Nitro System&lt;/a&gt; and &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; to deliver their solutions to their customers. All of this allows Maxar to scale HPC applications to tens of thousands of CPUs with the reliability, scalability, and agility of AWS that would otherwise be extremely difficult to achieve.&lt;/p&gt; 
&lt;p&gt;In this post, we will discuss how Maxar deploys all these tools to run short duration HPC workloads using the “&lt;a href="https://www.martinfowler.com/ieeeSoftware/failFast.pdf"&gt;fail fast&lt;/a&gt;” software development technique.&lt;/p&gt; 
&lt;h2&gt;Constraints&lt;/h2&gt; 
&lt;p&gt;HPC workloads come in all different shapes and sizes, but can generally be divided into two categories based on the degree of interaction between the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/scenarios.html"&gt;concurrently running parallel processes&lt;/a&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Loosely-coupled&lt;/strong&gt; are those where the multiple processes don’t strongly interact with each other in the course of a simulation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tightly coupled&lt;/strong&gt; are those where the parallel processes are simultaneously running and regularly exchanging information between cooperating processes at each step of a simulation.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Solutions like Maxar’s WeatherDesk platform using HPC for numerical weather prediction are tightly coupled due to the complexity of the calculations that go into making a numerical weather forecast. Mainly, the codependency of global weather parameters and computing algorithms require over two billion calculations per second spread across hundreds of Amazon Elastic Compute Cloud (Amazon EC2) instances within an AWS HPC cluster environment. Each of these calculations depends on each other – so the reliable exchange of lots of data in the least time, is important.&lt;/p&gt; 
&lt;p&gt;Additionally, HPC workloads – including Maxar’s WeatherDesk – are often constrained in other ways that can impact the final solution:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;HPC workloads are often very bursty&lt;/strong&gt;, performing computations only a few hours per day. This requires a large number of cores for a short time when they run.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Time-bound workloads&lt;/strong&gt; must complete on a specific schedule with the exact number of instances or physical cores available throughout that time period.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Spot instances&lt;/strong&gt; require workloads to “checkpoint” to handle interruptions, but not all can. Typically, tightly coupled applications, like weather simulation, find this hard.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Workloads often prefer homogeneous instance types or instances with the same underlying architecture and, as we discussed, they need reliable, fast, high-throughput connectivity between instances.&lt;/p&gt; 
&lt;p&gt;Maxar needs to ensure they’re able to launch the HPC clusters when needed, using the suite of architectures given the problem set, and they must dynamically adjust (or &lt;em&gt;fail fast&lt;/em&gt;) to minimize downstream impact.&lt;/p&gt; 
&lt;h2&gt;Solution overview&lt;/h2&gt; 
&lt;p&gt;With these constraints in mind, Maxar built a solution on AWS that uses &lt;a href="https://aws.amazon.com/ec2/instance-types/hpc6/"&gt;hpc6a&lt;/a&gt; instances. These instances are powered by 3rd generation AMD EPYC processors and offer up to 65% better price performance over comparable Amazon EC2 x86 based compute-optimized instances.&lt;/p&gt; 
&lt;p&gt;Their solution uses AWS ParallelCluster is an open-source cluster management tool that makes it easy for you to deploy and manage HPC clusters on AWS. ParallelCluster uses EFA,&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"&gt;cluster placement groups&lt;/a&gt;, and supports &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html"&gt;On-Demand Capacity Reservations (ODCR)&lt;/a&gt; – important technologies and techniques for creating a highly performant &lt;em&gt;and&lt;/em&gt; flexible&amp;nbsp;HPC solution.&lt;/p&gt; 
&lt;p&gt;They also use AWS CloudFormation to provision the head node and then use shell scripts to dynamically adjust input parameters to an AWS ParallelCluster CLI invocation &lt;em&gt;which builds a cluster&lt;/em&gt; of over 25,000 physical cores … and they do this &lt;em&gt;several times a day&lt;/em&gt;!&lt;/p&gt; 
&lt;p&gt;They manage the cluster provisioning workflow using CloudFormation but controlled via CI/CD pipelines like &lt;a href="https://aws.amazon.com/codecommit/"&gt;AWS CodeCommit &lt;/a&gt;and &lt;a href="https://aws.amazon.com/codebuild/"&gt;AWS CodeBuild&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The ODCR allows Maxar to reserve large numbers of a specific instance type in a specific availability zone (AZ) and using a cluster placement group for the duration of their workload. This can happen at any time of any day and without entering into a one-year or three-year commitment.&lt;/p&gt; 
&lt;p&gt;Based on the response from the synchronous &lt;code&gt;&lt;a href="https://docs.aws.amazon.com/cli/latest/reference/ec2/create-capacity-reservation.html"&gt;CreateCapacityReservation&lt;/a&gt;&lt;/code&gt; API call, Maxar can pivot to other instance types, AZs, or even other AWS Regions based on the proximity to the datasets and other variables. For Maxar, pivoting to a similar instance type (or two) in the same AZ, in the same region provided enough flexibility (and resiliency) to boost both performance &lt;em&gt;and&lt;/em&gt; confidence.&lt;/p&gt; 
&lt;h2&gt;Walkthrough&lt;/h2&gt; 
&lt;p&gt;Let’s walk through a sample CloudFormation template that incorporates the newly supported AWS ParallelCluster resources.&lt;/p&gt; 
&lt;p&gt;AWS CloudFormation provisions and configures AWS resources for you, so that you don’t have to individually create and configure them and determine resource dependencies. In our solution we’ll use c6g instances powered by Arm-based &lt;a href="https://aws.amazon.com/ec2/graviton/"&gt;AWS Graviton Processors&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_2641" style="width: 1018px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2641" loading="lazy" class="size-full wp-image-2641" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.42.09.png" alt="Figure 1: Solution architecture to outline the steps" width="1008" height="863"&gt;
 &lt;p id="caption-attachment-2641" class="wp-caption-text"&gt;Figure 1: Solution architecture to outline the steps&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In this walkthrough we’ll take you through six parts, including one that’s optional:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Prerequisites&lt;/li&gt; 
 &lt;li&gt;Deploying the solution&lt;/li&gt; 
 &lt;li&gt;Running the workload&lt;/li&gt; 
 &lt;li&gt;Results of the workload (&lt;em&gt;optional&lt;/em&gt;)&lt;/li&gt; 
 &lt;li&gt;Cluster cleanup&lt;/li&gt; 
 &lt;li&gt;Conclusion&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;For this to work in your own AWS account, you’ll need:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A list of instance types suitable for the workload (e.g. if the workload requires a GPU, g5, g5g, or g4dn might be suitable). In this example, we will use c6g.8xlarge and c6g.16xlarge.&lt;/li&gt; 
 &lt;li&gt;A list of availability zones that would meet the proximity requirement to the data for your workload.&lt;/li&gt; 
 &lt;li&gt;The duration of the workload and the number of instances required.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Deploying the solution&lt;/h3&gt; 
&lt;p&gt;You can find our one-click &lt;a href="https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=pcluster-odcr-demo&amp;amp;templateURL=https://hpc-odcr.s3.amazonaws.com/3.6.0/primary-cfn.yaml"&gt;Launch Stack&lt;/a&gt; to deploy the CloudFormation template with all the necessary components for this solution.&amp;nbsp;The solution is made up of:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;CloudFormation custom resources to create ODCRs&lt;/li&gt; 
 &lt;li&gt;A ParallelCluster head node with Slurm Workload Manager&lt;/li&gt; 
 &lt;li&gt;Security groups&lt;/li&gt; 
 &lt;li&gt;Cluster placement group&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The input parameter &lt;strong&gt;NodeInstanceTypes&lt;/strong&gt; is in comma delimited format used to specify the instance types you want to try to request On-demand Capacity Reservation (ODCR).&lt;/p&gt; 
&lt;p&gt;The input parameter &lt;strong&gt;NodeInstanceDurations&lt;/strong&gt; is in comma delimited format used to specify the durations you want to reserve for in minutes corresponding to the number of &lt;strong&gt;NodeInstanceTypes.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The input parameter &lt;strong&gt;NodeInstanceCounts&lt;/strong&gt; is in comma delimited formation used to specify the number of nodes you want to reserve for corresponding to the number of &lt;strong&gt;NodeInstanceTypes.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The input parameter &lt;strong&gt;KeyName&lt;/strong&gt; specifies the EC2 key pair you would like to use.&lt;/p&gt; 
&lt;p&gt;Be careful to ensure you specify a key pair you can access as you’ll need this later on!&lt;/p&gt; 
&lt;div id="attachment_2642" style="width: 999px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2642" loading="lazy" class="size-full wp-image-2642" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.43.19.png" alt="Figure 2: CloudFormation Input Parameters" width="989" height="822"&gt;
 &lt;p id="caption-attachment-2642" class="wp-caption-text"&gt;Figure 2: CloudFormation Input Parameters&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2643" style="width: 1063px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2643" loading="lazy" class="size-full wp-image-2643" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.43.34.png" alt="Figure 3: CloudFormation Deployment Stacks" width="1053" height="743"&gt;
 &lt;p id="caption-attachment-2643" class="wp-caption-text"&gt;Figure 3: CloudFormation Deployment Stacks&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Running the workload&lt;/h2&gt; 
&lt;p&gt;The workload is based on the &lt;a href="https://github.com/aws-samples/hpc-workshop-wrf"&gt;WRF on AWS HPC workshop&lt;/a&gt;. This uses the Weather Research and Forecasting (WRF) model – a mesoscale numerical weather prediction system designed for both atmospheric research and operational forecasting applications. It runs on AWS Graviton processors with AWS ParallelCluster.&lt;/p&gt; 
&lt;p&gt;To run the workload, login to the HPC head node created by the CloudFormation template you deployed above&amp;nbsp;using &lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html"&gt;AWS Systems Manager Session Manager&lt;/a&gt;&amp;nbsp;by following &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/session-manager.html"&gt;these instruction&lt;/a&gt;s.&lt;/p&gt; 
&lt;p&gt;Once you’ve logged in to the head node, you can execute some commands to start a weather forecast simulation. &lt;em&gt;Note that this will take approximately 2 hours to complete: one hour to download and prepare the data and software, and another hour to complete the simulation:&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;sudo -i
cd /shared
touch /var/log/user-data.log
./rundemo.sh &amp;amp;
tail -f /var/log/user-data.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The shell script implements the ODCR logic by using the &lt;code&gt;create-capacity-reservation&lt;/code&gt; API through the AWS Command Line Interface (CLI) to reserve the capacity you need. The script reserves a specific instance type in a specific AZ, which is tied to the cluster placement group. With ODCRs, the capacity becomes available and billing starts as soon as Amazon EC2 provisions the Capacity Reservation.&lt;/p&gt; 
&lt;p&gt;If the ODCR request fails, the script will pivot to alternative instance types until the ODCR request is successful. It’s also possible to loop through different AZs and regions if those are viable options for your workload.&lt;/p&gt; 
&lt;p&gt;Once the ODCR is successful, the script updates the instance type and then kicks off the HPC workload.&lt;/p&gt; 
&lt;p&gt;Navigating to the &lt;a href="https://us-east-1.console.aws.amazon.com/ec2/home?region=us-east-1#CapacityReservations:"&gt;Capacity Reservations&lt;/a&gt; section of the &amp;nbsp;Amazon EC2 Console you can see the Capacity Reservations that are actively being used by the solution.&lt;/p&gt; 
&lt;div id="attachment_2644" style="width: 997px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2644" loading="lazy" class="size-full wp-image-2644" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.45.01.png" alt="Figure 4: On-Demand Capacity Reservation" width="987" height="190"&gt;
 &lt;p id="caption-attachment-2644" class="wp-caption-text"&gt;Figure 4: On-Demand Capacity Reservation&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Visualize the results of the workload (optional)&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/dcv/"&gt;NICE DCV&lt;/a&gt; is a remote visualization technology that enables you to securely connect to graphic-intensive 3D applications hosted on a remote server.&lt;/p&gt; 
&lt;p&gt;To see the results,&amp;nbsp;you’ll need to connect to the head node through NICE DCV &lt;strong&gt;remotely&lt;/strong&gt; using &lt;a href="https://pypi.org/project/aws-parallelcluster/"&gt;AWS ParallelCluster python cluster management tool&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To do this, open a terminal from your local machine and run these commands to install the AWS ParallelCluster CLI and connect to the head node. You’ll need to specify the PEM file that corresponds to the EC2 key-pair you selected as an input parameter to your CloudFormation stack:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;# run pip3 command if you do not already have pcluster cluster management tool installed.
pip3 install aws-parallelcluster
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once you’re connected to the head node, launch a terminal, and run the following command to start ncview to visualize your results:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd $WRFWORK
ncview wrfout*
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here’s a couple of screenshots from the simulation results.&lt;/p&gt; 
&lt;div id="attachment_2645" style="width: 653px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2645" loading="lazy" class="size-full wp-image-2645" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.46.16.png" alt="Figure 5: ncview graphic application to visualize WRF output" width="643" height="581"&gt;
 &lt;p id="caption-attachment-2645" class="wp-caption-text"&gt;Figure 5: ncview graphic application to visualize WRF output&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2646" style="width: 634px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2646" loading="lazy" class="size-full wp-image-2646" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.46.22.png" alt="Figure 6: QVAPOR – Water Vapor Mixing Ratio" width="624" height="415"&gt;
 &lt;p id="caption-attachment-2646" class="wp-caption-text"&gt;Figure 6: QVAPOR – Water Vapor Mixing Ratio&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Cluster Cleanup&lt;/h2&gt; 
&lt;p&gt;To remove the cluster, go to&lt;a href="https://us-east-1.console.aws.amazon.com/cloudformation/home?region=us-east-1"&gt; AWS CloudFormation console&lt;/a&gt; and delete the CloudFormation stack that you created in this solution.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;With this solution, an ODCR reserves sufficient Amazon EC2 instances for the burstiness of the Maxar WeatherDesk workloads that run – at most – a few hours a day. They’re able to do this efficiently and cost-effectively without the interruptions from EC2 Spot or the need to sign up to a one-year or three-year commitment.&lt;/p&gt; 
&lt;p&gt;This method also introduces the concept of&amp;nbsp;&lt;em&gt;failing fast&lt;/em&gt; – pivoting to other HPC-optimized EC2 instance types &lt;em&gt;as needed&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;In the case of Maxar’s WeatherDesk, speed is everything…but so is dependability. If a workload can’t run because resources aren’t available, a quick pivot is essential so that Maxar’s customers still receive the critical Earth Intelligence information they need to operate their business, mission, or operation.&lt;/p&gt; 
&lt;p&gt;With a few lines of code, layering in ODCR workflows into their solution enabled the flexibility they need at the scale necessary for timeliness requirements.&lt;/p&gt; 
&lt;p&gt;Building Maxar’s WeatherDesk on AWS means they can rely on foundational concepts of reliability, scalability, and agility to address the rigid and demanding needs of weather workflows. By leveraging hpc6a instances, Maxar further reduced the runtime of its &lt;a href="https://blog.maxar.com/leading-the-industry/2020/maxar-wins-aws-2020-public-sector-partner-award-for-best-high-performance-computing-solution"&gt;award-winning numerical weather prediction HPC workload&lt;/a&gt; by 35% while also creating efficiency gains. This results in a solution that is more attractive to more users, particularly in terms of speed, ease of access and dependability, all of which are critical for supporting our customers’ decision making for their business, mission, and operations.&lt;/p&gt; 
&lt;p&gt;Using ODCRs gave them peace of mind regarding capacity, because they can dynamically shift EC2 instance types and still be on time, and keep their compute costs low. Since Maxar implemented this solution on AWS, their cost-to-run shrank by more than 50% and they’ve been able to take advantage of numerous AWS offerings that help reduce even further – while also increasing resiliency.&lt;/p&gt; 
&lt;p&gt;These efficiency gains result in a solution that is more attractive to more users, particularly in terms of cost, ease of access, and dependability – all of which are critical for supporting our customers’ decision making for their business, mission, and operation and delivering on Maxar’s purpose, For A Better World.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.57.16.png" alt="Christopher Cassidy" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Christopher Cassidy&lt;/h3&gt; 
  &lt;p&gt;Christopher Cassidy is a Principal DevOps Engineer at Maxar, working with the WeatherDesk team since 2006. He started as a data engineer before shifting gears to lead the initial cloud migration and ongoing cloud deployments. He is constantly looking for ways to optimize and improve how the organization utilizes the cloud. Outside of work, he enjoys watching Boston sports and Penn State University football, biking, gardening, and spending time with family.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.57.21.png" alt="Stefan Cecelski" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Stefan Cecelski&lt;/h3&gt; 
  &lt;p&gt;Stefan Cecelski is a Principal Data Scientist and Engineer at Maxar. As the technical lead for HPC solutions on the WeatherDesk team, he has a passion for applying innovative technologies that help Maxar better understand the weather and its impacts on customers. Beyond always keeping an eye on the sky, he enjoys hiking, birding, traveling internationally, and spending time with his family.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.57.24.png" alt="Travis Hartman" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Travis Hartman&lt;/h3&gt; 
  &lt;p&gt;Travis Hartman is the Director of Weather and Climate at Maxar. He has been with the organization since the early 2000s, beginning his career as an operational meteorologist and now leading and managing the Maxar portfolio of Weather and Climate products and programs. While he is always learning more about why the wind blows the way it does and why clouds do what they do, he also enjoys spending time with his family and trying to keep up with their busy, youth sports schedules.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt; 
&lt;p&gt;&lt;em&gt;The content and opinions in this blog are those of the third-party author and AWS is not responsible for the content or accuracy of this blog.&lt;/em&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>How Amazon’s Search M5 team optimizes compute resources and cost with fair-share scheduling on AWS Batch</title>
		<link>https://aws.amazon.com/blogs/hpc/how-amazons-search-m5-team-optimizes-compute-resources-and-cost-with-fair-share-scheduling-on-aws-batch/</link>
		
		<dc:creator><![CDATA[Kamalakannan Hari Krishna Moorthy]]></dc:creator>
		<pubDate>Tue, 05 Sep 2023 14:16:09 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Customer Solutions]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Thought Leadership]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<guid isPermaLink="false">fe8b346b20cec9af7046fccc16492d8c76275437</guid>

					<description>In this post, we share how Amazon Search optimizes their use of accelerated compute resources using AWS Batch fair-share scheduling to schedule distributed deep learning workloads.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed&amp;nbsp;by: Kamalakannan Hari Krishna Moorthy (SDE), Ameeta Muralidharan (SDE), Vijay Rajakumar (Sr SDE), R J (Principal Engineer), James Park (Sr Solutions Architect)&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The M5 program within Amazon Search owns the discovery learning strategy for Amazon and builds large-scale models across modalities: multilingual, multi-entity, and multitask. To build and train models with billions of parameters at scale, M5 uses accelerated compute such as Amazon Elastic Compute Cloud (Amazon EC2) instances with GPUs and AWS Trainium. One of our central tenets is to keep the infrastructure and operational costs under control.&lt;/p&gt; 
&lt;p&gt;In this post, we focus on how we evolved our systems to manage accelerated compute resources efficiently, and schedule the distributed deep learning workloads by leveraging AWS Batch &lt;strong&gt;fair-share scheduling&lt;/strong&gt;. By continuously improving the approach to managing compute resources and scheduling, we have: 1) reduced idle resources by 14%; 2) increased GPU utilization of our fleet by 19%; and 3) eliminated downtime during reallocation of compute.&lt;/p&gt; 
&lt;h2&gt;The evolution of our queue system over time&lt;/h2&gt; 
&lt;p&gt;Initially, M5 started as a one-pizza team and each developer was assigned some compute resources to run their experiments (Figure 1). However, this approach led to idle resources whenever a developer wasn’t actively running an experiment. When the team size increased, the number of experiments we ran also increased and eventually the compute required by the experiments exceeded the available compute. Due to this constraint, we needed a job queue to manage experiment lifecycle with compute that was available, in a scalable and reproducible manner.&lt;/p&gt; 
&lt;div id="attachment_2791" style="width: 460px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2791" loading="lazy" class="size-full wp-image-2791" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-12.36.50.png" alt="Figure 1: Our initial approach was to have each M5 developer be assigned a dedicated set of compute resources." width="450" height="279"&gt;
 &lt;p id="caption-attachment-2791" class="wp-caption-text"&gt;Figure 1: Our initial approach was to have each M5 developer be assigned a dedicated set of compute resources.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We chose AWS Batch as the solution to this requirement and were successful in efficiently sharing our compute resources across different AWS Regions, boosting our experiment velocity (Figure 2). Batch is a fully managed service that plans, schedules, and executes containerized ML workloads on different AWS compute offerings such as Amazon EC2 (both Spot and On-Demand Instances), Amazon ECS, Amazon EKS, AWS Fargate. Distributed training jobs in M5 are executed using a cluster of accelerated compute resources supported by multi-node-parallel jobs in Batch.&lt;/p&gt; 
&lt;div id="attachment_2792" style="width: 953px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2792" loading="lazy" class="size-full wp-image-2792" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-12.38.08.png" alt="Figure 2: As the team grew, we introduced AWS Batch job queues to manage the jobs on the compute resources using a first-in-first-out (FIFO) strategy." width="943" height="287"&gt;
 &lt;p id="caption-attachment-2792" class="wp-caption-text"&gt;Figure 2: As the team grew, we introduced AWS Batch job queues to manage the jobs on the compute resources using a first-in-first-out (FIFO) strategy.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As time progressed and our team size (and experiments and workloads) increased, we encountered another scheduling challenge. The Batch queue employed a first-in-first-out (FIFO) strategy for job placement, which sometimes resulted in a series of long-running jobs occupying compute resources for extended periods and dominating the queue. As a consequence, other jobs had to wait longer than we wanted before they could access the required compute resources and certain internal teams faced difficulties in obtaining compute resources promptly for their experiments. The single FIFO queue strategy was impacting their ability to meet business-driven timelines (Figure 3).&lt;/p&gt; 
&lt;div id="attachment_2793" style="width: 969px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2793" loading="lazy" class="size-full wp-image-2793" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-12.38.52.png" alt="Figure 3. First-in-first-out queue shown above is shared by 2 teams (blue and green) where order of submitting jobs and running duration of jobs favor team blue which acquired more compute resources resulting in an unfair allocation to team green, simply because blue submitted their jobs earlier in time." width="959" height="289"&gt;
 &lt;p id="caption-attachment-2793" class="wp-caption-text"&gt;Figure 3. First-in-first-out queue shown above is shared by 2 teams (blue and green) where order of submitting jobs and running duration of jobs favor team blue which acquired more compute resources resulting in an unfair allocation to team green, simply because blue submitted their jobs earlier in time.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To enable experiments from all teams to proceed at an equal pace, the total compute was divided and allocated to multiple new FIFO queues such that each team receives a part of the total compute dedicated to their experiments and decide prioritization.&lt;/p&gt; 
&lt;p&gt;Team-specific FIFO queues worked well for teams to schedule their jobs in a timely manner. However, we observed two main disadvantages over time:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Reallocation of resources across queues involved downtime.&lt;/strong&gt; Changes to resource allocation were carried out by scaling down compute from a source queue and scaling up in a target queue. In the background, this involved termination of EC2 instances from Batch compute environment linked to the source queue and re-provisioning them in the compute environment linked to the target queue. Due to limited availability of accelerated compute resources, the re-provisioning step was not predictably able to complete in a deterministic timeframe. This is because when you terminate an EC2 instance it becomes available to numerous other AWS accounts. In our experience, capacity reallocations ranged from few days to sometimes weeks, incurring cost and time during which no queue was able to use those instances.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Inefficient use of resources became a problem again.&lt;/strong&gt; We started observing unused/idle resources in the team specific FIFO queues. This was mainly coming from three factors: 1) random traffic of jobs across different teams led to under-saturated job queues (less jobs than available resources), 2) scheduling overhead —&amp;nbsp;which is the time the scheduler takes to prepare N instances to start a N-node job when all N nodes are not readily available, 3) fragmentation —&amp;nbsp;which refers to idle instances resulting across 20 queues. Though the fragmented instances could be from the same AWS Availability Zone (AZ), they cannot be utilized to schedule jobs if they belong to different Batch compute environments (CEs). CEs are the compute pools backing job queues. The total unused compute resources contributed to about 23.6% of our total fleet size, which is more than enough cause to explore solutions to reduce idle resources across our queues.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_2794" style="width: 913px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2794" loading="lazy" class="size-full wp-image-2794" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-12.39.31.png" alt="Figure 4. Next, we created dedicated FIFO job queues for each team, which provided dedicated compute reservations but leads to inefficient utilization. In the example with two queues shown, jobs wait for resources in first queue while there are idle resources and no pending jobs in the latter." width="903" height="604"&gt;
 &lt;p id="caption-attachment-2794" class="wp-caption-text"&gt;Figure 4. Next, we created dedicated FIFO job queues for each team, which provided dedicated compute reservations but leads to inefficient utilization. In the example with two queues shown, jobs wait for resources in first queue while there are idle resources and no pending jobs in the latter.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To overcome these disadvantages, we evaluated then newly-announced &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-fair-share-scheduling-for-aws-batch/"&gt;fair share scheduling policies&lt;/a&gt; (FSS) for Batch job queues. With FSS, Batch offers configurable parameters like share identifiers, weights for share identifiers, priority for jobs, fairness duration, etc., to ensure fair allocation of compute resources to several users while managing them in a single queue. The following is how we used these parameters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;shareIdentifier&lt;/code&gt;: represent internal teams that share the compute&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;weightFactor&lt;/code&gt;: share of compute allocated for each &lt;code&gt;shareIdentifier&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;shareDecaySeconds&lt;/code&gt;: period over which Batch calculates fairness of usage by a &lt;code&gt;shareIdentifier&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;computeReservation&lt;/code&gt;: share of compute to be set aside for inactive &lt;code&gt;shareIdentifiers&lt;/code&gt;. i.e., share identifiers which do not have active jobs&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;jobPriority&lt;/code&gt;: determines priority of jobs within a &lt;code&gt;shareIdentifier&lt;/code&gt;. i.e., a job with higher priority will be scheduled ahead of a job with lower priority irrespective of their creation times&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The first four parameters are configured using a scheduling policy which is attached to the job queue. Once configured, the Batch scheduler will keep track of compute usage and schedule jobs such that resources are allocated fairly. The &lt;code&gt;jobPriority&lt;/code&gt; parameter is set either within the job definition, or at the time of submitting a job. Using this information, we created a plan to move from team-specific FIFO queues to a unified queue with FSS where teams share capacity &lt;em&gt;while retaining the advantages of having team specific queues. &lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;As a first step, we converted team specific FIFO queues into FSS queues to reduce idle resources caused by scheduling overhead (~3% of total fleet) within individual queues. This was enabled by allowing jobs to be submitted with different priorities where developers could submit jobs with higher priorities to take advantage of instances that are idle due to scheduling overhead. For example, if there is a job needs 24 instances to become available and get scheduled while the queue only has 16 available, a new incoming job with higher priority can get scheduled immediately if it requires 16 nodes or less. Since CEs can be attached to more than one queue, we successfully switched over from FIFO to FSS in ~20 job queues without any downtime.&lt;/p&gt; 
&lt;p&gt;Next, we consolidated our capacity pools that were partitioned logically as several On-Demand Capacity Reservations (ODCR) into fewer ODCRs grouped by AZ. We then carried out the consolidation of 20 FSS queues in 2 phases. In the first phase, we reduced 20 FSS queues to 3 based on project boundaries. The CEs from 20 queues were retained and attached as-is to one of the 3 FSS queues to avoid downtime. This phase gave us an opportunity to dry run the new scheduling approach while limiting our blast radius from an unforeseen outage. At the end of phase 1, we reduced the idle compute resources from 23.6% to 20% while the GPU utilization of the fleet was at 69%.&lt;/p&gt; 
&lt;p&gt;In the second phase, we created the unified FSS queue with fewer CEs to reduce fragmentation and migrated the compute resources from staging FSS queues. Teams using the unified queue received a share of compute from our fleet which was configured using &lt;code&gt;shareIdentifiers&lt;/code&gt; and &lt;code&gt;weightFactors&lt;/code&gt; on the scheduling policy. Values for &lt;code&gt;weightFactor&lt;/code&gt; were based on the level of compute a team needed in the near future. Changes to resource allocation are handled by simply updating the weights, which eliminated any downtime while moving capacity between teams. After completion of phase 2, we saw a reduction in idle resources from 20% to 9.5% while our fleet’s aggregate GPU utilization increased from 69% to 88.7% due to reduction in idle compute resources. (If you are interested in how we measure GPU utilization, check out the AWS Deep Learning AMI &lt;a href="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-gpu-monitoring-gpumon.html"&gt;documentation&lt;/a&gt;).&lt;/p&gt; 
&lt;div id="attachment_2795" style="width: 892px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2795" loading="lazy" class="size-full wp-image-2795" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/24/CleanShot-2023-08-24-at-12.42.04.png" alt="Figure 5. Compute managed and share by teams with a single FSS queue improving utilization while retaining fairness in allocation " width="882" height="591"&gt;
 &lt;p id="caption-attachment-2795" class="wp-caption-text"&gt;Figure 5. Compute managed and share by teams with a single FSS queue improving utilization while retaining fairness in allocation&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;Figure 5. Compute managed and share by teams with a single FSS queue improving utilization while retaining fairness in allocation&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we described how Amazon’s Search M5 team continuously optimized their compute resources while owning and operating one of the largest accelerated compute clusters at Amazon. By moving to a unified fair-share scheduling queue on AWS Batch, M5 met its requirements with 14% less resources, and improved GPU utilization by 19% while also eliminating downtime during capacity allocation changes. We also published a self-paced workshop on how to leverage AWS Batch multi-node processing jobs to train deep learning models, and you can try it out in your own account &lt;a href="https://frameworks.hpcworkshops.com/05-batch-mnp-train-gpu.html"&gt;here&lt;/a&gt;. If you are interested in more detailed information about fair share, read the previous &lt;a href="https://aws.amazon.com/blogs/hpc/deep-dive-on-fair-share-scheduling-in-aws-batch/"&gt;deep dive blog post&lt;/a&gt; on fair share scheduling policy parameters.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Bursting your HPC applications to AWS is now easier with Amazon File Cache and AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/bursting-your-hpc-applications-to-aws-is-now-easier-with-amazon-file-cache-and-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Shun Utsui]]></dc:creator>
		<pubDate>Thu, 31 Aug 2023 14:35:36 +0000</pubDate>
				<category><![CDATA[Amazon File Cache]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Hybrid Cloud Management]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[hybrid]]></category>
		<guid isPermaLink="false">e9507e0b872282e4bb0b29af1c43a2a0ce6e8277</guid>

					<description>Today we're announcing the integration between Amazon File Cache and AWS ParallelCluster - super important for hybrid scenarios. We'll show you how it works and how to deploy it.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright wp-image-2829 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/30/File-Cache-and-ParallelCluster-1.png" alt="Bursting your HPC applications to AWS is now easier with Amazon File Cache and AWS ParallelCluster" width="380" height="212"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;This post was contributed by Shun Utsui, Senior HPC Solutions Architect, Austin Cherian, Senior Product Manager&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Combining on-premises and cloud&amp;nbsp;computing resources has emerged as a powerful solution for organizations seeking to meet their&amp;nbsp;computing needs. While you can target compute to be on-premises &lt;em&gt;or&lt;/em&gt; in the cloud, your workloads’ accessibility to data in a fast and secure manner is critical to its performance.&lt;/p&gt; 
&lt;p&gt;Customers have asked for more flexibility to send their workloads to AWS while keeping some data primarily on-premises, so today we’re announcing support for Amazon File Cache in ParallelCluster 3.7.&lt;/p&gt; 
&lt;p&gt;Amazon File Cache offers a high-speed cache on AWS to facilitate efficient file data processing, regardless of its storage location. File Cache serves as temporary, high-performance cache layer in the cloud, for data stored on-premises. Once configured, workload data moves from on-premises storage to AWS as the workload accesses it via File Cache, which appears to the compute client as a Lustre file system.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk you through the features of File Cache that are important for HPC environments, and show you, step-by-step, how you can quickly deploy this and try it out for yourself.&lt;/p&gt; 
&lt;h2&gt;Introducing Amazon File Cache&lt;/h2&gt; 
&lt;p&gt;Previous versions of ParallelCluster already supported mounting multiple FSx file systems. A File Cache can be attached to a cluster using the same familiar mechanisms. These configurations are useful for building “cloud bursting” scenarios: you can attach a File Cache to your ParallelCluster and submit jobs to the cluster either through a federated cluster on-premises like a stretch cluster in Slurm, or by logging into the ParallelCluster directly.&lt;/p&gt; 
&lt;p&gt;But beyond just data accessibility, there are other considerations that make Amazon File Cache an effective choice for your hybrid HPC use cases.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Scalability &amp;amp; Performance&lt;/strong&gt;: Amazon File Cache is built on Lustre, the popular, high-performance file system, and provides scale-out performance that &lt;em&gt;increases linearly with the cache storage capacity&lt;/em&gt;. File Cache is designed to accelerate cloud bursting workloads using cache storage designed to deliver sub-millisecond latencies, up to hundreds of GB/s of throughput, and millions of IOPS.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Data Management &amp;amp; Security:&lt;/strong&gt; Data encryption, access controls, and backup mechanisms ensure data integrity and confidentiality with robust storage solutions. File Cache provides data protection by encrypting data at rest and in-transit between the cache and Amazon Elastic Compute Cloud (Amazon EC2) instances. You can use AWS Key Management Service (KMS) to manage the encryption keys that are used to encrypt your data. File Cache also supports the use of AWS PrivateLink to access your cache resources without going over the internet. You can use Amazon Virtual Private Cloud (VPC) to control access to your cache resources by configuring the VPC Network ACLs to control access to your cache resources. Network ACLs enable you to create a firewall that controls inbound and outbound traffic at the subnet level.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Cost optimization&lt;/strong&gt;: &amp;nbsp;File Cache offers a flexible pricing model so you only pay for the resources you use and there are no minimum fees or setup charges. While pricing is quoted on a monthly basis, billing is &lt;em&gt;per-second&lt;/em&gt;.&lt;/p&gt; 
&lt;h2&gt;Reference Architecture&lt;/h2&gt; 
&lt;p&gt;Amazon File Cache can be associated with Amazon S3 buckets or Network File System (NFS) file systems that support NFS v3. Figure 1 shows a typical architecture for a hybrid HPC environment.&lt;/p&gt; 
&lt;p&gt;Customers with large-scale HPC resources in their own data centers often already have a parallel file system for storage, like Lustre which can be exported using NFS.&lt;/p&gt; 
&lt;p&gt;To deploy a hybrid infrastructure pattern like this, you’ll need to connect your corporate data center to your VPC using either an &lt;a href="https://aws.amazon.com/vpn/site-to-site-vpn/"&gt;AWS Site-to-Site VPN&lt;/a&gt;, or an &lt;a href="https://aws.amazon.com/directconnect/"&gt;AWS Direct Connect&lt;/a&gt;.&lt;/p&gt; 
&lt;div id="attachment_2818" style="width: 1026px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2818" loading="lazy" class="size-full wp-image-2818" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/30/CleanShot-2023-08-30-at-13.30.08.png" alt="Figure 1 – High-level architecture of Amazon File Cache in AWS ParallelCluster with a data repository association (DRA) to an on-premises HPC storage system. To link an on-premises parallel file system to File Cache you can export the file system over NFS v3. " width="1016" height="450"&gt;
 &lt;p id="caption-attachment-2818" class="wp-caption-text"&gt;Figure 1 – High-level architecture of Amazon File Cache in AWS ParallelCluster with a data repository association (DRA) to an on-premises HPC storage system. To link an on-premises parallel file system to File Cache you can export the file system over NFS v3.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Deployment details&lt;/h2&gt; 
&lt;p&gt;Attaching an Amazon File Cache to a ParallelCluster is straightforward. Let’s walk through a typical work flow so you get the idea. We’ll assume you already have an operational ParallelCluster.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 1.&lt;/strong&gt; Create a Security Group for Amazon File Cache following the “&lt;a href="https://docs.aws.amazon.com/fsx/latest/FileCacheGuide/limit-access-security-groups.html"&gt;Cache access control with Amazon VPC&lt;/a&gt;” section of the File Cache user guide.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 2.&lt;/strong&gt; Create a cache following the “&lt;a href="https://docs.aws.amazon.com/fsx/latest/FileCacheGuide/getting-started-step1.html"&gt;Create your cache&lt;/a&gt;” section of the File Cache User Guide. Copy the &lt;strong&gt;File Cache ID&lt;/strong&gt; to the clipboard.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 3.&lt;/strong&gt; Now we’ll modify the ParallelCluster configuration file. If you have an existing cluster deployed through ParallelCluster, you should have a YAML file that you specified during the deployment.&lt;/p&gt; 
&lt;p&gt;The ParallelCluster configuration file is written in YAML and it defines the resources for your HPC cluster. You can refer to the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/configuration-v3.html"&gt;ParallelCluster User Guide&lt;/a&gt; to find out more about the details the file format.&lt;/p&gt; 
&lt;p&gt;Under the &lt;code&gt;SharedStorage&lt;/code&gt; section of the configuration file, add &lt;code&gt;StorageType: FileCache&lt;/code&gt; with the name of the file system, the mount directory, and the File Cache ID that you copied onto your clipboard in the previous step.&lt;/p&gt; 
&lt;p&gt;In this example, we’re configuring the cache to be mounted under the &lt;code&gt;/cache&lt;/code&gt; directory.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;SharedStorage:
  - Name: FileCache0
    MountDir: /cache
    StorageType: FileCache
    FileCacheSettings:
      FileCacheId: &amp;lt;your_file_cache_ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You also need to specify the security group associated with the File Cache. Under the &lt;code&gt;HeadNode&lt;/code&gt; section and the &lt;code&gt;Scheduling&lt;/code&gt; section, enter the security group ID you just created in Step 1 (note that this entry has to be defined under &lt;strong&gt;both&lt;/strong&gt; the &lt;code&gt;HeadNode&lt;/code&gt; &lt;strong&gt;and&lt;/strong&gt; the &lt;code&gt;Scheduling&lt;/code&gt; section, otherwise the cache won’t be mounted on the entire cluster).&lt;/p&gt; 
&lt;p&gt;If you’re configuring multiple queues, you’ll need to specify the &lt;code&gt;AdditionalSecurityGroups&lt;/code&gt; string under the Networking section of &lt;em&gt;all your queue definitions&lt;/em&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Networking:
  SubnetId: &amp;lt;your_subnet_ID&amp;gt;
  AdditionalSecurityGroups:
    - &amp;lt;file_cache_security_group&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Step 4.&lt;/strong&gt; Update your running cluster.&lt;/p&gt; 
&lt;p&gt;First, you’ll need to stop the compute fleet so you can update the cluster.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-compute-fleet --status STOP_REQUESTED -n &amp;lt;cluster name&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the compute fleet is stopped, update your cluster with the update-cluster sub-command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-cluster -c &amp;lt;configuration file name&amp;gt; -n &amp;lt;cluster name&amp;gt;&amp;nbsp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will take about 10 minutes for the cluster update to complete. Once the update is complete, start the compute fleet again:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster update-compute-fleet --status START_REQUESTED -n &amp;lt;cluster name&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Confirm that your compute fleet’s status is “RUNNING” with the describe-compute-fleet sub-command.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster describe-compute-fleet -n &amp;lt;cluster name&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can now login to the head node and check that the cache is mounted under the directory you specified. In our example, we mounted the cache under the directory &lt;code&gt;/cache&lt;/code&gt;, which we’ve shown in Figure 2. You should now be able to access your cache like any other file system on a Linux machine.&lt;/p&gt; 
&lt;div id="attachment_2819" style="width: 974px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2819" loading="lazy" class="size-full wp-image-2819" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/30/CleanShot-2023-08-30-at-13.34.36.png" alt="Figure 2 – Amazon File Cache mounted on /cache in AWS ParallelCluster " width="964" height="294"&gt;
 &lt;p id="caption-attachment-2819" class="wp-caption-text"&gt;Figure 2 – Amazon File Cache mounted on /cache in AWS ParallelCluster&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Step 5.&lt;/strong&gt; Now run your HPC workload on the cluster, using your new cache, and then export results to the data repository. You should be able to list files that you have already uploaded into your data repository.&lt;/p&gt; 
&lt;p&gt;When a file is uploaded onto the data repository, Amazon File Cache will only fetch the metadata. At this point, the data itself is kept in the origin repository, and is only retrieved to the cache when accessed from an Amazon EC2 instance. This means the file access latency will be slightly higher for the first access, but will drop to a sub-millisecond levels once the content has cached. This is called lazy loading, and saves a lot of bandwidth between sites when you’re mounting a cache connected to a large remote file system.&lt;/p&gt; 
&lt;p&gt;Similarly, you can also release a file from the cache without removing the metadata on it. Once your job is completed and the cache is no longer needed, we recommend you export the data onto the data repository and clear the cache. To force exporting of the data from the cache, you can run a command like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;nohup find &amp;lt;target directory on cache&amp;gt; -type f -print0 | xargs -0 -n 1 sudo lfs hsm_archive &amp;amp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the data export task is finished, you can evict the cache to free up capacity. To do that, run the following command on the files you want released:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;lfs hsm_release &amp;lt;file&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Find out more about &lt;a href="https://docs.aws.amazon.com/fsx/latest/FileCacheGuide/export-changed-data.html"&gt;exporting changes to the data repository&lt;/a&gt; and &lt;a href="https://docs.aws.amazon.com/fsx/latest/FileCacheGuide/cache-eviction.html"&gt;cache eviction&lt;/a&gt; in the official Amazon File Cache user guide.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Step 6. (optional) &lt;/strong&gt;Remove the cache from the cluster and delete it. Once files are exported onto the data repository and the cache is cleared, you can remove the cache. To do that, open your ParallelCluster configuration file and remove the parts that you added in Step 3. Then, execute the exact same commands as in Step 4 to update the cluster. When the cluster is updated and the cache is not mounted anymore, you can safely delete it (follow &lt;a href="https://docs.aws.amazon.com/fsx/latest/FileCacheGuide/getting-started-step4.html"&gt;these steps&lt;/a&gt; for more detail) to save cost.&lt;/p&gt; 
&lt;h2&gt;Verifying the effects of Amazon File Cache and lazy loading&lt;/h2&gt; 
&lt;p&gt;To highlight the effect of Amazon File Cache and its lazy loading feature, we ran the &lt;a href="https://github.com/hpc/ior"&gt;IOR parallel I/O benchmarks&lt;/a&gt;. We created a multi-Availability Zone (AZ) environment to mimic a hybrid HPC set up. AZs are physically separated by a meaningful distance from other AZs in the same AWS Region, but they are all within 60 miles (~ 100 kilometers) of each other.&lt;/p&gt; 
&lt;p&gt;First, we created a ParallelCluster in an AZ that we randomly selected in the North Virginia Region (us-east-1c). Then, we created an EC2 instance with an Amazon EBS volume attached to it in the same region, but in another AZ (us-east-1b).&lt;/p&gt; 
&lt;p&gt;Next, we configured that EC2 instance to become an NFS server that exports the EBS volume we attached.&lt;/p&gt; 
&lt;p&gt;Back in us-east-1c, we created a cache in the same AZ as our ParallelCluster. Then, we linked the cache to the NFS serving EC2 instance in us-east-1c as its data repository.&lt;/p&gt; 
&lt;p&gt;Figure 3 illustrates this configuration.&lt;/p&gt; 
&lt;div id="attachment_2832" style="width: 1287px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2832" loading="lazy" class="size-full wp-image-2832" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/30/IOR-diagram-1.png" alt="Figure 3 – High-level architecture for IOR Parallel I/O benchmarks. The EC2 instance in us-east-1b is an NFS server that exports the EBS volume over NFS v3, which is then linked to Amazon File Cache in us-east-1c as a data repository." width="1277" height="606"&gt;
 &lt;p id="caption-attachment-2832" class="wp-caption-text"&gt;Figure 3 – High-level architecture for IOR Parallel I/O benchmarks. The EC2 instance in us-east-1b is an NFS server that exports the EBS volume over NFS v3, which is then linked to Amazon File Cache in us-east-1c as a data repository.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We submitted our first IOR job onto the cluster to see the write performance to the cache. For this job we used 8x hpc7g.16xlarge instances (which have 64 physical cores per instance), and using all 512 cores in parallel. Each core runs a process which writes a 3.2 GiB file onto the cache, 1.56 TiB of data in aggregate. We instructed the job to run 5 times, so we could see a consistent trend.&lt;/p&gt; 
&lt;p&gt;On a cache that we sized at 4.8 TiB (corresponding to a baseline performance of 4.8 GiB/s) we saw consistent write performance at around 4.48 GiB/s in all 5 iterations of the job. Results of the write performance test are shown in the graph in Figure 4.&lt;/p&gt; 
&lt;div id="attachment_2821" style="width: 729px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2821" loading="lazy" class="size-full wp-image-2821" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/30/CleanShot-2023-08-30-at-13.37.14.png" alt="Figure 4 – A bar graph of IOR write performance tests. Performance is consistent across all 5 iterations. " width="719" height="420"&gt;
 &lt;p id="caption-attachment-2821" class="wp-caption-text"&gt;Figure 4 – A bar graph of IOR write performance tests. Performance is consistent across all 5 iterations.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We wanted to see the difference in read performance for the first and subsequent reads, so next we ran a &lt;em&gt;read-performance&lt;/em&gt; test on the data set that we generated during our &lt;em&gt;write-performance&lt;/em&gt; test.&lt;/p&gt; 
&lt;p&gt;To avoid reading the data from the cache for the first iteration, we cleared the cache following the instructions in Step 5.&lt;/p&gt; 
&lt;p&gt;We also used the same compute resources to perform the write test that we used for the read test. Like before, we instructed the job to run 5 iterations so that during the first iteration, the files are ingested from the data repository into the cache. For the subsequent iterations, the job will read the files from the cache. Results of the read-performance tests are shown in the graph in Figure 5.&lt;/p&gt; 
&lt;div id="attachment_2822" style="width: 745px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2822" loading="lazy" class="size-full wp-image-2822" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/30/CleanShot-2023-08-30-at-13.37.37.png" alt="Figure 5 – A bar graph of IOR read performance tests. The first iteration is slower because the data is retrieved from the data repository through the lazy loading mechanism. The subsequent iterations are consistently fast because data is read from cache. " width="735" height="447"&gt;
 &lt;p id="caption-attachment-2822" class="wp-caption-text"&gt;Figure 5 – A bar graph of IOR read performance tests. The first iteration is slower because the data is retrieved from the data repository through the lazy loading mechanism. The subsequent iterations are consistently fast because data is read from cache.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The results show that the first iteration achieved around 0.12 GiB/s throughput, and in all the subsequent iterations it was consistently around 4.44 GiB/s (around 37 times better!).&lt;/p&gt; 
&lt;p&gt;During the first read, the workload loaded the entire data set of 1.56 TiB onto the cache from the data repository, which resides in another AZ, a physically distant location. Hence the performance was slower. From the second read onwards, the workload read the data only from the cache.&lt;/p&gt; 
&lt;p&gt;Of course, the performance gap between the first and subsequent iterations is dependent on a lot of factors including aggregate size of the data being transferred, some features of the data set (e.g. many small files vs one big file), the network bandwidth, and the NFS server specification. Your mileage &lt;em&gt;will&lt;/em&gt; vary, but you can test it quite quickly yourself, using the steps we’ve outlined in this post.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post we introduced you to the new Amazon File Cache integration feature with AWS ParallelCluster 3.7 to create a hybrid HPC environment with low latency to access your data.&lt;/p&gt; 
&lt;p&gt;We walked you through the details of how to set up File Cache with ParallelCluster and showed you some benchmarks to highlight how lazy-loading can drastically improve IO performance in a hybrid scenario.&lt;/p&gt; 
&lt;p&gt;When you consider Amazon File Cache as a solution for your hybrid workloads, you should consider the trade-offs between cost, performance, and (of course) business value. As we’ve seen with the IOR benchmarks, for data intensive workloads that read large data sets from the repository for the first time, it’ll take more time for a job to finish due to file access latency.&lt;/p&gt; 
&lt;p&gt;Customers who require predictability in performance for &lt;em&gt;every single job&lt;/em&gt;, might want to consider other AWS services. &lt;a href="https://aws.amazon.com/datasync/"&gt;AWS DataSync&lt;/a&gt;, for example, can sync data between your site and AWS to maintain a constantly up to date mirror. This will ensure the workload you run on AWS will access its input data with low latency.&lt;/p&gt; 
&lt;p&gt;Amazon File Cache is a powerful tool that enables customers to access their data on a cache with sub-millisecond latency. Support for Amazon File Cache is a great addition to AWS ParallelCluster for hybrid HPC workloads, making it easy for customers to deploy an HPC environment on AWS in a matter of minutes with a cache linked to data in their own data centers.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>The plumbing: best-practice infrastructure to facilitate HPC on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/the-plumbing-best-practice-infrastructure-to-facilitate-hpc-on-aws/</link>
		
		<dc:creator><![CDATA[Evan Bollig]]></dc:creator>
		<pubDate>Tue, 29 Aug 2023 15:02:56 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Best Practices]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<guid isPermaLink="false">53a6f1dcc9fb5f53203f89e0b63b36a7e608a879</guid>

					<description>If you want to build enterprise-grade HPC on AWS, what’s the best path to get started? Should you create a new AWS account and build from scratch? In this post we'll walk you through the best practices for getting setup cleanly from the start.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-2748" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/02/boofla88_complicate_plumbing_as_a_rube_golberg_machine_with_an__dc6d5623-255a-4f26-9e08-245491e934d8.png" alt="The plumbing: best-practice infrastructure to facilitate HPC on AWS" width="380" height="212"&gt;If you want to build enterprise-grade high performance computing on AWS, what’s the best path to get started? Should you create a new AWS account and build from scratch?&lt;/p&gt; 
&lt;p&gt;To answer those questions, consider an analogy of someone washing their hands at a sink. Many focus on a person washing their hands, but that act is an outcome or result. We’ll argue that the plumbing – the water, the sink, and the drain pipes – is critical to facilitate that outcome.&lt;/p&gt; 
&lt;p&gt;The plumbing often fades to the background, its value under-appreciated until something fails. Yet it provides a best-practices foundation for countless use-cases. In similar fashion, enterprise-grade HPC depends on a lot of critical plumbing.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll identify the hallmarks and best-practices for building a solid HPC foundation so you can get the plumbing right. We’ll assume you’re an HPC facilitator: a system admin, operator, or you work at an HPC center. Deep experience with AWS is &lt;strong&gt;not&lt;/strong&gt; required.&lt;/p&gt; 
&lt;p&gt;Our plan is that you come away with an appreciation for shared responsibilities when building on AWS, and aware of solutions that make enterprise-grade HPC a good deal easier than you might have expected.&lt;/p&gt; 
&lt;h2&gt;Shared responsibility&lt;/h2&gt; 
&lt;p&gt;Many customers, especially individuals or small teams assume that operating on the cloud requires bearing the entire burden of their architecture. The weight of compliance and security, the complexity of networking and identity – those burdens are much larger than one person or one team.&lt;/p&gt; 
&lt;p&gt;Even more HPC customers stumble because they fear that once they’ve entered their credit card to create an account, they own every single decision involved in building HPC on the cloud.&lt;/p&gt; 
&lt;p&gt;If you’re a small business – say a startup – you wouldn’t break ground for a new facility on day one, run network, water, and power lines, build an office building, and then populate it. Instead, you might rent an office – or go to a coffee shop table – where network, water, and power exist for you to tap into. Good plumbing allows you to focus first on building what matters most: your business.&lt;/p&gt; 
&lt;p&gt;At AWS, we often talk about the “&lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/shared-responsibility.html"&gt;shared responsibility model&lt;/a&gt;“ as core to our well-architected framework, a six-pillar framework for best-practices in building secure, high-performing, resilient and efficient infrastructures. Most customers understand shared responsibility regarding the security pillar: &amp;nbsp;customer responsibility &lt;em&gt;in the cloud&lt;/em&gt; vs. AWS responsibility &lt;em&gt;of the cloud&lt;/em&gt;. But shared responsibility also applies elsewhere – like &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/the-shared-responsibility-model.html"&gt;sustainability&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;It can also be multi-layered and differentiate between roles inside your own organization. For example, you as an HPC provider (a &lt;em&gt;facilitator&lt;/em&gt;) might focus on building an HPC cluster, while other teams manage incident response, account guardrails and boundaries, or identity and access management.&lt;/p&gt; 
&lt;p&gt;To make it clear, let’s say you represent an HPC center, and you serve end-users campus-wide at a university or research institution. As an HPC provider, you might be embedded within the campus-wide IT team, or maybe you operate with autonomy while pieces of your infrastructure are tied back through the central IT services. On-premises, you already have the concept of “&lt;em&gt;the plumbing&lt;/em&gt;” that you build on top of. You’re not operating alone. You focus effort on HPC, but your HPC system depends on enterprise shared services (like Active Directory where your users’ IDs and passwords live), your campus network, compliance reporting, and so on. This same pattern repeats itself whether you’re at a university, a national lab, or a non-profit organization. It’s true whether you’re public-sector or a company.&lt;/p&gt; 
&lt;p&gt;As a best-practice, HPC customers (especially those intending to operate enterprise-wide HPC clusters or HPC as a service offerings) leverage shared responsibility to build on top of good &lt;em&gt;plumbing&lt;/em&gt; infrastructure on AWS. The most successful collaborate with their central IT services to layer HPC on top of a broader enterprise cloud strategy.&lt;/p&gt; 
&lt;h2&gt;The plumbing&lt;/h2&gt; 
&lt;p&gt;So what is foundational &lt;em&gt;plumbing,&lt;/em&gt; and how does HPC attach to it?&lt;/p&gt; 
&lt;p&gt;Most enterprise customers establish an &lt;a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-migration/aws-landing-zone.html"&gt;AWS Landing Zone&lt;/a&gt;. This is a well-architected, pre-defined architecture for enterprise infrastructure. A Landing Zone is the foundation for multi-account architecture, Identity and Access Management (IAM), governance, data security, network design, and logging. It’s typically defined by central IT within an enterprise, and helps align accounts to AWS best-practices so they can meet compliance frameworks. Any infrastructure built within an enterprise, including HPC, affixes to the Landing Zone.&lt;/p&gt; 
&lt;p&gt;Therefore, in your role as an HPC facilitator, step one should be to ask your central IT team whether a Landing Zone exists, and how to get started. Share responsibility with that team, and let them guide your HPC build.&lt;/p&gt; 
&lt;p&gt;The fastest path to establish a Landing Zone is through the &lt;a href="https://aws.amazon.com/solutions/implementations/landing-zone-accelerator-on-aws/"&gt;Landing Zone Accelerator&lt;/a&gt; (LZA). This is an open source &lt;a href="https://docs.aws.amazon.com/whitepapers/latest/introduction-devops-aws/infrastructure-as-code.html"&gt;infrastructure as code&lt;/a&gt; solution that enables repeatable configuration and modification of a Landing Zone through low-code YAML files. &lt;a href="https://aws.amazon.com/professional-services/"&gt;AWS Professional Services&lt;/a&gt; and &lt;a href="https://aws.amazon.com/controltower/partners/"&gt;AWS Partners&lt;/a&gt; also offer fast-track options to build and support Landing Zones through LZA deployment and alternative solutions.&lt;/p&gt; 
&lt;p&gt;There is no one-size-fits-all Landing Zone configuration for HPC customers because HPC lives within every industry and compliance framework. However, Figure 1 shows a best-practices architecture pattern that’s repeated across reference LZA implementations from regulated industries like &lt;a href="https://aws.amazon.com/blogs/industries/introducing-landing-zone-accelerator-for-healthcare/"&gt;Health Care (e.g., HIPAA, C5, etc.)&lt;/a&gt;, and country/regional-specific compliance requirements like &lt;a href="https://aws.amazon.com/blogs/publicsector/data-security-governance-best-practices-education-state-local-government/"&gt;State and Local Governments (e.g., FISMA)&lt;/a&gt;, &lt;a href="https://github.com/awslabs/landing-zone-accelerator-on-aws/tree/main/reference/sample-configurations/aws-best-practices-education"&gt;Education (e.g., NIST 800-53, NIST 800-171, ITAR, etc.)&lt;/a&gt;, and &lt;a href="https://github.com/awslabs/landing-zone-accelerator-on-aws/tree/main/reference/sample-configurations/aws-best-practices-govcloud-us"&gt;US Federal and Department of Defense (e.g., ITAR, FedRAMP, CMMC, etc.)&lt;/a&gt;. Other reference implementations for LZAs (including territories outside the USA) are &lt;a href="https://aws.amazon.com/solutions/implementations/landing-zone-accelerator-on-aws/"&gt;inventoried here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;As a word of caution: linking to core infrastructure and adhering to constraints/guardrails imposed by a Landing Zone moves infrastructure closer to meeting compliance requirements, but may not be a complete solution for compliance. Just attaching an HPC cluster to a FedRAMP Landing Zone configuration doesn’t make it FedRAMP-compliant. Proper documentation of controls and internal audits (and in some cases, 3&lt;sup&gt;rd&lt;/sup&gt; party audits) might be required. Here again, share responsibility with central IT services and your enterprise security and compliance office to satisfy enterprise requirements.&lt;/p&gt; 
&lt;div id="attachment_2749" style="width: 990px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2749" loading="lazy" class="size-full wp-image-2749" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/02/CleanShot-2023-08-02-at-08.22.02.png" alt="Figure 1 - Landing Zone Accelerator provides the best-practices multi-account structure (the Plumbing for HPC). Shown here: an example configuration with shared services and shared networking for an enterprise. HPC is built within one or more Workload Accounts." width="980" height="550"&gt;
 &lt;p id="caption-attachment-2749" class="wp-caption-text"&gt;Figure 1 – Landing Zone Accelerator provides the best-practices multi-account structure (the Plumbing for HPC). Shown here: an example configuration with shared services and shared networking for an enterprise. HPC is built within one or more Workload Accounts.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Commonly, a Landing Zone is comprised of a root, or management, organizational unit (OU) with an account that centrally steers the multi-account structure through mandatory and preventative guard rails and other requirements for how accounts are provisioned.&lt;/p&gt; 
&lt;p&gt;The root account may define Service Control Policies and/or a Permissions Boundary to govern how accounts/roles in the organization tree provision resources, call APIs, etc. While it’s possible to self-build and manage a Landing Zone, best-practices (including for Landing Zone Accelerator) &lt;a href="https://aws.amazon.com/blogs/mt/scaling-landing-zone-with-aws-control-towers/"&gt;leverage AWS Control Tower&lt;/a&gt;, which is a managed service purpose-built for this task.&lt;/p&gt; 
&lt;p&gt;Below the top-level OU, a Security OU with Audit and Logging accounts manages organizational-wide services for security (like threat detection) and centralized logs (such as archive and forensics).&lt;/p&gt; 
&lt;p&gt;A third OU dedicated to shared Infrastructure might contain services like Active Directory, as well as a core Network account for firewall and/or packet inspection appliances, and a shared &lt;a href="https://aws.amazon.com/directconnect/"&gt;Direct Connect&lt;/a&gt; attachment.&lt;/p&gt; 
&lt;p&gt;Lastly, the final OU is dedicated to workload accounts – this is where an HPC team can operate to build dev, test, and production clusters and link to the shared infrastructure.&lt;/p&gt; 
&lt;h2&gt;HPC within a Landing Zone&lt;/h2&gt; 
&lt;p&gt;In practice, HPC teams build on top of a Landing Zone with minimal awareness of how the plumbing infrastructure is built. They only need to know when to tie in shared services and how to build HPC infrastructure for themselves.&lt;/p&gt; 
&lt;p&gt;For a deeper look at common HPC scenarios and key elements to ensure architectural best-practices, you can refer to the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/welcome.html"&gt;HPC Lens for the AWS Well-Architected Framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The most significant opportunity for HPC facilitators when they’re building on AWS is to augment their HPC design to exercise the elasticity and flexibility of the cloud. Compute nodes spin up as needed, and down when not. The compute node memory size, core-count, accelerators (and more) can be adjusted in minutes rather than the cluster hardware staying set in stone for a three to five year lifecycle.&lt;/p&gt; 
&lt;p&gt;Your HPC architecture might be a single monolithic HPC system, reproducing the on-premises cluster experience. Or you might be extending it (like in a hybrid HPC scenario) so end-users share a single scheduler and common storage resources. As an alternative, tools and services like &lt;a href="https://aws.amazon.com/blogs/hpc/choosing-between-batch-or-parallelcluster-for-hpc/"&gt;AWS ParallelCluster and AWS Batch&lt;/a&gt; present an opportunity to build smaller HPC solutions right-sized for workloads on a per-project or per-team basis – or even for individual users.&lt;/p&gt; 
&lt;p&gt;Whether you build one cluster or dozens, elasticity and flexibility are critical to conserve cost and evolve the end-user experience from “&lt;em&gt;what questions can I ask with the cores I’ve got?&lt;/em&gt;” (science limited by scale), to “&lt;em&gt;what do I need to do to get more cores?&lt;/em&gt;” (science is priority). The latter implies greater productivity and ability to innovate.&lt;/p&gt; 
&lt;p&gt;A Landing Zone amplifies flexibility in the case of multiple HPC clusters. For example, mapping users and workloads onto separate accounts in the multi-account structure is useful to partition data for residency and access constraints.&lt;/p&gt; 
&lt;p&gt;Second, a Landing Zone configuration dictates how new accounts are provisioned on creation, helping facilitators pre-warm dependencies for launching clusters (for example, required IAM roles, or a database that’s needed to enable accounting in the scheduler).&lt;/p&gt; 
&lt;p&gt;Third, Landing Zones can integrate &lt;a href="https://aws.amazon.com/servicecatalog/"&gt;AWS Service Catalog&lt;/a&gt;, which is a vending service for infrastructure as code. This creates a controlled mechanism for end-users to self-service, provision, and delete infrastructure in their account. Service Catalog Products define template configurations (a website, a database, or an HPC cluster) that are reviewed and approved by HPC facilitators or central IT services to satisfy compliance requirements or other guardrails. Optional parameters on Service Catalog Products give end-users control of facets of HPC relevant to their work, like instance-type selection, storage quota, or cost-center allocation. Through Service Catalog, facilitators distill the complexity of enterprise HPC design and management, but empower their users to innovate autonomously.&lt;/p&gt; 
&lt;p&gt;Landing Zones also support HPC facilitators with cost and billing strategies. Typically, central IT services receive &lt;a href="https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html"&gt;consolidated bills&lt;/a&gt; and reconcile costs with workload accounts through internal cost recovery and charge-back processes. With consolidated billing, organizations combine usage across all workload accounts to more effectively scale &lt;a href="https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html"&gt;Savings Plans&lt;/a&gt; or &lt;a href="https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/useconsolidatedbilling-effective.html#useconsolidatedbilling-discounts"&gt;Volume discounts&lt;/a&gt;. Whereas one HPC cluster with episodic utilization might not benefit from a Savings Plan, utilization &lt;em&gt;in the aggregate&lt;/em&gt; across &lt;em&gt;all&lt;/em&gt; HPC accounts might look sustained – and therefore justify making a commitment with a Savings Plan, and thus saving up to 72% on the organization’s overall compute bill.&lt;/p&gt; 
&lt;p&gt;The same logic applies when utilization is aggregated for HPC and &lt;em&gt;non-HPC&lt;/em&gt; workloads.&lt;/p&gt; 
&lt;p&gt;Consolidated billing doesn’t preclude charge-back to individuals (per-project, team, or user). Separate workload accounts provide a simple segmentation to track spend in the consolidated bill. Alternatively, cost allocation tags can be applied to resources within or across accounts, and differentiate between cost centers for utilization. Again, consult with your central IT services. They should be able to advise on how to best handle cost, billing, and tagging strategies for your HPC workload accounts.&lt;/p&gt; 
&lt;p&gt;If you need more information, review this &lt;a href="https://aws.amazon.com/blogs/apn/reducing-the-cost-of-managing-multiple-aws-accounts-using-aws-control-tower/"&gt;blog post related to Landing Zone best-practices&lt;/a&gt; for cost and billing, and this blog post about &lt;a href="https://aws.amazon.com/blogs/aws-cloud-financial-management/cost-tagging-and-reporting-with-aws-organizations/"&gt;tagging and reporting strategy for organizations&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Landing Zones are a best-practice “&lt;em&gt;plumbing&lt;/em&gt;” for enterprise-grade HPC and the most natural starting point for most HPC customers. Whether you intend to offer end-users flexible HPC as a service, or build a single monolithic HPC cluster shared by all users, we recommend you exercise &lt;strong&gt;shared responsibility&lt;/strong&gt; and collaborate with central IT services teams in your enterprise so you can layer HPC as a workload within the broader cloud strategy.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Deep-dive into Hpc7a, the newest AMD-powered member of the HPC instance family</title>
		<link>https://aws.amazon.com/blogs/hpc/deep-dive-into-hpc7a-the-newest-amd-powered-member-of-the-hpc-instance-family/</link>
		
		<dc:creator><![CDATA[Stephen Sachs]]></dc:creator>
		<pubDate>Tue, 22 Aug 2023 14:54:19 +0000</pubDate>
				<category><![CDATA[*Post Types]]></category>
		<category><![CDATA[Announcements]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[MPI]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">3edbe371730c2bf0764bb5a4d71c99ec5a30d5c5</guid>

					<description>Today we discuss the performance results we saw from the new hpc7a instance, running HPC workloads like CFD, molecular dynamics, and weather prediction codes.</description>
										<content:encoded>&lt;p&gt;Last week, &lt;a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-hpc7a-instances-powered-by-4th-gen-amd-epyc-processors-optimized-for-high-performance-computing/"&gt;we announced the Hpc7a instance type&lt;/a&gt;, the latest generation AMD-based HPC instance, purpose-built for tightly-coupled high performance computing workloads. This joins our family of HPC instance types in the Amazon Elastic Compute Cloud (Amazon EC2) which began with Hpc7a’s predecessor — the Hpc6a in January 2022.&lt;/p&gt; 
&lt;p&gt;Amazon EC2 Hpc7a instances, powered by 4th generation AMD EPYC processors, deliver up to 2.5x better performance compared to Amazon EC2 Hpc6a instances.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll discuss details of the new instance and show you some of the performance metrics we’ve gathered by running HPC workloads like computational fluid dynamics (CFD), molecular dynamics (MD) and numerical weather prediction (NWP).&lt;/p&gt; 
&lt;h2&gt;Introducing the Hpc7a instance&lt;/h2&gt; 
&lt;p&gt;We launched Hpc6a last year for customers to efficiently run their compute-bound HPC workloads on AWS. As their jobs grow in complexity, customers have asked for more cores with more compute performance, as well as more memory and network performance to reduce their time to results. The Hpc7a instances deliver on these asks, providing twice the number of cores, 1.5x the number of memory channels, and three-times the network bandwidth compared to the previous generation.&lt;/p&gt; 
&lt;p&gt;It’s based on the 4th Generation AMD EPYC (code name Genoa) processor with up to 192 physical cores, an&amp;nbsp;all-core turbo frequency of 3.7 GHz,&amp;nbsp;768 GiB of memory, and 300 Gbps of Elastic Fabric Adapter (EFA) network performance. This is all possible because of the &lt;a href="https://aws.amazon.com/ec2/nitro/"&gt;AWS Nitro System&lt;/a&gt;, a combination of dedicated hardware and a lightweight hypervisor&amp;nbsp;that offloads many of the traditional virtualization functions to dedicated hardware, result in performance that’s &lt;a href="https://aws.amazon.com/blogs/hpc/bare-metal-performance-with-the-aws-nitro-system/"&gt;virtually indistinguishable&lt;/a&gt; from bare metal.&lt;/p&gt; 
&lt;h2&gt;HPC instances sizes&lt;/h2&gt; 
&lt;p&gt;Many off the shelf HPC applications use per-core commercial licensing that is often much greater in price per core than the cores themselves. Those customers have asked for more available memory bandwidth, and more network throughput available &lt;em&gt;per-core&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Starting today, Hpc7a (along with other 7&lt;sup&gt;th&lt;/sup&gt; generation HPC instances) will be available in different sizes. Usually in Amazon EC2, a smaller instance size reflects a smaller slice of the underlying hardware. However, for the HPC instances — starting with Hpc7g and Hpc7a — each size option &lt;em&gt;will have the same engineering specs and price&lt;/em&gt;, and will differ only by the number of cores enabled.&lt;/p&gt; 
&lt;p&gt;You have always been able to manually disable cores or use process pinning (affinity) to carefully place threads around the CPUs. But doing this optimally needs an in-depth knowledge of the chip architecture – like the number of NUMA domains and the memory layout. It also means you have to know MPI well, and have a clear sight to what your job submission scripts do when the scheduler receives them.&lt;/p&gt; 
&lt;p&gt;By offering instance sizes that already have the &lt;em&gt;right pattern of cores turned off&lt;/em&gt;, you’ll be able to maximize the performance of your code with less work. This will be a boost for customers who need to achieve the absolute best performance &lt;em&gt;per core&lt;/em&gt; for their workloads, which often includes commercially-licensed ISV applications. In this case, customers are driven by pricing concerns to get the best possible results from each &lt;em&gt;per-core&lt;/em&gt; license they buy. You can find a &lt;a href="https://aws.amazon.com/blogs/hpc/instance-sizes-in-the-amazon-ec2-hpc7-family-a-different-experience/"&gt;detailed explanation in our post on this topic&lt;/a&gt;, along with performance comparisons that will help you understand our methodology.&lt;/p&gt; 
&lt;p&gt;With the HPC instance sizes, the cost stays the same for all sizes because you still get access to the entire node’s memory and network, with selected cores turned off to leave extra memory bandwidth available for the remaining cores. We encourage you to benchmark your own codes and find the right balance for your specific needs.&lt;/p&gt; 
&lt;p&gt;Hpc7a will be available in &lt;strong&gt;four different instances sizes&lt;/strong&gt;. Table 1 describes these in detail.&lt;/p&gt; 
&lt;div id="attachment_2788" style="width: 901px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2788" loading="lazy" class="wp-image-2788 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-14.18.26.png" alt="Table 1 - Available instance sizes for Hpc7a. Customers choosing a smaller instance type will still have access to the full memory and network performance of the largest instance size but a different number of CPU cores." width="891" height="263"&gt;
 &lt;p id="caption-attachment-2788" class="wp-caption-text"&gt;Table 1 – Available instance sizes for Hpc7a. Customers choosing a smaller instance type will still have access to the full memory and network performance of the largest instance size but a different number of CPU cores.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;Hpc7a shows significant performance gains over previous generations. While doubling the number of cores per instance, the performance, network, and memory bandwidth per-core have all increased. In many cases, this is leading to a doubling and more in overall simulation speed.&lt;/p&gt; 
&lt;p&gt;To illustrate this, we’ll look at the relative performance improvement of five common HPC codes across two generations of instance. We’ll look at:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Siemens Simcenter STAR-CCM+ for CFD&lt;/li&gt; 
 &lt;li&gt;Ansys Fluent for CFD&lt;/li&gt; 
 &lt;li&gt;OpenFOAM for CFD&lt;/li&gt; 
 &lt;li&gt;GROMACS for MD&lt;/li&gt; 
 &lt;li&gt;Weather Research and Forecasting Model (WRF) for NWP&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Let’s take a detailed look at each code with performance comparisons to the previous generation Hpc6a instance. For the readers convenience, the previous generations’ specs are Hpc6a.48xlarge: 96 physical cores (3&lt;sup&gt;rd&lt;/sup&gt; Generation AMD EPYC, code name Milan), 384 GB memory, 4 GB memory per core, 3.6 GHz CPU Frequency, and 100 Gbps EFA Network Performance.&lt;/p&gt; 
&lt;h2&gt;Siemens Simcenter STAR-CCM+&lt;/h2&gt; 
&lt;p&gt;First, we take a look at Siemens Simcenter STAR-CCM+. We chose the AeroSUV 320M cell automotive test case – a useful public case with similar characteristics to production automotive external aerodynamics models. We ran this with Siemens Simcenter STAR-CCM+ 2306, using Intel MPI 2021.6. As this is an ISV application, no further tuning to match the architecture is necessary. The graphs below show the &lt;em&gt;iterations per minute&lt;/em&gt; as a metric for performance.&lt;/p&gt; 
&lt;p&gt;We saw an up to 2.7x speed-up between Hpc7a and Hpc6a at 16 instances and similar scaling all the way to 12k cores, that’s around 26k cells per core.&lt;/p&gt; 
&lt;div id="attachment_2768" style="width: 980px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2768" loading="lazy" class="size-full wp-image-2768" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.36.02.png" alt="Figure 1 – A graph of performance of Siemens Simcenter STAR-CCM+ on the AeroSUV 320M cell dataset. The figure shows that Hpc7a outperforms Hpc6a up to 3.2x on a per instance basis." width="970" height="538"&gt;
 &lt;p id="caption-attachment-2768" class="wp-caption-text"&gt;Figure 1 – A graph of performance of Siemens Simcenter STAR-CCM+ on the AeroSUV 320M cell dataset. The figure shows that Hpc7a outperforms Hpc6a up to 2.7x on a per instance basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2786" style="width: 978px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2786" loading="lazy" class="wp-image-2786 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-14.14.12.png" alt="Figure 2 – A graph of performance of Siemens Simcenter STAR-CCM+ on the AeroSUV 320M cell dataset. Hpc7a outperforms Hpc6a up to 1.29x on a per core basis." width="968" height="537"&gt;
 &lt;p id="caption-attachment-2786" class="wp-caption-text"&gt;Figure 2 – A graph of performance of Siemens Simcenter STAR-CCM+ on the AeroSUV 320M cell dataset. Hpc7a outperforms Hpc6a up to 1.29x on a per core basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Ansys Fluent&lt;/h2&gt; 
&lt;p&gt;Next we took a look at ANSYS Fluent 2023R1 – where we ran the common public dataset &lt;a href="https://www.ansys.com/en-in/it-solutions/benchmarks-overview/ansys-fluent-benchmarks/ansys-fluent-benchmarks-release-19/external-flow-over-a-formula-1-race-car"&gt;External flow over a Formula-1 race car&lt;/a&gt; . The case has around 140-million cells and uses the realizable k-e turbulence model and the pressure-based coupled solver, least squares cell-based, pseudo transient solver. We ran it to over 9,000 cores, which is still well within the parallel scaling of this particular test case.&lt;/p&gt; 
&lt;p&gt;The graphs show Solver rating as defined by Ansys as the number of benchmarks that can be run on a given machine (in sequence) in a 24-hour period. We compute this by dividing the number of seconds in a day (86,400 seconds) by the number of seconds required to run the benchmark.&lt;/p&gt; 
&lt;p&gt;On a per instance basis Hpc7a exhibits up to 2.48x better performance than Hpc6a at 6 instances. The iso-core benefit to Hpc7as favor peaks with 1.29x at 6144 cores.&lt;/p&gt; 
&lt;div id="attachment_2770" style="width: 981px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2770" loading="lazy" class="size-full wp-image-2770" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.37.23.png" alt="Figure 3 – A graph of performance of Ansys Fluent on the F1 race car 140M dataset. Hpc7a outperforms Hpc6a up to 2.48x on a per instance basis." width="971" height="537"&gt;
 &lt;p id="caption-attachment-2770" class="wp-caption-text"&gt;Figure 3 – A graph of performance of Ansys Fluent on the F1 race car 140M dataset. Hpc7a outperforms Hpc6a up to 2.48x on a per instance basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2771" style="width: 979px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2771" loading="lazy" class="size-full wp-image-2771" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.37.43.png" alt="Figure 4 – A graph of performance of Ansys Fluent on the F1 race car 140M dataset. Hpc7a outperforms Hpc6a up to 1.29x on a per core basis." width="969" height="538"&gt;
 &lt;p id="caption-attachment-2771" class="wp-caption-text"&gt;Figure 4 – A graph of performance of Ansys Fluent on the F1 race car 140M dataset. Hpc7a outperforms Hpc6a up to 1.29x on a per core basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;OpenFOAM&lt;/h2&gt; 
&lt;p&gt;Next, we tested OpenFOAM with the &lt;em&gt;DrivAer fastback vehicle&lt;/em&gt; (from the &lt;a href="http://autocfd.eng.ox.ac.uk/"&gt;AutoCFD workshop&lt;/a&gt;) with 128M cells (generated using the ANSA preprocessing software by BETA-CAE Systems), and ran the case in hybrid RANS-LES mode using the pimpleFoam solver. We used OpenFOAM v2206 compiled with GNU C++ Compiler v12.3.0 and Open MPI v4.1.5. We use the architecture specific flags to tune the compilation for each instance type (“-march=x86-64-v4 -mtune=x86-64-v4” for Hpc7a and “-march=znver3 -mtune=znver3” for Hpc6a). We ran the case using 192 and 96 MPI ranks per instance, respectively, (fully-populated) and scaled to 3072 cores. The graphs below show the &lt;em&gt;iterations per minute&lt;/em&gt; as a metric for performance.&lt;/p&gt; 
&lt;p&gt;We saw an up to 2.7x speed-up at 4 instances as this is the core count where Hpc7a is showing super linear scaling curve. Super linear scaling stops at 1536 cores for both instances types and further decreases towards 3072. This is 42k cores per MPI rank which is the expected scaling limit for OpenFOAM.&lt;/p&gt; 
&lt;div id="attachment_2772" style="width: 977px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2772" loading="lazy" class="size-full wp-image-2772" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.38.19.png" alt="Figure 5 – A graph of performance of OpenFOAM on the DrivAer 128M dataset. Hpc7a outperforms Hpc6a by up 2.7x on a per instance basis." width="967" height="537"&gt;
 &lt;p id="caption-attachment-2772" class="wp-caption-text"&gt;Figure 5 – A graph of performance of OpenFOAM on the DrivAer 128M dataset. Hpc7a outperforms Hpc6a by up 2.7x on a per instance basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2773" style="width: 979px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2773" loading="lazy" class="size-full wp-image-2773" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.38.49.png" alt="Figure 6 – A graph of performance of OpenFOAM on the DrivAer 128M dataset. Hpc7a outperforms Hpc6a by up 1.3x on a per core basis." width="969" height="540"&gt;
 &lt;p id="caption-attachment-2773" class="wp-caption-text"&gt;Figure 6 – A graph of performance of OpenFOAM on the DrivAer 128M dataset. Hpc7a outperforms Hpc6a by up 1.3x on a per core basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;GROMACS&lt;/h2&gt; 
&lt;p&gt;Next, we looked at Max Planck Institute provided test case for &lt;a href="https://www.mpinat.mpg.de/grubmueller/bench"&gt;2M atoms ribosome in water (benchRIB)&lt;/a&gt; using GROMACS version 2021.5. We used the Intel compiler version 2022.1.2 and Intel MPI 2021.5.1 to compile and run GROMACS. In this case we used the best matching Intel Compiler flags for Hpc7a (“-march=skylake-avx512 -mtune=skylake-avx512”) and Hpc6a (“-march=core-avx2”) and Intel MKL for the Fast Fourier Transform.&lt;/p&gt; 
&lt;p&gt;For each scaling data point we used the optimal MPI rank versus OpenMP thread distribution, which means we start with 1 OpenMP thread and an MPI rank on each core at 1 instances and steadily increase the number of OpenMP threads when scaling further to better balance the workload. The maximum number of threads used is 2 threads at 8 instances and 4 at 16 instances for Hpc7a and Hpc6a, respectively. The graphs show simulated time per day (ns/day), higher is better.&lt;/p&gt; 
&lt;p&gt;We saw up to 2.05x speed-up on 2 instances and similar scaling when comparing Hpc7a to Hpc6a. We also see an up to 1.2x speed-up at 768 cores when comparing on a per core level.&lt;/p&gt; 
&lt;div id="attachment_2774" style="width: 974px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2774" loading="lazy" class="size-full wp-image-2774" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.39.25.png" alt="Figure 7 – A graph of performance of GROMACS on the benchRIB dataset. Hpc7a outperforms Hpc6a up to 2.05x on a per instance basis." width="964" height="536"&gt;
 &lt;p id="caption-attachment-2774" class="wp-caption-text"&gt;Figure 7 – A graph of performance of GROMACS on the benchRIB dataset. Hpc7a outperforms Hpc6a up to 2.05x on a per instance basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2775" style="width: 976px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2775" loading="lazy" class="size-full wp-image-2775" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.39.50.png" alt="Figure 8 – A graph of performance of GROMACS on the benchRIB dataset. Hpc7a outperforms Hpc6a up to 1.2x on a per core basis." width="966" height="535"&gt;
 &lt;p id="caption-attachment-2775" class="wp-caption-text"&gt;Figure 8 – A graph of performance of GROMACS on the benchRIB dataset. Hpc7a outperforms Hpc6a up to 1.2x on a per core basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;WRF&lt;/h2&gt; 
&lt;p&gt;We looked at &lt;a href="https://www2.mmm.ucar.edu/wrf/src/conus2.5km.tar.gz"&gt;CONUS 2.5km benchmark&lt;/a&gt; performance using WRF v4.2.2. We used the Intel compiler version 2022.1.2 and Intel MPI 2021.9.0 to compile and run WRF using the same compiler flags as for GROMACS. We used 48 MPI ranks per instances filling up the remaining cores with 4 OpenMP threads to use all 192 cores per instance.&lt;/p&gt; 
&lt;p&gt;We ran the scaling test up to 128 instances (24,576 cores) and used the total elapsed time to calculate the simulation speed as runs per day (higher is better). Comparing the instances types, we saw an up to 2.6x speed up at 8 instances, and better scalability for Hpc7a compared to previous generation Hpc6a due to the increase of on-node traffic.&lt;/p&gt; 
&lt;div id="attachment_2776" style="width: 975px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2776" loading="lazy" class="size-full wp-image-2776" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.40.19.png" alt="Figure 9 – A graph of performance of WRF on the Conus 2.5km dataset. Hpc7a outperforms Hpc6a by up 2.6x on a per instance basis." width="965" height="535"&gt;
 &lt;p id="caption-attachment-2776" class="wp-caption-text"&gt;Figure 9 – A graph of performance of WRF on the Conus 2.5km dataset. Hpc7a outperforms Hpc6a by up 2.6x on a per instance basis.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2777" style="width: 976px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2777" loading="lazy" class="size-full wp-image-2777" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.40.40.png" alt="Figure 10 – A graph of performance of WRF on the Conus 2.5km dataset. Hpc7a retains better scalability at &gt;10k cores due to increased on-node traffic." width="966" height="533"&gt;
 &lt;p id="caption-attachment-2777" class="wp-caption-text"&gt;Figure 10 – A graph of performance of WRF on the Conus 2.5km dataset. Hpc7a retains better scalability at &amp;gt;10k cores due to increased on-node traffic.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Performance is more than just higher core counts&lt;/h2&gt; 
&lt;p&gt;Hpc7a shows a great performance increase over the previous generation and not only due to doubling the numbers of cores. Figure 11 shows the performance gain for each workload on a per-core basis, so this is additional to that doubling. We ran all instance comparisons using either real production workloads or a close substitute.&lt;/p&gt; 
&lt;p&gt;Hpc7a instances show, on average, a performance improvement of 29% compared to the previous generation on a &lt;em&gt;per-core&lt;/em&gt; basis.&lt;/p&gt; 
&lt;div id="attachment_2778" style="width: 977px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2778" loading="lazy" class="size-full wp-image-2778" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/08/18/CleanShot-2023-08-18-at-10.41.13.png" alt="Figure 11 - Relative performance of Hpc6a and Hpc7a for various applications. The performance results shown here use the same number of cores on HPC6a.48xlarge and Hpc7a.96xlarge to highlight the expected iso-core improvement. This translates to using only half the number Hpc7a instances per test case." width="967" height="539"&gt;
 &lt;p id="caption-attachment-2778" class="wp-caption-text"&gt;Figure 11 – Relative performance of Hpc6a and Hpc7a for various applications. The performance results shown here use the same number of cores on HPC6a.48xlarge and Hpc7a.96xlarge to highlight the expected iso-core improvement. This translates to using only half the number Hpc7a instances per test case.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog post we introduced the Amazon EC2 Hpc7a instance, which offers up to 2.5x better compute performance compared to previous generation AMD HPC instance. It has twice the cores per instance and yet still has increased per-core compute performance, better memory bandwidth, and greater network performance per core.&lt;/p&gt; 
&lt;p&gt;We hope you’ll try them for your workloads, too. Reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt; and let us know how you fare.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>How computer vision is enabling a circular economy</title>
		<link>https://aws.amazon.com/blogs/hpc/how-computer-vision-is-enabling-a-circular-economy/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Tue, 15 Aug 2023 15:04:30 +0000</pubDate>
				<category><![CDATA[Amazon Machine Learning]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">593227d1a134845db1c20527dfb0d67ff5c347eb</guid>

					<description>In this post, we show how Reezocar uses computer vision to change the way they detect damage and price used vehicles for re-sale in secondary markets. This reduces landfill and helps achieve the goals of the circular economy.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright wp-image-2675 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/boofla88_computer_vision_as_part_of_the_circular_economy_62b0baed-7a6b-49a3-9fc0-848575717090-1.png" alt="How computer vision is enabling a circular economy" width="380" height="212"&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;This post was contributed by Laurent Fabre, Chief Technology Officer and his team from Reezocar, and &lt;/em&gt;&lt;em&gt;Ilan Gleiser, Pr. Specialist, Global Impact Computing, AWS and our team.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Computer vision has enormous potential to revolutionize the way we think about sustainability and the circular economy. One of the key ways in which computer vision is already being applied to these areas is using powerful graphics processing units (GPUs) and AWS cloud computing services.&lt;/p&gt; 
&lt;p&gt;One of the primary applications of computer vision in the circular economy is in product lifecycles. By using advanced computer vision algorithms and machine learning models, manufacturers and supply chain managers can more accurately track and analyze the lifecycle of products, from raw materials sourcing all the way through to end-of-life disposal. This allows companies to better understand the environmental impacts of their products and make more informed decisions about how to design and produce products that are more sustainable and can be easily reused or recycled.&lt;/p&gt; 
&lt;p&gt;Another area in which computer vision is being applied to the circular economy is in waste reduction. By using advanced image recognition algorithms and computer vision technologies, waste management companies can better sort and categorize recyclable materials, making it easier and more cost-effective to recover and reuse valuable resources.&lt;/p&gt; 
&lt;p&gt;In addition to these applications, computer vision is also being used to identify opportunities for energy and resource savings, and to monitor and analyze environmental impacts at both a local and global scale. For example, satellite imagery and other forms of remote sensing data can be used to track deforestation, monitor ocean pollution levels, and even create early warning systems to predict the impact of natural disasters on ecosystems.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll explore how Reezocar is using computer vision to change the way they detect car damage and price used vehicles for re-sale in secondary markets. This technology can be used to determine the useful life of a car and potentially reduce the need for landfill waste, therefore aligning with the goals of the circular economy: designing-out waste and pollution from the environment.&lt;/p&gt; 
&lt;p&gt;Secondary markets play a critical role in achieving a circular economy by extending the lifespan of products and reducing waste. The benefits of secondary markets go beyond environmental impact, as they also generate economic opportunities, job creation, and community empowerment.&lt;/p&gt; 
&lt;p&gt;In this context, computer vision is revolutionizing the way we detect car damage. By leveraging the power of machine learning, computer vision can detect even the smallest dents and scratches on a car’s body.&lt;/p&gt; 
&lt;p&gt;Overall, the applications of computer vision in the circular economy are wide-ranging and growing rapidly. By enabling more efficient and sustainable resource use, computer vision has the power to drive real change in our economy and help create a more sustainable future for us all.&lt;/p&gt; 
&lt;h2&gt;Who is Reezocar?&lt;/h2&gt; 
&lt;p&gt;Reezocar is an online platform for buying and selling used cars in France. It offers customers a safe purchase guarantee and a bespoke shopping help service. With access to over six-million car ads, Reezocar makes it easy to select makes and models that match a buyer’s search criteria. Customers also have the option of purchasing a certified car, with a 15-day money back guarantee included.&lt;/p&gt; 
&lt;p&gt;Reezocar uses GPU-accelerated machine learning algorithms, convolutional neural networks, and a damage estimation model to calculate car prices. This system helps customers get the best deal on their purchase, as well as an accurate assessment of the vehicle’s condition. The result is a fair price estimate of the car and reassurance that they’re getting a good value for their money.&lt;/p&gt; 
&lt;p&gt;More than 10,000 customers have already been won over by the Reezocar experience, making it one of the top on-line marketplaces for buying and selling used cars. With its reliable customer service and advanced technology, Reezocar is quickly becoming the go-to destination for those looking to purchase a used car.&lt;/p&gt; 
&lt;p&gt;Reezocar is committed to its environmental mission and employs various strategies to uphold its climate-conscious values. Here are four key approaches they employ:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Extending Vehicle Lifespan&lt;/strong&gt;: By refurbishing selected vehicles, they have successfully prolonged their lifespan by up to five years, using approximately 5% of their manufacturer’s suggested retail price (MSRP) as refurbishing budget. This initiative has prevented over 30,000 tons of waste from ending up in landfills.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Carbon Offset Initiatives&lt;/strong&gt;: To counterbalance the CO2 emissions produced by these vehicles, they consistently engage in carbon offset practices through their partnership with ReforestAction. Through this collaboration, they have planted 33,000 trees, effectively trapping an &lt;a href="https://www.reforestaction.com/reezocar"&gt;estimated 5,000 tons of CO2.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Discouraging New Car Production&lt;/strong&gt;: Recognizing that the most environmentally friendly car is the one that doesn’t need to be built, they actively discourage the production of new cars by prioritizing the refurbishment of used vehicles. This approach has resulted in an estimated avoidance of 40,000 tons of material, considering the increasing weight of modern cars.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient Supply Chain&lt;/strong&gt;: By advocating for a shorter supply chain, they significantly reduce energy consumption during the delivery of vehicles. Additionally, this approach helps them avoid the costs associated with waste management while minimizing the pollution caused by car-related materials such as plastics, thus safeguarding the surrounding ecosystem.&lt;br&gt; With a decade of experience and numerous satisfied customers, Reezocar remains dedicated to leading the way in climate action.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;The rise of computer vision&lt;/h2&gt; 
&lt;p&gt;For decades, car buyers and dealers have relied on manual inspection to detect body damage on used cars. Until recently, physical inspections were the only way to get a full picture of the car’s condition. Even then, most damage could only be detected with close examination from a trained eye.&lt;/p&gt; 
&lt;p&gt;The technology leverages a combination of machine learning algorithms and convolutional neural networks to detect dents in vehicles, resulting in a faster, more streamlined, and accurate process.&lt;/p&gt; 
&lt;p&gt;Initially, the technology employs machine learning algorithms to estimate an initial value of the car based on features such as its model and gearbox type (automatic vs. standard).&lt;/p&gt; 
&lt;p&gt;Next, Computer vision is used to process the car’s image and identify any form of damage, like scratches, dents, or severe degradation. Any detected damage will be factored into the initial value estimate by subtracting the repair costs. The proposed computer vision system determines the specific parts of the car that are damaged, which helps identify the necessary repairs and their costs.&lt;/p&gt; 
&lt;p&gt;After computing the repair cost, Reezocar generates a post-repair estimate of the car’s value. This estimate considers factors like age, condition, and make to ensure maximum accuracy. With these tools at their disposal, dealerships can offer more competitive pricing and decide whether a car should be refurbished or sent to the landfill.&lt;/p&gt; 
&lt;h2&gt;Reezocar reference architecture&lt;/h2&gt; 
&lt;div id="attachment_2656" style="width: 1079px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2656" loading="lazy" class="size-full wp-image-2656" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.31.43.png" alt="Figure 1: Reezocar’s architecture schema " width="1069" height="509"&gt;
 &lt;p id="caption-attachment-2656" class="wp-caption-text"&gt;Figure 1: Reezocar’s architecture schema&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Reezocar’s Reference architecture aims to detect car dents using computer vision, as illustrated in Figure 1. Their approach consists of three steps: acquiring a dataset, fine tuning a convolutional neural network, and measuring the success of the model.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, they acquired and combined a synthetic and a real-world dataset to ensure a diverse and robust sample dataset. Using Amazon EC2 instances, they generated a synthetic dataset by algorithmically deforming 3D CAD models of cars. This helped to create a labeled synthetic dataset of images. Afterwards, they used Amazon SageMaker Ground Truth to annotate a set of real-world images. To achieve this, Reezocar uses Amazon SageMaker Ground Truth to acquire and annotate data, and then employs machine learning models running on GPUs to train a damage estimation model.&lt;/p&gt; 
&lt;p&gt;By using Amazon SageMaker Ground Truth, Reezocar was able to avoid the need to manage their own data labeling workforce, which would have slowed down innovation and increased costs. This approach allowed Reezocar to focus on their core competencies and optimize their resources for maximum efficiency.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Second&lt;/strong&gt;, they leveraged GPUs to fine tune a convolutional neural network (CNN) such as Mask R-CNN or Detectron to do object segmentation and damage detection. This CNN model is used to segment car parts and detect car dents.&lt;/p&gt; 
&lt;p&gt;Reezocar adopted a gradual approach to tackle the dent detection problem. They aimed to ensure that their system could effectively differentiate between damaged and undamaged cars. Once they were satisfied with the performance of the initial model, they submitted images with low confidence for undamaged cars and images of damaged cars to Amazon SageMaker Ground Truth to obtain around 4000 more detailed and accurate annotations, which indicate which parts are damaged. This annotated dataset was then used to train an object detection model that could accurately identify the damaged parts of a car.&lt;/p&gt; 
&lt;p&gt;Reezocar uses AWS Batch to run inference and augment their data with damage information. Reezocar triggers AWS Batch conditionally by leveraging AWS Step Functions and an Amazon SQS queue. The Step Functions workflow showcased in Reference 1 checks for the arrival of a certain number of events related to the car damage data in the SQS queue. Once the required number of events is present, AWS Step Functions triggers AWS Batch to process the data using the proposed computer vision algorithm on P4d instances. AWS Batch dynamically provisions the optimal quantity and type of compute resources based on the volume and specific resource requirements of the batch jobs submitted.&lt;/p&gt; 
&lt;p&gt;P4d instances are a specific type of Amazon EC2 instance family that is optimized for high-performance computing and includes powerful NVIDIA GPUs. These instances provide the necessary computational power for running machine learning models and computer vision algorithms efficiently, making them suitable for Reezocar’s car damage detection system.&lt;/p&gt; 
&lt;p&gt;This combination of Step Functions, SQS, and AWS Batch allows Reezocar to efficiently process the data.&lt;/p&gt; 
&lt;h2&gt;Evaluating the model is in the eye of the beholder&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Finally&lt;/strong&gt;, to evaluate the performance of their dent detection models, Reezocar used a combination of accuracy and mean Average Precision (mAP) metrics. The accuracy metric was used to assess the classifier’s performance, while the mAP metric was used to evaluate the object detection model’s performance. They set a target accuracy of 0.85 and a target mAP of 0.9 to ensure high levels of accuracy and reliability in their results. These metrics allowed Reezocar to measure the success of their object detection model and assess the quality of the system’s output. The process is illustrated in Figure 2.&lt;/p&gt; 
&lt;div id="attachment_2657" style="width: 1015px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2657" loading="lazy" class="size-full wp-image-2657" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.32.56.png" alt="Figure 2: The damage detection system developed by Reezocar combines the generation of a synthetic dataset with real-world data labeling to ensure a diverse and robust dataset. The system is designed to detect relevant images that require labeling, which contributes to the overall efficiency and effectiveness of the damage detection system." width="1005" height="716"&gt;
 &lt;p id="caption-attachment-2657" class="wp-caption-text"&gt;Figure 2: The damage detection system developed by Reezocar combines the generation of a synthetic dataset with real-world data labeling to ensure a diverse and robust dataset. The system is designed to detect relevant images that require labeling, which contributes to the overall efficiency and effectiveness of the damage detection system.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Reezocar’s objective is to determine the refurbishing cost of cars, which involves an additional step to detect the severity of the damage in parts. The severity assessment process is illustrated in Figure 3, and the severity estimation is used to determine the repair cost of the damaged part. Table 1 shows that the repair cost estimation method involves applying a percentage of the initial price of the part, which varies according to the severity of the damage. In other words, different percentages are used based on the severity of the damage to estimate the repair cost accurately.&lt;/p&gt; 
&lt;div id="attachment_2658" style="width: 852px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2658" loading="lazy" class="size-full wp-image-2658" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.33.30.png" alt="Figure 3: Damage severity assessment pipeline. The severity assessment is based on the normal consistency of the damaged part." width="842" height="604"&gt;
 &lt;p id="caption-attachment-2658" class="wp-caption-text"&gt;Figure 3: Damage severity assessment pipeline. The severity assessment is based on the normal consistency of the damaged part.&lt;/p&gt;
&lt;/div&gt; 
&lt;table width="624"&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td width="156"&gt;&lt;strong&gt;Severity&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="156"&gt;Scratched&lt;/td&gt; 
   &lt;td width="156"&gt;Damaged&lt;/td&gt; 
   &lt;td width="156"&gt;Wrecked&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td width="156"&gt;&lt;strong&gt;Repair cost&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="156"&gt;12%&lt;/td&gt; 
   &lt;td width="156"&gt;60%&lt;/td&gt; 
   &lt;td width="156"&gt;100%&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;em&gt;Table 1: Repair cost. Reezocar uses a percentage of the initial price of the damaged part to estimate the repair cost. The repair cost estimation process involves applying different percentages based on the severity of the damage.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Through the implementation of their damage detection pipeline, Reezocar has achieved a high degree of automation with a precision rate of 86.7%. However, due to challenging light conditions and reflective materials, some cars are being mislabeled as damaged. Reezocar has identified a mislabeling rate of 15.7%, and are currently labeling data in order to retrain the damage detection model.&lt;/p&gt; 
&lt;p&gt;The team was able to achieve a notable performance improvement by using Amazon Sagemaker Ground Truth to obtain 4000 labeled images, resulting in an increase in their mean Average Precision (MAP) from 0.22 to 0.3. This success has validated the effectiveness of their data-labeling pipeline and has motivated the company to pursue further data acquisition for labeling to enhance the performance of their system. As part of this effort, Reezocar is actively collecting images of damaged cars and labeling them to reach their goal of a MAP of 0.9.&lt;/p&gt; 
&lt;p&gt;In the figures below, we see some qualitative results of the proposed damage detection model. For example, in Figure 4, an image of a car with damaged parts is displayed, and the proposed model accurately detects and segments the damaged parts, as shown in the second row, first column. Additionally, they used a car-part segmentation model (See Row 1; Column 3 of Figure 4) to identify which specific part is damaged.&lt;/p&gt; 
&lt;div id="attachment_2659" style="width: 1009px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2659" loading="lazy" class="size-full wp-image-2659" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.34.17.png" alt="Figure 4: example of a wrecked car." width="999" height="458"&gt;
 &lt;p id="caption-attachment-2659" class="wp-caption-text"&gt;Figure 4: example of a wrecked car.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 5 shows an image of an undamaged car, while Figure 6 showcases the performance of the proposed method on scratched parts of a car, providing a closer look at the results.&lt;/p&gt; 
&lt;div id="attachment_2660" style="width: 1003px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2660" loading="lazy" class="size-full wp-image-2660" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.34.51.png" alt="Figure 5: Example of an undamaged car." width="993" height="392"&gt;
 &lt;p id="caption-attachment-2660" class="wp-caption-text"&gt;Figure 5: Example of an undamaged car.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2661" style="width: 998px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2661" loading="lazy" class="size-full wp-image-2661" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.35.19.png" alt="Figure 6: Example of a car with a scratched part." width="988" height="519"&gt;
 &lt;p id="caption-attachment-2661" class="wp-caption-text"&gt;Figure 6: Example of a car with a scratched part.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;The use of computer vision to detect car dents is quickly becoming a game-changer in the automotive industry. Machine learning and convolutional neural networks are allowing for a more accurate detection of car dents than ever before, leading to improved repair and maintenance processes.&lt;/p&gt; 
&lt;p&gt;Companies like Reezocar are leveraging computer vision-based damage estimation models to accurately and efficiently calculate car prices. Thanks to these models, Reezocar is now able to calculate the car price with a Mean Absolute Percentage (MAP) error of just 6%, improving the accuracy and efficiency of their pricing process.&lt;/p&gt; 
&lt;p&gt;This technology is not only helping to extend the life of cars but also to reduce landfill waste. It’s also helping to make the car buying process simpler and more transparent. As computer vision technology continues to improve, the way we detect and repair car dents will likely change as well.&lt;/p&gt; 
&lt;p&gt;By combining the power of Amazon EC2 instances, NVIDIA GPUs, AWS Batch linked by Elastic Fabric Adapter (EFA), and machine learning powered computer vision models, Reezocar can accurately detect car dents, ultimately extending the lifespan of selected cars by up to 5 years. By refurbishing these cars and reselling them in the secondary market, Reezocar helps reduce landfill waste and promote circularity.&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.41.23.png" alt="Laurent Fabre" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Laurent Fabre&lt;/h3&gt; 
  &lt;p&gt;Laurent Fabre is the CTO of Reezocar, a leading online platform for buying and selling used cars. Laurent brings over two decades of experience in the IT industry. He is a highly skilled Cybersecurity expert, having trained at Airbus, where he developed expertise in cryptography and secure programming techniques. He also studied mathematics, with a focus on operations research, which has enabled him to leverage data analytics and metrics to solve complex problems and achieve optimal solutions. Throughout his career, Laurent has taken on numerous mission-critical tasks for both civilian and military-grade projects, often on short notice. Outside of work, Laurent indulges his passion for interpretive dancing of quantum physics, which allows him to combine his love of science and art.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.41.26.png" alt="Atef Shaar" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Atef Shaar&lt;/h3&gt; 
  &lt;p&gt;Atef Shaar is currently a Lead Data at Reezocar. He is developing new machine-learning-based applications for the automobile industry. Additionally, he is managing a team of data engineers and data scientists. Prior to this work, Atef Shaar worked as a Research Engineer at Télécom Paris. He graduated with a Ph.D. degree from Télécom Paris in 2018. He was a visiting research student at the National University of Singapore in 2015. He completed a master’s study in International Business at Grenoble Graduate School of Business, after earning an engineering degree. His research work is related to machine learning and its application in multiple domains including marketing, distributed storage systems, and the automotive industry.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.46.26.png" alt="Julien Maksoud" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Julien Maksoud&lt;/h3&gt; 
  &lt;p&gt;Julien Maksoud is a highly skilled engineer with over a decade of experience in the Oil &amp;amp; Gas industry. As a consultant, he supported various companies in Europe and Africa in analyzing their data using machine learning-based methods, providing valuable insights to decision-makers. Julien’s passion for solving complex problems using machine learning led him to pursue an advanced Master’s degree from Télécom Paris, which equipped him with a deeper understanding of data science, including machine learning and deep learning frameworks. Currently, Julien is working as a Data Engineer at Reezocar, where he leverages his expertise to create data pipelines on AWS for ETL and data analysis. His experience in both data engineering and data science enables him to assist with model deployment and maintenance as well. Julien is committed to delivering results that enable organizations to make informed decisions based on data-driven insights.&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-16.41.30.png" alt="Tarek " width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Tarek Ben Charrada&lt;/h3&gt; 
  &lt;p&gt;Tarek Ben Charrada graduated with distinction, ranking 28th out of over 2800 candidates in the highly competitive “concours d’entrée aux grandes écoles” to go and earn his engineering degree from Ecole polytechnique de Tunisie. He then pursued his PhD in 3D reconstruction from a single image, demonstrating his commitment to advancing the field of computer vision and computer graphics. Today, Tarek’s expertise is sought-after as a data scientist, where he leverages his skills in computer vision, speech processing and differentiable privacy to push the boundaries of what’s possible in these exciting fields.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Automate scheduling of jobs on AWS Batch and AWS Fargate with Amazon EventBridge</title>
		<link>https://aws.amazon.com/blogs/hpc/automate-scheduling-of-jobs-on-aws-batch-and-aws-fargate-with-amazon-eventbridge/</link>
		
		<dc:creator><![CDATA[Jeff Blakely]]></dc:creator>
		<pubDate>Tue, 08 Aug 2023 15:32:15 +0000</pubDate>
				<category><![CDATA[Amazon EventBridge]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS Fargate]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<guid isPermaLink="false">4c4efcd2cd5e250317ee6cf4c26db59117f0d766</guid>

					<description>In this post we'll show how to use AWS Batch, AWS Fargate and&amp;nbsp;Amazon Event Bridge to create a job scheduling solution for containers that's fully managed, serverless, and event-driven.</description>
										<content:encoded>&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;One of the simplest ways to help streamline your automation needs is to employ a job scheduler and remove the requirement to manually execute tasks. As businesses embrace automation to do more with less, job schedulers have established themselves as a critical part of any business’s IT operations. As such, the reliability, scalability, performance, and cost effectiveness of the scheduling platform becomes a bigger consideration. It’s no surprise that we are often engaged in conversations with customers asking us how they may leverage an AWS service to replace legacy on-premise job scheduler solutions.&lt;/p&gt; 
&lt;p&gt;In this blog post we will explain how you can use &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt; and&amp;nbsp;&lt;a href="https://aws.amazon.com/eventbridge/"&gt;Amazon Event Bridge&lt;/a&gt; to create a fully managed and serverless, cloud-native, event-driven, job scheduling solution, with a specific focus on containerized jobs. We will provide a technical overview of the different components that make up this solution and cover some of the key considerations to help you determine if this solution is a good fit for your needs.&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;The following solution showcases an &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/fargate.html"&gt;AWS Batch on AWS Fargate&lt;/a&gt; environment where jobs, which run on container images provided by the end user, can be configured to run on a schedule or in response to some event. It also covers how to pass secrets into these container jobs to connect to back-end systems like databases, and how to centralize the logs from these jobs. This solution also caters to jobs which require central storage for generated artifacts like reports.&lt;/p&gt; 
&lt;p&gt;Additionally, the architecture is designed with cost optimization and operational efficiency in mind, so all services used take advantage of low cost &lt;a href="https://aws.amazon.com/serverless/"&gt;AWS serverless technologies&lt;/a&gt; or managed services. This helps remove the undifferentiated heavy lifting of managing the underlying compute resources.&lt;/p&gt; 
&lt;div id="attachment_2614" style="width: 1562px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2614" loading="lazy" class="size-full wp-image-2614" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/13/hpcblog-180-fig1v2.png" alt="Figure 1: High-Level architecture of the solution showing how schedules defined in Amazon EventBridge initiate AWS Batch jobs, which in turn can pull container images from Amazon ECR or secrets from AWS Secrets Manger, and push data and logs to Amazon EFS and Amazon CloudWatch, respectively." width="1552" height="1232"&gt;
 &lt;p id="caption-attachment-2614" class="wp-caption-text"&gt;Figure 1: High-Level architecture of the solution showing how schedules defined in Amazon EventBridge initiate AWS Batch jobs, which in turn can pull container images from Amazon ECR or secrets from AWS Secrets Manger, and push data and logs to Amazon EFS and Amazon CloudWatch, respectively.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Before we jump into the implementation, lets quickly introduce each of the services called out in the architecture and its function:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; helps you to run batch computing workloads on the AWS Cloud. A job definition is created to specify how a job is to be run, and includes details such as the job container image, and any required parameters and secrets.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt; is a serverless, pay-as-you-go compute engine that can be used with AWS Batch to easily run and scale containerized data processing workloads without the need to own, run, or manage compute infrastructure.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/secrets-manager/"&gt;AWS Secrets Manager&lt;/a&gt; is used to securely store and manage secrets which can be passed securely to the jobs in AWS Batch.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html"&gt;Amazon CloudWatch Logs&lt;/a&gt; enables you to centralize the logs from all of your systems, applications, and AWS services that you use including AWS Batch, in a single, highly scalable service.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/batch-cwe-target.html"&gt;Amazon EventBridge&lt;/a&gt; can be used to&amp;nbsp;schedule automated actions that are invoked at certain times using cron or rate expressions. By setting an AWS Batch job as a target, EventBridge can run the job on a schedule, or in response to some event.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/ecr/"&gt;Amazon Elastic Container Registry (Amazon ECR)&lt;/a&gt;is an AWS managed container image registry service where users can store their own job container images for selection in AWS Batch job definitions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/efs/"&gt;Amazon Elastic File System (EFS)&lt;/a&gt; provides serverless, fully elastic file storage so that you can share file data without provisioning or managing storage capacity and performance, and is a useful option for storing and sharing job artifacts.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Implementation&lt;/h2&gt; 
&lt;p&gt;For this solution, we are using AWS Fargate as the compute resource and AWS Batch to run various job definitions. As shown in Figure 2, we are using a public ECR image which is available from the&amp;nbsp;Amazon Public ECR Gallery. Alternatively, you can upload your own image to a private ECR repository and pass the image URL in the AWS Batch job definition.&amp;nbsp;If you use a private ECR repository, you will need to verify that the Batch execution IAM role is able to access it.&lt;/p&gt; 
&lt;div id="attachment_2610" style="width: 889px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2610" loading="lazy" class="size-full wp-image-2610" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/12/hpcblog-180-fig2.png" alt="Figure 2: AWS Batch Job Definitions example depicting one active Fargate job" width="879" height="235"&gt;
 &lt;p id="caption-attachment-2610" class="wp-caption-text"&gt;Figure 2: AWS Batch Job Definitions example depicting one active Fargate job&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We utilized Secrets Manager to store three secure variables that will be used later in this example. They represent a username, a password, and a connection string. Review the &lt;a href="https://docs.aws.amazon.com/secretsmanager/latest/userguide/create_secret.html"&gt;Create an AWS Secrets Manager secret documentation&lt;/a&gt; for more information on this process.&lt;/p&gt; 
&lt;p&gt;Shell scripts can be used as the entry point to the container to perform any desired task, such as start an ETL process, run SQL scripts against an AWS RDS database, or pull and analyze data from S3. As seen in Figure 3 below, we pass multiple variables to the shell script.&lt;/p&gt; 
&lt;div id="attachment_2609" style="width: 889px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2609" loading="lazy" class="size-full wp-image-2609" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/12/hpcblog-180-fig3.png" alt="Figure 3:&amp;nbsp; AWS Batch Job Definitions – Container properties details depicting bash commands" width="879" height="368"&gt;
 &lt;p id="caption-attachment-2609" class="wp-caption-text"&gt;Figure 3: AWS Batch Job Definitions – Container properties details depicting bash commands&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As part of the AWS Batch job definition, we define two environment variables, &lt;code&gt;$Var1&lt;/code&gt; and &lt;code&gt;$Var2&lt;/code&gt;, along with AWS Secrets &lt;code&gt;username&lt;/code&gt;, &lt;code&gt;password&lt;/code&gt; and &lt;code&gt;connectionstring&lt;/code&gt;, as shown in Figure 4 below. For demonstration purpose, we are simply writing these parameters to an EFS file system that is mounted to the container. You can leverage these features to customize how you pass parameters to run application or database jobs through AWS Batch. You can create multiple job definitions and can pass variables and secrets based on specific job or application requirements.&lt;/p&gt; 
&lt;div id="attachment_2608" style="width: 889px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2608" loading="lazy" class="size-full wp-image-2608" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/12/hpcblog-180-fig4.png" alt="Figure 4:&amp;nbsp; AWS Batch Job Definitions – Linux operating systems configuration depicting environment variables &amp;amp; EFS mount points" width="879" height="758"&gt;
 &lt;p id="caption-attachment-2608" class="wp-caption-text"&gt;Figure 4: AWS Batch Job Definitions – Linux operating systems configuration depicting environment variables &amp;amp; EFS mount points&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To run these Batch job definitions on a schedule, we are relying on Amazon EventBridge&amp;nbsp;rules. With Amazon EventBridge rules, we can create complex schedules to run these jobs,&amp;nbsp;such as every first Tuesday of the month. Figure 5 below shows how we are running a job every 10 minutes, and which Batch job queue and job definition to use.&lt;/p&gt; 
&lt;div id="attachment_2607" style="width: 889px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2607" loading="lazy" class="size-full wp-image-2607" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/12/hpcblog-180-fig5.png" alt="Figure 5: Amazon EventBridge Schedule screen highlighting the EventBridge rules with an example target" width="879" height="602"&gt;
 &lt;p id="caption-attachment-2607" class="wp-caption-text"&gt;Figure 5: Amazon EventBridge Schedule screen highlighting the EventBridge rules with an example target&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;You can customize the EventBridge rule based on your specific needs. You can run these events using a cron expressions as mentioned in Figure 6, or you can specify regular intervals. This gives you the flexibility to schedule daily, weekly, or monthly jobs to run and reduce maintenance cost.&lt;/p&gt; 
&lt;div id="attachment_2606" style="width: 987px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2606" loading="lazy" class="size-full wp-image-2606" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/12/hpcblog-180-fig6.png" alt="Figure 6: EventBridge Rule - Scheduling pattern showing a fine-grained schedule that runs the rule every day at 12:00pm UTC+0" width="977" height="593"&gt;
 &lt;p id="caption-attachment-2606" class="wp-caption-text"&gt;Figure 6: EventBridge Rule – Scheduling pattern showing a fine-grained schedule that runs the rule every day at 12:00pm UTC+0&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The AWS Batch management console has a built in &lt;a href="https://console.aws.amazon.com/batch/home?#dashboard"&gt;dashboard&lt;/a&gt; where we can see how many jobs are ready to run and the number of successful and failed jobs. As we have shown in Figure 7, you can observe the status of previously executed jobs and take necessary action based on requirements. AWS Batch retains job information for a minimum of 7 days.&lt;/p&gt; 
&lt;div id="attachment_2605" style="width: 889px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2605" loading="lazy" class="size-full wp-image-2605" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/12/hpcblog-180-fig7.png" alt="Figure 7: AWS Batch Success/Running Dashboard showing multiple successful job executions" width="879" height="395"&gt;
 &lt;p id="caption-attachment-2605" class="wp-caption-text"&gt;Figure 7: AWS Batch Success/Running Dashboard showing multiple successful job executions&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;If there are any failed jobs, you can use CloudWatch Logs to troubleshoot further. In our example, the job definition is using the &lt;strong&gt;&lt;em&gt;awslogs&lt;/em&gt;&lt;/strong&gt; Log driver to send logs to a CloudWatch log group called &lt;code&gt;batch-loggroup&lt;/code&gt;, as shown in Figure 8.&lt;/p&gt; 
&lt;div id="attachment_2604" style="width: 889px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2604" loading="lazy" class="size-full wp-image-2604" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/12/hpcblog-180-fig8.png" alt="Figure 8: AWS Batch Job&amp;nbsp;Definition&amp;nbsp;- Logging configuration detail panel with awslogs driver options" width="879" height="264"&gt;
 &lt;p id="caption-attachment-2604" class="wp-caption-text"&gt;Figure 8: AWS Batch Job&amp;nbsp;Definition&amp;nbsp;– Logging configuration detail panel with awslogs driver options&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 9 shows the log files with date and time information of respective batch jobs. You can get a detailed analysis of those jobs and troubleshoot as necessary.&lt;/p&gt; 
&lt;div id="attachment_2603" style="width: 889px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2603" loading="lazy" class="size-full wp-image-2603" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/12/hpcblog-180-fig9.png" alt="Figure 9: CloudWatch Batch Log Group log stream details" width="879" height="512"&gt;
 &lt;p id="caption-attachment-2603" class="wp-caption-text"&gt;Figure 9: CloudWatch Batch Log Group log stream details&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Considerations&lt;/h2&gt; 
&lt;p&gt;AWS Batch runs jobs at scale and optimizes for throughput and cost, while giving you powerful queueing and scaling capabilities. However, not every workload is best run on Batch on Fargate. Here are some important things to consider when configuring AWS Batch jobs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Very short jobs. &lt;/strong&gt;If your jobs run for only a handful of seconds, the overhead to schedule your jobs may be more than the runtime of the jobs themselves, resulting in less-than-desired utilization. In this case, &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; might be a better option. Or, staying with Batch, a workaround is to binpack your tasks together before submitting them in Batch, then having your Batch jobs iterate over your tasks. A general guidance is to binpack your jobs is to: 1) stage the individual tasks arguments into an Amazon DynamoDB table or as a file in an Amazon S3 bucket, ideally group the tasks in order to get your AWS Batch jobs to last of 3-5 minutes each 2) loop through your task groups within your AWS Batch job.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Jobs required to run immediately.&lt;/strong&gt; While Batch can process jobs rapidly, it optimizes for cost, priority, and throughput rather than instantaneous job execution. This implies that some queue time is desirable to allow Batch to make the right choice for compute. If you need a response time ranging from milliseconds to seconds between job submissions, you may want to use a service-based approach using Amazon Elastic Container Service (&lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon ECS&lt;/a&gt;) or Amazon Elastic Kubernetes Service (&lt;a href="https://aws.amazon.com/eks/"&gt;Amazon EKS&lt;/a&gt;) instead of using a delayed-start, batch-processing architecture.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Batch Compute – Fargate vs Amazon Elastic Compute Cloud (Amazon EC2).&amp;nbsp;&lt;/strong&gt;AWS Fargate requires less initial setup and configuration than Amazon EC2 and you don’t need to manage servers, handle capacity planning, or isolate container workloads for security. So, if your jobs must start quickly, specifically less than 30 seconds, or the requirements of your jobs are 16 vCPUs or less, no GPUs, and 120 GiB of memory or less, then Fargate is the recommended choice. However, if you require specific control over instance types, a high level of throughput or concurrency, custom Amazon Machine Images (AMI’s), or have requirements for resources Fargate does not provide, such as GPU or more memory for example, we recommend &lt;a href="https://aws.amazon.com/pm/ec2"&gt;Amazon EC2&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Spot Instance usage. &lt;/strong&gt;Most AWS Batch customers use Amazon EC2 Spot Instances because of the savings over EC2 On-Demand instances. However, if your workload runs for multiple hours and can’t be interrupted, On-Demand instances might be more suitable for you. You can always try Spot Instances first and switch to On-Demand if necessary.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more information, please review the &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/best-practices.html"&gt;Best Practices for AWS Batch&lt;/a&gt; documentations.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;AWS Batch on AWS Fargate is an excellent option when looking for a low cost, scalable solution for running batch jobs, with low operational overhead. In this blog we hoped to cover some of the common use cases when configuring batch jobs, as well as considerations when identifying when Amazon Batch may or may not be a good fit. To get started, out the&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/Batch_GetStarted.html"&gt;Getting Started with AWS Batch documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;footer&gt; 
 &lt;div class="blog-author-box"&gt; 
  &lt;div class="blog-author-image"&gt;
   &lt;img src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/13/varsh-raju-profile.png" alt="Varsha Raju" width="125"&gt;
  &lt;/div&gt; 
  &lt;h3 class="lb-h4"&gt;Varsha Raju&lt;/h3&gt; 
  &lt;p&gt;Varsha Raju is a Container Specialist Consultant with the Professional Services team at Amazon Web Services.&lt;/p&gt; 
 &lt;/div&gt; 
&lt;/footer&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Improving NFL player health using machine learning with AWS Batch</title>
		<link>https://aws.amazon.com/blogs/hpc/improving-nfl-player-health-using-machine-learning-with-aws-batch/</link>
		
		<dc:creator><![CDATA[Chris Boomhower]]></dc:creator>
		<pubDate>Thu, 03 Aug 2023 15:48:49 +0000</pubDate>
				<category><![CDATA[*Post Types]]></category>
		<category><![CDATA[Amazon Machine Learning]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[Customer Solutions]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<guid isPermaLink="false">495891f2c2f9205adede6e53337c85fcffc62687</guid>

					<description>In this post we’ll show you how the NFL used AWS to scale their ML workloads and produce the first comprehensive dataset of helmet impacts across multiple NFL seasons. They were able to reduce manual labor by 90% and the results beats human labelers in accuracy by 12%!</description>
										<content:encoded>&lt;h2&gt;&lt;img loading="lazy" class="alignright size-full wp-image-2726" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/26/boofla88_an_american_football_players_helmet_and_a_football_sit_46243c39-bad2-4945-845f-82b4efa4487e.png" alt="Improving NFL player health using machine learning with AWS Batch" width="380" height="212"&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;The National Football League (NFL) is the most popular sports league in America and home to&amp;nbsp;&lt;a href="https://web.archive.org/web/20200416082746/http:/www.nfl.com/news/story/0ap3000001106247/article/nfl-players-approve-cba-impact-on-league-in-2020-and-beyond"&gt;1,500+ professional athletes&lt;/a&gt;. The NFL is committed to better understanding the frequency and severity of player injuries to reduce their occurrence and make the game of football safer. This has led them to establish the NFL Player Health and Safety (PH&amp;amp;S) initiative for which more details are available on&amp;nbsp;&lt;a href="https://www.nfl.com/playerhealthandsafety/"&gt;NFL’s website&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;A core focus of the NFL PH&amp;amp;S initiative is to reduce helmet impacts among its athletes and to mitigate their effects when they do occur. To accomplish this, they first needed better insight to how often helmet collisions occur at the individual player level over the course of several games, spanning multiple seasons. These insights would empower stakeholders to make meaningful, strategic decisions that prioritize the sport’s safety.&lt;/p&gt; 
&lt;p&gt;In this post we’ll show you how the NFL, in partnership with AWS Professional Services, leveraged the scalable compute of AWS to run their ML workloads at scale to produce the first comprehensive dataset of helmet impacts across multiple NFL seasons.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;Historically, human annotators were tasked with sampling a very small subset of plays to label individual impacts, frame-by-frame, to provide a minimal level of insight to leaders. However, manually annotating videos in this capacity highlights some serious limitations. For one, cost is an issue since human annotators are expensive to employ but also slow to generate accurate labels. It takes even the most experienced, accurate annotators about an hour to produce quality labels for a single play.&lt;/p&gt; 
&lt;p&gt;This brings us to the second issue: accuracy. Human annotators demonstrate high variability in labeled outcomes between one another (and even with themselves when relabeling the same play).&lt;/p&gt; 
&lt;p&gt;Finally, it’s difficult and expensive to generate high quality helmet impact labels at scale since it is only feasible to sample and annotate a small subset of plays using human annotators. For example, it would take over 270 annotators six months (each working 24 hours per day, 7 days per week) to label just a single season. Given these limitations, the NFL needed an alternative solution to identify helmet impacts.&lt;/p&gt; 
&lt;p&gt;To gain the needed insights without the limitations accompanying manual annotation, the NFL decided to use machine learning (ML) at scale in the AWS cloud. This was to be the first, fully comprehensive, historical dataset of its kind and would require immense resources to achieve. Running their workload using AWS Batch and other services allowed the NFL to minimize cost, optimize for reliability, and maximize throughput. The resulting catalog of helmet impacts allows NFL leaders, league owners, and coaches to make informed decisions to improve game safety through better equipment standards, rule updates, and enhanced coaching strategies.&lt;/p&gt; 
&lt;h2&gt;Task overview&lt;/h2&gt; 
&lt;p&gt;The NFL’s machine learning workload considers videos of NFL plays, such as the one displayed in Video 1, and NFL’s&amp;nbsp;&lt;a href="https://nextgenstats.nfl.com/glossary"&gt;Next Gen Stats&lt;/a&gt; player-tracking data to identify and assign helmet impacts. Multiple steps are necessary to combine and leverage these video and player-tracking data. These steps may be summarized into three primary tasks as follows.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Endzone trimmed" width="500" height="281" src="https://www.youtube-nocookie.com/embed/pABlWIfIKLg?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p style="text-align: left"&gt;&lt;em&gt;Video 1 – This is a raw input video example. Videos such as this serve as input to the workflow that identifies and assigns helmet impacts to players.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Snap detection –&lt;/strong&gt;&amp;nbsp;We determine when in the video the ball is &lt;a href="https://en.wikipedia.org/wiki/Snap_(gridiron_football)"&gt;snapped&lt;/a&gt; so that videos can be aligned to each other and to the NFL Next Gen Stats data. This step uses an image segmentation ML model to identify players on the field and stabilize the image with respect to the camera motion. Then, the result is fed to a change point detection model to determine the frame when the players start moving, corresponding to the ball being snapped. This step requires a GPU instance in order to do the image segmentation efficiently.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Helmet detection and player assignment&amp;nbsp;&lt;/strong&gt;– For this step, a fine-tuned helmet detector, using a GPU, identifies helmets at every frame of the video and tracks them through the play. To assign them to specific players, we next match the detected player tracks to the known player positions observed by the NFL Next Gen Stats player-tracking system. The result is the set of x-y coordinates for every player’s helmet throughout the play.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Impact classification&amp;nbsp;&lt;/strong&gt;– We use a deep learning-based action recognition model to identify when helmets are undergoing an impact. The input to this step is a set of cropped frames centered around a particular helmet detected in the previous step.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Once a game play’s video and respective NFL Next Gen Stats data have been processed through all three steps, impacts and player assignments are available to help inform NFL PH&amp;amp;S decision makers. To demonstrate these output results, we’ve overlayed those produced for the input video (Video 1) in Video 2.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Endzone trimmed impact" width="500" height="281" src="https://www.youtube-nocookie.com/embed/7Z2t2aVv1Vk?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p style="text-align: left"&gt;&lt;em&gt;Video 2 – This is an example of the video shared in Video 1 with helmet bounding box, player assignment, and classified helmet impact results overlayed onto each frame. Notice the solution only produces results for frames after ball snap occurs. The video has been slowed after ball snap, and more so surrounding each impact, for the reader’s benefit and interpretation.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Scaling with AWS Batch&lt;/h2&gt; 
&lt;p&gt;Each of these tasks requires a different combination of memory and compute resources, so we matched each one with the Amazon EC2 instance family best suited for it. Since each job utilizes a GPU for deep learning tasks, we utilize instances in the P3 and G4dn families. While P3 instances are optimized for model training, their NVIDIA Tesla V100 GPUs and higher ratio of CPU per GPU make them an appealing choice for debugging and quick tasks where speed is the most important factor. On the other hand, the G4dn family of instances has NVIDIA T4 Tensor GPUs and a lower cost per task.&lt;/p&gt; 
&lt;p&gt;Since we are mostly a team of data scientists and machine learning engineers, we wanted a managed way to implement AWS best practices including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Running workloads across multiple availability zones&lt;/li&gt; 
 &lt;li&gt;Allowing as wide a variety of instance types as possible for each task&lt;/li&gt; 
 &lt;li&gt;Maintaining a strong security posture by running the entire workload in a VPC&lt;/li&gt; 
 &lt;li&gt;Scaling down the entire cluster to zero vCPU when it’s not in use to save on cost&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AWS Batch supported these requirements with just a few lines of code (AWS CDK in our case), allowing us to spend our time focusing on improving the models’ accuracy rather than debugging networking or a multi-Availability Zone setup.&lt;/p&gt; 
&lt;p&gt;We configured our AWS Batch environment with two compute environments to match the G4dn and P3 instance families totalling 9,248 vCPU across three Availability Zones in a single AWS Region. This setup gives us the flexibility to change the priority of the G4dn and P3 compute environments while maintaining access to the widest array of instance types in case of limited AWS capacity. For example, when a new model is ready and we want to repopulate data for entire NFL seasons, we prioritize G4dn instances to minimize total cost. But, if we are experimenting on a few plays, we prioritize P3 instances as shown in Figure 1 to have the shortest possible iteration time.&lt;/p&gt; 
&lt;p&gt;In the future, we plan to experiment with G5 and P4 instance families which can have up to 3.3x higher performance for ML workloads than G4 instances and 2.5x higher performance for ML workloads than P3 instances respectively.&lt;/p&gt; 
&lt;div id="attachment_2708" style="width: 952px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2708" loading="lazy" class="size-full wp-image-2708" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/26/CleanShot-2023-07-26-at-13.34.56.png" alt="Figure 1 – This is a figure demonstrating an AWS Batch configuration used for this solution. It shows how one Batch compute environment was setup for P3 instance types with a max vCPU count of 544 and a second Batch compute environment for G4dn instance types with a max vCPU count of 8,704. We were able to prioritize one compute environment over the other based on our immediate needs at any given time." width="942" height="336"&gt;
 &lt;p id="caption-attachment-2708" class="wp-caption-text"&gt;Figure 1 – This is a figure demonstrating an AWS Batch configuration used for this solution. It shows how one Batch compute environment was setup for P3 instance types with a max vCPU count of 544 and a second Batch compute environment for G4dn instance types with a max vCPU count of 8,704. We were able to prioritize one compute environment over the other based on our immediate needs at any given time.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Serverless architecture design patterns&lt;/h2&gt; 
&lt;div id="attachment_2709" style="width: 929px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2709" loading="lazy" class="size-full wp-image-2709" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/26/CleanShot-2023-07-26-at-13.35.33.png" alt="Figure 2 – This figure shows the inference AWS Step Functions state machine on the right (Block C) and the standardized, nested Step Functions state machine on the left (Block B). It serves as a reference visual throughout this section and is described in greater detail in the subsequent paragraphs." width="919" height="631"&gt;
 &lt;p id="caption-attachment-2709" class="wp-caption-text"&gt;Figure 2 – This figure shows the inference AWS Step Functions state machine on the right (Block C) and the standardized, nested Step Functions state machine on the left (Block B). It serves as a reference visual throughout this section and is described in greater detail in the subsequent paragraphs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As we mentioned in the Task Overview section, it was important to split up the steps into three different tasks. But the challenge to coordinate these steps while maintaining our ability to scale, handling the occasional failure, and providing visibility into each underlying step was glaring. That’s where AWS Step Functions comes in.&lt;/p&gt; 
&lt;p&gt;Since an AWS Batch job can be submitted directly from a Step Functions state machine, we were able to start simple and adapt as our needs changed. Today, we utilize a design, shown in Figure 2, consisting of an inference Step Functions state machine (Block C) that organizes the results from related videos (i.e., those belonging to plays from the same NFL game) using a Step Functions&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-map-state.html"&gt;Map&lt;/a&gt;&amp;nbsp;state where each map iteration represents a different video. From within each map iteration, we invoke a standardized, nested state machine (Block B) which is responsible for submitting jobs to AWS Batch while standardizing logging, error handling, retry, and caching logic.&lt;/p&gt; 
&lt;p&gt;This design approach simplifies maintenance, improves scalability, and promotes modularity. It ensures that our solution easily scales to meet the demands of the project. Since an inference Step Functions state machine (Block C) execution could contain anywhere between 1 to 150+ plays (this count is determined at execution launch), we use the Step Functions Map state to define our inference steps only once and then dynamically scale operations as the input data necessitates. Furthermore, we designed the nested state machine (Block B) to standardize supporting engineering and operations logic before and after Batch job submission while yielding to specific task needs. Specifically, AWS Lambda and AWS Batch job definition resource names (ARNs) are passed to the nested Step Functions state machine each time it is executed from an inference step associated with one of our primary tasks. This is how we ensure our solution is highly scalable while maximizing reusability and utilization by any type of task, even those belonging to non-inference pipelines.&lt;/p&gt; 
&lt;p&gt;While the Map state does allow the inference Step Functions state machine (Block C) to dynamically scale parallel workloads, we made additional efforts to prioritize parallelism even further to make the very best use of AWS Batch.&lt;/p&gt; 
&lt;p&gt;Given Step Functions’ Map state limitation of no more than 40 concurrent Map iterations at any given time, we decided to define our inference pipeline operation in a way that would mitigate the effects of this limitation while maintaining a simple operations tracking strategy.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Before we go on (a quick aside)&lt;/strong&gt;: &lt;em&gt;The Step Functions Map state limitation is associated with Map’s Inline mode at the time we performed this work. Our design was created before the Step Functions team announced &lt;a href="https://aws.amazon.com/blogs/aws/step-functions-distributed-map-a-serverless-solution-for-large-scale-parallel-data-processing/"&gt;Step Functions Distributed Map&lt;/a&gt;, which supports a maximum concurrency of 10,000 Map iterations at any given time. We’d encourage readers with similar use cases to ours to investigate this option for much better parallelism.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;We chose to run an inference Step Functions state machine execution on subsets of all play videos belonging to a game. So, if a game has 300 unique videos where, for example, half of the videos were recorded from one viewing angle and the other half from another, we’d run two separate executions of the inference state machine, each processing 150 videos. Prioritizing parallelism in our operational design enabled the NFL to maximize their AWS Batch Compute Environment resource utilization, whether processing an entire season’s worth of data or only a couple games at once.&lt;/p&gt; 
&lt;p&gt;As we alluded to already, it’s important to have visibility into the status of our executions when we’re processing so much data with AWS Batch. Doing so helps us understand the conditions of our executions, improve our debugging processes, and ensure that our solution maintains an accepted level of “explainability.” To address this need, we created engineering hooks, using AWS Lambda, Amazon DynamoDB, and Amazon CloudWatch, throughout the inference (Block C) and nested (Block B) Step Functions state machines to track custom events specific to our use case. These mechanisms laid the foundation for a separate observability pipeline which provides structure to data processing status, raises errors when they occur, and provides long-term persistence of all metadata needed for thorough review, audit, or debug.&lt;/p&gt; 
&lt;p&gt;Retaining and consolidating all metadata associated with our executions is especially important when using AWS Batch since Batch job descriptions are deleted from the Batch API 7 days after their job’s completion. By preserving this information in searchable form, we can easily refer back to any previous execution for debugging purposes or to gain insights from past runs. Also worth noting, since application of our pipeline may be characterized as spiky with somewhat unpredictable periods of virtually no utilization broken up by bursts of extremely high utilization, we chose to use&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/capacity.html"&gt;DynamoDB on-demand capacity&lt;/a&gt;&amp;nbsp;to maximize reliability and reduce tracking costs.&lt;/p&gt; 
&lt;p&gt;Finally, we designed our orchestration workflows to reduce cost, wasted compute, and time lost due to redundant re-processing and to utilize Batch resources efficiently. By skipping snap detection, helmet detection &amp;amp; player assignment, and impact classification Batch job submission in cases where previous Batch job executions already produced output data using the same input data, we’re able to save more than $1,000 per hour in on-demand EC2 costs. We accomplished this by profiling task input data, metadata, code versions, and model versions as part of our nested Step Functions state machine (Block B) auxiliary logic. If the task profile matches one accompanying a previous, successful execution, the Batch job is skipped and the pre-existing output data is passed as output from the current nested Step Functions state machine execution. Alternatively, if no successful matches are identified due to a change in task metadata, code, the model, or upstream tasks, the state machine proceeds to submit the Batch job to produce new output results.&lt;/p&gt; 
&lt;h2&gt;Inference and beyond&lt;/h2&gt; 
&lt;div id="attachment_2710" style="width: 918px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2710" loading="lazy" class="size-full wp-image-2710" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/26/CleanShot-2023-07-26-at-13.36.15.png" alt="Figure 3 – This is a simplified representation of the modular relationships between various Step Functions state machines in the solution. It highlights how both the standardized, nested state machine, or task state machine, (Block B) and the inference state machine (Block C) are treated as modules that may be leveraged by the training and evaluation state machines (Blocks A and D). Designing for such use further amplifies the design’s ability to scale AWS Batch utilization while improving maintainability." width="908" height="638"&gt;
 &lt;p id="caption-attachment-2710" class="wp-caption-text"&gt;Figure 3 – This is a simplified representation of the modular relationships between various Step Functions state machines in the solution. It highlights how both the standardized, nested state machine, or task state machine, (Block B) and the inference state machine (Block C) are treated as modules that may be leveraged by the training and evaluation state machines (Blocks A and D). Designing for such use further amplifies the design’s ability to scale AWS Batch utilization while improving maintainability.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Using AWS Batch in conjunction with nested Step Functions state machines to automate the step-by-step execution of tasks for various pipelines on the project further promotes reusability of individual components, and in turn the design’s scalability beyond inference. This high modularity in design and the dependencies between different pipelines leveraged on the project are showcased in the figure above.&lt;/p&gt; 
&lt;p&gt;Referring to Block D of the figure, whenever the Evaluation Step Functions state machine is triggered to generate evaluation metrics for a given set of task models, it invokes the Inference Step Functions state machine (Block C) automatically to generate the assigned helmet impact predictions for a specified subset of videos. These results produced by the Inference pipeline are then consolidated and evaluated against a set of model evaluation metrics by the Evaluation pipeline.&lt;/p&gt; 
&lt;p&gt;Similarly, our design enables the Training Step Functions state machine (Block A) to leverage the standardized, nested Step Functions state machine (Block B) for triggering feature generation, hyper-parameter tuning, and training AWS Batch jobs. This way, training pipeline tasks may also take advantage of the same tracking, error handling, retry, and caching logic discussed already.&lt;/p&gt; 
&lt;p&gt;Through these design choices and implementation strategies, we produced a massively-scalable orchestration framework around AWS Batch. NFL PH&amp;amp;S support teams benefited from a highly robust set of automated workflows that leverage serverless resources on AWS, and stakeholders quickly received a comprehensive dataset that would guide them to make powerful, informed decisions to improve player health and safety.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Through AWS Batch, the NFL fixed one of their biggest pain points with identifying helmet impacts in their video data. Compared to hiring human annotators, our solution on AWS Batch achieves 1) $700,000/season in cost savings, 2) 90% reduction in hours of manual labor, and 3) beats human labelers in accuracy by 12%. In addition, the use of AWS Batch allows our solution to scale in massively parallel ways, which allowed us to compress more than 24 years’ worth of computation time into less than 6 weeks. With this solution, the NFL was able to create the first comprehensive dataset of helmet impacts for all players dating back to 2016. This helps inform and guide NFL league owners and coaches in their decision-making process to make the game safer for all players.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Diving Deeper into Fair-Share Scheduling in AWS Batch</title>
		<link>https://aws.amazon.com/blogs/hpc/deep-dive-on-fair-share-scheduling-in-aws-batch/</link>
		
		<dc:creator><![CDATA[Angel Pizarro]]></dc:creator>
		<pubDate>Tue, 01 Aug 2023 15:42:17 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Best Practices]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[Scheduling]]></category>
		<guid isPermaLink="false">ff9ba54be1e67f8c9fd3a5548f335dd2e9cf09ec</guid>

					<description>Today we dive into details of AWS Batch fair share policies and show how they affect job placement. You'll see the result of different share policies, and hear about practical use cases where you can benefit from fair share job queues in Batch.</description>
										<content:encoded>&lt;p&gt;In our previous &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-fair-share-scheduling-for-aws-batch/"&gt;blog post&lt;/a&gt;, we introduced fair share scheduling policies for AWS Batch job queues. We thought it would be worth digging deeper into what the scheduling policy parameters mean in detail and in practice, showing how policies influence the placement of jobs on compute resources with a few examples. Specifically, we will cover the effects of &lt;code&gt;weightFactor&lt;/code&gt;, &lt;code&gt;shareDecayInSeconds&lt;/code&gt;, and &lt;code&gt;computeReservation&lt;/code&gt; parameters. We’ll finish off the post with some use cases where fair share queues may be applicable in your AWS Batch environments. Let’s get started!&lt;/p&gt; 
&lt;h2&gt;Understanding the allocation of available resources for jobs in the queue&lt;/h2&gt; 
&lt;p&gt;As we mentioned in that initial post, a scheduling policy’s purpose is to allow the Batch job scheduler to meter out equitable access to a set of shared compute resources for different workloads. The different workloads are identified by supplying a &lt;code&gt;shareIdentifier&lt;/code&gt; within the job queue’s &lt;code&gt;schedulingPolicy&lt;/code&gt;, and then submitting jobs with that specific share identifier.&lt;/p&gt; 
&lt;p&gt;To allocate resources across a set of active share identifiers, the scheduler in AWS Batch calculates the aggregate usage of all actively running and recently completed jobs by summing the total vCPU seconds across all eligible jobs. Shares with a higher value for aggregate usage are the less likely than shares with a lower value to be assigned available resources. Over time, this function results in your compute resources being fairly distributed across shares according to your job queue’s fair share policy.&lt;/p&gt; 
&lt;p&gt;A share’s aggregate usage is calculated within a limited time window, known as the &lt;strong&gt;share decay&lt;/strong&gt;. Specifically, any job &lt;code&gt;STARTING&lt;/code&gt; or &lt;code&gt;RUNNING&lt;/code&gt; is considered active and counts toward the share’s aggregate usage metric. Jobs that completed (&lt;code&gt;SUCCESSFUL&lt;/code&gt; or &lt;code&gt;FAILED&lt;/code&gt; state) within the share decay time window are also considered in the aggregate usage metric.&lt;/p&gt; 
&lt;p&gt;The net effect is that more recent jobs will have a higher value for aggregate usage compared to jobs with share identifiers that ended at an earlier time. This is worth repeating:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A &lt;em&gt;higher&lt;/em&gt; aggregate usage for a share means that a &lt;em&gt;lower&lt;/em&gt; percentage of resources will be assigned to that share, as resources become available.&lt;/li&gt; 
 &lt;li&gt;Since fewer resources are allocated, the aggregate usage of that share will decrease.&lt;/li&gt; 
 &lt;li&gt;As the aggregate usage becomes lower than other shares, it will preferentially be assigned resources again.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Over time this job scheduling and placement behavior converges to your scheduling policy for the active shares identifiers.&lt;/p&gt; 
&lt;h3&gt;Verifying Share Decay Behavior&lt;/h3&gt; 
&lt;p&gt;To better understand how the scheduling behaves, let’s take the following example fair share policy that designates two share IDs that should get an equal amount of resources over time:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-json"&gt;{
  "fairsharePolicy": {
    "shareDistribution": [
      {
        "shareIdentifier": "yellow"
      },
      {
        "shareIdentifier": "blue"
      }
]}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;We also defined the following Batch resources:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A compute environment with a maximum of 64 vCPUs, consisting of m5.2xlarge instances.&lt;/li&gt; 
 &lt;li&gt;A fair share job queue with this policy we just defined.&lt;/li&gt; 
 &lt;li&gt;A job definition that sleeps randomly between 2 to 5 minutes.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With these pieces in place, we submitted an array job of 500 using the &lt;strong&gt;yellow &lt;/strong&gt;share identifier, waited three minutes, then submitted another array job using the &lt;strong&gt;blue&lt;/strong&gt; share identifier. Since we submitted &lt;strong&gt;yellow&lt;/strong&gt; jobs first, we expected all available job slots would be occupied by &lt;strong&gt;yellow&lt;/strong&gt; jobs. When resources become available, &lt;strong&gt;blue&lt;/strong&gt; jobs should preferentially get assigned resources until the &lt;strong&gt;blue&lt;/strong&gt; share’s aggregate usage becomes greater than &lt;strong&gt;yellow&lt;/strong&gt;, and the scheduler starts to prefer &lt;strong&gt;yellow&lt;/strong&gt; jobs. This behavior is shown in Figure 3.&lt;/p&gt; 
&lt;div id="attachment_2681" style="width: 987px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2681" loading="lazy" class="size-full wp-image-2681" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/25/HPCBlog-174-fig1.png" alt="Figure 1: A line chart showing the number of yellow and blue pending and running jobs over time. The data shows that AWS Batch will use historical data to preferentially place jobs from a share that does not have recent usage, balancing over time to equal amount of compute resources over time." width="977" height="706"&gt;
 &lt;p id="caption-attachment-2681" class="wp-caption-text"&gt;Figure 1: A line chart showing the number of yellow and blue pending and running jobs over time. The data shows that AWS Batch will use historical data to preferentially place jobs from a share that does not have recent usage, balancing over time to equal amount of compute resources over time.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The key to interpreting these graphs is to note that the dashed lines indicate pending jobs, while the solid lines (toward the bottom) show actual running jobs jostling for a share of the compute resources. As each chart proceeds to the right along the time axis, you can see how various policies impact the rate at which pending jobs from each &lt;code&gt;shareIdentifier&lt;/code&gt; get depleted sooner, or later.&lt;/p&gt; 
&lt;h3&gt;Extending the share decay time window&lt;/h3&gt; 
&lt;p&gt;The default minimum time window for share decay is ten minutes (600 seconds), but you can define a larger time window in the fair share policy using the &lt;code&gt;shareDecaySeconds&lt;/code&gt; parameter. This lets you to tweak how the scheduler calculates a share’s aggregate usage by considering jobs that finished longer than ten minutes in the past. This would be something you should explore if you have jobs that significantly differ in job counts or run times between share identifiers and are finding that the allocation of resources is not meeting your needs. Otherwise, stick with the default.&lt;/p&gt; 
&lt;h3&gt;Adjusting a share’s aggregate usage of compute with &lt;code&gt;weightFactor&lt;/code&gt;&lt;/h3&gt; 
&lt;p&gt;So far, we’ve seen how AWS Batch uses the recent history of jobs to calculate their aggregate vCPU usage, and then use that value to allocate resources over time to different shares. In our example, we assumed that you wanted each share to be equal in how it gets assigned resources.&lt;/p&gt; 
&lt;p&gt;Sometimes, however, you want to give preference to a particular share ID over others. For example, say your share identifiers are aligned to organizational departments, in this case human resources (“HR”) and production machine learning services (“MLOps”). You may want to designate that jobs from “MLOps” get a higher percentage of resources than “HR”. The way to do this is to modify the calculated aggregate usage of a share to increase or decrease it using the &lt;code&gt;weightFactor&lt;/code&gt; parameter.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;weightFactor&lt;/code&gt; is a modifier on the computed aggregate usage metric of a given share. The default is &lt;strong&gt;1.0&lt;/strong&gt; – and means &lt;em&gt;don’t modify the calculated aggregate usage of the share&lt;/em&gt;. A value less than 1.0 for &lt;code&gt;weightFactor&lt;/code&gt; will result in a lower value for aggregate usage, and hence that share will be allocated a higher percentage of available resources than it would normally get. Higher values for weight factor will result in the opposite — a higher value for aggregate usage for the share and lower percentage of resource allocated than Batch would normally assign.&lt;/p&gt; 
&lt;p&gt;To see how this works, let’s adapt our previous example fair share policy, adjusting the weight factor of the &lt;strong&gt;blue&lt;/strong&gt; share identifier to &lt;strong&gt;0.5&lt;/strong&gt;. The policy also defines &lt;strong&gt;yellow &lt;/strong&gt;share’s as the default weight factor is &lt;strong&gt;1.0&lt;/strong&gt;.&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-json"&gt;{
  "fairsharePolicy": {
    "shareDistribution": [
      {
        "shareIdentifier": "yellow",
        &lt;strong&gt;"weightFactor": 1.0&lt;/strong&gt;
      },
      {
        "shareIdentifier": "blue",
        &lt;strong&gt;"weightFactor": 0.5&lt;/strong&gt;
      }
]}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;What we expect from this policy is that jobs with the &lt;strong&gt;blue&lt;/strong&gt; share identifier should get twice as many compute resources than jobs with the &lt;strong&gt;yellow&lt;/strong&gt; share identifier. Figure 2 shows our expected result: Batch gives more resources to the blue jobs, and as a result these jobs complete faster despite having been submitted later in time compared to the yellow jobs.&lt;/p&gt; 
&lt;div id="attachment_2686" style="width: 987px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2686" loading="lazy" class="size-full wp-image-2686" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/25/HPCBlog-174-fig2.png" alt="Figure 2: A graph of pending and running jobs over time. The chart shows that the blue share, which is given a weightFactor of 0.5, is allocated more resources than the yellow share, resulting in the blue pending queue being completed before yellow, despite these jobs being submitted at a later time than yellow." width="977" height="708"&gt;
 &lt;p id="caption-attachment-2686" class="wp-caption-text"&gt;Figure 2: A graph of pending and running jobs over time. The chart shows that the &lt;strong&gt;blue&lt;/strong&gt; share, which is given a &lt;code&gt;weightFactor&lt;/code&gt; of 0.5, is allocated more resources than the &lt;strong&gt;yellow&lt;/strong&gt; share, resulting in the&lt;strong&gt; blue&lt;/strong&gt; pending queue being completed before &lt;strong&gt;yellow&lt;/strong&gt;, despite these jobs being submitted at a later time than &lt;strong&gt;yellow&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;An analogy for weight factor would be like a drag coefficient on a car&lt;/strong&gt; — a car with a lower drag requires less energy to move forward with less effort than other cars with a higher drag. But drag is not the only factor impacting how far or fast a car can go: it also depends on the size of the road and how many other cars are traveling alongside it. So, while a lower &lt;code&gt;weightFactor&lt;/code&gt; value can increase the likelihood a job will get resources, it’s not guaranteed to get them right away.&lt;/p&gt; 
&lt;h2&gt;Saving some capacity for other shares – compute reservations&lt;/h2&gt; 
&lt;p&gt;Sometimes you want to set aside a small amount of your total capacity of compute resource just in case some high-priority work arrives and needs to run right away. Fair share policies allow you to do this by defining a &lt;em&gt;compute reservation&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;A &lt;em&gt;compute reservation&lt;/em&gt; allows AWS Batch to reserve a certain amount of the total capacity for share identifiers that are not yet active in the job queue. Let’s take a look at the formula used to calculate the compute reservation of a job queue, and then use some examples to understand its implications.&lt;/p&gt; 
&lt;h3&gt;Understanding the compute reservation formula&lt;/h3&gt; 
&lt;p&gt;Batch uses the &lt;code&gt;computeReservation&lt;/code&gt; parameter to determine the percentage of total capacity to hold in reserve for inactive shares. It works like this:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Reserved Capacity = (computeReservation/100)^activeShareIds&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;strong&gt;&lt;code&gt;computeReservation&lt;/code&gt;&lt;/strong&gt; value is just an integer expressing the percentage of total maximum compute capacity you want to reserve for inactive share identifiers. It’s modified by the total number of &lt;code&gt;&lt;strong&gt;activeFairShareIds&lt;/strong&gt;&lt;/code&gt; — the number of share identifiers that are currently active in the job queue. As the number of &lt;code&gt;&lt;strong&gt;activeShareIds&lt;/strong&gt;&lt;/code&gt; &lt;em&gt;increases&lt;/em&gt;, the fraction of computed capacity reserved for inactive shares &lt;em&gt;decreases&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;To see how this works in practice, let’s consider a contrived example where we reserved 50% of our total capacity in a compute environment for share identifiers not in the job queue. To do that, we defined the following fair share policy:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-json"&gt;{
  "fairsharePolicy": {
    &lt;strong&gt;"computeReservation": 50&lt;/strong&gt;,
    "shareDistribution": [
      {
        "shareIdentifier": "yellow",
        "weightFactor": 1.0
      },
      {
        "shareIdentifier": "blue",
        "weightFactor": 1.0
      }
]}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;In the case where the job queue only has jobs with the &lt;strong&gt;yellow&lt;/strong&gt; share identifier running, the job queue will reserve &lt;strong&gt;50%&lt;/strong&gt; of the capacity for jobs with &lt;em&gt;other&lt;/em&gt; share identifiers because: &lt;strong&gt;(50/100)^1 = 0.50.&lt;/strong&gt; We’ve shown this in Figure 3.&lt;/p&gt; 
&lt;div id="attachment_2685" style="width: 505px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2685" loading="lazy" class="size-full wp-image-2685" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/25/HPCBlog-174-fig3.png" alt="Figure 3: An illustration of the AWS Batch job queue and compute environment showing that when capacity reservation is set to 50 and there is only one share identifier active, 50% of the capacity is reserved for jobs with a different share identifier." width="495" height="239"&gt;
 &lt;p id="caption-attachment-2685" class="wp-caption-text"&gt;Figure 3: An illustration of the AWS Batch job queue and compute environment showing that when capacity reservation is set to 50 and there is only one share identifier active, 50% of the capacity is reserved for jobs with a different share identifier.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As more share identifiers become active in the job queue, the amount of compute reserved for new shares identifiers &lt;em&gt;decreases&lt;/em&gt;. This ensures that “fairness” is maintained, and newer share identifiers are not disproportionately allocated compute resources.&lt;/p&gt; 
&lt;p&gt;Now let’s consider a situation again where two share identifiers — &lt;strong&gt;blue&lt;/strong&gt; and &lt;strong&gt;yellow — &lt;/strong&gt;are active in the job queue, and the compute reservation is set at &lt;code&gt;50&lt;/code&gt;. The capacity that will remain unused in this case will be &lt;strong&gt;(50/100)^2 = 0.25&lt;/strong&gt; or &lt;strong&gt;25%&lt;/strong&gt;. We’ve shown this in Figure 4.&lt;/p&gt; 
&lt;div id="attachment_2684" style="width: 512px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2684" loading="lazy" class="size-full wp-image-2684" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/25/HPCBlog-174-fig4.png" alt="Figure 4: An illustration of the AWS Batch job queue and compute environment showing that when capacity reservation is set to 50 and there are two share identifiers active, 25% of the capacity is reserved for jobs with a different share identifier." width="502" height="241"&gt;
 &lt;p id="caption-attachment-2684" class="wp-caption-text"&gt;Figure 4: An illustration of the AWS Batch job queue and compute environment showing that when capacity reservation is set to 50 and there are two share identifiers active, 25% of the capacity is reserved for jobs with a different share identifier.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We ran the experiment as before with this new policy and you can see the results in Figure 5. When there is only a single share active, only 50% of the compute resources are used, and this increases to 75% once two shares are active. You’ll also note that blue jobs received an immediate job placement since resources were kept reserved for them.&lt;/p&gt; 
&lt;div id="attachment_2683" style="width: 987px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2683" loading="lazy" class="size-full wp-image-2683" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/25/HPCBlog-174-fig5.png" alt="Figure 5: A line chart showing the number of yellow and blue pending and running jobs over time in a AWS Batch compute environment that has a compute reservation value of 50. The data shows that Batch reserves a portion of compute resources for inactive shares, corresponding to the CR 50% and CR 25% lines." width="977" height="706"&gt;
 &lt;p id="caption-attachment-2683" class="wp-caption-text"&gt;Figure 5: A line chart showing the number of &lt;strong&gt;yellow&lt;/strong&gt; and &lt;strong&gt;blue&lt;/strong&gt; pending and running jobs over time in a AWS Batch compute environment that has a compute reservation value of 50. The data shows that Batch reserves a portion of compute resources for inactive shares, corresponding to the CR 50% and CR 25% lines.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Something to note is that &lt;em&gt;at the end of the run&lt;/em&gt; shown in Figure 5, &lt;em&gt;utilization dropped below 50% even though there are two active shares! &lt;/em&gt;This is due to Batch’s smart scale-down behavior. As the number of jobs in the queue start to decrease, Batch will start to pack new jobs on a smaller number of instances, scaling down other compute resources sooner, saving you money.&lt;/p&gt; 
&lt;p&gt;The last example used equal weight factors for the shares, but what happens if the policy determined that &lt;strong&gt;blue&lt;/strong&gt; jobs had a weight factor of &lt;code&gt;0.5&lt;/code&gt; as before? The answer is that the available shares are allocated according to your weights, which we’ve shown in Figure 6.&lt;/p&gt; 
&lt;div id="attachment_2682" style="width: 987px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2682" loading="lazy" class="size-full wp-image-2682" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/25/HPCBlog-174-fig6.png" alt="Figure 6: A line chart showing the number of yellow and blue pending and running jobs over time in an AWS Batch compute environment that has a compute reservation value of 50. The data shows that Batch allocates resources based on a share’s weight factor, and reserves a portion of compute resources for inactive shares." width="977" height="708"&gt;
 &lt;p id="caption-attachment-2682" class="wp-caption-text"&gt;Figure 6: A line chart showing the number of &lt;strong&gt;yellow&lt;/strong&gt; and &lt;strong&gt;blue&lt;/strong&gt; pending and running jobs over time in an AWS Batch compute environment that has a compute reservation value of 50. The data shows that Batch allocates resources based on a share’s weight factor, and reserves a portion of compute resources for inactive shares.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;You’ll notice at the end of the run that the compute capacity remains at 75% utilization, even though there is only one type of job running. That’s because &lt;strong&gt;blue&lt;/strong&gt; jobs completed within the share decay time window, and hence the &lt;strong&gt;blue&lt;/strong&gt; share is still considered “active” even though there are no currently running jobs.&lt;/p&gt; 
&lt;h3&gt;Determining the size of the reserve&lt;/h3&gt; 
&lt;p&gt;While our example used a large value for &lt;code&gt;computeReservation&lt;/code&gt; to illustrate how the parameter works, in practice you should define a much smaller value for this. The intent of the &lt;code&gt;computeReservation&lt;/code&gt; parameter is to hold a small amount of reserve capacity for urgent requests, or allow room for you to meet a minimum SLA across shares.&lt;/p&gt; 
&lt;p&gt;The exact value of &lt;code&gt;computeReservation&lt;/code&gt; will depend on your job sizes and expected maximum number of active shares. It should be &lt;em&gt;as small as possible&lt;/em&gt; while still allowing for jobs with an inactive share identifier to start quickly. For our example of two active shares, a value of 50 is way too high, but if you expect at least 5 active share IDs, a value of 50 would translate to ~3% of your total maximum capacity reserved for inactive shares.&lt;/p&gt; 
&lt;h2&gt;One more thing – setting job priority within a share&lt;/h2&gt; 
&lt;p&gt;One final resource allocation modifier is the &lt;code&gt;sharePriority&lt;/code&gt; parameter. This parameter only effects the relative ordering of jobs belonging to a single share ID — it does not affect the ordering of jobs belonging to other shares.&lt;/p&gt; 
&lt;p&gt;Expanding our previous example of the organizational groups “HR” and “MLOps” we used &lt;code&gt;weightFactor&lt;/code&gt; to give preferential treatment to an MLOps jobs. Within MLOps, though, you want to define that production get jobs higher priority than dev/test jobs. To do that, you set higher value for the &lt;code&gt;sharePriority&lt;/code&gt; parameter (&lt;code&gt;sharePriority=100&lt;/code&gt;) for production jobs than dev/test jobs (&lt;code&gt;sharePriority=1&lt;/code&gt;). This would result in production jobs to be placed on available resources before dev/test jobs, even if the dev/test jobs were submitted earlier in time to the job queue.&lt;/p&gt; 
&lt;h2&gt;Practical use cases for fair share policies&lt;/h2&gt; 
&lt;p&gt;Fair share policies are useful in a lot of scenarios. Here’s just a few examples to seed your thinking:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The share decay time window of fair share queues &lt;strong&gt;helps smooth out resource usage over time&lt;/strong&gt; by considering the history of the jobs assigned to a share. It ensures that variations in job count or run times between different share identifiers do not result in giving more compute time to a share than is specified within your policy. This is particularly beneficial when a share has a spike in job submissions. In a FIFO queue, spikes would dominate the compute resources to the detriment of other jobs in other shares.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fair share queues allow new work to get running as quickly as possible&lt;/strong&gt;. Jobs with a share that have not been in the job queue recently will have an extremely low aggregate usage (possibly even zero) and so Batch’s scheduler will preferentially assign resources to them when they become available. This allows for dynamic resource allocation, ensuring that jobs submitted later receive their fair share of compute resources as soon as possible.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fair share queues allow you to run heavy and light jobs simultaneously&lt;/strong&gt;. It allows important lighter jobs to be executed alongside heavy jobs, instead of waiting for the heavy jobs to complete first.&lt;/li&gt; 
 &lt;li&gt;Fair share allows efficient use of resources within a single queue. With FSS, a single job queue can be used equitably and efficiently for different types of workloads, &lt;strong&gt;maximizing your overall resource efficiency&lt;/strong&gt;. This might save you splitting your compute capacity across different job queues and compute environments.&lt;/li&gt; 
 &lt;li&gt;Fair sharing enables fair resource allocation among multiple users or workloads, ensuring &lt;strong&gt;equitable distribution of your total compute capacity&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;Priority scheduling lets &lt;strong&gt;important jobs in each share run sooner&lt;/strong&gt;, because they’re given a higher priority.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Fair share policies are not magic&lt;/h2&gt; 
&lt;p&gt;While fair share policies can help to overcome some of the drawbacks of a single FIFO job queue, they are not magic. When you are expecting jobs that have special requirements, such as GPU accelerators or very large memory requirements, you should take care to design your Batch environment for multiple job queues. In the case of GPUs, you don’t want CPU only jobs to run on these instances since it is both a waste of resources and they might block another job that actually needs the GPU from starting. Similarly&amp;nbsp; very large memory jobs may block the queue for a time as an instance waits for current jobs to complete to make room for the big job.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog post, we delved into the parameters of AWS Batch fair share scheduling policies, including compute reservations, share decay time windows, weight factor, and share priority. We explored how these parameters affect resource allocation, fairness over time, and job prioritization within share identifiers. And we discussed practical use cases where fair share proves beneficial.&lt;/p&gt; 
&lt;p&gt;By leveraging fair share queues in AWS Batch, organizations can optimize job execution, enhance resource management, and meet critical deadlines effectively. Read more about scheduling policies in &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/scheduling-policies.html"&gt;the documentation&lt;/a&gt; or try creating your own fair share policies using &lt;a href="https://console.aws.amazon.com/batch/home?#schedulingPolicies"&gt;the AWS Batch management console&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>How to make digital technologies for the circular economy work for your business</title>
		<link>https://aws.amazon.com/blogs/hpc/how-to-make-digital-technologies-for-the-circular-economy-work-for-your-business/</link>
		
		<dc:creator><![CDATA[Ilan Gleiser]]></dc:creator>
		<pubDate>Thu, 27 Jul 2023 16:03:20 +0000</pubDate>
				<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">544fba8a33f7ade32dc34fb32cd925f949d183f2</guid>

					<description>In this post, we discuss the benefits of digital technology for the circular economy, and show how businesses can implement these technologies to get the most out of them for the wellbeing of everyone.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-2638" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/circular-economy-ready.png" alt="How to make digital technologies for the circular economy work for your business" width="380" height="212"&gt;Ilan Gleiser – Principal Machine Learning Specialist, Global Impact Computing, AWS&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Digital technology for circular economy is an innovative approach that has the potential to revolutionize how businesses operate. It combines technology, data, and creativity to create a more efficient and sustainable way of doing business. By integrating digital technologies into their operations, businesses can benefit from improved resource management, increased efficiency, and reduced waste.&lt;/p&gt; 
&lt;p&gt;In this blog post, we will discuss the benefits of digital technology for the circular economy, as well as how businesses can implement these technologies to get the most out of them.&lt;/p&gt; 
&lt;h2&gt;What is a circular economy?&lt;/h2&gt; 
&lt;p&gt;A circular economy (CE) gradually decouples growth from resource consumption. Keeping products and materials in use, reducing waste, and regenerating natural systems are the principles of CE. The World Economic Forum’s (WEF) CE definition is “&lt;em&gt;an industrial system that is restorative or regenerative by intention and design&lt;/em&gt;”. Resources and products are re-used, recycled, and then remanufactured by the use of a closed loop (circular) process. The advantages of such an approach are substantial. As evidence of this, the Ellen McArthur Foundation did a study that estimates that, by 2030, the circular economy in Europe can produce net benefits of €1.8 trillion while addressing various resource-related challenges, creating jobs, and sparking innovation. Over the next 20 years, the current linear economic model will present huge challenges and negative impacts that are cumulative and set to grow.&lt;/p&gt; 
&lt;p&gt;Thankfully, the advances of digital technologies have enabled more innovative methods that make the best use of a CE’s potential. They include IoT sensors that track assets and guide decisions; blockchains to increase traceability and validate provenance for secondary markets of recycled rare metals; artificial intelligence (AI) and machine learning (ML) models to enable predictive maintenance; high performance computing (HPC) capabilities to design new materials; additive manufacturing (AM) techniques to enable quick prototyping; and big data analytics to measure circularity performance metrics.&lt;/p&gt; 
&lt;p&gt;In short, digital technologies have the potential to revolutionize how we think about production and energy usage, bringing us one step closer to realizing business benefits but also living in harmony with our planet.&lt;/p&gt; 
&lt;h2&gt;What are the benefits of implementing circular economy in business?&lt;/h2&gt; 
&lt;p&gt;First, it prolongs the life of the product and its components. Circular economy principles want design innovation to keep products, components, and materials at their best use at any time. Technical materials, such as those from airplanes or cars, can empower recycling cycles through repairing, refurbishing, reusing, and, when no other option is available, recycling the part or object.&lt;/p&gt; 
&lt;p&gt;Second, to make a system of reuse and recycle successful, companies need to identify obstacles like fluctuating supply and demand and the condition of the returned goods. For a company to decide the next use of a product they have taken back, they would have to consider the various aspects of the product, as well as the current state of the market. This is where artificial intelligence (AI), driven by machine-learning (ML) and high-performance computing (HPC) come in. We can use AI/ML to predict customer needs based on trends, while HPC enables quick crunching of the large amounts of data needed in decisions related to supply-chain efficiency.&lt;/p&gt; 
&lt;p&gt;Finally, a common challenge to generate value from used material streams (from kitchen waste to used computers) is that these streams are mixed and heterogenous in materials, products and by-products, both biological and technical. The recovery of valuable materials requires homogeneous, pure flows. This often necessitates sorting, cleaning and grading steps within an industrial process before the resources can be put back into circulation. To reduce this sorting time and cost, computer vision algorithms can be combined with robotic automation solutions to enable a more efficient selection and classification of materials and help automate the sorting processes, therefore enabling rapid assessment of resources for reuse for end-of-life sorting intelligence. With digital technologies driving the circular economy forward, businesses will be able to efficiently turn what was once considered “waste” into an evergreen resource.&lt;/p&gt; 
&lt;h2&gt;Which digital technologies can help in achieving a circular economy?&lt;/h2&gt; 
&lt;p&gt;With that scenario in mind, let’s look at some real-life examples of how customers are using AWS services and technologies to accelerate the path towards a circular economy.&lt;/p&gt; 
&lt;p&gt;Artificial intelligence (AI), machine-learning (ML) algorithms, high-performance computing (HPC), internet of things (IOT), big data analytics (BDA), Blockchain (BC) and additive manufacturing (3D printing) can be combined with robotic automation to create an efficient, automated process that uses resources more efficiently. The combination of these emerging digital technologies enables organizations to identify opportunities, reduce waste and improve operations on multiple levels, allowing companies to track materials used through their value chain, and helping them move towards a circular economy.&lt;/p&gt; 
&lt;p&gt;By implementing AI/ML algorithms with IoT sensors, companies can identify opportunities for reuse or recycling materials, optimize resource use and help build a zero-waste supply chain. HPC provides the compute power needed to run ML models quickly, and 3D printing provides customized parts on-demand while reducing material waste. In short, these digital technologies allow companies to shift away from linear production systems to achieve the sustainability benefits associated with a circular economy.&lt;/p&gt; 
&lt;h3&gt;HPC&lt;/h3&gt; 
&lt;p&gt;High-performance computing is a critical technology for simulation and one of the main accelerators for the circular economy transition. It enables processing of massive amounts of data, quickly and accurately and helps organizations solve complex problems faster than ever before.&lt;/p&gt; 
&lt;p&gt;Companies can also use HPC to develop new products or services based on the analysis of the data they have collected. HPC makes it easier to identify opportunities for cost savings or potential risks. Ultimately, this allows businesses to become more efficient and competitive in a dynamic and ever-evolving market.&lt;/p&gt; 
&lt;p&gt;HPC systems are composed of large clusters of computers, specialized software, and high-speed networks. They are used in a variety of fields, like engineering, finance, medicine, and science, to solve complex problems that would otherwise be impossible to solve. Companies use HPC systems to develop new products and services – and to improve existing ones. As the world becomes increasingly complex, HPC is increasingly necessary. The circular economy is one of the most exciting ways that HPC can help make the world more sustainable.&lt;/p&gt; 
&lt;p&gt;At AWS, we partnered with &lt;a href="https://aws.amazon.com/blogs/hpc/massively-scaling-quantum-chemistry-to-support-a-circular-economy/"&gt;Good Chemistry&lt;/a&gt;, an AWS cloud-native computational chemistry company based in Vancouver, to use the power of HPC to eliminate PFAS from the environment by simulating the exact compound that destroys the “forever plastic” molecule. By designing and building the &lt;strong&gt;largest ever Kubernetes cluster on the AWS cloud&lt;/strong&gt;, the company not only accelerated the path towards a more sustainable future but also did it in a sustainable way.&lt;/p&gt; 
&lt;p&gt;The PFAS family comprises around 4,700 man-made compounds which are primarily used for waterproofing, creating non-stick surfaces, packaging food, or making firefighting foam and paint. It has been found in water systems and the human body, leading to various health problems.&lt;/p&gt; 
&lt;p&gt;The paths PFAS take to be broken down can be simulated on the Good Chemistry platform on AWS. This platform (“QMIST Cloud”) uses a combination of ML and HPC to run computational chemistry simulations on AWS. This method is increasingly popular among businesses for a number of benefits that extend to various industries, including drug design and food innovation, to new battery material discovery and carbon capture. &lt;a href="https://aws.amazon.com/blogs/hpc/the-benefits-of-computational-chemistry-for-the-circular-economy/"&gt;In a previous post&lt;/a&gt;, we described the benefits of computational chemistry for a circular economy.&lt;/p&gt; 
&lt;p&gt;In addition, Accenture, the World Economic Forum, and AWS, working together via &lt;a href="https://thecirculars.org/"&gt;The Circulars Accelerator&lt;/a&gt;, have been pushing digital technologies for a circular economy forward. Our aim is that start-up companies with circularity enabling businesses should have access to supercomputers on the cloud to develop sustainable solutions like these. By doing so, we all hope to contribute towards a cleaner future with fewer environmental impacts.&lt;/p&gt; 
&lt;p style="text-align: center"&gt;“&lt;em&gt;By supporting leading startups and fostering innovative partnerships through The Circulars Accelerator, we hope to help companies move from insights and commitments to actions, outcomes, and results that can be measured and commercially successful.&lt;/em&gt;”&lt;/p&gt; 
&lt;p style="text-align: right"&gt;Wes Spindler – Accenture, Managing Director, Sustainability,&lt;br&gt; Co-Author &lt;a href="https://www.amazon.com/Circular-Economy-Handbook-Realizing-Advantage/dp/1349959677/ref=pd_lpo_2?pd_rd_w=k1d9d&amp;amp;content-id=amzn1.sym.116f529c-aa4d-4763-b2b6-4d614ec7dc00&amp;amp;pf_rd_p=116f529c-aa4d-4763-b2b6-4d614ec7dc00&amp;amp;pf_rd_r=2FMSHHBS9YZNKHN2CGER&amp;amp;pd_rd_wg=ZJlYi&amp;amp;pd_rd_r=439b48b0-2804-43dc-bc4c-044addf94307&amp;amp;pd_rd_i=1349959677&amp;amp;psc=1"&gt;The circular economy Handbook&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Big data analytics&lt;/h3&gt; 
&lt;p&gt;Data is essential to answering questions and accelerating action on the transition to a circular economy. Evidence for the impact of circular strategies is steadily increasing, but much work remains to be done in order to build a comprehensive understanding of the effects of this transition. We need data to establish a reasonable baseline, assess interventions, and measure outcomes. But data capture can be challenging as data for employment, emissions, and material consumption can be spotty and siloed. To bridge this gap, surveys and projects that collect specific data related to the circular economy can help to better understand how it affects different systems and help inform decision making. Digital solutions and data arising from artificial intelligence can also unlock circular economy opportunities.&lt;/p&gt; 
&lt;p&gt;As an example, AI can process large amounts of data to identify patterns, fill in data gaps and uncover trends which we can use for measuring impact. Additionally, we can apply predictive analytics and agent-based simulations to forecast future economic changes and consumer behavior. Going further still, data analysis tools can provide insights into customer sentiment regarding certain initiatives, like those relating to waste minimization or energy efficiency.&lt;/p&gt; 
&lt;p&gt;Big data provides powerful insights into the economic benefits associated with certain initiatives. Companies use big data analytics to measure the impact of their sustainability initiatives by tracking both direct costs and savings resulting from various measures. This provides them with a detailed view into the cost effectiveness of their efforts and give valuable insights into areas for potential improvement.&lt;/p&gt; 
&lt;p&gt;Finally, big data – when harmonized into a circular economy data lake and properly cleaned and prepared – enables organizations to track metrics such as the most common key performance indicators (KPIs) to measure circularity.&lt;/p&gt; 
&lt;p&gt;Some examples we use to measure circularity are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Resource efficiency&lt;/strong&gt;: This metric measure how efficiently resources are used throughout the product life cycle. This includes raw materials, water, energy, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Recyclability&lt;/strong&gt;: This metric looks at how easy it is to recycle a product or reuse it at the end of its lifecycle.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Waste reduction&lt;/strong&gt;: This metric looks at the amount of waste generated throughout the product life cycle. Companies can use this metric to evaluate their waste management efforts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Renewable energy use&lt;/strong&gt;: This metric looks at the percentage of energy sourced from renewable sources used by a company. This helps companies reduce their carbon footprint and become more sustainable.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Product design&lt;/strong&gt;: This evaluates the design of a product to assess how well designed it is for longevity and reuse. This can include looking at the parts used in the product and how easily we can replace or repair them.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Measuring these KPIs can help companies understand where they are in their journey towards becoming more circular and identify areas where they need to improve. With big data, companies can gain insights into their performance and optimize their operations to achieve better results.&lt;/p&gt; 
&lt;div id="attachment_2633" style="width: 1095px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2633" loading="lazy" class="size-full wp-image-2633" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.15.01.png" alt="Fig 1: The infographic above exemplifies some common use cases of digital technologies for circular economy with a Circular Data Lake at the Center enabling the calculation of metrics and KPIs" width="1085" height="611"&gt;
 &lt;p id="caption-attachment-2633" class="wp-caption-text"&gt;Fig 1: The infographic above exemplifies some common use cases of digital technologies for circular economy with a Circular Data Lake at the Center enabling the calculation of metrics and KPIs&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;AI/ML&lt;/h3&gt; 
&lt;p&gt;AI and ML algorithms can monitor and optimize energy use in real-time, identify inefficient processes ,and recommend changes to reduce waste. Machine learning is being implemented across several parts of the economy in order to improve circularity. Data analysis, especially for innovation, forecasting, and optimization, is the most significant application of artificial intelligence in the circular economy according to UNEP (United Nations Environmental Programme).&lt;/p&gt; 
&lt;p&gt;In the energy sector, an example is the Distributed Energy Resource Management Systems (DERMs), which employs reinforcement learning algorithms to optimize energy usage across horizontal micro-grids of prosumers, and across shared renewable energy platforms. &lt;a href="https://aws.amazon.com/solutions/case-studies/edf/"&gt;EDF Energy&lt;/a&gt;, UK’s largest producer of low-carbon electricity, aims to transform UK’s energy supply by decarbonizing, digitizing, and democratizing it. The company, which uses AWS to scale Powershift, an energy flexibility platform, enables customers to manage and monetize their assets in an intelligent, innovative way.&lt;/p&gt; 
&lt;p&gt;In retail, more specifically in the consumer-packaged goods (CPG) industry, companies can use generative AI to design packaging with a lower carbon footprint. Customers will be delighted with minimal packaging, fewer damaged goods, faster deliveries and by contributing to a more sustainable world …“&lt;em&gt;saving over a million tons of packaging material in the process&lt;/em&gt;” – &lt;strong&gt;Bill McGrath, Amazon&lt;/strong&gt;, describing the benefits of Amazon’s Ship in its Own Container (SIOC) and Frustration-Free programs.&lt;/p&gt; 
&lt;p&gt;In the auto sector, AWS customer Reezocar, one of the largest buyers and sellers of used vehicles in France, uses AWS Batch and NVIDIA GPUs on Amazon Elastic Compute Cloud (Amazon EC2) instances to render synthetic data of simulated dents in cars to train a machine learning model to automatically identify dents in images of real cars. The output of the ML model informs the price the company pays for used cars, before they refurbish and resell them in the market. By doing so, the company extends the useful life of the vehicles by 5 years, reducing the amount of waste that goes to landfills or gets incinerated.&lt;/p&gt; 
&lt;h3&gt;IOT&lt;/h3&gt; 
&lt;p&gt;IOT or &lt;em&gt;Internet of Things&lt;/em&gt;, is the extension of the digital world into the physical world. Essentially, IoT is a network of physical objects linked by software, sensors, and the Internet for the purpose of exchanging data. Sensors can collect data about the behavior of processes, conditions, or materials, like temperature and moisture, production and machine conditions, or customer usage performance. We can guide circular economy strategies using IoT data that describes and monitors the type, quantity, and timing of current material flows in real time.&lt;/p&gt; 
&lt;p&gt;The biggest gains from IoT are likely to be in the B2B sector, with machine-to-machine communications from IOT devices being responsible for generating more data than human to human communications. We can use machine learning to analyze trends and detect patterns that lead to better decision making. IOT, combined with AI, allows for better monitoring and management of resources. It is now considered a crucial element of a circular system, giving organizations and smart cities better visibility of supply chains, CO2 emission or traffic patterns, enabling the creation and processing of data, coming from humans or machine to machine communications. IoT can accelerate the transition to a circular economy by helping to track the movement of materials, goods, and services in real-time.&lt;/p&gt; 
&lt;div id="attachment_2634" style="width: 1050px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2634" loading="lazy" class="size-full wp-image-2634" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/07/20/CleanShot-2023-07-20-at-13.16.37.png" alt="Fig 2: This architecture demonstrates how information flows and is analyzed as goods move through the supply chain from supplier to manufacturer to consumer. It is an event-driven architecture that comprises physical and business events managed in a secure, role-based access-controlled multi-tenant AWS event processing framework. The AWS event processing framework leverages key AWS services for data exchange with the Edge, Data Storage (Event Lake), Analytics, and AI/ML in the cloud. Events are in the form of messages that are exchanged between the AWS cloud and authorized entities and sub-entities in the supply chain (Supplier, Transporter, Manufacturer (receiving, plant floor, shipping), Fulfillment). The AWS cloud serves as the broker that the entities use to publish and subscribe to events." width="1040" height="581"&gt;
 &lt;p id="caption-attachment-2634" class="wp-caption-text"&gt;Fig 2: This architecture demonstrates how information flows and is analyzed as goods move through the supply chain from supplier to manufacturer to consumer. It is an event-driven architecture that comprises physical and business events managed in a secure, role-based access-controlled multi-tenant AWS event processing framework. The AWS event processing framework leverages key AWS services for data exchange with the Edge, Data Storage (Event Lake), Analytics, and AI/ML in the cloud. Events are in the form of messages that are exchanged between the AWS cloud and authorized entities and sub-entities in the supply chain (Supplier, Transporter, Manufacturer (receiving, plant floor, shipping), Fulfillment). The AWS cloud serves as the broker that the entities use to publish and subscribe to events.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Blockchain&lt;/h3&gt; 
&lt;p&gt;One of the latest technological developments that can help to create a more sustainable world is blockchain. We can apply the digital ledger in several industries.&lt;/p&gt; 
&lt;p&gt;Blockchain can enhance circular economy practices by enabling transparent and traceable supply chains. This technology creates an immutable and secure record of every transaction or process &lt;em&gt;within&lt;/em&gt; a supply chain. By creating transparency, blockchain can reduce waste, fraud, and inefficiencies while increasing trust and accountability.&lt;/p&gt; 
&lt;p&gt;Blockchain can also enable a closed-loop system by tracking the movement of goods, materials, and resources, from raw material sourcing to final disposal. This ensures that waste materials are recycled and reused rather than being sent to landfills or incinerators. It can also ensure that we can trace the origin of recycled materials, promoting a more sustainable and responsible value chain.&lt;/p&gt; 
&lt;p&gt;Furthermore, blockchain technology can also incentivize consumers to participate in circular economy by rewarding them for making environmentally conscious decisions. For instance, consumers could earn tokens for returning used products to companies that they can use to purchase new products, thereby reducing waste. Companies can also sell used parts of machines, like cars, trucks, airplanes or hard drives on a secondary market that runs on an Amazon Managed Blockchain. Some potential examples of this type of marketplace include the resale of used clothing, furniture, and electronics. As consumers become more environmentally conscious, the demand for sustainable options is likely to increase. With blockchain technology, it is possible to create a secure and trustworthy marketplace for these used products.&lt;/p&gt; 
&lt;p&gt;Overall, the use of blockchain technology for creating secondary markets for used products is a promising development for sustainability and social impact. By facilitating the reuse and resale of goods, blockchain can help reduce waste and promote a circular economy.&lt;/p&gt; 
&lt;h2&gt;Additive manufacturing&lt;/h2&gt; 
&lt;p&gt;Additive manufacturing is playing an important role in the circular economy, reducing the use of resources by eliminating wasteful manufacturing practices. The Ellen McArthur Foundation has identified additive manufacturing as a key technology in their Circular Design Toolkit, designed to help companies reduce their environmental impact and increase sustainability.&lt;/p&gt; 
&lt;p&gt;By using 3D printing, companies can manufacture complex parts with little waste, resulting in cost savings. 3D printing has the potential to significantly reduce resource use and minimize environmental impacts. In addition, with additive manufacturing, components can be tailored to the end user’s specific needs, making mass customization possible. This means we can make products on-demand without stockpiling inventory, and this further helps to minimize waste. It lets companies quickly and cost-effectively create products without the need for traditional production processes like injection molding.&lt;/p&gt; 
&lt;p&gt;By using additive manufacturing, businesses can gain significant savings in resources while meeting customer demands.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Digital technologies are revolutionizing the way businesses operate and achieve their goals. Leveraging these technologies, businesses can now create an effective and sustainable circular economy that reduces waste, maximizes resource utilization, and minimizes environmental impacts.&lt;/p&gt; 
&lt;p&gt;With these tools, companies can create closed-loop systems &lt;em&gt;designed&lt;/em&gt; to reuse and recycle resources, resulting in a more sustainable business model. To make the most of these digital technologies, businesses should focus on creating flexible systems that can be tailored to meet the changing needs of their customers.&lt;/p&gt; 
&lt;p&gt;Using AWS technologies such as artificial intelligence, machine-learning, high-performance computing, IOT and Blockchain, the AWS Global Impact Computing Team is actively involved in driving this transformation. If you are interested in learning more how we can support your business, please reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;. With the right digital transformation strategy and AWS support, companies can successfully implement a circular economy business model that brings long-term economic, social, and environmental benefits.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Call for participation: RADIUSS Tutorial Series 2023</title>
		<link>https://aws.amazon.com/blogs/hpc/call-for-participation-radiuss-tutorial-series-2023/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Thu, 20 Jul 2023 14:43:29 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Research]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">bf60ac5fc9d32b2a05eccff237b773604ff35782</guid>

					<description>Lawrence Livermore National Laboratory (LLNL) and AWS are again joining forces to provide a training opportunity for emerging HPC tools and application. In this post you'll find out the details of those tutorials, and find out how to participate.</description>
										<content:encoded>&lt;p&gt;Lawrence Livermore National Laboratory (LLNL) and AWS are again joining forces to provide a training opportunity for emerging HPC tools and application. RADIUSS (&lt;a href="http://software.llnl.gov/radiuss/"&gt;Rapid Application Development via an Institutional Universal Software Stack&lt;/a&gt;) is a broad suite of open-source software projects originating from LLNL. Together we’re hosting a tutorial series to give attendees hands-on experience with these cutting-edge technologies. We’re calling on you to participate in these events.&lt;br&gt; &lt;img loading="lazy" class="size-full wp-image-1549 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2022/07/22/HPCBlog-149-fig1.png" alt="Various logos for the RADIUSS projects. " width="400" height="166"&gt;&lt;/p&gt; 
&lt;p&gt;This &lt;a href="https://software.llnl.gov/radiuss/event/2023/07/11/radiuss-on-aws/"&gt;virtual series&lt;/a&gt; consists of &lt;strong&gt;nine distinct tutorials&lt;/strong&gt; through the month of August (two more than last year). Each tutorial focuses on a specific package from the RADIUSS suite. You’ll get hands-on experience with each project and learn more about HPC deployments on AWS. Each event will be hosted on AWS resources and use &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; to provide an HPC environment for you to use, and gain experience with.&lt;/p&gt; 
&lt;p&gt;LLNL has a long history of delivering some of the world’s largest supercomputers and the supporting software to effectively use them. This &lt;a href="https://www.llnl.gov/news/llnl-amazon-web-services-cooperate-standardized-software-stack-hpc"&gt;collaboration between LLNL and AWS&lt;/a&gt; brings these projects and industry best practices to a broader community. Together we’re hoping to democratize access to next generation computational research tools and HPC resources.&lt;/p&gt; 
&lt;h2&gt;How to Participate&lt;/h2&gt; 
&lt;p&gt;Come join us in learning how these projects can help advance your HPC research and improve efficiency. These tutorials provide a great opportunity to learn modern techniques and tools in an adaptive cloud environment.&lt;/p&gt; 
&lt;p&gt;To register your interest, sign up for the tutorials through the &lt;a href="https://forms.gle/F1o6L6phiaruYWNp8"&gt;RADIUSS event form&lt;/a&gt;. Details of each event and registration deadlines are listed in the following table.&lt;/p&gt; 
&lt;table class="aligncenter" style="height: 1352px" border="1" width="895"&gt; 
 &lt;thead&gt; 
  &lt;tr style="background-color: #000000"&gt; 
   &lt;td width="87"&gt;&lt;span style="color: #ffffff"&gt;&lt;strong&gt;Package&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;&lt;span style="color: #ffffff"&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td width="267"&gt;&lt;span style="color: #ffffff"&gt;&lt;strong&gt;Tutorial Date&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
   &lt;td width="147"&gt;&lt;span style="color: #ffffff"&gt;&lt;strong&gt;Signup Deadline&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;BLT&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;BLT is a streamlined&amp;nbsp;CMake-based foundation for&amp;nbsp;Building,&amp;nbsp;Linking and&amp;nbsp;Testing large-scale high performance computing (HPC) applications.&amp;nbsp;BLT makes it easy to get up and running on a wide range of HPC compilers, operating systems and technologies&lt;/td&gt; 
   &lt;td&gt;August 3rd&lt;br&gt; 9am-11am PST&lt;/td&gt; 
   &lt;td&gt;2 business days prior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Spack&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;Spack is a multi-platform package manager that builds and installs multiple versions and configurations of software. It works on Linux, macOS, and many supercomputers.&lt;/td&gt; 
   &lt;td&gt;August 8th and 9th&lt;br&gt; 8am-11:30am PST&lt;/td&gt; 
   &lt;td&gt;2 business days prior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;MFEM&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;MFEM&amp;nbsp;is a modular parallel C++ library for finite element methods. Its goal is to enable high-performance scalable finite element discretization research and application development on a wide variety of platforms, ranging from laptops to supercomputers.&lt;/td&gt; 
   &lt;td&gt;August 10th&lt;br&gt; 9am-11am PST&lt;/td&gt; 
   &lt;td&gt;2 business days prior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Caliper&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;Caliper is a library to integrate performance profiling capabilities into applications. Caliper can be used for lightweight always-on profiling or advanced performance engineering use cases, such as tracing, monitoring, and auto-tuning.&lt;/td&gt; 
   &lt;td rowspan="3"&gt;August 14th&lt;br&gt; 9am-12pm PST&lt;/td&gt; 
   &lt;td&gt;2 business days prior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Hatchet&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;Hatchet is a Python-based library that allows&amp;nbsp;Pandas&amp;nbsp;dataframes to be indexed by structured tree and graph data. It is intended for analyzing performance data that has a hierarchy (for example, serial or parallel profiles that represent calling context trees, call graphs, nested regions’ timers, etc.).&lt;/td&gt; 
   &lt;td&gt;2 business days prior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thicket&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;Thicket is a Python-based toolkit for analyzing ensemble performance data. You can find detailed documentation, along with tutorials of Thicket in the&amp;nbsp;ReadtheDocs.&lt;/td&gt; 
   &lt;td&gt;2 business days prior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;RAJA&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;RAJA is a library of C++ software abstractions, primarily developed at Lawrence Livermore National Laboratory (LLNL), that enables architecture and programming model portability for HPC applications.&lt;/td&gt; 
   &lt;td rowspan="2"&gt;August 17th&lt;br&gt; 9am-11am PST&lt;/td&gt; 
   &lt;td&gt;2 business days prior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Umpire&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;Umpire is a resource management library that allows the discovery, provision, and management of memory on machines with multiple memory devices like NUMA and GPUs.&lt;/td&gt; 
   &lt;td&gt;2 business days prior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Ascent&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;Ascent is an open source many-core capable lightweight in situ visualization and analysis infrastructure for multi-physics HPC simulations.&lt;/td&gt; 
   &lt;td&gt;August 22nd&lt;br&gt; 9am-11am PST&lt;/td&gt; 
   &lt;td&gt;2 business days prior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Axom&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;Axom provides robust, flexible software infrastructure for the development of multi-physics applications and computational tools.&lt;/td&gt; 
   &lt;td&gt;August 24th&lt;br&gt; 9am-11am PST&lt;/td&gt; 
   &lt;td&gt;2 business days prior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;WEAVE&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;WEAVE is a collection of open source workflow tools. WEAVE includes Sina and Maestro which can be used together to perform and analyze runs of your code.&lt;/td&gt; 
   &lt;td&gt;August 29th&lt;br&gt; 9am-11am PST&lt;/td&gt; 
   &lt;td&gt;2 business days prior&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Flux&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="395"&gt;Flux is a flexible framework for resource management, built for your site. The framework consists of a suite of projects, tools, and libraries which may be used to build site-custom resource managers for High Performance Computing centers.&lt;/td&gt; 
   &lt;td&gt;August 31st&lt;br&gt; 9am-11am PST&lt;/td&gt; 
   &lt;td&gt;2 business days prior&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;We’re excited about this opportunity to give back to the HPC community alongside LLNL and look forward to meeting you all over the duration of the event.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Deploying and running HPC applications on AWS Batch</title>
		<link>https://aws.amazon.com/blogs/hpc/deploying-and-running-hpc-applications-on-aws-batch/</link>
		
		<dc:creator><![CDATA[Dario La Porta]]></dc:creator>
		<pubDate>Tue, 18 Jul 2023 15:46:07 +0000</pubDate>
				<category><![CDATA[Amazon Elastic File System (EFS)]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[AWS Service Catalog]]></category>
		<category><![CDATA[AWS Step Functions]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<guid isPermaLink="false">c84fd32f4acbd74debeef9e525762984afdbb0d3</guid>

					<description>In this post AWS Professional Services describes how they recommend using AWS Batch and Amazon Elastic Container Service (Amazon ECS) managed services for running HPC applications like GROMACS and RELION.</description>
										<content:encoded>&lt;p&gt;In this document, we introduce the use of &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; and &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon Elastic Container Service (Amazon ECS)&lt;/a&gt; managed services for running HPC applications like &lt;a href="https://www.gromacs.org/"&gt;GROMACS&lt;/a&gt; and &lt;a href="https://relion.readthedocs.io/"&gt;RELION&lt;/a&gt;. The use of containers is combined with the traditional approach of building and provisioning HPC application binaries, which helps in keeping the container image small and avoiding the proliferation of per-application containers.&lt;/p&gt; 
&lt;h2&gt;Executive Summary&lt;/h2&gt; 
&lt;p&gt;The purpose of this document is to share the discoveries and best practices for migrating a simple HPC workflow, which includes batch and legacy X11 applications, to &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;. The goal is to showcase how to architect infrastructure using managed AWS services such as &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt;, &lt;a href="https://aws.amazon.com/efs/"&gt;Amazon Elastic File System&lt;/a&gt;, and &lt;a href="https://aws.amazon.com/step-functions/"&gt;AWS Step Functions&lt;/a&gt;, with the help of &lt;a href="https://aws.amazon.com/cdk/"&gt;AWS Cloud Development Kit(AWS CDK)&lt;/a&gt;, and make the solution available as a Service in &lt;a href="https://aws.amazon.com/servicecatalog/"&gt;AWS Service Catalog&lt;/a&gt; for a self-service user experience.&lt;/p&gt; 
&lt;p&gt;The solution described here can be used for a broad range of HPC applications, including those with the most demanding CPU/GPU and networking requirements. In this case, we have selected GROMACS and RELION. GROMACS (GROningen MAchine for Chemical Simulations) is a Molecular Dynamics package used for simulating proteins, lipids, and nucleic acids. RELION (REgularised LIkelihood OptimisatioN) is used for processing electron cryo-microscopy (cryo-EM) images to identify 3D macromolecular structures. Both applications can run on CPUs or GPUs to accelerate compute-intensive steps, and they can scale out on multiple nodes through MPI (Message Passing Interface). The design of this solution is suitable for a Proof-of-Concept activity — &amp;nbsp;you should expect to do more integration work to enable these workloads in your production deployments.&lt;/p&gt; 
&lt;h2&gt;Motivations&lt;/h2&gt; 
&lt;p&gt;AWS offers a broad range of products and services that can be combined in various ways to achieve optimal price/performance and, more importantly, to attain business agility. The business outcomes underlying this project are as follows:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Performance optimization: This involves reducing costs by optimizing the application binaries and the underlying software stack of libraries. Costs-performance optimization can be achieved in two main ways: by selecting different &lt;a href="https://aws.amazon.com/ec2/instance-types/"&gt;Amazon EC2 instance types&lt;/a&gt; or by choosing the &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html"&gt;Spot&lt;/a&gt; pricing model.&lt;/li&gt; 
 &lt;li&gt;Reduction of dependency on legacy software applications: This goal aims to minimize the need for ongoing maintenance for software updates and fixes.&lt;/li&gt; 
 &lt;li&gt;Highly scalable HPC infrastructure: The objective is to automatically scale out/in the compute environment to meet the demands of the workload.&lt;/li&gt; 
 &lt;li&gt;Improved availability of the service to end users.&lt;/li&gt; 
 &lt;li&gt;Enhanced flexibility in balancing between minimum time and maximum cost savings.&lt;/li&gt; 
 &lt;li&gt;Software-defined infrastructure: This enables building and deploying through any CI/CD pipeline and facilitates self-service delivery.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The goal of this post is to assess the feasibility of running highly demanding HPC applications in a fully managed and highly scalable environment based on fully managed HPC services such as AWS Batch and Amazon ECS.&lt;/p&gt; 
&lt;h2&gt;Solution Overview&lt;/h2&gt; 
&lt;p&gt;HPC applications like GROMACS and RELION, originally designed for traditional HPC systems, can be run on AWS using the same software stack utilized in on-premises static data centers. This can be achieved by leveraging the EC2 virtualization layer or employing EC2 metal instances, while benefiting from the high-performance networking offered by &lt;a href="https://aws.amazon.com/hpc/efa/"&gt;Elastic Fabric Adapter (EFA)&lt;/a&gt;. To handle scale-out/in, a job scheduler such as &lt;a href="https://slurm.schedmd.com/"&gt;SLURM&lt;/a&gt; can be deployed within an &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; instance.&lt;/p&gt; 
&lt;p&gt;However, in this project, we have designed a cloud-native architecture based on managed services like AWS Batch, eliminating the reliance on third-party software for job scheduling (e.g., SLURM) and infrastructure elasticity management (e.g., extensions in ParallelCluster for scale-out/in based on the job queue).&lt;/p&gt; 
&lt;p&gt;The use of Amazon ECS containers for HPC applications and workflows provides an additional level of flexibility, but it raises several key questions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;How does application performance on AWS Batch/ECS compare to running on-premises/EC2?&lt;/li&gt; 
 &lt;li&gt;What is the optimal cost-performance tradeoff considering CPU and GPU instances, Spot, and On-Demand pricing?&lt;/li&gt; 
 &lt;li&gt;How efficiently does AWS Batch execute tightly-coupled and multi-node parallel applications in containers while leveraging EFA and high-performance file systems?&lt;/li&gt; 
 &lt;li&gt;What containerization strategy should be employed: one container for each distinct application or for a group of applications?&lt;/li&gt; 
 &lt;li&gt;How can Spot or On-Demand pricing be selectively used to further optimize costs?&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Addressing these questions will help determine the best approach for running HPC workloads on AWS and optimizing performance and cost-efficiency.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;The following architecture demonstrates how AWS services are combined to centrally manage applications and associated resources, achieving consistent governance and transforming workflows into IT services for end users.&lt;/p&gt; 
&lt;div id="attachment_2545" style="width: 843px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2545" loading="lazy" class="size-full wp-image-2545" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/06/26/HPCBlog-194-fig1.png" alt="Figure 1: The architecture for deploying HPC applications on AWS Batch" width="833" height="618"&gt;
 &lt;p id="caption-attachment-2545" class="wp-caption-text"&gt;Figure 1: The architecture for deploying HPC applications on AWS Batch.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To identify, manage, and audit the solution, the administrator can deploy the architecture using AWS CDK. This approach ensures flexibility and manages all content relevant to both the architecture itself and architecture governance processes.&lt;/p&gt; 
&lt;p&gt;The architecture includes the following components:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An AWS Batch compute environment used to run GROMACS and other containerized HPC applications&lt;/li&gt; 
 &lt;li&gt;A multi-layer storage solution consisting of Amazon S3, Amazon FSx for Lustre, and Amazon EFS&lt;/li&gt; 
 &lt;li&gt;An orchestration layer based on Step Functions&lt;/li&gt; 
 &lt;li&gt;A remote visualization layer using the NICE DCV remote display protocol for running RELION&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The AWS Batch compute environment offers multiple queues with various EC2 instance types, including those with GPUs, as some workflows benefit from the NVIDIA Tensor Cores. Spot and On-Demand compute environments are utilized to prioritize either the cost, or completion time of simulations, aiming to avoid unnecessary expenses and control job completion time as suggested in the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/welcome.html"&gt;Cost Optimization Pillar&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For job monitoring and orchestration, AWS Step Functions are employed. The Step Functions workflow is designed to optimize job costs by first attempting execution on Spot Instances and then falling back to On-Demand instances in case of failure or pending status. This configuration ensures quick recovery from spot termination failures and meets demand, as described in the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/welcome.html"&gt;Reliability Pillar&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Considering the requirement for handling multiple categories of data, the solution incorporates different layers of storage. This configuration is based on the &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/performance-efficiency-pillar/welcome.html"&gt;Performance Efficiency Pillar&lt;/a&gt;, aiming to optimize workload performance.&lt;/p&gt; 
&lt;p&gt;Amazon S3 is used as a high-resiliency, low-cost storage for input and output files. In a typical use case, the input file is downloaded from the S3 bucket during job bootstrap, and the generated output files are uploaded back to S3 upon completion. The bucket can be configured with a lifecycle policy to &lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html"&gt;migrate files between storage classes&lt;/a&gt; based on file access or age, further optimizing costs.&lt;/p&gt; 
&lt;p&gt;A high-performance scratch file system based on Amazon FSx for Lustre is utilized for staging GROMACS job input and output files. This file system is mounted on each EC2 instance running AWS Batch jobs, and the Docker environment is configured to export the mounted file system to the container.&lt;/p&gt; 
&lt;p&gt;To simplify the compilation and installation of application binaries, the environment is integrated with the &lt;a href="https://spack.io/"&gt;Spack&lt;/a&gt; package manager and provided recipes. With Spack, it is possible to build packages with multiple versions, configurations, platforms, and compilers, all coexisting on the same machine. The &lt;a href="https://github.com/TACC/Lmod"&gt;Lmod&lt;/a&gt; environment manager is used to launch the installed applications.&lt;/p&gt; 
&lt;p&gt;The application binaries are located in Amazon Elastic File System (EFS), which is also used to store job launch configurations and scripts. Similar to the FSx for Lustre file system, EFS is mounted on each EC2 instance running AWS Batch jobs and exported to the Docker container.&lt;/p&gt; 
&lt;p&gt;The ECS container image has been created with an OS and software stack suitable for running both targeted applications. The use of a single container simplifies infrastructure deployment by using a general-purpose container compatible with the required applications. This configuration is possible because the container does not contain the application binaries; instead, they are installed with Spack in the shared file system.&lt;/p&gt; 
&lt;p&gt;Post-execution analysis plays a crucial role in the scientists’ workflow. Therefore, the environment is configured to use &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;NICE DCV&lt;/a&gt;, allowing applications to run on CPU for software rendering or GPU for 3D/OpenGL rendering support. The ability to choose between software rendering or 3D/OpenGL rendering optimizes costs by selecting the appropriate instance type for the specific application.&lt;/p&gt; 
&lt;p&gt;For system issue and application error collection, system logs are monitored and retrieved using Amazon CloudWatch.&lt;/p&gt; 
&lt;p&gt;The IT personnel provision and update the infrastructure using the AWS Cloud Development Kit (CDK), ensuring reproducibility and simplifying maintenance. The infrastructure CDK can be designed to be general enough for reuse through AWS Service Catalog, supporting different business units and/or users located in different AWS Regions.&lt;/p&gt; 
&lt;p&gt;The availability of AWS Service Catalog for end users transforms application workflows into IT services that are more reproducible and easier to maintain.&lt;/p&gt; 
&lt;h2&gt;User Experience&lt;/h2&gt; 
&lt;p&gt;The end user’s experience begins with AWS Service Catalog, where they will find the entry for “RELION”. Clicking on this entry will launch an AWS CloudFormation (CF) template, which has been created through CDK. Behind the scenes, the CF template will create a DCV Virtual Session running the RELION application in an X11 Linux Desktop and provide a link for connection using a common web browser or the DCV Client.&lt;/p&gt; 
&lt;p&gt;RELION will be pre-configured with a custom script to submit a distinct Step Functions instance for each GROMACS job, implementing the desired data and job orchestration functions.&lt;/p&gt; 
&lt;p&gt;The Step Functions task will retrieve input data and set up a work environment on FSx for Lustre for the job. It will then submit the job to a Spot queue in AWS Batch and monitor its execution. If the job remains pending, the task will terminate and resubmit it to an On-Demand queue. Once the job is completed, the Step Function will transfer the job results back to S3.&lt;/p&gt; 
&lt;p&gt;The end user’s experience with RELION is identical to running the application on a static infrastructure with a rigid software stack like Slurm and VDI clusters. However, in this case, the user is presented with a simple entry in AWS Service Catalog. The use of ECS containers in conjunction with Batch’s scale-out/in capabilities and Step Functions workflows allows IT administrators to conceal the implementation details of the HPC infrastructure and data/job orchestration functions while minimizing costs.&lt;/p&gt; 
&lt;h2&gt;Future Work&lt;/h2&gt; 
&lt;p&gt;The AWS Batch compute environment depicted in Figure 1 is suitable for benchmarking purposes, as it allows for identifying the instance types that offer the best cost/performance ratio. However, for production environments, the Batch compute environment should be designed to span multiple Availability Zones, and the selection of instance types should be tailored to the specific workloads of the company.&lt;/p&gt; 
&lt;p&gt;In addition to basic logging and event monitoring, it is recommended to collect job statistics, including execution times and costs, in production environments. These statistics can be aggregated into a dashboard that provides valuable insights on key business metrics for management and operational teams.&lt;/p&gt; 
&lt;p&gt;Since the infrastructure is created through AWS CDK, it can be integrated into a higher-level system that automates the execution of an entire workflow. This workflow may include cluster deployment, job submission, retrieval and archiving of results, and can be triggered by an event such as dropping data into an S3 bucket.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we have outlined the best practices for deploying an HPC application stack on AWS using a range of AWS services. Through this project, we have successfully demonstrated the feasibility of using ECS containers and AWS Batch to run demanding HPC applications such as GROMACS and RELION.&lt;/p&gt; 
&lt;p&gt;Compared to a software stack based on job schedulers like SLURM, our solution relies on fully managed services for job scheduling, resource allocation, and resource provisioning. This simplifies maintenance and reduces the operational costs of the system.&lt;/p&gt; 
&lt;p&gt;We have designed the infrastructure using AWS CDK, enabling seamless integration with CI/CD pipelines and providing the potential for an HPC-as-a-Service solution. Additionally, we have optimized the storage architecture by implementing three layers of storage to achieve the best possible cost-to-performance ratio.&lt;/p&gt; 
&lt;p&gt;The ability to run applications in containers combines the use of the widely adopted package manager, Spack, with containers that are optimized to leverage GPU acceleration. This is further facilitated by AWS Batch, which allows for the execution of tightly coupled MPI jobs on multiple nodes, leveraging the high-performance networking provided by EFA.&lt;/p&gt; 
&lt;p&gt;To learn more, read about how to use &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, &lt;a href="https://aws.amazon.com/step-functions/"&gt;AWS Step Functions&lt;/a&gt;, &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt; and &lt;a href="https://aws.amazon.com/cdk/"&gt;AWS CDK&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>High performance actuarial reserve modeling using AWS Batch</title>
		<link>https://aws.amazon.com/blogs/hpc/high-performance-actuarial-reserve-modeling-using-aws-batch/</link>
		
		<dc:creator><![CDATA[Max Tybar]]></dc:creator>
		<pubDate>Tue, 11 Jul 2023 15:38:57 +0000</pubDate>
				<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">46fc942561b214e6fd0387b20c94142703c00df0</guid>

					<description>In this blog post, we’ll discuss how to use AWS Batch to improve actuarial modeling workloads which are important in the insurance industry, used for analyzing and predicting risks and potential losses.</description>
										<content:encoded>&lt;p&gt;Actuarial modeling is a key component in the insurance industry, used for analyzing and predicting various risks and potential losses. Due to the complexity of calculations involved, actuarial modeling requires significant computing power and resources. This is where &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, a high performance computing (HPC) service comes in.&lt;/p&gt; 
&lt;p&gt;In this blog post, we’ll discuss how to use AWS Batch to improve actuarial modeling workloads. We’ll provide an overview of Batch and its features, and we will show you how to use it to run actuarial modeling jobs. We’ll also discuss some of the benefits of using Batch for this, like scalability, cost-effectiveness, and integration with other AWS services.&lt;/p&gt; 
&lt;h2&gt;Our reference architecture&lt;/h2&gt; 
&lt;p&gt;Figure 1 describes the architecture we’ll create to show how you can achieve this.&lt;/p&gt; 
&lt;div id="attachment_2468" style="width: 911px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2468" loading="lazy" class="size-full wp-image-2468" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/06/16/CleanShot-2023-06-16-at-16.38.28.png" alt="Figure 1. Our architecture showing AWS Batch managing EC2 Instances that mount an Amazon FSx for Lustre shared filesystem. Data are synced with Amazon S3 and containers images with actuarial models are pulled from Amazon ECR registries." width="901" height="525"&gt;
 &lt;p id="caption-attachment-2468" class="wp-caption-text"&gt;Figure 1. Our architecture showing AWS Batch managing EC2 Instances that mount an Amazon FSx for Lustre shared filesystem. Data are synced with Amazon S3 and containers images with actuarial models are pulled from Amazon ECR registries.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;To manage data for our batch jobs, we’ll use Amazon Simple Storage Service (Amazon S3) for storing both input &lt;em&gt;and&lt;/em&gt; final output data sets. Amazon S3 offers a range of benefits, including scalability, security, lifecycle management – and integration with other AWS services.&lt;/p&gt; 
&lt;p&gt;We’ll also use Amazon FSx for Lustre to access the input dataset on Amazon S3 through regular POSIX file operations. By configuring FSx for Lustre to import object metadata into the filesystem, all S3 bucket additions will be synchronized with the filesystem. When file data is accessed, FSx for Lustre will retrieve the S3 object data and store it in the filesystem. you can find out more about FSx for Lustre in &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/what-is.html"&gt;our service documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;With the FSx for Lustre filesystem in place, multiple compute instances can access the data concurrently. To make this happen, we’ll use AWS Batch to manage those compute instances and the overall batch processing job. AWS Batch dynamically provisions the optimal quantity and type of compute resources (like CPU or memory-optimized instances) based on the specific requirements of each job submitted.&lt;/p&gt; 
&lt;h2&gt;Key services and elements&lt;/h2&gt; 
&lt;p&gt;There are several factors that are important for making actuarial modeling workloads run well in any environment. Let’s consider the most important ones that informed the choices we made when developing this architecture.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;High performance compute&lt;/strong&gt;: AWS provides access to compute instances with high performance CPUs, GPUs, and dedicated high-speed network connectivity for compute clusters. This means that your actuarial calculations can be performed faster, resulting in quicker model iteration and more accurate predictions.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: Actuarial modeling is a resource-intensive process, requiring a significant &lt;em&gt;scale&lt;/em&gt; of resources. AWS Batch provides elastic scalability, meaning that resources can be scaled up or down depending on demand. This allows you to efficiently allocate resources based on your needs – and scale down to save costs during periods of low use.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Cost-effectiveness&lt;/strong&gt;: There’s no additional charge for using AWS Batch. You only pay for the resources you use. Additionally, you can optimize your consumption further by using &lt;a href="https://aws.amazon.com/ec2/spot/?cards.sort-by=item.additionalFields.startDateTime&amp;amp;cards.sort-order=asc&amp;amp;trk=46b0eefc-8c98-474e-8590-b407d7fe3181&amp;amp;sc_channel=ps&amp;amp;ef_id=CjwKCAjw9pGjBhB-EiwAa5jl3NOdAEYvMs746ofIXcZVq0qY0qMqgbxjjjyPpLFjqDlM9_A5trCLTBoCBcwQAvD_BwE:G:s&amp;amp;s_kwcid=AL!4422!3!651751059279!e!!g!!amazon%20ec2%20spot%20instances!19852662173!145019250457"&gt;Amazon EC2 Spot Instances&lt;/a&gt;, which provide access to spare EC2 capacity at a reduced cost.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Data storage&lt;/strong&gt;: AWS provides a variety of data storage options, including Amazon S3, which is highly scalable and durable, and stores large volumes of data reliably. This is important in actuarial modeling where massive amounts of data is necessary to get accurate predictions.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Collaboration&lt;/strong&gt;: Actuarial modeling often involves collaboration between different teams and individuals. The cloud is an effective medium for collaboration, allowing teams to work together in a single environment, share data, and use the same computing resources.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Security&lt;/strong&gt;: AWS has robust security protocols in place to protect data and computing resources. Actuarial modeling requires sensitive data, so the security measures provided by AWS will allow you to ensure you protect data from unauthorized access.&lt;/p&gt; 
&lt;h2&gt;Walkthrough&lt;/h2&gt; 
&lt;p&gt;To create this architecture in your own AWS account, follow these steps.&lt;/p&gt; 
&lt;p&gt;The steps we will perform are as follows:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Run CDK deployment process to provision infrastructure in your account&lt;/li&gt; 
 &lt;li&gt;Submit sample &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; jobs&lt;/li&gt; 
 &lt;li&gt;Modify &lt;a href="https://aws.amazon.com/pm/eventbridge/?ef_id=Cj0KCQjw27mhBhC9ARIsAIFsETG3pXnxxvjaCWF5kwyQ8xGPx4CwYaDM-fGaQhhFEMeqARpZzWwpTk0aAk9xEALw_wcB:G:s&amp;amp;s_kwcid=AL!4422!3!629393325337!!!g!!!16080176144!133788117598"&gt;Amazon EventBridge&lt;/a&gt; rule&lt;/li&gt; 
 &lt;li&gt;Look for the reserves results in &lt;a href="https://aws.amazon.com/pm/cloudwatch/?ef_id=Cj0KCQjw27mhBhC9ARIsAIFsETEJxGIkQMIrUaYBamTjxW501BuvapBaasTV-QVtQZDsMOKrzYY9698aAsrdEALw_wcB:G:s&amp;amp;s_kwcid=AL!4422!3!629393325805!!!g!!!16080176300!133788122638"&gt;AWS CloudWatch&lt;/a&gt; logs&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;p&gt;For smooth running, we advise &lt;a href="https://docs.aws.amazon.com/cloud9/latest/user-guide/create-environment.html"&gt;spinning up&lt;/a&gt; an &lt;a href="https://aws.amazon.com/cloud9/"&gt;AWS Cloud9&lt;/a&gt; instance.&lt;/p&gt; 
&lt;p&gt;However, if you elect to follow the deployment process using your local desktop, just make sure you have the following installed:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html"&gt;AWS CDK version 2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://docs.docker.com/get-docker/"&gt;Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;AWS CLI &lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html"&gt;installed&lt;/a&gt; and &lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html"&gt;configured&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Clone GitHub Repository and install dependencies&lt;/h3&gt; 
&lt;p&gt;We’ll clone the &lt;a href="https://github.com/aws-samples/actuarial-reserve-modelling"&gt;GitHub repository&lt;/a&gt; with the source-code and install-dependencies for the AWS CDK stack.&lt;/p&gt; 
&lt;p&gt;In a terminal, run the following command to clone the repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;git clone https://github.com/aws-samples/actuarial-reserve-modelling.git&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After you cloned the repository, navigate to the &lt;code&gt;infrastructure&lt;/code&gt; subfolder located in the cloned repository’s root directory, and run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;python -m pip install -r requirements.txt&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Deploy AWS CDK stack&lt;/h3&gt; 
&lt;p&gt;This will build a docker image locally and push it to Amazon Elastic Container Registry (&lt;a href="https://aws.amazon.com/ecr/"&gt;Amazon ECR&lt;/a&gt;), deploy the infrastructure and copy &lt;code&gt;policy_[*].csv&lt;/code&gt; files to the &lt;a href="https://aws.amazon.com/pm/serv-s3/?trk=fecf68c9-3874-4ae2-a7ed-72b6d19c8034&amp;amp;sc_channel=ps&amp;amp;ef_id=Cj0KCQjw27mhBhC9ARIsAIFsETHlt-ac34H0eE8wNihbUBYiSx67n2a8l0GyN71omjwCKpT9J3cNkYcaAncKEALw_wcB:G:s&amp;amp;s_kwcid=AL!4422!3!536452728638!e!!g!!s3!11204620052!112938567994"&gt;Amazon S3&lt;/a&gt; bucket. The process can take anywhere between 15 to 30 minutes depending on the machine you’re running deployment from.&lt;/p&gt; 
&lt;p&gt;Continuing in the infrastructure directory, deploy the CDK stack with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cdk bootstrap --require-approval never &amp;amp;&amp;amp; cdk deploy --require-approval never&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After the &lt;code&gt;cdk&lt;/code&gt; command successfully executes, you’ll see output results in the console. It should look something like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;Outputs:
ActuaryCalculationStack.EventBridgeRuleName = ActuaryCalculationStack-BatchJobCompletedEventBrid-1CH4ZMV0LYL71
ActuaryCalculationStack.FsxFileSystemId = fs-09ae7e84627d424bf
ActuaryCalculationStack.LambdaFunctionName = ActuaryCalculationStack-sumReservesFunctionA86431B-FaA5vkjhj7vR
ActuaryCalculationStack.VPCId = vpc-037c8f46fb1927d8b
Stack ARN:
arn:aws:cloudformation:us-west-2:111122223333:stack/ActuaryCalculationStack/5144a3f0-d4b7-11ed-a662-02cf31d82b91
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, copy the &lt;code&gt;ActuaryCalculationStack.EventBridgeRuleName&lt;/code&gt;&amp;nbsp;and &lt;code&gt;ActuaryCalculationStack.LambdaFunctionName&lt;/code&gt; values and &lt;em&gt;save them somewhere where you can access later&lt;/em&gt; (we’ll be asking you for these values soon).&lt;/p&gt; 
&lt;h3&gt;Submit sample AWS Batch jobs using AWS CLI&lt;/h3&gt; 
&lt;p&gt;In our example, each Batch &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/array_jobs.html"&gt;array job&lt;/a&gt; worker selects its own discrete list of input files to process. The worker determines the files based on all the file names available on the FSx for Lustre filesystem, using the &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/job_env_vars.html"&gt;AWS Batch-provided&lt;/a&gt; &lt;code&gt;AWS_BATCH_JOB_ARRAY_INDEX&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;p&gt;If you want to learn more about the structure of the &lt;code&gt;entrypoint.sh&lt;/code&gt; file that our docker image uses, see this &lt;a href="https://aws.amazon.com/blogs/hpc/ml-training-with-aws-batch-and-amazon-fsx"&gt;blog post&lt;/a&gt; for an example.&lt;/p&gt; 
&lt;p&gt;Before we begin, let’s take a quick look at a code snippet from a simulation we perform for actuarial modeling calculations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-clike"&gt;    // Run the simulation
    let mut total_reserves = 0.0;
    for _ in 0..NUM_SIMULATIONS {
        let mut reserves = 0.0;
        for policy in &amp;amp;policies {
            // Calculate the number of claims for this policy
            let num_claims = thread_rng().sample(Exp::new(1.0 / (policy.term / CLAIM_INTERVAL)).unwrap());

            // Calculate the reserves for this policy
            for _ in 0..num_claims as usize {
                let claim_amount = thread_rng().sample(Normal::new(100.0, 10.0).unwrap());
                reserves += claim_amount;
            }
        }
        total_reserves += reserves;
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This code snippet, written in the Rust programming language, simulates insurance claim payouts for a set of policies over multiple iterations. The purpose of the simulation is to estimate the expected size of reserves that an insurance company will need to hold to cover claims for the given set of policies.&lt;/p&gt; 
&lt;p&gt;In our example, we’ll perform simulations using an exponential distribution and calculate the amount of each claim using a normal distribution. We calculate the reserves for each simulation by adding up the total number of claims for all policies. Finally, the total reserves across all simulations are added up to get an estimate of the expected volume of reserves needed to cover the given set of policies.&lt;/p&gt; 
&lt;p&gt;Now, we’ll submit Array job with ten workers using AWS CLI.&lt;/p&gt; 
&lt;p&gt;Navigate to the root directory of the cloned repository and run the following to submit a job with ten workers &lt;em&gt;and&lt;/em&gt; to store the output for &lt;code&gt;jobId&lt;/code&gt; in a local variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;jobId=$(aws batch submit-job --cli-input-json file://test-10-workers.json 2&amp;gt;&amp;amp;1 | awk -F'"jobId": "' '/"jobId": "/{print $2}' | cut -d '"' -f1)&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can run the same command with 2 or 5 workers using json files in the root directory (&lt;code&gt;test-2-workers.json&lt;/code&gt; or &lt;code&gt;test-5-workers.json&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;Next, substitute&amp;nbsp;&lt;code&gt;ActuaryCalculationStack.EventBridgeRuleName&lt;/code&gt; with the value we asked to record during the&amp;nbsp;&lt;strong&gt;Deploy AWS CDK stack&lt;/strong&gt; section.&lt;/p&gt; 
&lt;p&gt;After that, run the command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws events put-rule --name "ActuaryCalculationStack.EventBridgeRuleName"&amp;nbsp;--event-pattern "{\"source\":[\"aws.batch\"],\"detail-type\":[\"Batch Job State Change\"],\"detail\":{\"jobId\":[\"$jobId\"],\"status\":[\"SUCCEEDED\"]}}"&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will modify our AWS EventBridge Rule to only be triggered on the&amp;nbsp;&lt;code&gt;SUCCEEDED&lt;/code&gt;&amp;nbsp;status after all our jobs complete their execution.&lt;/p&gt; 
&lt;p&gt;Finally:&lt;/p&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Open the &lt;a href="https://console.aws.amazon.com/batch"&gt;AWS Batch dashboard&lt;/a&gt; to view job status.&lt;/li&gt; 
 &lt;li&gt;In the left navigation pane, choose &lt;strong&gt;Jobs&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;For &lt;strong&gt;Job queue&lt;/strong&gt;, select &lt;code&gt;actuary-computing-job-queue&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Wait until status of&amp;nbsp;&lt;code&gt;test-10-workersjob&lt;/code&gt; changes from &lt;code&gt;PENDING&lt;/code&gt; to &lt;code&gt;SUCCEEDED&lt;/code&gt;&amp;nbsp;and navigate to your &lt;a href="https://us-west-2.console.aws.amazon.com/lambda/home?region=us-west-2#/functions"&gt;AWS Lambda console&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Use&amp;nbsp;&lt;code&gt;LambdaFunctionNamevalue&lt;/code&gt; in the search box to find the Lambda function you deployed with CDK.&lt;/li&gt; 
 &lt;li&gt;Open the Lambda function and navigate to &lt;strong&gt;Monitor&lt;/strong&gt;→ &lt;strong&gt;View CloudWatch logs&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Look for &lt;code&gt;The total reserves value is&lt;/code&gt; line where you can see the aggregated value for the reserves across your workers. Here’s an example of what you can expect:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_2469" style="width: 915px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2469" loading="lazy" class="size-full wp-image-2469" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/06/16/CleanShot-2023-06-16-at-16.46.44.png" alt="Figure 1 - an example of the output from the workload, recorded in CloudWatch logs" width="905" height="192"&gt;
 &lt;p id="caption-attachment-2469" class="wp-caption-text"&gt;Figure 1 – an example of the output from the workload, recorded in CloudWatch logs&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In our test, this job completed in 11 minutes and 40 seconds. Since we used an input dataset of 10 files, each worker processed just one file. We used 10,000 as the &lt;strong&gt;number of simulations per claim&lt;/strong&gt; as a benchmark for actuarial computing in our Rust code.&lt;/p&gt; 
&lt;div id="attachment_2478" style="width: 777px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2478" loading="lazy" class="size-full wp-image-2478" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/06/16/CleanShot-2023-06-16-at-16.58.59.png" alt="Table 1 – Summary of jobs and runtimes." width="767" height="204"&gt;
 &lt;p id="caption-attachment-2478" class="wp-caption-text"&gt;Table 1 – Summary of jobs and runtimes.&lt;/p&gt;
&lt;/div&gt; 
&lt;h1&gt;Pricing&lt;/h1&gt; 
&lt;p&gt;There’s no additional charge for using AWS Batch, and you only pay for resources you use to store or run your workloads. For actuarial modeling with Batch, you can save on your costs by using Reserved Instances or leveraging Savings Plans. You can also use EC2 Spot Instances to realize significant cost reduction by specifying this in your compute-types when setting up your Batch compute environments.&lt;/p&gt; 
&lt;p&gt;To understand the cost efficiency of this solution, you can just calculate a normalized cost per simulation.&lt;/p&gt; 
&lt;p&gt;Pricing is &lt;em&gt;per instance-hour&lt;/em&gt; consumed for each instance (from the time you launch an instance until you terminate it, or stop it). Billing is &lt;em&gt;per-second.&lt;/em&gt; Let’s consider the following cost using an Amazon EC2 C4.2xLarge instance type, with a cost of $0.40 per hour.&lt;/p&gt; 
&lt;div id="attachment_2479" style="width: 677px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2479" loading="lazy" class="size-full wp-image-2479" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/06/16/CleanShot-2023-06-16-at-17.00.44.png" alt="Table 2 – Test results: Number of simulations-per-claim and Normalized compute cost per simulation." width="667" height="210"&gt;
 &lt;p id="caption-attachment-2479" class="wp-caption-text"&gt;Table 2 – Test results: Number of simulations-per-claim and Normalized compute cost per simulation.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_2470" style="width: 790px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2470" loading="lazy" class="size-full wp-image-2470" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/06/16/CleanShot-2023-06-16-at-16.47.38.png" alt="Figure 2 – Computing Cost efficiency improves as the number of simulations-per-claim increases." width="780" height="573"&gt;
 &lt;p id="caption-attachment-2470" class="wp-caption-text"&gt;Figure 2 – Computing Cost efficiency improves as the number of simulations-per-claim increases.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 2 shows that as we increase the number of &lt;em&gt;simulations-per-claim&lt;/em&gt;, the normalized compute cost decreases. By increasing the number of &lt;em&gt;simulation-per-claim&lt;/em&gt;, the actuarial models can significantly improve performance and accuracy while improving the cost efficiency.&lt;/p&gt; 
&lt;h1&gt;Cleaning up:&lt;/h1&gt; 
&lt;p&gt;To clean up, navigate to the &lt;code&gt;infrastructure&lt;/code&gt; directory, and run the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cdk destroy --force&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This process will take somewhere between 10 to 20 minutes and will completely remove all the infrastructure that you provisioned following the steps in this blog post.&lt;/p&gt; 
&lt;p&gt;If you followed this blog using AWS Cloud9, &lt;a href="https://docs.aws.amazon.com/cloud9/latest/user-guide/delete-environment.html"&gt;delete&lt;/a&gt; the environment to stop incurring any charges from that service.&lt;/p&gt; 
&lt;h1&gt;Conclusion&lt;/h1&gt; 
&lt;p&gt;In this post, you learned about how you can use AWS Batch and Amazon FSx for Lustre to speed up actuarial reserves computing for insurance claims using cost-effective batch processing.&lt;/p&gt; 
&lt;p&gt;You evaluated a design that enables simple scaling of the number of concurrent jobs deployed to process a set of input files. Based on the tests results, we determined that running with 10 jobs speeds up the process of actuarial reserves computing by almost 10 times compared to a traditional single batch test. And this resulted in saving both cost &lt;em&gt;and&lt;/em&gt; time for insurance companies. Even better, you saw evidence that using more workers in the actuarial process can speed up processing even more.&lt;/p&gt; 
&lt;p&gt;If you’re interested in learning more about using these tools together, this post just serves as a starting point. We’d love to hear your suggestions or proposed feature improvements, so please create a pull request on our GitHub repo or reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Automate your clusters by creating self-documenting HPC with AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/automate-your-clusters-by-creating-self-documenting-hpc-with-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Matt Vaughn]]></dc:creator>
		<pubDate>Thu, 06 Jul 2023 15:12:15 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Best Practices]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<guid isPermaLink="false">1c830f4c28f55e0d23f1553eed0f3c946f3f4af4</guid>

					<description>Today we're going to show you how you can automate cluster deployment and create self-documenting infrastructure at the same time, which leads to more repeatable results that are easier to manage (and replicate).</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-2522" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/06/23/boofla_over_the_shoulder_shot_of_a_female_engineer_holding_a_di_7f44b14d-fcd7-4e93-adb4-85467ae5b306-2.png" alt="Cloud automation" width="380" height="212"&gt;To build and operate your own HPC cluster in the cloud, you need to connect several types of resources. This includes compute and login nodes, shared filesystems, networking, identity management, and a scheduler, as well as the user-land software environment. AWS ParallelCluster helps make that process easy. You design your cluster, describing it using a specially-formatted file, which you can write by hand or in a web-based graphic user interface. Then, you use this file with AWS ParallelCluster to launch and manage your system.&lt;/p&gt; 
&lt;p&gt;The ParallelCluster web interface and CLI are interactive tools, which means they expect a human present to drive them. Customers have asked to be able to fully automate ParallelCluster so they can embed it in automated workflows, CI/CD pipelines, and their own integrated solutions.&lt;/p&gt; 
&lt;p&gt;We’ve added two new features to explicitly enable this kind of automation. &lt;strong&gt;The first&lt;/strong&gt;, released with ParallelCluster 3.5, is a Python package that encapsulates cluster management functions. &lt;strong&gt;The second&lt;/strong&gt;, new in AWS ParallelCluster 3.6, builds on the Python library to help you to use AWS CloudFormation to manage most aspects of cluster management.&lt;/p&gt; 
&lt;p&gt;In this post, the first in a series covering&amp;nbsp;DevOps with AWS ParallelCluster, we will show you how you can use these capabilities to&amp;nbsp;automate your HPC infrastructure.&lt;/p&gt; 
&lt;h2&gt;The AWS ParallelCluster Python package&lt;/h2&gt; 
&lt;p&gt;ParallelCluster 3.5 features an importable Python library that can programmatically control HPC clusters. Its operations map 1:1 to&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/api-ref-v3.html"&gt;ParallelCluster API&lt;/a&gt; actions. Each management operation and its parameters are just the&amp;nbsp;&lt;code&gt;snake_case&lt;/code&gt;-formatted versions of the &lt;code&gt;pascalCase&lt;/code&gt;&amp;nbsp;methods in the ParallelCuster API.&lt;/p&gt; 
&lt;p&gt;For example, the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/list-clusters.html"&gt;listClusters API&lt;/a&gt;&amp;nbsp;method, which returns all the current user’s clusters, maps to &lt;code&gt;pcluster.list_clusters(region, next_token, cluster_status)&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To illustrate these concepts, let’s walk through some examples that use the Python library.&lt;/p&gt; 
&lt;p&gt;First, install ParallelCluster and its dependencies as we describe in the&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-parallelcluster.html"&gt;ParallelCluster documentation&lt;/a&gt;. Next, provide authentication credentials for your AWS account. We recommend that you &lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html"&gt;install the AWS CLI &lt;/a&gt;and configure it with your preferred AWS credentials as the default profile. However, ParallelCluster uses the &lt;a href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html"&gt;boto3&lt;/a&gt; library to interact with AWS services, so any mechanism supported by boto3, like setting the &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt;&amp;nbsp;and &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt;&amp;nbsp;environment variables, will also work.&lt;/p&gt; 
&lt;h3&gt;Try out the library interactively&lt;/h3&gt; 
&lt;p&gt;Let’s explore some Python library basics and ensure that authentication is set up correctly by trying it out interactively.&lt;/p&gt; 
&lt;p&gt;Launch a Python 3 prompt and import the &lt;code&gt;pcluster&lt;/code&gt; library as shown. Then, use &lt;code&gt;list_official_images()&lt;/code&gt; to retrieve Amazon Machine Images (AMI) supported by the current version of AWS ParallelCluster.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;&amp;gt;&amp;gt;&amp;gt; import pcluster.lib as pc
&amp;gt;&amp;gt;&amp;gt; pc.list_official_images()
{'images': [{'amiId': 'ami-0a7309444dfeeb105', 'os': 'alinux2', 'name': 'aws-parallelcluster-3.6.0-amzn2-hvm-x86_64-202302151337 2023-02-15T13-40-45.464Z', 'version': '3.6.0', 'architecture': 'x86_64'},...]}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, try &lt;code&gt;list_clusters()&lt;/code&gt; to find your HPC systems that are already running in the &lt;code&gt;us-east-2&lt;/code&gt; region.&amp;nbsp;Note how you can pass parameters for the cluster status and region, as defined in the API documentation.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;&amp;gt;&amp;gt;&amp;gt; pc.list_clusters(cluster_status='CREATE_COMPLETE', region='us-east-2')
{'clusters': []}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you already have clusters in the &lt;code&gt;us-east-2&lt;/code&gt; region, you’ll see them listed in place of the empty list shown in the example.&lt;/p&gt; 
&lt;h3&gt;Use the library in a script&lt;/h3&gt; 
&lt;p&gt;Now, let’s write a little program that uses the &lt;code&gt;pcluster&lt;/code&gt; library to&amp;nbsp;create an HPC cluster.&lt;/p&gt; 
&lt;p&gt;First, gather values for two parameters you will need to test out the library:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The public subnet ID where you wish to launch a test HPC cluster (use the &lt;a href="https://console.aws.amazon.com/vpc/#subnets"&gt;Amazon VPC console&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;The name of your preferred SSH login key (use the &lt;a href="https://console.aws.amazon.com/ec2/#KeyPairs"&gt;Amazon EC2 console&lt;/a&gt;)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Export these values to your shell environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;export HEAD_NODE_SUBNET=subnet-abcdef01234567890
export SSH_KEY_NAME=MY-SSH-KEY-NAME
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, create an HPC cluster by creating and running this Python script.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-python"&gt;#!/usr/bin/env python3
import os
import pprint

import pcluster.lib as pc

pp = pprint.PrettyPrinter()

HEAD_NODE_SUBNET = os.environ['HEAD_NODE_SUBNET']
COMPUTE_NODE_SUBNET = os.environ['HEAD_NODE_SUBNET']
KEY_NAME = os.environ['SSH_KEY_NAME']

CONFIG = {'Image': {'Os': 'alinux2'},
          'HeadNode': {'InstanceType': 't2.large',
                       'Networking': {'SubnetId': HEAD_NODE_SUBNET},
                       'Ssh': {'KeyName': KEY_NAME}},
          'Scheduling': {'Scheduler': 'slurm',
                         'SlurmQueues':
                         [{'Name': 'q0',
                           'ComputeResources':
                           [{'Name': 'cr1', 'InstanceType': 't2.micro',
                             'MinCount': 0, 'MaxCount': 10}],
                           'Networking': {'SubnetIds': [COMPUTE_NODE_SUBNET]}}]}}

pp.pprint(
    pc.create_cluster(
        cluster_name='mycluster', cluster_configuration=CONFIG))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this example, the cluster configuration (&lt;code&gt;CONFIG&lt;/code&gt;) is a Python dictionary whose structure maps 1:1 to elements in the AWS ParallelCluster &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/cluster-configuration-file-v3.html"&gt;YAML configuration file&lt;/a&gt;&amp;nbsp;schema. What’s interesting is that we construct&amp;nbsp;&lt;code&gt;CONFIG&lt;/code&gt;&amp;nbsp;programmatically — we insert the name of the SSH key and the subnet IDs for the head node and compute resource using the environment variables we defined before launching the Python interpreter.&lt;/p&gt; 
&lt;p&gt;We could easily extend the example. For instance, we could:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Load the configuration from a template file rather than hard-coding it.&lt;/li&gt; 
 &lt;li&gt;Dynamically generate names for the queues and compute resources.&lt;/li&gt; 
 &lt;li&gt;Look up the public subnet using the &lt;code&gt;boto3&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Pre-validate the cluster configuration by calling &lt;code&gt;&lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/create-cluster.html"&gt;create_cluster&lt;/a&gt;&lt;/code&gt; with &lt;code&gt;dryrun=True&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;When the script runs successfully, it will print details of the new cluster, including its status, to standard output.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;{'cluster': {'cloudformationStackArn': 'arn:aws:cloudformation:us-east-2:000000000000:stack/mycluster/00000000-aaaa-1111-999-000000000000',
             'cloudformationStackStatus': 'CREATE_IN_PROGRESS',
             'clusterName': 'mycluster',
             'clusterStatus': 'CREATE_IN_PROGRESS',
             'region': 'us-east-2',
             'scheduler': {'type': 'slurm'},
             'version': '3.6.0'}}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The cluster status will initially be &lt;code&gt;CREATE_IN_PROGRESS&lt;/code&gt;. You can continue checking its status with &lt;code&gt;pc.describe_cluster(cluster_name='mycluster')&lt;/code&gt;. If the cluster launches successfully, the status will change to &lt;code&gt;CREATE_COMPLETE&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Manually polling for status isn’t very automated. We could add a loop to the example script that uses &lt;code&gt;describe_cluster()&lt;/code&gt; to monitor the cluster launch but this raises a few questions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you don’t want to run your automation code locally, where can it run?&lt;/li&gt; 
 &lt;li&gt;How do you safely get AWS credentials into the automation environment?&lt;/li&gt; 
 &lt;li&gt;How do you handle other situations like cluster updates or deletes?&lt;/li&gt; 
 &lt;li&gt;Can you automatically provision other resources that the cluster depends upon?&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As it turns out, &lt;strong&gt;AWS CloudFormation&lt;/strong&gt; is a popular answer to these questions.&lt;/p&gt; 
&lt;h2&gt;What is AWS CloudFormation?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/cloudformation/"&gt;AWS CloudFormation&lt;/a&gt; is an infrastructure-as-code service. It helps you model, provision, and manage cloud resources, like Amazon Elastic Compute Cloud (EC2) instances, Amazon Simple Storage Service (S3) buckets, and Amazon FSx filesystems in a predictable and repeatable way. It uses template files, which you can manage under source control, to define the infrastructure and configurations required to deploy your application or service.&lt;/p&gt; 
&lt;p&gt;You can use CloudFormation templates to create &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacks.html"&gt;stacks&lt;/a&gt;, which are collections of AWS resources that you can create, update, or delete together. Stacks can be &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html"&gt;nested&lt;/a&gt;, allowing you to compose sophisticated AWS deployments from modular building blocks.&lt;/p&gt; 
&lt;p&gt;For example, you could define your HPC networking, storage, and compute infrastructure in separate CloudFormation stacks, then combine them into a single nested configuration. This could simplify your deployment and management processes quite substantially.&lt;/p&gt; 
&lt;h3&gt;A Primer on AWS CloudFormation templates&lt;/h3&gt; 
&lt;p&gt;A CloudFormation template file defines a stack. You write it in JSON or YAML format and it can have several sections. Here’s a&amp;nbsp;simple example that creates an S3 bucket. Let’s walk through its structure.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;AWSTemplateFormatVersion: '2010-09-09'
Description: &amp;gt;
  Demonstrate creation of an Amazon S3 bucket
  
Parameters:
  BucketName:
    Description: "Name of the S3 bucket"
    Type: String

Resources:
  MyS3Bucket:
    Type: "AWS::S3::Bucket"
    Properties:
      BucketName: !Ref BucketName

Outputs:
  S3BucketARN:
    Description: "ARN of the S3 bucket"
    Value: !GetAtt [MyS3Bucket, Arn]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt; describe the cloud resources you want to create or modify, such as EC2 instances or S3 buckets. Each has a data type. You can choose one of the &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html"&gt;AWS-supported types&lt;/a&gt;, such as&amp;nbsp;&lt;code&gt;AWS::S3::Bucket&lt;/code&gt; or&amp;nbsp;&lt;code&gt;AWS::Batch::ComputeEnvironment&lt;/code&gt;,&amp;nbsp;or a third-party extension available through the AWS CloudFormation Registry. Only resources are mandatory in a CloudFormation template.&lt;/p&gt; 
&lt;p&gt;The &lt;strong&gt;Parameters&lt;/strong&gt; section defines variables that you use to set up the stack.&amp;nbsp;They also have &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html#parameters-section-structure-properties-type"&gt;data type&lt;/a&gt;s. They can be simple types like &lt;code&gt;String&lt;/code&gt; or &lt;code&gt;Number&lt;/code&gt;, but also AWS-specific types like&amp;nbsp;&lt;code&gt;AWS::EC2::Instance::Id&lt;/code&gt; or&amp;nbsp;&lt;code&gt;AWS::EC2::VPC::Id.&lt;/code&gt; We’re naming a storage bucket, so &lt;code&gt;String&lt;/code&gt; is the right data type. If we wanted to, we could add some validation, or set a default value. Parameter values can be referenced elsewhere in the template using the &lt;code&gt;Ref&lt;/code&gt; &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-ref.html"&gt;intrinsic function&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Finally, &lt;strong&gt;Outputs&lt;/strong&gt; are values that the stack returns for use by downstream consumers. They are &lt;em&gt;always&lt;/em&gt; &lt;code&gt;String&lt;/code&gt; types. In this example, we use the &lt;code&gt;&lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html"&gt;GetAtt&lt;/a&gt;&lt;/code&gt; intrinsic function to query the ARN for the newly-created bucket and expose it as&amp;nbsp;&lt;code&gt;&lt;em&gt;S3BucketARN&lt;/em&gt;&lt;/code&gt;. As a human, I can read these outputs, but it’s also possible for other stacks to access them. This is a key way that stacks can be combined together.&lt;/p&gt; 
&lt;p&gt;A template can also include &lt;strong&gt;Mappings&lt;/strong&gt; for table-style value lookups, and &lt;strong&gt;Conditions&lt;/strong&gt; that define conditional statements. We haven’t illustrated them in the current example, though.&lt;/p&gt; 
&lt;h3&gt;Deploying AWS CloudFormation Stacks&lt;/h3&gt; 
&lt;p&gt;You can use the AWS console, the &lt;a href="https://awscli.amazonaws.com/v2/documentation/api/latest/reference/cloudformation/index.html#cli-aws-cloudformation"&gt;AWS Command Line Interface&lt;/a&gt;, or a programming framework like &lt;a href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html"&gt;boto3&lt;/a&gt; to deploy these templates as AWS CloudFormation stacks.&lt;/p&gt; 
&lt;p&gt;Start by saving the example template to a file named &lt;code&gt;template.yml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can deploy it as a stack using the AWS console. Navigate to the AWS CloudFormation service. Choose &lt;strong&gt;Create stack/With new resources&lt;/strong&gt;. Then, at &lt;strong&gt;Specify template&lt;/strong&gt;, choose &lt;strong&gt;Upload file&lt;/strong&gt;. Select &lt;code&gt;template.yml&lt;/code&gt; for upload, then choose &lt;strong&gt;Next&lt;/strong&gt;. Under &lt;strong&gt;Specify stack details&lt;/strong&gt;, provide a name for the stack, such as &lt;em&gt;demos3&lt;/em&gt;. Under &lt;strong&gt;Parameters&lt;/strong&gt;, provide a distinctive name for the new storage bucket. Choose &lt;strong&gt;Next &lt;/strong&gt;twice more, then choose &lt;strong&gt;Submit &lt;/strong&gt;to create the stack.&lt;/p&gt; 
&lt;div id="attachment_2523" style="width: 927px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2523" loading="lazy" class="size-full wp-image-2523" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/06/23/CleanShot-2023-06-23-at-13.52.54.png" alt="Figure 1. AWS CloudFormation console view of a stack that creates an S3 bucket" width="917" height="417"&gt;
 &lt;p id="caption-attachment-2523" class="wp-caption-text"&gt;Figure 1. AWS CloudFormation console view of a stack that creates an S3 bucket&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 1 shows the CloudFormation console after deploying a stack named demos3 from our example template. We’ve selected the &lt;strong&gt;Outputs&lt;/strong&gt; tab to highlight the output&amp;nbsp;&lt;code&gt;&lt;em&gt;S3BucketARN&lt;/em&gt;&lt;/code&gt; and its value. The &lt;strong&gt;Events&lt;/strong&gt; tab shows all actions taken by AWS CloudFormation during deployment of the stack, and &lt;strong&gt;Parameters&lt;/strong&gt; shows the variables we used to configure the stack at creation.&lt;/p&gt; 
&lt;p&gt;If you wanted to deploy this stack without using a GUI, you could use the AWS CLI. Using the same &lt;code&gt;template.yml&lt;/code&gt; file, you’d run this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws cloudformation deploy –stack-name demos3 \
  --parameter-overrides BucketName=USERNAME-demo-s3 \
  --template-file ./template.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Similarly, you could use the boto3 Python library, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-python"&gt;#!/usr/bin/env python3
import boto3
client = boto3.client('cloudformation')
with open("./template.yml", "r") as tfile:
    tbody = tfile.read()
    client.create_stack(
        StackName="demos3",
        TemplateBody=tbody,
        Parameters=[{
            "ParameterKey": "BucketName",
            "ParameterValue": "USERNAME-demo-s3"
        }]
    )
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Change is good … and controlled, too&lt;/h2&gt; 
&lt;p&gt;Now, let’s say you want to change the configuration of this new Amazon S3 bucket, like its encryption or tag settings. You’d first make the &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-s3-bucket.html"&gt;relevant changes&lt;/a&gt; to the resource in &lt;code&gt;template.yml&lt;/code&gt;. Then, you would re-deploy using any of the interfaces shown above, being sure to use the same &lt;strong&gt;StackName&lt;/strong&gt; value to ensure it’s treated as an update not a request to launch a new stack.&lt;/p&gt; 
&lt;p&gt;CloudFormation will determine if the resources themselves can be updated in place, or need to be recreated with the new properties. It’ll then take the appropriate actions on your behalf to make the infrastructure match your definition written in code.&lt;/p&gt; 
&lt;p&gt;Should you want to delete the Amazon S3 bucket, you can simply delete the stack. AWS CloudFormation will delete all the associated resources for you. This can be helpful when there are many interdependent resources associated with the stack!&lt;/p&gt; 
&lt;p&gt;For a more comprehensive introduction to CloudFormation, you can consult the &lt;a href="https://docs.aws.amazon.com/cloudformation/"&gt;online documentation&lt;/a&gt;, which offers a general user guide, an API reference, and a command-line guide.&lt;/p&gt; 
&lt;h2&gt;Using AWS ParallelCluster with AWS CloudFormation&lt;/h2&gt; 
&lt;p&gt;To manage AWS ParallelCluster in a CloudFormation template, we define our HPC cluster as an AWS CloudFormation resource.&lt;/p&gt; 
&lt;p&gt;However, ParallelCluster isn’t a managed AWS service, and so it doesn’t have an official CloudFormation resource type. What we do instead is create the cluster from a CloudFormation &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html"&gt;custom resource&lt;/a&gt;. The custom resource, in turn, relies on a “provider” stack that we deploy using a template published by the AWS ParallelCluster team.&lt;/p&gt; 
&lt;p&gt;This may sound a little Inception-like, but it makes sense. The provider stack combines the&amp;nbsp;ParallelCluster Python library and additional management logic to implement asynchronous create, update, delete, and list operations for our AWS ParallelCluster custom resource. Details of how it works are in the&amp;nbsp;&lt;a href="http://LINKY-LINKY"&gt;ParallelCluster documentation&lt;/a&gt;, and we’d encourage you to be familiar with that before you build your empire on this. Today, we’ll just focus on how to use it.&lt;/p&gt; 
&lt;h3&gt;A Simple Cluster&lt;/h3&gt; 
&lt;p&gt;Let’s start with an example CloudFormation template for a simple cluster.&lt;/p&gt; 
&lt;p&gt;The template defines two resources: &lt;code&gt;PclusterClusterProvider&lt;/code&gt;, which is an instance of the provider stack, and &lt;code&gt;PclusterCluster&lt;/code&gt;, which is our HPC cluster. It has three parameters:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;HeadNodeSubnet&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ComputeNodeSubnet&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;KeyName&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The subnet parameters define where the head node and compute instances will be launched. The key name is an Amazon EC2 SSH key you can use to access the cluster once it’s running. To keep things simple, we hard-code options like instance type and the maximum size of the cluster in the file, but you can choose to set them using parameters as well.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;AWSTemplateFormatVersion: '2010-09-09'
Description: AWS ParallelCluster CloudFormation Template

Parameters:
  HeadNodeSubnet:
    Description: Subnet where the HeadNode will run
    Type: AWS::EC2::Subnet::Id
  ComputeSubnet:
    Description: Subnet where the Compute Nodes will run
    Type: AWS::EC2::Subnet::Id
  KeyName:
    Description: KeyPair to login to the head node
    Type: AWS::EC2::KeyPair::KeyName

Resources:
  PclusterClusterProvider:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: !Sub
        - https://${AWS::Region}-aws-parallelcluster.s3.${AWS::Region}.${AWS::URLSuffix}/parallelcluster/${Version}/templates/custom_resource/cluster.yaml
        - { Version: 3.6.0, Region: !Ref AWS::Region }

  PclusterCluster:
    Type: Custom::PclusterCluster
    Properties:
      ServiceToken: !GetAtt [ PclusterClusterProvider , Outputs.ServiceToken ]
      DeletionPolicy: Retain
      ClusterName: !Sub 'c-${AWS::StackName}'
      ClusterConfiguration:
        Image:
          Os: alinux2
        HeadNode:
          InstanceType: t2.medium
          Networking:
            SubnetId: !Ref HeadNodeSubnet
          Ssh: 
            KeyName: !Ref KeyName
        Scheduling:
          Scheduler: slurm
          SlurmSettings:
            QueueUpdateStrategy: DRAIN
          SlurmQueues:
            - Name: q1
              ComputeResources:
                - Name: cr1
                  InstanceType: t2.micro
                  MinCount: 0
                  MaxCount: 4
              Networking:
                SubnetIds:
                  - !Ref ComputeSubnet
Outputs:
  HeadNodeIp:
    Description: The Public IP address of the HeadNode
    Value: !GetAtt [ PclusterCluster, headNode.publicIpAddress ]
  ValidationMessages:
    Description: Warnings from cluster create or update operations.
    Value: !GetAtt PclusterCluster.validationMessages

&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We establish the cluster configuration in &lt;code&gt;ClusterConfiguration&lt;/code&gt;. This is a YAML dictionary whose structure corresponds to the AWS ParallelCluster &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/cluster-configuration-file-v3.html"&gt;configuration file&lt;/a&gt;. Notice how we incorporate the values from template parameters into it – that’s one of the benefits of AWS CloudFormation.&lt;/p&gt; 
&lt;p&gt;The &lt;strong&gt;cluster&lt;/strong&gt; resource depends on the &lt;strong&gt;provider&lt;/strong&gt; resource. The relationship between them is defined by the &lt;code&gt;ServiceToken&lt;/code&gt; property of &lt;code&gt;PclusterCluster&lt;/code&gt;, which is set to an output from the &lt;code&gt;PclusterClusterProvider&lt;/code&gt;. CloudFormation resolves this dependency by creating the provider stack before attempting to create the HPC cluster (you can find out more about service tokens in the &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html#aws-resource-cfn-customresource-properties"&gt;CloudFormation documentation&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Finally, there are two outputs. The first is &lt;code&gt;HeadNodeIp&lt;/code&gt;, the public IP address you can use to access the head node. The second is &lt;code&gt;ValidationMessages&lt;/code&gt;, which is a list of warnings or errors that arise when the cluster is created.&lt;/p&gt; 
&lt;h3&gt;Launch a cluster with an AWS CloudFormation template&lt;/h3&gt; 
&lt;p&gt;You can use the example template to create a simple HPC cluster.&amp;nbsp;Save it as a file named (for example) &lt;code&gt;cluster.yml&lt;/code&gt;. As with the Amazon S3 demo, you can use the &lt;a href="https://console.aws.amazon.com/cloudformation/"&gt;AWS CloudFormation console&lt;/a&gt; to launch the stack with &lt;code&gt;cluster.yml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;At the end of the stack launch workflow, you’ll be asked to acknowledge that, in order to create your stack, CloudFormation may create IAM resources with custom names, &lt;em&gt;and&lt;/em&gt; that it requires &lt;code&gt;CAPABILITY_AUTO_EXPAND&lt;/code&gt;. The former means that the stack could result in new IAM resources. You can review any that it creates in the stack’s &lt;strong&gt;Resources&lt;/strong&gt; or in the AWS Console &lt;a href="https://console.aws.amazon.com/iamv2/"&gt;Identity and Access Dashboard&lt;/a&gt;. The latter means that the &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_UpdateStack.html"&gt;stack template contains macros&lt;/a&gt;, meaning that some aspects of the stack get computed dynamically. Both are quite standard capabilities, but CloudFormation asks that you consent to them in the interest of transparency and cost control.&lt;/p&gt; 
&lt;div id="attachment_2524" style="width: 971px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2524" loading="lazy" class="size-full wp-image-2524" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/06/23/CleanShot-2023-06-23-at-13.57.56.png" alt="Figure 2: Example AWS ParallelCluster stack deployed with AWS CloudFormation" width="961" height="493"&gt;
 &lt;p id="caption-attachment-2524" class="wp-caption-text"&gt;Figure 2: Example AWS ParallelCluster stack deployed with AWS CloudFormation&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;You can follow the progress of your cluster deployment in the AWS CloudFormation console (Figure 2). Once the stack status reaches&amp;nbsp;&lt;code&gt;CREATE_COMPLETE&lt;/code&gt;, you can inspect the various new resources it created.&lt;/p&gt; 
&lt;p&gt;The provider (1) and HPC cluster resources (4) will be in the &lt;strong&gt;Resources&lt;/strong&gt; section. The management resource creates two nested stacks (2, 3), but those don’t have any parameters you need to configure. You can consider them pure infrastructure and ignore them. The actual HPC cluster will be an independent AWS CloudFormation stack (5), since that’s how ParallelCluster implements clusters in the cloud.&lt;/p&gt; 
&lt;p&gt;In the Outputs (6) section, you’ll find important details like the IP address for the head node and warnings or errors that might have occurred during cluster creation.&lt;/p&gt; 
&lt;p&gt;Deploying the cluster template using the AWS CLI is like what we just did for the Amazon S3 example.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws cloudformation deploy --stack-name cfn-demo \
  --parameter-overrides KeyName=SSH-KEY-NAME \
                        HeadNodeSubnet=subnet-0123456789 \
                        ComputeSubnet=subnet-9876543210 \
  --template-file ./cluster.yml \
  --capabilities CAPABILITY_NAMED_IAM CAPABILITY_AUTO_EXPAND
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this case, the parameter overrides refer to different key names in the cluster template, and the the &lt;code&gt;--capabilities&lt;/code&gt; option allows additional capabilities. The mapping between AWS CLI and the Python boto3 library is like the Amazon S3 example, too, so we’ve left it out – for the sake of brevity.&lt;/p&gt; 
&lt;h3&gt;Updating the cluster configuration&lt;/h3&gt; 
&lt;p&gt;You can update many (but not all) aspects of your cluster configuration using CloudFormation. Essentially, if we’ve documented an attribute in the ParallelCluster configuration file to be changeable after cluster creation, you should be able to update it by changing the CloudFormation template and re-deploying the cluster.&lt;/p&gt; 
&lt;p&gt;We recommend you do two things before you attempt to update your cluster.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, set your cluster’s queue update strategy to &lt;code&gt;DRAIN&lt;/code&gt; or &lt;code&gt;TERMINATE&lt;/code&gt;. Once this setting is in place several attributes in the cluster configuration can be dynamically updated without you needing to pause the fleet. &lt;strong&gt;Second&lt;/strong&gt;, carefully review the documentation for the ParallelCluster &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/cluster-configuration-file-v3.html"&gt;configuration options&lt;/a&gt; you wish to change, including the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/using-pcluster-update-cluster-v3.html#update-policy-fail-v3"&gt;specifics of their update policies&lt;/a&gt;. This will help you interpret and respond to any messages or unexpected behaviors you encounter in updating your system.&lt;/p&gt; 
&lt;h3&gt;Debugging cluster operations&lt;/h3&gt; 
&lt;p&gt;The CloudFormation template is a high-level orchestrator for your HPC cluster deployment. If there are issues at deployment, CloudFormation will capture and expose them in the &lt;code&gt;ValidationMessages&lt;/code&gt; output.&lt;/p&gt; 
&lt;p&gt;Examples errors you might find there are syntax errors in the &lt;code&gt;ClusterConfiguration&lt;/code&gt; field, time-out errors for the head node to boot up, and so on. Once the cluster has fully launched, you can monitor and debug it via its &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/cloudwatch-dashboard-v3.html"&gt;Amazon CloudWatch dashboard&lt;/a&gt; and the log files that are accessible on your cluster’s head node.&lt;/p&gt; 
&lt;h3&gt;Deleting the cluster&lt;/h3&gt; 
&lt;p&gt;Just like with the Amazon S3 example, you can delete your cluster deployment by deleting the AWS CloudFormation stack that created it. For the simple case we’ve illustrated in this article, it makes sense to delete the cluster provider &lt;em&gt;and&lt;/em&gt; the cluster. They were just for illustration and you likely no longer need any of the resources they created.&lt;/p&gt; 
&lt;p&gt;As you build more sophisticated AWS CloudFormation deployments, be aware that you can set specific stacks to be &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-protect-stacks.html"&gt;protected from termination&lt;/a&gt;. This in turn prevents their resources from being deleted, too. For example, if your stack can optionally create an Amazon FSx filesystem, you may wish to protect it when you’re deleting the cluster itself. The same might be true for Amazon Aurora databases you used to power Slurm accounting, or Amazon Cognito user pools used by the ParallelCluster UI.&lt;/p&gt; 
&lt;h3&gt;Using quick-create links&lt;/h3&gt; 
&lt;p&gt;If you create a cluster template that you’d like other people to be able to use, you can take advantage of CloudFormation quick-create links. These are URLs that streamline deployment of CloudFormation stacks by predefining parameters and configurations. They provide a simplified way to share and start stack deployments.&lt;/p&gt; 
&lt;p&gt;You can find more on quick-create links in the &lt;a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-console-create-stacks-quick-create-links.html"&gt;CloudFormation documentation&lt;/a&gt;. Briefly: you create a template file, upload it to an Amazon S3 bucket, and make it accessible either to your target user or the general public. Next, you construct the quick-create link, which includes that Amazon S3 URL, a name for the stack, and any parameters you want to pre-populate. Finally, you share the link by whatever means you wish to. Users can simply click on it to begin deploying your stack in their own AWS account.&lt;/p&gt; 
&lt;p&gt;This is an incredibly useful pattern for offering others access to identical resources you’ve used to pioneer a solution to a problem.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this article, we’ve shown you how to automated deployment and management of HPC infrastructure on AWS. We started by demonstrating the AWS ParallelCluster Python library, then introduced AWS CloudFormation as a tool for modeling and managing cloud resources. We then showed an example of using the newly-released integration between ParallelCluster and CloudFormation to deploy and manage simple cluster from a template file. Finally, we introduced quick-create links, which help streamline sharing and deployment of CloudFormation stacks, so you can help others use the solutions you’ve created.&lt;/p&gt; 
&lt;p&gt;There’s a lot more to learn, and we’ll cover this in future posts.&lt;/p&gt; 
&lt;p&gt;While you’re waiting on these, try &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/tutorials_09_cfn-custom-resource-v3.html"&gt;launching a cluster using AWS CloudFormation&lt;/a&gt; and let us know how we can improve the experience. You can reach us on Twitter at &lt;a href="https://twitter.com/techhpc/"&gt;@TechHPC&lt;/a&gt; or by email at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Running protein structure prediction at scale using a web interface for researchers</title>
		<link>https://aws.amazon.com/blogs/hpc/running-protein-structure-prediction-at-scale-using-a-web-interface-for-researchers/</link>
		
		<dc:creator><![CDATA[Chiaki Ishio]]></dc:creator>
		<pubDate>Wed, 05 Jul 2023 14:58:09 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Life Sciences]]></category>
		<category><![CDATA[Drug Discovery]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[Protein Folding]]></category>
		<guid isPermaLink="false">5ce79e678bb6dcf7f451ddf276cec5303fa160ee</guid>

					<description>Today, we'll show you our open-source sample implementation of a web frontend and cloud HPC backend to support researchers using AI tools like AlphaFold for drug discovery and design.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-2422" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/06/09/AdobeStock_373127444-2.jpeg" alt="Protein enzymes fold into their structure to fulfill their function - 3d illustration" width="380" height="208"&gt;This post was contributed by Chiaki Ishio, Solutions Architect, Daisuke Miyamoto, Senior Specialist Solutions Architect, Compute/HPC, Shingo Chiyoda, Solutions Architect, Daiki Kuriyama, Senior Prototyping Engineer, AWS Japan&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Unraveling the proteins structure is crucial not only for understanding its function, but also for drug discovery. Protein structure prediction software like &lt;a href="https://github.com/deepmind/alphafold"&gt;AlphaFold2&lt;/a&gt; and &lt;a href="https://github.com/aqlaboratory/openfold"&gt;OpenFold&lt;/a&gt; have become indispensable to researchers since their introduction in 2021. However, researchers may find it complicated to build an environment to run this software &lt;em&gt;and&lt;/em&gt; manage the hardware. IT admins also may find it laborious to secure and control the computing resources for the researchers.&lt;/p&gt; 
&lt;p&gt;In this blog post, we present &lt;a href="https://github.com/aws-samples/alphafold-protein-structure-prediction-with-frontend-app"&gt;a sample implementation&lt;/a&gt; to solve these problems. It consists of a purpose-built web frontend and a cloud HPC backend environment. Using this implementation, researchers can run protein structure prediction jobs at the backend HPC cluster through an easy-to-use web frontend.&lt;/p&gt; 
&lt;p&gt;IT admins can reduce the time and effort required to prepare computing resources by taking advantage of a scalable HPC environment, and they can also easily manage permissions by restricting user access to the web interface. In this way, the cloud is an especially productive environment for HPC, because we can combine a variety of interfaces with serious computing horsepower, depending on the purposes and our users’ needs.&lt;/p&gt; 
&lt;h2&gt;Overview of our sample application&lt;/h2&gt; 
&lt;p&gt;In this section, we’ll take a look at a &lt;a href="https://github.com/aws-samples/alphafold-protein-structure-prediction-with-frontend-app"&gt;sample application published as an AWS Samples&lt;/a&gt; (see the link for the deployment method).&lt;/p&gt; 
&lt;p&gt;This implementation provides a web application, as shown in Figure 1. Through this web frontend, you can easily run three-dimensional protein structure prediction using AlphaFold2. To run a job, input the FASTA format text at the screen top and click the ‘Create Job’ button. The job execution status is displayed in the list below the text field. You can also visualize the results of completed jobs and download the results in PDB format.&lt;/p&gt; 
&lt;div id="attachment_2415" style="width: 905px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2415" loading="lazy" class="size-full wp-image-2415" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/06/09/CleanShot-2023-06-09-at-11.16.48.png" alt="Figure 1 – A sample web frontend application. Researchers can enter FASTA format text into a simple form and obtain results of the protein structure prediction following a short wait." width="895" height="851"&gt;
 &lt;p id="caption-attachment-2415" class="wp-caption-text"&gt;Figure 1 – A sample web frontend application. Researchers can enter FASTA format text into a simple form and obtain results of the protein structure prediction following a short wait.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Architecture: HPC cluster integrated with web frontend&lt;/h2&gt; 
&lt;p&gt;What is happening behind the scenes when you use the web frontend above? We’ll illustrate the process using an architectural diagram (Figure 2). Note that the numbers in the following text match the numbers in the diagram.&lt;/p&gt; 
&lt;p&gt;When you interact with the web frontend, it sends an instruction to the compute environment via an API (1). For example, when you start a job with a FASTA format text, the file gets stored in an S3 bucket via &lt;a href="https://aws.amazon.com/api-gateway/"&gt;Amazon API Gateway&lt;/a&gt; and &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; (2).&lt;a href="https://aws.amazon.com/systems-manager/"&gt; Also, via &lt;u&gt;AWS Systems Manager&lt;/u&gt;, the AlphaFold2 script is executed&lt;/a&gt; on the HPC cluster managed by&lt;a href="https://aws.amazon.com/hpc/parallelcluster"&gt; &lt;u&gt;AWS ParallelCluster&lt;/u&gt; &lt;/a&gt;(3). The databases required to run AlphaFold2 are stored in &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Amazon FSx for Lustre&lt;/a&gt;, a distributed file system which offers rapid access from the HPC cluster (4). Since FSx for Lustre synchronizes bi-directionally with the S3 bucket (5), FASTA files in the S3 bucket can be accessed from the Lustre file system. Finally, we use &lt;a href="https://slurm.schedmd.com/documentation.html"&gt;Slurm&lt;/a&gt; to manage jobs in ParallelCluster, and &lt;a href="https://aws.amazon.com/rds/aurora/serverless/"&gt;Amazon Aurora Serverless v1&lt;/a&gt; to store the job execution history (6).&lt;/p&gt; 
&lt;div id="attachment_2416" style="width: 1329px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-2416" loading="lazy" class="size-full wp-image-2416" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2023/06/09/CleanShot-2023-06-09-at-11.18.24.png" alt="Figure 2 — Architectural diagram of a web application for predicting protein structures. When a FASTA file is submitted from the web frontend, the job is submitted to AWS ParallelCluster via Amazon API Gateway/AWS Lambda." width="1319" height="674"&gt;
 &lt;p id="caption-attachment-2416" class="wp-caption-text"&gt;Figure 2 — Architectural diagram of a web application for predicting protein structures. When a FASTA file is submitted from the web frontend, the job is submitted to AWS ParallelCluster via Amazon API Gateway/AWS Lambda.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We’ve walked through the detailed architecture, but application users don’t need to be aware of the actual mechanisms behind the scenes. If you’re a researcher and you want to try protein structure prediction on the cloud, but have no access to the AWS management console in your organization, you can still use this web application.&lt;/p&gt; 
&lt;p&gt;Best of all, if you’re not familiar with HPC operations, you can still run the AlphaFold2 job via the GUI.&lt;/p&gt; 
&lt;h2&gt;Deep Dive on components in the architecture&lt;/h2&gt; 
&lt;p&gt;So far, we’ve explained the architecture at a high level. In this section, we’ll pick up some AWS services and components in this application.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;u&gt;AWS CDK:&lt;/u&gt;&lt;/strong&gt; The entire web application is implemented using the &lt;a href="https://aws.amazon.com/cdk/"&gt;AWS CDK (Cloud Development Kit)&lt;/a&gt;, a tool that allows you to define a cloud application as code. If you’re the IT admin, you can build this almost automatically. First, you deploy the backend and frontend with the CDK, connect the two, then launch an HPC cluster and configure a protein structure prediction tool (detail steps are &lt;a href="https://github.com/aws-samples/alphafold-protein-structure-prediction-with-frontend-app"&gt;described in the sample repository&lt;/a&gt;) to complete the entire application.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;u&gt;AWS ParallelCluster:&lt;/u&gt;&lt;/strong&gt; Now, we’ll take a closer look at the options for building the HPC cluster environment. Both &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; and &lt;a href="https://github.com/aws-samples/alphafold-protein-structure-prediction-with-frontend-app"&gt;AWS Batch&lt;/a&gt; are suitable compute environment options for running protein structure prediction software as in this blog post. Here’s a brief recap of each service: ParallelCluster makes it easy for you to deploy and manage HPC clusters on AWS, and is recommended for those who are familiar with any HPC cluster environments. Meanwhile, AWS Batch allows you to run batch computing jobs at any scale, and it automatically provisions computing resources. It is recommended for users who are comfortable using containers. Based on the characteristics of these two options, the reason we chose ParallelCluster is flexibility. That is, even if new software other than AlphaFold2 or ColabFold are released in the future, they won’t always immediately support container environments; ParallelCluster is useful in these kinds of cases.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;u&gt;Cost:&lt;/u&gt;&lt;/strong&gt; Cost is also an important perspective when considering any architecture. The sample application described above includes two types of costs: recurring and job-execution based. The breakdown of each cost type is described in the next paragraph (note that these cost calculations are based on US East (Northern Virginia) Region pricing as of the publish date of this blog post).&lt;/p&gt; 
&lt;p&gt;First, the recurring cost is &lt;a href="https://calculator.aws/#/estimate?id=fd5b780e164920cb80c0cc009b68e85ddc9da2a4"&gt;about $290 per month&lt;/a&gt;, mostly due to FSx for Lustre. In this sample application, we applied the data compression setting in order to minimize the cost of FSx for Lustre. The remainder of the cost is associated with ParallelCluster: an EC2 instance acting as the head node, a NAT gateway, and the Aurora Serverless v1 that manages the job history. If you choose Batch instead of ParallelCluster for your HPC environment, you could reduce the recurring cost associated with ParallelCluster’s head node.&lt;/p&gt; 
&lt;p&gt;Next, let’s look at the cost incurred &lt;em&gt;per-job&lt;/em&gt; for execution. For example, using g4dn.2xlarge for the GPU instance costs 0.752 USD per hour. The cost is proportional to the time required to run the job. Note that unit pricing for AWS services, including EC2 instances, varies by region. You can consider selecting a suitable region to optimize your costs.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog post, we described a sample implementation of a web application that can run protein structure prediction jobs with AlphaFold2 via GUI, using AWS ParallelCluster as backend. This web application is built to be easy for researchers to use &lt;em&gt;and&lt;/em&gt; for IT admins to manage the infrastructure.&lt;/p&gt; 
&lt;p&gt;Also, as we discussed, you can use this implementation interchangeably with other protein structure prediction software. That is, you can replace the backend software with alternatives, like &lt;a href="https://github.com/HeliXonProtein/OmegaFold"&gt;OmegaFold&lt;/a&gt; and &lt;a href="https://github.com/aqlaboratory/openfold"&gt;OpenFold&lt;/a&gt;. And you can customize and extend the frontend, like offering users options for which software to use for the protein structure prediction jobs.&lt;/p&gt; 
&lt;p&gt;If you’re an IT admin, you might want to add a user-management mechanism with Amazon Cognito or a cost capping mechanism using AWS Budgets. AWS allows you to combine these services and build an HPC environment in the form of a solution. We hope you will bring your own ideas to improve this implementation to build an even better research environment.&lt;/p&gt; 
&lt;p&gt;For more background on the protein folding topic generally, check out some of our other posts, including “&lt;a href="https://aws.amazon.com/jp/blogs/industries/predicting-protein-structures-at-scale-using-aws-batch/"&gt;Predicting protein structures at scale using AWS Batch (architecture for running RoseTTAFold on AWS Batch)&lt;/a&gt;” and the one about &lt;a href="https://aws.amazon.com/jp/blogs/hpc/optimize-protein-folding-costs-with-openfold-on-aws-batch/"&gt;optimizing protein folding costs with OpenFold on AWS Batch (benchmark results)&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Acknowledgements&lt;/h3&gt; 
&lt;p&gt;Special thanks go to the following members at AWS Japan for their contributions in building the sample application presented in this post: &lt;em&gt;Judge Saki Ito (Solutions Architect), Tomochika Kato (Solutions Architect), Fuminori Abe (Solutions Architect), Hitoshi Anji (Manager, Solutions Architect), Hokuto Akimoto (Account Manager), Takehiro Suzuki (Prototyping Engineer), and Hiroshi Kobayashi (HPC Specialist Solutions Architect).&lt;/em&gt;&lt;/p&gt;</content:encoded>
					
		
		
			</item>
	</channel>
</rss>