<?xml version="1.0" encoding="UTF-8" standalone="no"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:slash="http://purl.org/rss/1.0/modules/slash/" xmlns:sy="http://purl.org/rss/1.0/modules/syndication/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" version="2.0">

<channel>
	<title>AWS HPC Blog</title>
	<atom:link href="https://aws.amazon.com/blogs/hpc/feed/" rel="self" type="application/rss+xml"/>
	<link>https://aws.amazon.com/blogs/hpc/</link>
	<description>Just another Amazon Web Services site</description>
	<lastBuildDate>Wed, 24 Jul 2024 15:56:23 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	
	<item>
		<title>Deploying generative AI applications with NVIDIA NIMs on Amazon EKS</title>
		<link>https://aws.amazon.com/blogs/hpc/deploying-generative-ai-applications-with-nvidia-nims-on-amazon-eks/</link>
		
		<dc:creator><![CDATA[Abhishek Sawarkar]]></dc:creator>
		<pubDate>Wed, 24 Jul 2024 15:56:23 +0000</pubDate>
				<category><![CDATA[Amazon Machine Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[MPI]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">864e8d92882395e358a5517106e0e927701768ae</guid>

					<description>Learn how to deploy AI models at scale with @AWS using NVIDIA's NIM and Amazon EKS! This step-by-step guide shows you how to create a GPU cluster for inference. Don't miss part 1 of this 2-part blog series!</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img class="alignright size-full wp-image-4072" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/24/Deploying-generative-AI-applications-with-NVIDIA-NIMs-on-Amazon-EKS-copy.png" alt="" width="380" height="212"&gt;This post was contributed by Abhishek Sawarkar (NVIDIA), Alex Iankoulski (AWS), Aman Shanbhag (AWS), Deepika Padmanabhan (NVIDIA), Jiahong Liu (NVIDIA), Joey Chou (AWS)&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Today, we’ll take you through a step-by-step -guide to help you to create a cluster of g5.48xlarge instances, accelerated by 8 x NVIDIA A10G Tensor Core GPUs, using Amazon Elastic Kubernetes Service (Amazon EKS) to host your inference solution via &lt;a href="https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/"&gt;NVIDIA NIM&lt;/a&gt;, NVIDIA’s optimized microservices for deploying AI models at scale. This blog is part 1 of a 2-part series.&lt;/p&gt; 
&lt;p&gt;Amazon EKS is a managed service for running Kubernetes workloads on AWS. We can use EKS to orchestrate NVIDIA NIM (plural: NIMs) pods across multiple nodes, because it automatically manages the availability and scalability of the Kubernetes control plane nodes responsible for scheduling containers, managing application availability, storing cluster data, and other key tasks. EKS also integrates with AWS networking, security, and infrastructure – leveraging all the performance, scale, reliability, &lt;em&gt;and&lt;/em&gt; availability of AWS infrastructure. You can find all the details in the &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html"&gt;EKS documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To follow along, you can find the code in our &lt;code&gt;awsome-inference&lt;/code&gt; &lt;a href="https://github.com/aws-samples/awsome-inference"&gt;GitHub repository&lt;/a&gt;. This repo contains reference architectures and test cases for inference on AWS. The examples cover a variety of models, use-cases, and frameworks for inference optimization.&lt;/p&gt; 
&lt;h2&gt;Introducing NVIDIA NIM&lt;/h2&gt; 
&lt;p&gt;NVIDIA NIM (or just NIM for the rest of this post) is part of the NVIDIA AI Enterprise, available on the &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-ozgjkov6vq3l6"&gt;AWS Marketplace&lt;/a&gt;. NIM is a set of easy-to-use microservices designed for secure, reliable deployment of high-performance AI model inference supporting the next generation of world-class &lt;a href="https://developer.nvidia.com/generative-ai"&gt;generative AI &lt;/a&gt;applications. NIM offers developers a number of benefits including ease of use, performance at scale, and security. For more information on NIM, check out the &lt;a href="https://docs.nvidia.com/nim/large-language-models/latest/introduction.html?nvid=nv-int-tblg-432774"&gt;NIM Documentation&lt;/a&gt; or &lt;a href="https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html"&gt;Getting Started&lt;/a&gt; pages.&lt;/p&gt; 
&lt;h3&gt;Ease of use&lt;/h3&gt; 
&lt;p&gt;NIM speeds up time-to-market with pre-built, cloud-native containerized microservices that are continuously maintained to deliver optimized inference on NVIDIA GPU accelerated instances.&lt;/p&gt; 
&lt;h3&gt;Performance and scale&lt;/h3&gt; 
&lt;p&gt;NIM can improve the Total Cost of Ownership (TCO) by providing low latency, high throughput inference for AI applications, that scales on EKS depending upon the number of incoming requests. We’ll cover NIM performance and benchmarking in-depth in a future blog post. If you’re interested in learning more about the performance advantage of NIM now, you can check out &lt;a href="https://developer.nvidia.com/blog/nvidia-collaborates-with-hugging-face-to-simplify-generative-ai-model-deployments/"&gt;NVIDIA’s blog with Hugging Face&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Security&lt;/h3&gt; 
&lt;p&gt;By hosting NIM on EKS, you can leverage enterprise-grade software with dedicated feature branches, security patching, rigorous validation processes, and enterprise support, including access to NVIDIA AI experts. This way, you can maintain security, and control, of your generative AI applications.&lt;/p&gt; 
&lt;p&gt;To learn more about hosting on AWS, check out &lt;a href="https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing"&gt;Amazon EC2 Accelerated Computing Instances&lt;/a&gt;, &lt;a href="https://aws.amazon.com/about-aws/whats-new/2024/03/amazon-sagemaker-integration-nvidia-nim-microservices/"&gt;Amazon SageMaker&lt;/a&gt;, and Amazon EKS. You can also explore the latest community-built AI models optimized and accelerated by NVIDIA NIM on NVIDIA AI.&lt;/p&gt; 
&lt;p&gt;This post focuses on Amazon EKS, and walks you through deploying the Llama3-8B NIM on a single node of &lt;a href="https://aws.amazon.com/ec2/instance-types/g5/"&gt;Amazon EC2 G5 instances&lt;/a&gt;. The g5.48xlarge instance is powered by 8 x &lt;a href="https://www.nvidia.com/en-us/data-center/products/a10-gpu/"&gt;NVIDIA A10G Tensor Core GPUs&lt;/a&gt;. To learn more about g5 EC2 instances, check out &lt;a href="https://aws.amazon.com/ec2/instance-types/g5/"&gt;Amazon EC2 G5 Instances&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You &lt;em&gt;can&lt;/em&gt; also use a smaller g5 instance (for example, the g5.12xlarge). The default profile of the Llama3-8B NIM is fp16, which means it takes up 16GB of GPU memory just for the model. If you want to follow along, we recommend having at least 24 GB of GPU memory so you can fully leverage the optimization benefits provided by NVIDIA NIM. To learn more about NIM &lt;em&gt;profiles&lt;/em&gt;, check out the &lt;a href="#_Deploying_the_NIM"&gt;Deploying the NIM&lt;/a&gt; section in this post.&lt;/p&gt; 
&lt;h2&gt;Steps to deploy the Llama3-8B NIM onto a single g5.48xlarge instance&lt;/h2&gt; 
&lt;h3&gt;Architecture: What you will be deploying&lt;/h3&gt; 
&lt;div id="attachment_4063" style="width: 1106px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-4063" loading="lazy" class="size-full wp-image-4063" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/24/IMG-2024-07-24-12.25.21.png" alt="Figure 1 – This is the architecture diagram of what you will be provisioning during this 2-part-blog series. During this blog, we will provision EKS Resources (the Control Plane VPC is automatically provisioned in AWS’ account, and is fully managed), including the pods and worker node groups (g5.48xlarge in this blog). Additionally, we will also provision the Data Plane VPC and associated resources – Subnets (Public &amp;amp; Private), NAT Gateways, and the Internet Gateway. In part 2, we will cover Autoscaling (for both the cluster and pods) and Load Balancing (Ingress of type Application Load Balancer)." width="1096" height="614"&gt;
 &lt;p id="caption-attachment-4063" class="wp-caption-text"&gt;Figure 1 – This is the architecture diagram of what you will be provisioning during this 2-part-blog series. During this blog, we will provision EKS Resources (the Control Plane VPC is automatically provisioned in AWS’ account, and is fully managed), including the pods and worker node groups (g5.48xlarge in this blog). Additionally, we will also provision the Data Plane VPC and associated resources – Subnets (Public &amp;amp; Private), NAT Gateways, and the Internet Gateway. In part 2, we will cover Autoscaling (for both the cluster and pods) and Load Balancing (Ingress of type Application Load Balancer).&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Since this post focuses on the Llama3-8B model in fp16, &lt;em&gt;we won’t need multiple instances provisioned to accommodate it&lt;/em&gt; (since the model fits on one GPU). We’ll talk about the shared file system (Amazon EFS) in a future post – when we talk about larger model NIMs.&lt;/p&gt; 
&lt;h3&gt;Deploying into your AWS account&lt;/h3&gt; 
&lt;h4&gt;Prerequisites&lt;/h4&gt; 
&lt;p&gt;Before you proceed, please ensure you have the &lt;em&gt;NVIDIA AI Enterprise License (NVAIE) &lt;/em&gt;to access the NIMs. To learn more about how you can get access, go to &lt;a href="https://build.nvidia.com/explore/discover"&gt;build.nvidia.com &lt;/a&gt;and provide your corporate email address.&lt;/p&gt; 
&lt;p&gt;To get started, clone the &lt;code&gt;awsome-inference&lt;/code&gt; &lt;a href="https://github.com/aws-samples/awsome-inference.git"&gt;GitHub Repository&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;git clone https://GitHub.com/aws-samples/awsome-inference.git&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Make sure that the following packages are installed:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;aws cli&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;eksctl&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;docker&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;helm&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You can find shell scripts to install each one of these packages in the &lt;a href="https://github.com/aws-samples/aws-do-eks/tree/main/Container-Root/eks/ops/setup"&gt;aws-do-eks repository&lt;/a&gt; in the directory &lt;code&gt;aws-do-eks/Container-Root/eks/ops/setup/&lt;/code&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;./install-aws-cli.sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;./install-eksctl.sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;./install-kubectl.sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;./install-docker.sh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;./install-helm.sh&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Alternatively, you may choose to install the packages manually. You can find instructions for this in the &lt;a href="https://github.com/aws-samples/awsome-inference/blob/main/1.infrastructure/README.md"&gt;README&lt;/a&gt; file on the GitHub repo.&lt;/p&gt; 
&lt;p&gt;Set up your &lt;a href="https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html#generating-personal-api-key"&gt;NGC Personal API Key&lt;/a&gt;, so that you are ready to download the NIM Docker Images. Once you have that set up, export it as a command-line variable:&lt;/p&gt; 
&lt;div class="hide-language"&gt; 
 &lt;pre&gt;&lt;code class="lang-bash"&gt;export NGC_CLI_API_KEY=”key from ngc”&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;p&gt;The NGC Personal API Key is required for using the NGC Service through the Docker client – or via the NGC CLI. To learn more about the NGC Service, please check out the &lt;a href="https://catalog.ngc.nvidia.com/?filters=&amp;amp;orderBy=weightPopularDESC&amp;amp;query=&amp;amp;page=&amp;amp;pageSize="&gt;NGC catalog&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Lastly, run &lt;code&gt;aws configure&lt;/code&gt; to authenticate with AWS. The examples on GitHub – and in this post – use the region &lt;code&gt;us-east-2&lt;/code&gt; to provision resources. Feel free to change this, as required. Make sure you consistently use the same region throughout the deployment process.&lt;/p&gt; 
&lt;h4&gt;Setting up your VPC and EKS cluster&lt;/h4&gt; 
&lt;p&gt;You can find all the resources for this sub-section in the sub-directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd 1.infrastructure/&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this directory, you’ll find the files needed to get a &lt;a href="https://aws.amazon.com/vpc/"&gt;Virtual Private Cloud&lt;/a&gt; (VPC), and an EKS cluster, set up in your own environment.&lt;/p&gt; 
&lt;p&gt;To set up your VPC, go to the directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd 0_setup_vpc/&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this directory, you’ll find &lt;code&gt;vpc-cf-example.yaml&lt;/code&gt;, which is an AWS CloudFormation template for deploying a VPC with a pair of public and private subnets, spread across two Availability Zones (AZ). It also includes an Internet gateway, a default route on the public subnets, a pair of NAT gateways (one in each AZ), and default routes for them in the private subnets. For more information on this CloudFormation template, check out &lt;a href="https://docs.aws.amazon.com/codebuild/latest/userguide/cloudformation-vpc-template.html"&gt;AWS CloudFormation VPC Template&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To deploy the VPC Stack, please download the VPC Template (called &lt;code&gt;vpc-cf-example.yaml&lt;/code&gt;), and navigate to the CloudFormation console. Once there, hit &lt;strong&gt;Create Stack&lt;/strong&gt;. Under step 1, hit &lt;strong&gt;Choose an existing template&lt;/strong&gt;, and &lt;strong&gt;Upload a template file&lt;/strong&gt;. You can leave the rest of the steps as defaults.&lt;/p&gt; 
&lt;p&gt;Note: The instructions described below walk you through setting up an EKS Cluster only if you do not use &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-blocks.html"&gt;&lt;em&gt;Capacity Blocks for ML&lt;/em&gt;&lt;/a&gt;. If you wish to use &lt;em&gt;Capacity Blocks for ML&lt;/em&gt;, please follow the instructions in the &lt;a href="https://github.com/aws-samples/awsome-inference/tree/main/1.infrastructure#capacity-blocks"&gt;Capacity Blocks section&lt;/a&gt; of the &lt;code&gt;awsome-inference/1.infrastructure&lt;/code&gt; README to set up the EKS Cluster and &lt;em&gt;self-managed node groups&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Next, to set up your EKS Cluster, navigate back to the 1.infrastructure/ and then go to the 1_setup_cluster/ directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd 1_setup_cluster/&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This sub-directory contains YAML cluster configurations, templates, and examples to aid you setting up clusters for inference. Within this directory, you should find &lt;code&gt;nims-cluster-config-example.yaml&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;This cluster configuration is an example configuration that you can use to spin up a managed EKS cluster of p5.48xlarge instances that you have reserved via the &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-capacity-reservations.html"&gt;On-Demand Capacity Reservation&lt;/a&gt; (ODCR). You can use this example configuration as-is, or with modifications to set up your EKS cluster.&lt;/p&gt; 
&lt;p&gt;We’ve commented out the capacityReservation field in this example configuration. If you want to use an ODCR, please uncomment lines 63-65 on the configuration file and fill in your own details that map to &lt;em&gt;your&lt;/em&gt; ODCR.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;# Uncomment the lines below (capacityReservation) if you use an ODCR
# capacityReservation:
#   capacityReservationTarget:
#     capacityReservationID: "cr-0ce6be6d411d2f43f"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next are some of the parameters that you may need to change in the &lt;code&gt;nims-cluster-config-example.yaml&lt;/code&gt; file:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;$PLACEHOLDER_VPC_ID&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$PLACEHOLDER_SUBNET_PRIVATE_1&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$PLACEHOLDER_SUBNET_PRIVATE_2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$PLACEHOLDER_SUBNET_PUBLIC_1&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$PLACEHOLDER_SUBNET_PUBLIC_2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$INSTANCE_TYPE&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$PUBLIC_KEYPAIR_NAME&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;$SECURITY_GROUP_IDS&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Parameters 1-5 can be found using your CLI:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;aws cloudformation describe-stacks --stack-name &amp;lt;&lt;strong&gt;YOUR_VPC_STACK_NAME&lt;/strong&gt;&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Parameters 7 and 8 have to be changed if you’d like ssh access to your worker nodes. For the rest of this post, we’ll set parameter 5 (&lt;code&gt;$INSTANCE_TYPE&lt;/code&gt;) to &lt;code&gt;g5.48xlarge&lt;/code&gt;. If you’d like to make any other changes, just consult &lt;a href="https://github.com/aws-samples/awsome-inference/tree/main/1.infrastructure"&gt;README for 1.infrastructure&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Deploying the NIM&lt;/h4&gt; 
&lt;p&gt;Before deploying the NIM, it is important to understand the different &lt;a href="https://docs.nvidia.com/nim/large-language-models/latest/support-matrix.html"&gt;profiles&lt;/a&gt; available for this. With each NIM, NVIDIA releases different profiles that you can choose based on what infrastructure you wish to run on, or what you wish to test.&lt;/p&gt; 
&lt;p&gt;This profile is chosen by default based on your infrastructure. You can customize the infrastructure you run your NIM on with EKS – for example, you can specify how many GPUs are available per pod with the following lines in your manifest, and a NIM profile that is optimized for 1 GPU is automatically chosen.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;resources:
  limits:
    nvidia.com/gpu: 1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you may choose to specify a profile, if multiple profiles are available. For example, you can choose an &lt;code&gt;fp8&lt;/code&gt; profile that lets you use an fp8-quantized version of the base model.&lt;/p&gt; 
&lt;p&gt;To learn more about the NIM profiles available for you to use, follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Ensure you have your EKS cluster up and running (following the steps described in the section above), and that you have your NIM pod(s) deployed.&lt;/li&gt; 
 &lt;li&gt;Once your EKS cluster and pods are up and running, to execute into the pod, run:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;kubectl exec -it &amp;lt;pod name&amp;gt; -- cat /etc/nim/config/model_manifest.yaml&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;If you want to use a specific profile, you can specify the profile’s SHA value in the yaml file like this:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;...
# @param env [array] Adds arbitrary environment variables to the main container
 env: 
 - name: NIM_MODEL_PROFILE
   value: 30b562864b5b1e3b236f7b6d6a0998efbed491e4917323d04590f715aa9897dc"
... 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now that you have your infrastructure set up, you can start provisioning pods to host your NIM(s).&lt;/p&gt; 
&lt;p&gt;Navigate to the &lt;code&gt;nims-inference&lt;/code&gt; directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd 2.projects/nims-inference&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once there, you should see a helm example in the &lt;code&gt;nim-deploy&lt;/code&gt; directory. This directory is cloned from NVIDIA’s &lt;a href="https://github.com/NVIDIA/nim-deploy/tree/main"&gt;nim-deploy repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Before running the &lt;code&gt;helm&lt;/code&gt; command below, please ensure that you’ve set the NGC CLI API key as described in the “Prerequisites” section.&lt;/p&gt; 
&lt;p&gt;To get your NIM up and running without any configuration changes, you may run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd nim-deploy/helm
helm install my-nim nim-llm/ --set model.ngcAPIKey=$NGC_CLI_API_KEY --set persistence.enabled=true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This &lt;code&gt;helm install&lt;/code&gt; command uses the default &lt;code&gt;values.yaml&lt;/code&gt; file. You also have the option to use custom values helm files. More on this in the &lt;a href="https://github.com/NVIDIA/nim-deploy/tree/main/helm"&gt;README on NVIDIA’S nim-deploy repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you run into any issues here, refer to the Troubleshooting section later in this post.&lt;/p&gt; 
&lt;p&gt;Once you deploy the pod, running &lt;code&gt;kubectl get pods&lt;/code&gt; should show something like:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;my-nim-0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 1/1&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Running&amp;nbsp;&amp;nbsp; 0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 10s&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;To test whether you can query the NIM pod, try port forwarding like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;kubectl port-forward service/my-nim-nim-llm 8000:8000&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, in another terminal, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;curl -X 'POST' \
'http://localhost:8000/v1/chat/completions' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "messages": [
    {
      "content": "You are a polite and respectful chatbot helping people plan a vacation.",
      "role": "system"
    },
    {
      "content": "What should I do for a 4 day vacation in Spain?",
      "role": "user"
    }
  ],
  "model": "meta/llama3-8b-instruct",
  "max_tokens": 16,
  "top_p": 1,
  "n": 1,
  "stream": false,
  "stop": "\n",
  "frequency_penalty": 0.0
}'
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This should return something like:&lt;/p&gt; 
&lt;p class="Code"&gt;&lt;code&gt;{"id":"cmpl-228f8ceabea1479caff6142c33478f3b","object":"chat.completion","created":1720470045,"model":"meta/llama3-8b-instruct","choices":[{"index":0,"message":{"role":"assistant","content":"Spain is a wonderful destination! With four days, you can definitely get a taste"},"logprobs":null,"finish_reason":"length","stop_reason":null}],"usage":{"prompt_tokens":42,"total_tokens":58,"completion_tokens":16}}&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;And: Congrats! You now have a NIM pod up and running on a single g5.48xlarge instance. You can also now stop the port forward service.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;The most common issues we’ve seen relate to Kubernetes Persistent Volume Claims (PVC). This is because, by default, the helm charts provided don’t install the ebs-csi controllers for you. If you find yourself blocked by PVC errors, try this:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check all pods with &lt;code&gt;kubectl get pods -A&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Get the logs of the &lt;code&gt;aws-node-xxxx&lt;/code&gt; pod and check the three containers (with the &lt;code&gt;-c&lt;/code&gt; flag): 
  &lt;ol&gt; 
   &lt;li&gt;&lt;code&gt;aws-node&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;aws-eks-nodeagent&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;aws-vpc-cni-init&lt;/code&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;Check the status of your PVCs with the &lt;code&gt;kubectl get pvc -A&lt;/code&gt; If the PVC controller isn’t installed – or if you see an error here – run the steps below.&lt;/li&gt; 
 &lt;li&gt;Install the PVC controller using:&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;kubectl apply -k "GitHub.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.27"&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="5"&gt; 
 &lt;li&gt;Once you’ve run this, you should have the EBS CSI controller installed on your cluster, and you should be able to deploy your pod without any errors. Redeploy (using &lt;code&gt;helm install&lt;/code&gt;) your configuration &lt;em&gt;after &lt;/em&gt;making these changes.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Benchmarking&lt;/h2&gt; 
&lt;p&gt;The GitHub repository also contains some &lt;a href="https://github.com/aws-samples/awsome-inference/tree/main/2.projects/nims-inference/benchmark"&gt;example code&lt;/a&gt; to get you started with collecting performance metrics against your deployed NIM. To do this, we use the NVIDIA &lt;code&gt;genai-perf&lt;/code&gt; tool. &lt;code&gt;genai-perf&lt;/code&gt; is a command-line benchmarking tool for measuring inference metrics of models served through an inference server.&lt;/p&gt; 
&lt;p&gt;Some of the metrics provided by this tool are: output token throughput, time to first token, inter token latency, and request throughput. For more on the tool, check out the &lt;a href="https://docs.nvidia.com/nim/benchmarking/llm/latest/overview.html"&gt;GenAI-Perf Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Once you’ve tested your NIM, you can set up a service of type ClusterIP to expose a port on your container.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;kubectl apply -f nim-deploy/helm/nim-llm/nodeport.yaml&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We have a sample benchmarking script under benchmark/benchmark_concurrency.sh, that will run benchmarking with multiple concurrencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd benchmark/
./benchmark_concurrency.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can then copy the benchmarking data from the genai-perf pod to your local:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;kubectl cp &amp;lt;POD_NAME&amp;gt;:/workspace/benchmarks &amp;lt;YOUR_LOCAL_PATH&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once done, you should terminate the benchmark deployment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;./terminate_benchmark.sh&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can directly change any of the flags used by &lt;code&gt;genai-perf&lt;/code&gt; tool in the &lt;code&gt;genai-perf.yaml&lt;/code&gt; manifest found in the same benchmark/ directory, too.&lt;/p&gt; 
&lt;p&gt;This process saves a &lt;code&gt;.csv&lt;/code&gt; and &lt;code&gt;.json&lt;/code&gt; file onto your local system. These files contain the benchmarking metrics we mentioned. Alternatively, you can check which pod has prefix &lt;code&gt;genai-perf&lt;/code&gt;, and run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;kubectl logs &amp;lt;pod-name&amp;gt;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is what an example output will look like:&lt;/p&gt; 
&lt;div id="attachment_4064" style="width: 1087px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-4064" loading="lazy" class="size-full wp-image-4064" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/24/IMG-2024-07-24-12.38.30.png" alt="Figure 2 – Screenshot of an example log of running the genai-perf tool. You may safely ignore any ERROR messages that you see in these logs, and the cut off values too – the benchmarking scripts on the awsome-inference GitHub Repository get you the relevant CSV and JSON files that contain the complete benchmarking numbers." width="1077" height="1034"&gt;
 &lt;p id="caption-attachment-4064" class="wp-caption-text"&gt;Figure 2 – Screenshot of an example log of running the genai-perf tool. You may safely ignore any ERROR messages that you see in these logs, and the cut off values too – the benchmarking scripts on the awsome-inference GitHub Repository get you the relevant CSV and JSON files that contain the complete benchmarking numbers.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We’ll focus on benchmarking metrics in depth in a future blog post. For now, we hope this post provided you with enough to get started benchmarking your NIM deployments.&lt;/p&gt; 
&lt;h2&gt;What’s next?&lt;/h2&gt; 
&lt;p&gt;Now you’re able to have the Llama3-8B NIM deployed on a single g5.48xlarge instance using EKS as an orchestrator. In a future blog post, we will focus on:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Scaling your NIM with EKS&lt;/li&gt; 
 &lt;li&gt;Load Balancing across NIM pods with EKS&lt;/li&gt; 
 &lt;li&gt;Benchmarking: Which instance should you run on? How many NIM pods can you fit per GPU? What scale can you take this to?&lt;/li&gt; 
 &lt;li&gt;Using other NIMs.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we’ve shown you how to leverage Amazon EKS to orchestrate the deployment of pods containing NVIDIA NIMs, to enable quick-to-setup and optimized large-scale Large Language Model (LLM) inference on AWS G5 instances.&lt;/p&gt; 
&lt;p&gt;NVIDIA NIM can empower researchers and developers to run optimized inference in their applications, and Amazon EKS can help with the orchestration. Together, these scale NIM easily and efficiently to thousands of GPUs, resulting in accelerated time-to-market for cutting edge AI applications.&lt;/p&gt; 
&lt;p&gt;To learn more about deploying NVIDIA NIM and the associated infrastructure, refer to &lt;a href="https://github.com/aws-samples/awsome-inference/tree/main/2.projects/nims-inference"&gt;AWS Samples&lt;/a&gt;. To learn more about Amazon EKS and NIMs, check out the &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/getting-started.html"&gt;EKS User Guide&lt;/a&gt;, and &lt;a href="https://github.com/NVIDIA/nim-deploy/tree/main/helm"&gt;NVIDIA NIM&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Lastly, as mentioned in the “What’s Next?” section, keep on the lookout for part 2, where we dive deep into scaling, load balancing, and benchmarking.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Implementing e-mail and SMS notifications in AWS ParallelCluster with Slurm</title>
		<link>https://aws.amazon.com/blogs/hpc/implementing-e-mail-and-sms-notifications-in-aws-parallelcluster-with-slurm/</link>
		
		<dc:creator><![CDATA[Neil Munday]]></dc:creator>
		<pubDate>Tue, 23 Jul 2024 11:46:40 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<guid isPermaLink="false">11c5a3466d18240a784c1c4c99d0cbee592d5299</guid>

					<description>Learn how to configure email and SMS alerts for job events to stay on top of your HPC workloads with AWS ParallelCluster using Slurm.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3938" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/03/boofla88_a_stream_of_airmail_envelopes_with_wings_flying_throug_d3ff59ca-fae6-4531-9a2e-476912ebd20f.png" alt="Implementing e-mail and SMS notifications in AWS ParallelCluster with Slurm" width="380" height="212"&gt;AWS ParallelCluster allows customers to deploy HPC clusters using AWS. You can select the size you need for your cluster, the operating system (OS), file systems, and CPU or GPU architectures to meet your needs – and only pay for what you use. Slurm manages jobs on the cluster, and informs the autoscaling functions in ParallelCluster.&lt;/p&gt; 
&lt;p&gt;In this post we’ll demonstrate how you can use a post-install script to add e-mail notifications and SMS notifications to ParallelCluster for Slurm job events. And we’ll show how you can use this to interact with other AWS resources – to create your own workflows. This could be a “job completion” notification from Slurm that triggers a post-processing AWS Lambda function to run on output data that a job wrote to an Amazon Simple Storage Service (Amazon S3) bucket.&lt;/p&gt; 
&lt;h2&gt;Enabling Slurm job notifications&lt;/h2&gt; 
&lt;p&gt;Slurm has an option to send e-mails to users when a job changes state, for example when the job starts and ends. To do this, the server that’s running the &lt;code&gt;slurmctld&lt;/code&gt; daemon must already be configured to send e-mails using SMTP.&lt;/p&gt; 
&lt;p&gt;In the example that follows, we’re instructing Slurm to send notifications via e-mail for all job events:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;#SBATCH --mail-user=jdoe@example.com
#SBATCH --mail-type=ALL
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When Slurm sends e-mails for these events, they don’t include an e-mail body – it packs everything it knows about the job in the subject of the e-mail, as we’ve shown in Figure 1. That empty e-mail body is just begging to be filled with additional job information.&lt;/p&gt; 
&lt;div id="attachment_3927" style="width: 1006px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3927" loading="lazy" class="size-full wp-image-3927" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/03/IMG-2024-07-03-13.05.06.png" alt="Figure 1 - Example Slurm e-mail for a “job began” event – note that all the information about the job is in the subject line, and the body of the e-mail is left blank." width="996" height="470"&gt;
 &lt;p id="caption-attachment-3927" class="wp-caption-text"&gt;Figure 1 – Example Slurm e-mail for a “job began” event – note that all the information about the job is in the subject line, and the body of the e-mail is left blank.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;What is Slurm-Mail?&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://github.com/neilmunday/slurm-mail"&gt;Slurm-Mail&lt;/a&gt; is an open source add-on for Slurm that allows HPC administrators to configure their Slurm installations to send HTML5 e-mails to their users about their jobs. The e-mails it generates contain additional information about users’ jobs from Slurm’s job accounting database. You can customize the e-mails using templates depending on your individual needs.&lt;/p&gt; 
&lt;p&gt;Slurm-Mail &lt;a href="https://neilmunday.github.io/slurm-mail/repo/"&gt;provides packages&lt;/a&gt; for RHEL-based operating systems, including Amazon Linux. It also supports Ubuntu and SLES.&lt;/p&gt; 
&lt;h2&gt;How can we use Slurm-Mail with AWS ParallelCluster?&lt;/h2&gt; 
&lt;p&gt;The Slurm-Mail &lt;a href="https://neilmunday.github.io/slurm-mail/repo/"&gt;package repository&lt;/a&gt; provides RPMs for Amazon Linux 2. We can therefore install Slurm-Mail on the headnode of our ParallelCluster. However, we need to tell Slurm-Mail where to deliver e-mails. For this we can use &lt;a href="https://aws.amazon.com/ses/"&gt;Amazon Simple Email Service (SES)&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Amazon SES gives you the ability to send e-mails from your cloud-based resources, e.g. from Amazon Compute Cloud (Amazon EC2) hosts, Lambda functions, and more. This eliminates the need for you to set-up and operate your own mail servers. You can send messages using Amazon SES with either the &lt;a href="https://docs.aws.amazon.com/ses/latest/dg/send-email-api.html"&gt;Amazon SES API&lt;/a&gt;, or an SMTP interface. In this blog post, we’ll make use of the SMTP interface to send e-mails from the headnode on our ParallelCluster, using Slurm-Mail.&lt;/p&gt; 
&lt;p&gt;In the Amazon SES console you can create an e-mail identity that will send the e-mails. To help prevent fraud and abuse, and to help protect your reputation as a sender, Amazon SES places all new accounts in the &lt;em&gt;SES sandbox&lt;/em&gt;. Here, you’ll only be able to send and receive e-mails to the e-mail identities you have defined in Amazon SES. If you’re only sending e-mails to yourself or e-mail addresses you are able to verify, then the sandbox will be enough for your needs. If not, you’ll have to cut a ticket to customer support to migrate out of the Amazon SES sandbox.&lt;/p&gt; 
&lt;p&gt;Once you’ve created and verified your e-mail identity in the Amazon SES console you can create SMTP credentials (shown in Figure 1).&lt;/p&gt; 
&lt;div id="attachment_3928" style="width: 1008px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3928" loading="lazy" class="size-full wp-image-3928" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/03/IMG-2024-07-03-13.05.37.png" alt="Figure 2 – Accessing AWS SES SMTP settings via the console" width="998" height="429"&gt;
 &lt;p id="caption-attachment-3928" class="wp-caption-text"&gt;Figure 2 – Accessing AWS SES SMTP settings via the console&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The process of creating SMTP credentials will create a new IAM (Identity and Access Management) user. Take note of the credentials displayed to you when this happens, because you’ll need these to configure Slurm-Mail. Also note your SMTP server settings – they’re available in the SES console (as shown in Figure 2).&lt;/p&gt; 
&lt;p&gt;With this information, you’re now ready to make use of the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/slurm_accounting_with_email"&gt;AWS ParallelCluster Slurm-Mail recipe&lt;/a&gt;, which is part of the &lt;a href="https://hpc.news/recipes"&gt;HPC Recipes Library&lt;/a&gt;. This is a launchable AWS CloudFormation template which will ask you for a number of input values to define your cluster. These include the SMTP settings that Slurm-Mail will use. We’ve detailed the input parameters you need in Table 1, and there’s an example of these being used, in Figure 3.&lt;/p&gt; 
&lt;div id="attachment_3929" style="width: 816px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3929" loading="lazy" class="size-full wp-image-3929" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/03/IMG-2024-07-03-13.07.09.png" alt="Table 1 - Template input parameters" width="806" height="428"&gt;
 &lt;p id="caption-attachment-3929" class="wp-caption-text"&gt;Table 1 – Template input parameters&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3930" style="width: 1015px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3930" loading="lazy" class="size-full wp-image-3930" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/03/IMG-2024-07-03-13.07.43.png" alt="Figure 3 - SES configuration settings for CloudFormation using the Amazon SES parameters which we established in this guide." width="1005" height="510"&gt;
 &lt;p id="caption-attachment-3930" class="wp-caption-text"&gt;Figure 3 – SES configuration settings for CloudFormation using the Amazon SES parameters which we established in this guide.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;During the creation of the headnode resource, we invoke the &lt;code&gt;&lt;a href="https://github.com/aws-samples/aws-hpc-recipes/blob/main/recipes/pcluster/slurm_accounting_with_email/assets/postinstall.sh"&gt;postinstall.sh&lt;/a&gt;&lt;/code&gt; script to install and configure Slurm-Mail. The script will create a &lt;code&gt;yum&lt;/code&gt; configuration file on the headnode to help ensure that you can keep your Slurm-Mail installation up to date.&lt;/p&gt; 
&lt;p&gt;It will take approximately 30-40 minutes per cluster to provision the resources. Once the stack has finished deploying, you can check the output values to find the public IP address of the headnode (Figure 4).&lt;/p&gt; 
&lt;div id="attachment_3931" style="width: 735px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3931" loading="lazy" class="size-full wp-image-3931" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/03/IMG-2024-07-03-13.08.24.png" alt="Figure 4 - Stack output values. This is how you find out the IP number of the headnode of the cluster." width="725" height="719"&gt;
 &lt;p id="caption-attachment-3931" class="wp-caption-text"&gt;Figure 4 – Stack output values. This is how you find out the IP number of the headnode of the cluster.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Using the headnode IP and your SSH key you will be able to login to the headnode of your cluster to submit jobs. Slurm-Mail will then send you job notification emails. We’ve shown an example e-mail in Figure 5.&lt;/p&gt; 
&lt;div id="attachment_3932" style="width: 672px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3932" loading="lazy" class="size-full wp-image-3932" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/03/IMG-2024-07-03-13.08.49.png" alt="Figure 5 - Example e-mail generated by Slurm-Mail running on AWS ParallelCluster" width="662" height="1015"&gt;
 &lt;p id="caption-attachment-3932" class="wp-caption-text"&gt;Figure 5 – Example e-mail generated by Slurm-Mail running on AWS ParallelCluster&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Adding e-mail notifications to an existing cluster&lt;/h2&gt; 
&lt;p&gt;Before proceeding to update an &lt;em&gt;existing&lt;/em&gt; ParallelCluster deployment, make sure you have an AWS SES e-mail identity configured along with a SMTP user and password (see the previous section).&lt;/p&gt; 
&lt;p&gt;Now you need to login to the headnode of your ParallelCluster and execute some commands to download the post-install script:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;wget https://raw.githubusercontent.com/aws-samples/aws-hpc-recipes/main/recipes/pcluster/slurm_accounting_with_email/assets/postinstall.sh -O ./post-install.sh
chmod +x ./post-install.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now you can execute the script with the necessary command line options to configure Slurm-Mail on your headnode. The script will perform some actions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Determines the OS version&lt;/li&gt; 
 &lt;li&gt;Configures the &lt;code&gt;yum&lt;/code&gt; or Ubuntu repository for Slurm-Mail based on the OS.&lt;/li&gt; 
 &lt;li&gt;Installs the &lt;code&gt;Slurm-Mail&lt;/code&gt; package&lt;/li&gt; 
 &lt;li&gt;Updates the &lt;code&gt;Slurm-Mail&lt;/code&gt; configuration file to use your SMTP settings from SES&lt;/li&gt; 
 &lt;li&gt;Updates your Slurm configuration file to use Slurm-Mail for e-mail notifications&lt;/li&gt; 
 &lt;li&gt;Restarts your Slurm controller daemon (&lt;code&gt;slurmctld&lt;/code&gt;)&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You can then run the script as root to perform the installation and configuration – make sure you replace the &lt;code&gt;$SES_*&lt;/code&gt; variables with the values for your set-up.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;sudo ./post-install.sh -c /opt/slurm/etc/slurm.conf -d /opt/slurm/bin \
    -e $SES_email_address -u $SES_user -p $SES_password \
    -n $SES_port -s $SES_server
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;How can we take this further?&lt;/h2&gt; 
&lt;p&gt;So far we’ve been able to receive notifications about our Slurm jobs running on AWS ParallelCluster via e-mail. But what if we wanted to receive an SMS text message – or even trigger another AWS service to perform an action? For example we could invoke an AWS Lambda function to post-process files that a Slurm job wrote to an Amazon S3 bucket.&lt;/p&gt; 
&lt;p&gt;It turns out we can do this, thanks to the ability of SES to send a notification that includes the e-mail headers to an Amazon Simple Notification Service (SNS) topic of our choosing when the e-mails are successfully delivered.&lt;/p&gt; 
&lt;p&gt;From here we can configure the SNS topic to invoke a Lambda function that uses the SMS sending facilities in SES. Since we include the e-mail headers in the Amazon SNS notification, we can just extract “Subject” and include this in the text message that we want to send.&lt;/p&gt; 
&lt;p&gt;In the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/slurm_accounting_with_email"&gt;recipe repository&lt;/a&gt; we’ve included &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/blob/main/recipes/pcluster/slurm_accounting_with_email/assets/launch-sms.yaml"&gt;a recipe&lt;/a&gt; that demonstrates this set-up. The recipe makes use of a CloudFormation custom resource to configure SES to send successful e-mail delivery notifications to a SNS &lt;em&gt;topic&lt;/em&gt; that we also create in that same template.&lt;/p&gt; 
&lt;p&gt;Figure 6 illustrates the flow of information all the way from Slurm on the headnode, through Slurm-Mail, to Amazon SES, through to Amazon SNS to send the SMS text message (you may need to read that sentence a couple of times, but we promise this makes sense). All the resources we show in that diagram are created by the recipe.&lt;/p&gt; 
&lt;div id="attachment_3933" style="width: 1008px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3933" loading="lazy" class="size-full wp-image-3933" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/03/IMG-2024-07-03-13.10.43.png" alt="Figure 6 - Generating SMS text messages with AWS ParallelCluster" width="998" height="417"&gt;
 &lt;p id="caption-attachment-3933" class="wp-caption-text"&gt;Figure 6 – Generating SMS text messages with AWS ParallelCluster&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;This code snippet shows how we can send a SMS message using SNS in a Lambda function, using the Python runtime:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-python"&gt;import json
import os
import boto3

def handler(event, context):          
    message = json.loads(event['Records'][0]['Sns']['Message'])
    if "mail" in message:
        # send to phone number
        sns = boto3.client("sns")
        print(f'sending text message to {os.environ["SMS_NUMBER"]}')
        try:
            sns.publish(
                PhoneNumber=os.environ["SMS_NUMBER"],
                Message=message["mail"]["commonHeaders"]["subject"]
            )
        except Exception as err:
            print(f'Unable to send message: {err}')
            return
        print('message sent ')
    return
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Sending a SMS text message is only &lt;em&gt;one&lt;/em&gt; possibility, however. By using Amazon SNS to trigger an AWS Lambda function, we can interact with many other AWS services to perform tasks for us by using the &lt;a href="https://boto3.amazonaws.com"&gt;Boto3&lt;/a&gt; module for Python for example.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;We’ve shown you how you can provision a working AWS ParallelCluster with just a few mouse clicks that – once deployed – is ready and waiting to send to us job notification e-mails. We can submit our jobs without having to manually check-up on them. The content provided in the e-mails is further enhanced, thanks to Slurm-Mail.&lt;/p&gt; 
&lt;p&gt;Finally, we’ve also shown you how to take this further and deploy a ParallelCluster capable of also sending us &lt;em&gt;SMS text messages&lt;/em&gt; when our jobs’ change state. This is just one example of how Slurm job notifications can be used to trigger – and pass information to – other AWS services.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Gang scheduling pods on Amazon EKS using AWS Batch multi-node processing jobs</title>
		<link>https://aws.amazon.com/blogs/hpc/gang-scheduling-pods-on-amazon-eks-using-aws-batch-multi-node-processing-jobs/</link>
		
		<dc:creator><![CDATA[Angel Pizarro]]></dc:creator>
		<pubDate>Fri, 19 Jul 2024 18:46:20 +0000</pubDate>
				<category><![CDATA[Amazon Elastic Kubernetes Service]]></category>
		<category><![CDATA[Amazon Machine Learning]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Molecular Modeling]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">28ee190df91f8c8cfafa11f91493f9f8e189961b</guid>

					<description>AWS Batch multi-node parallel jobs can now run on Amazon EKS to provide gang scheduling of pods across nodes for large scale distributed computing like ML model training. More details here.</description>
										<content:encoded>&lt;p&gt;Machine learning, weather forecasting, computational fluid dynamics, and other workloads all require distributing computation across many compute resources. These workloads are often too large for a single machine, and can benefit from scaling across multiple nodes. But with that scaling comes the need to continuously communicate with each other.&lt;/p&gt; 
&lt;p&gt;AWS Batch &lt;a href="https://aws.amazon.com/about-aws/whats-new/2024/07/aws-batch-gang-scheduling-eks-multi-node-parallel-jobs/"&gt;now offers&lt;/a&gt; a multi-node parallel (MNP) job type for gang scheduling of pods across nodes on your Amazon Elastic Kubernetes Service (Amazon EKS) clusters, to enable just these types of workloads. Previously MNP jobs were only available if you used Amazon Elastic Container Service (Amazon ECS), so today we’re excited to bring this feature for Batch users running on Amazon EKS, too.&lt;/p&gt; 
&lt;p&gt;In this post, we will show you how to use AWS Batch MNP jobs to set up a Dask distributed processing job on EKS.&lt;/p&gt; 
&lt;h2&gt;Introduction to Dask and AWS Batch&lt;/h2&gt; 
&lt;p&gt;Dask is a flexible library for parallel computing in Python, used in many data science applications including&amp;nbsp;&lt;a href="https://examples.dask.org/applications/image-processing.html"&gt;image processing&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href="https://ml.dask.org/"&gt;machine learning&lt;/a&gt;. It’s able to schedule computational tasks dynamically across data collections that can be larger than the available memory on a single Amazon Elastic Compute Cloud (Amazon EC2) instance, or distribute computation across many instances in a moderately-sized (~100s of instances) compute cluster.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;&lt;a href="https://distributed.dask.org/en/stable/"&gt;Dask.distributed&lt;/a&gt;&lt;/code&gt;&amp;nbsp;is a lightweight library within the project that creates a dynamic task scheduler coordinates the actions of workers spread across multiple machines. A script can connect to the scheduler and send work items to it for processing.&lt;/p&gt; 
&lt;p&gt;While you can use Dask &lt;a href="https://aws.amazon.com/blogs/machine-learning/machine-learning-on-distributed-dask-using-amazon-sagemaker-and-aws-fargate/"&gt;in an interactive environment like a Jupyter Notebook&lt;/a&gt; to develop methods on example data, data scientists often want to scale their analysis on much larger datasets in an asynchronous fashion. When they do this, they’ll typically use an HPC cluster or cloud-native scheduler like AWS Batch to scale their workloads with &lt;code&gt;dask.distributed&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you’re new to AWS, then &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;&amp;nbsp;is a fully-managed service that adds batch-style semantics on top of our managed container orchestration services,&amp;nbsp;&lt;a href="https://aws.amazon.com/ecs"&gt;Amazon ECS&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href="https://aws.amazon.com/eks/"&gt;Amazon EKS&lt;/a&gt;. This includes price-capacity optimized scaling of Spot or On-demand AWS Fargate or EC2 instances based on the jobs in a queue, and advanced capabilities like automated job retries and fair share job scheduling.&lt;/p&gt; 
&lt;p&gt;There are multiple ways of running Dask analysis clusters on AWS, but for this post I’ll focus on using AWS Batch&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/multi-node-parallel-jobs.html"&gt;multi-node parallel (MNP)&lt;/a&gt;&amp;nbsp;jobs on top of a &lt;em&gt;pre-existing&lt;/em&gt; Amazon EKS cluster across a set of interconnected Amazon EC2 instances.&lt;/p&gt; 
&lt;p&gt;When you define an MNP job, you choose what type and how many instances you want to launch. AWS Batch will launch the same instance size for all nodes, designating one of them to be the “main node”. Figure 1 illustrates the deployment strategy we used for our example distributed Dask workload, a simple print out of “Hello world!” across all workers.&lt;/p&gt; 
&lt;p&gt;The main node hosts the Dask scheduler, along with our analysis script. Dask workers are launched on each child node. The Dask workers connect to the Dask scheduler on the main node using details encoded in the Batch environment variable. Once the analysis is complete, the application container will send a shutdown signal to the scheduler and workers, causing them to cleanly shut down.&lt;/p&gt; 
&lt;div id="attachment_3984" style="width: 1003px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3984" loading="lazy" class="size-full wp-image-3984" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/10/IMG-2024-07-10-19.24.14.png" alt="Figure 1 – Our deployment strategy for the distributed Dask workload. Dask workers are launched on each child node and connect to the Dask scheduler on the main node. When complete, the application container will send a shutdown signal to the scheduler and workers, causing them to cleanly shut down." width="993" height="600"&gt;
 &lt;p id="caption-attachment-3984" class="wp-caption-text"&gt;Figure 1 – Our deployment strategy for the distributed Dask workload. Dask workers are launched on each child node and connect to the Dask scheduler on the main node. When complete, the application container will send a shutdown signal to the scheduler and workers, causing them to cleanly shut down.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Overview of our example&lt;/h2&gt; 
&lt;p&gt;All of the code and steps needed to stand up this example are available in our&amp;nbsp;“&lt;a href="https://catalog.workshops.aws/running-batch-on-eks/en-US"&gt;Running batch workloads on Amazon EKS&lt;/a&gt;” workshop (the Dask-specific exercise is “&lt;a href="https://catalog.workshops.aws/running-batch-on-eks/en-US/exercises/run-mnp-job"&gt;Run a multi-node parallel job&lt;/a&gt;”). But let’s check out some specific parts of that exercise for additional context, so you can get an idea of how this all works.&lt;/p&gt; 
&lt;h3&gt;$&amp;gt; dask_hello_world.py&lt;/h3&gt; 
&lt;p&gt;First, we created a Python “hello world!” script with a method that returns a string with the Dask task index and hostname of the instance that the worker ran on:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-python"&gt;# Hello world! method
def hello(task_index):
    return "%s: Hello from worker %s" % (str(task_index), os.uname()[1])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, the script connects the client to the Dask scheduler using details from a configuration file, then waits on the scheduler to report that it has all the expected workers connected &lt;em&gt;before&lt;/em&gt; sending any work.&lt;/p&gt; 
&lt;p&gt;This is a choice you’re able to make: we &lt;em&gt;could&lt;/em&gt; have started sending tasks to workers as soon as a single worker was ready. It depends on your whether workload &lt;em&gt;requires&lt;/em&gt; all the workers to be up at the same time (&lt;em&gt;tightly-coupled&lt;/em&gt; workloads) or not (&lt;em&gt;pleasingly-parallel&lt;/em&gt; workloads). Both the scheduler configuration file &lt;em&gt;and&lt;/em&gt; the expected number of workers are provided as positional arguments at runtime.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-python"&gt;# Connect the client to the Dask scheduler service
dask_client = Client(scheduler_file=argv[1])

# Wait until the expected number of workers are ready
# before submitting tasks to workers. 
expected_num_workers = int(argv[2])  
while True:
    num_workers = len(dask_client.scheduler_info()["workers"].keys())
    logger.info("Workers %d of %d up" % (num_workers, num_worker_nodes)) 
    if num_workers == expected_num_workers:
        break
    sleep(10)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The script then uses the &lt;code&gt;dask_client.map()&lt;/code&gt;&amp;nbsp;function to run our workload across the available workers. As a reminder – the scheduler will assign a task to a Dask worker, which will run that task, and then return the result. It may get another task if there are any left on the Dask scheduler’s queue. In our example, we submitted the same method 50 times, so we expected each worker to complete multiple tasks during its runtime.&lt;/p&gt; 
&lt;p&gt;Results are gathered with &lt;code&gt;as_completed()&lt;/code&gt; and the return messages are printed using Python’s standard logging capabilities:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-python"&gt;# submit 50 tasks to the Dask scheduler
futures =  dask_client.map(hello, range(50))
# gather the results and print them out
for f in as_completed(futures):
    logger.info(f.result())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the tasks are complete, the Dask client sends a shutdown message to the scheduler and workers. Without that message, the script would exit but the containers (and nodes) for both the Dask scheduler &lt;em&gt;and&lt;/em&gt; workers would remain running – waiting for work.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-python"&gt;dask_client.shutdown()&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Process initialization&lt;/h3&gt; 
&lt;p&gt;In order to give the script the expected number of workers, we need to use the context that Batch provides to nodes, namely the &lt;code&gt;$AWS_BATCH_JOB_NUM_NODES&lt;/code&gt; environment variable. Kubernetes Pod specifications do not interpolate environment variables in either the &lt;code&gt;command&lt;/code&gt; or &lt;code&gt;args&lt;/code&gt; parameters, so we create an initialization shell script to interpolate any necessary environment variables and launch the script correctly. The example below showcases an initialization script that accepts the location of the dask scheduler configuration as a positional argument and leverages Batch environment variables to calculate the number of expected worker nodes. It then starts our analysis script with those values.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;#!/bin/bash

SCHEDULER_FILE=${1:-"/tmp/dask-scheduler-info.json"}
NUM_WORKER_NODES=$((${AWS_BATCH_JOB_NUM_NODES} – 1))
SCRIPT="/usr/local/bin/dask-mnp-hello_world.py"

exec $SCRIPT $SCHEDULER_FILE $NUM_WORKER_NODES
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Application container&lt;/h3&gt; 
&lt;p&gt;Now that we have the Dask analysis and initialization scripts ready, it’s time to create the application container. We could build our own container from scratch, but the Dask project provides a&amp;nbsp;&lt;a href="https://docs.dask.org/en/stable/deploying-docker.html"&gt;Docker image&lt;/a&gt;&amp;nbsp;with the full Dask Conda package (including the distributed scheduler), Numpy, and Pandas. It also has several other goodies – like having an entry point that activates the proper Conda environment and the potential to define an extra set of Python modules to install on launch. For more information on what the Dask container can do, refer to the&amp;nbsp;&lt;a href="https://github.com/dask/dask-docker"&gt;dask-docker GitHub repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We’ll use the provided container as the base Docker image and add our &lt;code&gt;dask_hello_world.py&lt;/code&gt; and initialization scripts to it. This example copies over the scripts as an executables but otherwise leaves the container be.&lt;/p&gt; 
&lt;p&gt;&lt;code class="lang-bash"&gt;FROM ghcr.io/dask/dask:2024.6.2-py3.12&lt;br&gt; # Copy the local analysis and initialization scripts&lt;br&gt; COPY --chmod=755 src/dask-mnp-hello.py /usr/local/bin/dask-mnp-hello.py&lt;br&gt; COPY --chmod=755 src/run-dask-mnp.py /usr/local/bin/run-dask-mnp.sh&lt;br&gt; &lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;We complete this part of the exercise by building the application container image and pushing it to a private Amazon Elastic Container Registry (Amazon ECR) that’s accessible from our EKS nodes.&lt;/p&gt; 
&lt;p&gt;Next, we create the AWS Batch job definition.&lt;/p&gt; 
&lt;h2&gt;Creating a Batch on EKS multi-node parallel job definition&lt;/h2&gt; 
&lt;p&gt;The AWS Batch MNP job determines the type and number of nodes that will be launched for our Dask analysis. The job definition also allows for defining other things, like the security context of the pods, but that’s outside the scope of this blog post. The documentation for EKS MNP job definitions provides all the possible configurations, so we’ll only cover the subset that are important for us.&lt;/p&gt; 
&lt;h3&gt;Providing launch context&lt;/h3&gt; 
&lt;p&gt;We need a way to give the Dask scheduler and workers – and our script – the context they need to launch correctly. Specifically, these processes need to know whether they’re running on the main node or a child node. In our design, the main node should host the Dask scheduler and the application script. The child nodes should only be running Dask workers.&lt;/p&gt; 
&lt;p&gt;Batch helps us in two ways. First, it provides environment variables that define the node index number, the main node’s index number, and the private IP of the main node. Second, MNP job definitions allow you to define multiple containers so you can encapsulate a process in its own container. Third, MNP has the concept of node ranges, so you can differentially define the type and number of containers on a node, the security context (via different service accounts), and/or host data volume mounts based on the node’s index range.&lt;/p&gt; 
&lt;p&gt;In our example, we will create two node ranges, one for the main node, and a second for the worker nodes.&lt;/p&gt; 
&lt;p&gt;For the main node, we define the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Set the main node index to “0”.&lt;/li&gt; 
 &lt;li&gt;Create a single-node node range as &lt;code&gt;"targetNodes": "0:0"&lt;/code&gt;. 
  &lt;ul&gt; 
   &lt;li&gt;Note that Batch uses closed interval, meaning that the latter index &lt;em&gt;is included in the range&lt;/em&gt;. Most programming languages use &lt;code&gt;[closed, open)&lt;/code&gt; intervals, meaning that the last index value will &lt;em&gt;not&lt;/em&gt; be in the range.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Define two containers – one for Dask scheduler and the other for the analysis script.&lt;/li&gt; 
 &lt;li&gt;Define host volume mount that allows the containers to share configuration information.&lt;/li&gt; 
 &lt;li&gt;Set the number of expected workers to be one less than the number of Batch nodes, since the main node will not run a Dask worker.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can review the complete JSON example for the job definition in the workshop.&lt;/p&gt; 
&lt;p&gt;For the worker nodes, we define a single container that leverages the Dask-provided container:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;{
  "targetNodes": "1:",
  "instanceTypes": [
    "c5.xlarge"
  ],
  "eksProperties": {
    "podProperties": {
      "hostNetwork": true,
      "containers": [
        {
          "name": "dask-workers",
          "image": "ghcr.io/dask/dask:2024.6.2-py3.12",
          "command": [
            "/bin/sh",
            "-c"
          ],
          "args": [
            "dask worker --nworkers auto \"${AWS_BATCH_JOB_MAIN_NODE_PRIVATE_IPV4_ADDRESS}:8786\""
          ],
          "resources": {
            "requests": {
              "cpu": "3",
              "memory": "4000Mi"
            },
            "limits": {
              "cpu": "4"
            }
          }
        }
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There’s a couple of things to call out here. First, AWS Batch MNP jobs only place a single pod &lt;em&gt;per node&lt;/em&gt;. The pod containers can either use the host’s network, or the &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html"&gt;VPC CNI EKS add-on&lt;/a&gt; which can assign IP addresses to individual containers. Our example uses host networking since we know the individual containers will not have port clashes. Frankly, most batch processing jobs don’t need to assign individual IPs to containers, and doing so risks IP exhaustion &lt;em&gt;and&lt;/em&gt; can slow down pod placement. We recommend using host networking whenever possible for these types of workloads.&lt;/p&gt; 
&lt;p&gt;Second, we created an ECR &lt;a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/pull-through-cache-working-pulling.html"&gt;pull-through cache rule&lt;/a&gt; to cache the public Dask container image in a private repository, accessible to nodes. This allows us to scan the container image when caching, improve download performance, and mitigate against downstream rate limits from the remote registry.&lt;/p&gt; 
&lt;p&gt;Third, Batch provides the main node’s IP address to worker nodes via the &lt;code&gt;$AWS_BATCH_JOB_MAIN_NODE_PRIVATE_IPV4_ADDRESS&lt;/code&gt; environment variable. Dask workers can leverage that variable to connect to the Dask scheduler, but, as we mentioned previously, Pod specifications do not interpolate environment variables to pass the correct value in the &lt;code&gt;command&lt;/code&gt; or &lt;code&gt;args&lt;/code&gt; parameters. For our application container we leveraged an initialization script. For the worker containers, we opt to use a complete command line string as the args to a shell command which will do the interpolation of the variable and start the Dask worker correctly.&lt;/p&gt; 
&lt;p&gt;Finally, we set the resource requirements for the container for CPU and memory. Batch uses the soft-limits in the &lt;code&gt;requests&lt;/code&gt; parameter for scheduling and placement of pods on nodes. We also set the hard-limits &lt;code&gt;limits&lt;/code&gt; parameter to a higher value for CPU. The reason for this is that Batch will copy over values from &lt;code&gt;requests&lt;/code&gt; if &lt;code&gt;limits&lt;/code&gt; is not set, and vice versa. By defining both, you can let Batch place a container using minimal CPU requirements, but allow the pod to scale to a higher limit if the resources are available. Note that Batch assumes memory is always a hard limit, and the values must be the same across both &lt;code&gt;requests&lt;/code&gt; and &lt;code&gt;limits&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Submitting the job and viewing results&lt;/h2&gt; 
&lt;p&gt;Now that all the parts are in place, you can submit the job using the AWS CLI or AWS Batch console. Once you do, Batch will get busy launching instances into your EKS cluster.&lt;/p&gt; 
&lt;p&gt;When the Nodes show as &lt;strong&gt;Ready&lt;/strong&gt;, Batch will start placing Pods on the Nodes. Batch provides Pod placement using &lt;code&gt;nodeName&lt;/code&gt; and tolerations to the Kubernetes scheduler, bypassing much of the scheduler placement logic. You can view the status of the Nodes and Pods using either the EKS console or &lt;code&gt;kubectl&lt;/code&gt;. Throughout the lifecycle of the job, Batch can provide information on its status, and any attempts that were made to run the analysis.&lt;/p&gt; 
&lt;p&gt;The workshop exercise sets up a new EKS cluster with the &lt;a href="https://docs.aws.amazon.com/eks/latest/userguide/cloudwatch.html"&gt;CloudWatch managed add-on&lt;/a&gt; configured to send the &lt;code&gt;STDOUT&lt;/code&gt; and &lt;code&gt;STDERR&lt;/code&gt; of all containers to an AWS CloudWatch log stream, as well as the necessary &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/batch-eks-cloudwatch-logs.html"&gt;AWS Batch toleration&lt;/a&gt;. To see the output message from the workers, you can go to the CloudWatch management console and locate the right CloudWatch log stream using the pod container identifiers.&lt;/p&gt; 
&lt;p&gt;Here’s an example message from task 6 that ran on one of the Batch managed nodes (with the relevant log message highlighted in bold):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-json"&gt;{
    "time": "2024-07-11T00:00:00.000000000Z",
    "stream": "stderr",
    "_p": "F",
    &lt;strong&gt;"log": "2024-07-11 0:00:00,000 - distributed - INFO - 6: Hello from Worker ip-10-1-123-123.us-east-1.compute.internal",&lt;/strong&gt;
    "kubernetes": {
        "pod_name": "aws-batch.a1b2c3d4-1a2b-1234-a1b2d-a0123456789b1",
        "namespace_name": "my-aws-batch-namespace",
        "pod_id": "abcdef12-3456-7890-abcd-ef0123456789",
        "host": "ip-10-3-133-64.us-east-1.compute.internal",
   // . . . other JSON log attributes 
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once the workers complete all of the tasks, the script will send the shutdown signal to the scheduler and worker containers, and Batch will scale down the Nodes. Kubernetes will no longer be able to provide information on the Pods that ran, or even their exit status. This makes it difficult to find logs after the job is complete. Its a good thing, then, that Batch keeps a record of the job status.&lt;/p&gt; 
&lt;p&gt;Given the job’s ARN you can use the AWS CLI to find the Pod’s name, and then the logs since the CloudWatch EKS add-on leverages this in its log-stream naming convention. Here is an example querying for the names of the pod and node of the main node’s index from our example run.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;# To query the main node pod name, use the main node index
$&amp;gt; aws batch-eks-mnp --endpoint $BATCH_ENDPOINT --region $BATCH_REGION --no-cli-pager  describe-jobs --jobs $JOB_ID#0 --query "jobs[].eksAttempts[].{podName: podName, nodeName: nodeName}"
[
    {
        "podName": "aws-batch.a1b2c3d4-1a2b-1234-a1b2d-a0123456789b1",
        "nodeName": "ip-10-1-123-123.us-east-1.compute.internal"
    }
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We can use these values to search for any relevant logs in the CloudWatch log streams management console (Figure 2).&lt;/p&gt; 
&lt;div id="attachment_3985" style="width: 999px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3985" loading="lazy" class="size-full wp-image-3985" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/10/IMG-2024-07-10-19.32.01.png" alt="Figure 2 – The AWS CloudWatch management console showing the filtered log streams for applications running on the EKS cluster. The filter dialog shows using the Pod name to retrieve the individual container logs for our Batch job. The log stream name also contains the container’s name, allowing us to identify the correct log stream to view for our results." width="989" height="491"&gt;
 &lt;p id="caption-attachment-3985" class="wp-caption-text"&gt;Figure 2 – The AWS CloudWatch management console showing the filtered log streams for applications running on the EKS cluster. The filter dialog shows using the Pod name to retrieve the individual container logs for our Batch job. The log stream name also contains the container’s name, allowing us to identify the correct log stream to view for our results.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we showed how you can leverage AWS Batch multi-node parallel (MNP) jobs to &lt;strong&gt;gang schedule&lt;/strong&gt; pods across a set of instances on your Amazon EKS cluster. We used Dask, a popular data science framework, as an example workload to launch a distributed analysis.&lt;/p&gt; 
&lt;p&gt;You can try out this exercise by working through the exercises in the &lt;a href="https://catalog.workshops.aws/running-batch-on-eks/en-US"&gt;Running Batch workloads on Amazon EKS workshop&lt;/a&gt;. If you do, let us know what you think!&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Strategies for distributing executable binaries across grids in financial services</title>
		<link>https://aws.amazon.com/blogs/hpc/strategies-for-distributing-executable-binaries-grids-financial-services/</link>
		
		<dc:creator><![CDATA[Kirill Bogdanov]]></dc:creator>
		<pubDate>Tue, 16 Jul 2024 15:02:38 +0000</pubDate>
				<category><![CDATA[Amazon Elastic Block Store (Amazon EBS)]]></category>
		<category><![CDATA[Amazon Elastic File System (EFS)]]></category>
		<category><![CDATA[Amazon File Cache]]></category>
		<category><![CDATA[Amazon FSx]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[Amazon FSx for NetApp ONTAP]]></category>
		<category><![CDATA[Amazon FSx for OpenZFS]]></category>
		<category><![CDATA[Amazon Simple Storage Service (S3)]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[FSI]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Lustre]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">ca9e22473e1200ab94d24f287d767e37cfaf1f73</guid>

					<description>You can boost the performance of your compute grids by strategically distributing your binaries. Our experts looked at lots of strategies for fast &amp;amp; efficient compute grid operations - to save you some work.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3906" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/24/AdobeStock_852512916.png" alt="Strategies for distributing executable binaries across grids in financial services" width="380" height="212"&gt;Financial Services Institutions (FSIs) rely heavily on compute grids for their daily operations. One key challenge of operating such grids is the distribution of data, including the distribution of binaries required for computation.&lt;/p&gt; 
&lt;p&gt;In this blog post, we’ll focus on classifying different binary distribution techniques and providing intuition and guidance for customers about when to use each technique in connection with their business objectives and requirements. We’ll also offer insights and recommendations that others can apply outside of the FSI industry.&lt;/p&gt; 
&lt;h2&gt;The problem&lt;/h2&gt; 
&lt;p&gt;Most customer concerns regarding data distribution revolve around business objectives, and cost. Some of these concerns include cost-effectively distributing large volumes of data regularly, and doing it quickly, to ensure the rapid start of compute instances. This helps to avoid subsequent delays throughout the execution, and often translates into cost savings, too.&lt;/p&gt; 
&lt;p&gt;Consider a grid configured to scale to 5,000 x Amazon Elastic Compute Cloud (Amazon EC2) instances. With fluctuating demand causing frequent scaling events, each new instance has to download the binary files it needs. Assuming up to 5 scaling events daily, with a 1-minute download time, this results in over 416 hours or 17 days of wasted EC2 time – &lt;em&gt;highlighting the value of efficient binary distribution&lt;/em&gt;.&lt;/p&gt; 
&lt;h2&gt;How to assess your options&lt;/h2&gt; 
&lt;p&gt;Choosing the suitable data layer is not just a technical decision but a strategic one that impacts the cost efficiency of the entire computing grid. The wrong choice can lead to significant inefficiencies and increased costs. Therefore, it’s crucial to consider the options and their implications carefully.&lt;/p&gt; 
&lt;p&gt;What factors should customers consider when evaluating their options for binary distribution based on workload requirements and characteristics?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Size – &lt;/strong&gt;What is the total size of the &lt;em&gt;super-set of all binaries&lt;/em&gt; that must be present on the Amazon EC2 instance? What is the size of the &lt;em&gt;most commonly used&lt;/em&gt; binary files sufficient to start the workload in a lazy loading situation (i.e., waiting for the rest of the files to be delivered later)?&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Size distribution: &lt;/strong&gt;Does the binary set consist of a large number of small files, or a small number of large files?&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scaling: &lt;/strong&gt;What’s the maximum number of instances you need to have running simultaneously? What’s the maximum scaling speed (the higher the speed, the more bandwidth necessary)?&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantics: &lt;/strong&gt;Does the workload require POSIX file access? Will it be necessary to have excess capacity, to allow adding to, or altering, binary sets during the production stage?&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Options&lt;/h2&gt; 
&lt;h3&gt;Amazon Simple Storage Service (Amazon S3)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="mailto:https://aws.amazon.com/s3/"&gt;Amazon S3&lt;/a&gt; is the AWS object storage service, allowing users to store and retrieve any data from anywhere. Amazon S3 can act as a common destination for all built and versioned binary files, and distribute these files among Amazon EC2 instances.&lt;/p&gt; 
&lt;p&gt;To illustrate typical tradeoffs, we begin with the most intuitive approach of copying the required files from Amazon S3. A common destination could be EC2 Instance Store or an Elastic Block Service (EBS) volume attached to the EC2 instance. A grid administrator can script the copy operation at instance boot time (e.g., using &lt;code&gt;&lt;a href="https://docs.aws.amazon.com/cli/latest/reference/s3/sync.html"&gt;aws s3 sync&lt;/a&gt;&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;The primary advantage is that once files are copied, the workload has the lowest latency and highest bandwidth for accessing these files. Performance isn’t impacted by the total number of EC2 instances as each instance has its own volume.&lt;/p&gt; 
&lt;p&gt;Furthermore, Amazon S3 provides very high throughput, sufficient even for the most demanding use cases. Combined with the pay-as-you-go model, grid administrators don’t need to scale S3 to match fluctuating bandwidth demands, simplifying administration.&lt;/p&gt; 
&lt;p&gt;Amazon S3 is a regional service with no charges for data transfers across Availability Zones (AZs) and has very low storage costs. In the context of binary distribution, the bulk of the cost will be associated with the number of files transferred, which comes from API calls. This is important to consider for binaries consisting of many small files.&lt;/p&gt; 
&lt;p&gt;The key tradeoff is the initial delay associated with the time it takes to copy the files from S3. During this time, EC2 will have to idle until all the data is copied. You can partially mitigate this by splitting your binaries and downloading them in parts – at the expense of added complexity to manage this.&lt;/p&gt; 
&lt;h3&gt;Amazon S3 Express One Zone (EOZ)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/s3/storage-classes/express-one-zone/"&gt;Amazon S3 Express One Zone&lt;/a&gt; (EOZ) stores data in a single Availability Zone, reducing data access latency and increasing bandwidth from within the AZ. Because FSI grids often require significant computing capacity our recommendation is to diversify EC2 instance types and use as many AZs as possible. However, this conflicts with the benefits of single AZ solutions.&lt;/p&gt; 
&lt;p&gt;Grid administrators faced with this have two main options. They can have a single instance of the storage service running in a single AZ, which will partially alleviate latency and bandwidth advantages when instances from other AZs access data. Alternatively, grid administrators can have a separate instance of the storage in each AZ – this will increase storage costs and add complexity to the synchronization of data between S3 EOZ buckets before the start of the workload.&lt;/p&gt; 
&lt;h3&gt;Mountpoint for Amazon S3&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/s3/features/mountpoint/"&gt;Mountpoint for Amazon S3&lt;/a&gt; – an open-source file client that enables easy access to objects in an S3 bucket through standard file system APIs on your EC2 instance.&lt;/p&gt; 
&lt;p&gt;With Mountpoint, files stay in S3, and only get locally cached when accessed, unlike the previously described technique. This makes it possible to start the workload instantaneously and load files on-demand throughout the execution process. This approach particularly useful when you don’t know beforehand which binary files you require for the workload execution, or if only a small fraction of files are likely to be needed.&lt;/p&gt; 
&lt;p&gt;However, the first execution of the workload will still experience higher latency than subsequent executions due to the need to cache files locally.&lt;/p&gt; 
&lt;h3&gt;Amazon Elastic File System (EFS)&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/efs/"&gt;EFS&lt;/a&gt; is a fully managed, network-attached storage service that supports NFS protocols. It provides a POSIX file access API and supports up to 20 GB/sec of read throughput, up to 250,000 read IOPS, and up to 25,000 connections. The actual performance will depend on the throughput mode selected for EFS: &lt;em&gt;Elastic&lt;/em&gt;, &lt;em&gt;Provisioned&lt;/em&gt;, or &lt;em&gt;Bursting &lt;/em&gt;which you should choose after assessing your workload requirements (see more details in the &lt;a href="https://docs.aws.amazon.com/efs/latest/ug/performance.html"&gt;EFS documentation&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Unlike S3-based solutions there are no charges associated with accessing individual files stored on EFS. Instead, there is a cost associated with the amount of data transferred.&lt;/p&gt; 
&lt;p&gt;Similarly to S3 Express One Zone, is EFS One Zone (OZ). However, in addition to tradeoffs associated with one AZ solutions (which we described earlier) EFS OZ has cross-AZ data transfer charges, thus we don’t recommend using EFS OZ for this task.&lt;/p&gt; 
&lt;h3&gt;Amazon FSx&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/fsx/"&gt;&lt;strong&gt;Amazon FSx&lt;/strong&gt;&lt;/a&gt; lets you choose between several widely-used, distributed, network-attached file systems: &lt;a href="https://aws.amazon.com/fsx/lustre/"&gt;Lustre&lt;/a&gt;, &lt;a href="https://aws.amazon.com/fsx/netapp-ontap/"&gt;NetApp ONTAP&lt;/a&gt;, &lt;a href="https://aws.amazon.com/fsx/openzfs/"&gt;OpenZFS&lt;/a&gt;, and &lt;a href="https://aws.amazon.com/fsx/windows/"&gt;Windows File Server&lt;/a&gt;. Each of these file systems provides even greater throughput and lower latency than EFS. You can explore a more detailed comparison of these services &lt;a href="https://aws.amazon.com/fsx/when-to-choose-fsx/"&gt;using our published advice&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This post focuses on FSx for Lustre (FSxL), which can scale to &lt;a href="https://aws.amazon.com/blogs/hpc/build-and-deploy-a-1-tb-s-file-system-in-under-an-hour/"&gt;provide 1TByte/sec throughput&lt;/a&gt; (or more), &lt;em&gt;millions&lt;/em&gt; of IOPs, and sub-millisecond access latency, making it the best choice for many large-scale workloads.&lt;/p&gt; 
&lt;p&gt;FSxL also integrates with Amazon S3, allowing users POSIX-level file access to files stored on S3. When a client attempts to access a file, FSx first retrieves it from S3 and caches it at the FSxL server, making it readily available for subsequent requests by other users.&lt;/p&gt; 
&lt;p&gt;FSxL can be configured to match target capacity and/or throughput. This is a task the admin must invoke manually. FSxL deploys into a single AZ, meaning file access across AZs will incur additional data charges.&lt;/p&gt; 
&lt;p&gt;Both FSxL and EFS offer full POSIX file-access support, allowing writes by any Amazon EC2 instance to be available to other instances. While binary files are generally immutable and don’t need this feature, it’s beneficial for the same data layer to be used for other needs like providing computational input, storing results, or maintaining shared state.&lt;/p&gt; 
&lt;h3&gt;Amazon File Cache&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/filecache/"&gt;&lt;strong&gt;Amazon File Cache&lt;/strong&gt;&lt;/a&gt; is a high-speed caching service similar to FSxL but with a number of distinct differences.&lt;/p&gt; 
&lt;p&gt;First, in addition to supporting Amazon S3, File Cache can support data repositories using NFSv3, which means that the source files can be located virtually anywhere, including on-premises. This can simplify the process of delivering quant libraries – especially if they are built on-premises.&lt;/p&gt; 
&lt;p&gt;File Cache automatically loads files and metadata from the origin and releases the least recently-used cached files to ensure the most active files are available in the cache for your applications. File Cache is built using Lustre, which means effective I/O throughput can be very high.&lt;/p&gt; 
&lt;h3&gt;Amazon EBS volumes&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/ebs/"&gt;&lt;strong&gt;Amazon EBS volumes&lt;/strong&gt;&lt;/a&gt; are persistent block storage devices that can be attached to an EC2 instance. They function like HDD/SSD drives, with varying performance characteristics and costs. For example, &lt;em&gt;io2 Block Express&lt;/em&gt; volumes offer a maximum throughput of 4000 MB/s and 256,000 IOPS, per volume.&lt;/p&gt; 
&lt;p&gt;To use EBS volumes for binary distribution, you can follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Create a source EBS volume for the binary distributions, you can do this as part of CI/CD pipeline.&lt;/li&gt; 
 &lt;li&gt;Create a persistent EBS snapshot, which is a point-in-time copy of an EBS volume saved in S3. You can use EBS snapshots to restore and create multiple copies of the original EBS volume.&lt;/li&gt; 
 &lt;li&gt;Configure an EC2 Launch Template to include an EBS volume based on the newly created snapshot.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;When an EBS volume is created from a snapshot and attached to an EC2 instance, only the metadata of the volume is loaded initially. The volume’s data blocks are not loaded from S3 into the EBS block storage until they’re accessed or needed by the instance. These blocks will be lazy-loaded as they’re being accessed. As a result, the very first access to these blocks (and consequently the first workload invocation) will incur additional latency (we explain this in more detail in &lt;a href="https://docs.aws.amazon.com/ebs/latest/userguide/ebs-initialize.html"&gt;our documentation&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;When creating EBS volumes from a snapshot within the same region, there are no additional charges for data transfer, i.e., you only pay for the GBs of storage capacity.&lt;/p&gt; 
&lt;p&gt;For completeness, we should mention that &lt;code&gt;io1&lt;/code&gt; and &lt;code&gt;io2&lt;/code&gt; volume types enable you to attach a single EBS volume to up to 16 x EC2 instances with the same AZ, reducing the total number of EBS volumes needed. However, this comes at the tradeoff of additional management efforts – mapping volumes to EC2 instances. Also, the total IOPS performance across all the attached instances cannot exceed the volume’s maximum provisioned IOPS. Finally, you should also consider EBS &lt;em&gt;performance&lt;/em&gt; when sharing a volume among multiple concurrent workers running on a large EC2 instance.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"&gt;&lt;strong&gt;AMIs&lt;/strong&gt;&lt;/a&gt; are a template that contains the software configuration required to launch an Amazon EC2 instance. An AMI often includes a reference to one or more EBS Snapshots. In the context of binary distribution, AMIs can be thought of as an abstraction layer on top of EBS volumes, bypassing the need to manually attach an EBS volume to an instance. There are no performance or cost benefits in prebaking binary files into AMIs over using EBS volumes. Therefore, the choice is based on the managerial overhead and the grid’s architecture. We explore this topic in an article about &lt;a href="https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/prebaking-vs.-bootstrapping-amis.html"&gt;Prebaking vs. bootstrapping AMIs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html"&gt;Warm Pools&lt;/a&gt; in EC2 Auto Scaling let you pre-initialize EC2 instances and then stop them, &lt;em&gt;until you need to scale out&lt;/em&gt;. The EBS volume continues to hydrate in the background while the instance is stopped, saving you time and resources when you’re ready to scale, since your instances will spin up quickly and EBS volumes will be hydrated.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://docs.aws.amazon.com/ebs/latest/userguide/ebs-fast-snapshot-restore.html"&gt;&lt;strong&gt;Fast Snapshot Restore&lt;/strong&gt; (FSR)&lt;/a&gt; is a feature of Amazon EBS that optimizes EBS snapshots for faster volume restoration. When an EBS snapshot is marked for FSR, the EBS volumes restored from this snapshot can benefit from full performance immediately as the volume is created, without the need for a lazy loading mechanism. There is a &lt;a href="https://docs.aws.amazon.com/ebs/latest/userguide/ebs-fast-snapshot-restore.html#volume-creation-credits"&gt;credit-based system&lt;/a&gt; associated with the rate at which you can create EBS volumes from an FSR-enabled snapshot. These limits might render this method unsuitable for large-scale grids.&lt;/p&gt; 
&lt;h2&gt;Summary of options discussed&lt;/h2&gt; 
&lt;p&gt;Let’s now summarize the relevant characteristics of these services in a table (Figure 1). The &lt;em&gt;Source&lt;/em&gt; column shows where the binaries come from, while the &lt;em&gt;Destination&lt;/em&gt; column indicates if files are relocated or accessed remotely.&lt;/p&gt; 
&lt;p&gt;The next column indicates if &lt;em&gt;all files are accessible as soon as EC2 boots up&lt;/em&gt;. For example, if the files are stored on Amazon S3 and need to be copied to EBS before the workload can start, this step requires manual implementation. Next, some services &lt;em&gt;deliver full performance upon the first invocation&lt;/em&gt;, while other are not. For example, EFS provides full performance right away, but restoring an EBS volume from a snapshot may result in longer access times initially, due to hydration.&lt;/p&gt; 
&lt;p&gt;The cost column indicates workload dimensions that are most likely to account for the bulk cost as your service scales horizontally. We’ve shown this column only for an indicative purpose – review the costs of each service yourself to get an accurate estimation. For example, for a workload with many small files stored on Amazon S3 and accessed via S3 Mount Point, there is a cost associated with API calls per file. In contrast, the cost of FSxL varies based on provisioned throughput and total data transferred.&lt;/p&gt; 
&lt;p&gt;Finally, the last three &lt;em&gt;general&lt;/em&gt; columns indicate if the same service can be reused to deliver more files at runtime. For example, you can place a new version of a binary distribution on S3 and retrieve it via S3 Mount Point. But if you distribute the files using EBS or an AMI, then a new EBS needs to be mounted – or a different method needs to be used – to get the updates. The &lt;em&gt;global file system capabilities&lt;/em&gt; indicates if writes from one instance are available for reading by other instances in the grid.&lt;/p&gt; 
&lt;div id="attachment_3897" style="width: 1238px" class="wp-caption aligncenter"&gt;
 &lt;a href="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/27/IMG-2024-06-27-12.57.52.png" target="_new" rel="noopener"&gt;&lt;img aria-describedby="caption-attachment-3897" loading="lazy" class="size-full wp-image-3897" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/27/IMG-2024-06-27-12.57.52.png" alt="Figure 1: Comparison of the discussed binary distribution techniques in the relationship to different workload requirements. Click on the image to get a larger (more readable) version." width="1228" height="605"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-3897" class="wp-caption-text"&gt;Figure 1: Comparison of the discussed binary distribution techniques in the relationship to different workload requirements. Click on the image to get a larger (more readable) version.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Some single AZ services have costs associated with cross-AZ network traffic, so refer to the documentation of each service for details.&lt;/p&gt; 
&lt;h2&gt;Conclusion and recommendations&lt;/h2&gt; 
&lt;p&gt;The purpose of this post is to provide a general guide on how to reason about the challenge of binary distribution in large FSI grids.&lt;/p&gt; 
&lt;p&gt;Our suggestion is to begin by evaluating the problem dimensions outlined in this post, in relation to your workload. Then, consider whether binaries need to be relocated closer to the computing resources or if they can be accessed over the network. We recommend starting with the simplest solution that fits the requirements, testing it, and then determining if further improvements are necessary.&lt;/p&gt; 
&lt;p&gt;When moving files to a compute instance, there may be a high initial overhead, but it can ensure consistent, isolated high performance. When it comes to network file systems, it’s important to keep in mind that they are shared among all compute instances that access them. Thus, NFS should be pre-provisioned for the load and scale you anticipate. Finally, you should always consider cross-AZ traffic costs and latency when making your choice.&lt;/p&gt; 
&lt;p&gt;Complexity often requires balancing performance and costs. You can incorporate your knowledge of your workloads and some simple heuristics in how you select the distribution methods presented in this post to mitigate many trade-offs, and feel good about your decisions.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Running FSI workloads on AWS with YellowDog</title>
		<link>https://aws.amazon.com/blogs/hpc/running-fsi-workloads-on-aws-with-yellowdog/</link>
		
		<dc:creator><![CDATA[Kirill Bogdanov]]></dc:creator>
		<pubDate>Wed, 10 Jul 2024 14:51:21 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[Financial Services]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Amazon FSx]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[EC2 Spot]]></category>
		<category><![CDATA[FSI]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">d573e34c0811266707460763324d754054fc016c</guid>

					<description>Financial services firms: we stress-tested YellowDog's HPC environment to see if it could handle a 10m task batch at 3,000 tasks per second. Check out the results.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt; &lt;img loading="lazy" class="alignright size-full wp-image-3913" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/24/Running-FSI-workloads-on-AWS-with-YellowDog.png" alt="Running FSI workloads on AWS with YellowDog 2" width="380" height="212"&gt;This post was contributed by Kirill Bogdanov and Alan Parry, CTO at YellowDog&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Large compute grids are critical components of any financial services industry (FSI) organisation today. They’re used in daily operations from pricing products, to risk management and monitoring, and even regulatory reporting. Historically, though, compute grids have been complex to operate and expensive to run. Even worse, some organizations’ demand for compute regularly exceeds their limited on-premises capacity. All of this has led to FSI firms exploring cloud for modernisation and as a cost effective alternative.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://yellowdog.co/"&gt;YellowDog&lt;/a&gt; is a high performance computing (HPC) environment that specializes in large-scale workloads and hybrid-cloud deployments, allowing customers to use their on-premises capacity while simultaneously taking advantage of the scale and capabilities of AWS.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll look at YellowDog’s performance and its operational characteristics that are important for FSI customers including: scheduling latency, throughput, and integration with AWS services.&amp;nbsp;This is the second post in a series – in our &lt;a href="https://aws.amazon.com/blogs/hpc/running-a-3-2m-vcpu-hpc-workload-on-aws-with-yellowdog/"&gt;previous post&lt;/a&gt;, we showed you how YellowDog can scale up to 3.2 million vCPUs on AWS in just under 33 minutes.&lt;/p&gt; 
&lt;h2&gt;The challenge familiar to FSIs&lt;/h2&gt; 
&lt;p&gt;Today, new financial regulatory requirements like FRTB (the &lt;em&gt;Fundamental Review of the Trading Book&lt;/em&gt;), RWA (&lt;em&gt;Risk Weighted Assets&lt;/em&gt;), and ESG (&lt;em&gt;Environmental, Social, and Governance&lt;/em&gt;) have resulted in a significant increase (up to 10x) in computational demand, compelling FSI firms to find additional compute capacity.&lt;/p&gt; 
&lt;p&gt;Furthermore, FSI workloads often involve large numbers of short-running tasks, requiring a variation of HPC known as high throughput computing (HTC). It’s not uncommon to process more than 100 million tasks per day, with a substantial portion of these tasks taking less than two seconds to run. These characteristics can make many ‘normal’ job schedulers ill-suited for the task.&amp;nbsp;But the ability to perform more computations, sooner, translates directly into improved quality of service and market competitiveness for an institution.&lt;/p&gt; 
&lt;p&gt;These factors are driving FSI firms to reimagine their computing grids and to explore cloud-based modernization pathways in pursuit of improved functionality and cost efficiency.&lt;/p&gt; 
&lt;p&gt;YellowDog is a leading cloud provisioning and job scheduling stack that enables customers across FSI to optimise their compute infrastructure and run complex workloads across multiple AWS regions, simultaneously. Today we’re evaluating the suitability of running YellowDog on AWS for FSI HTC workloads, specifically.&lt;/p&gt; 
&lt;h2&gt;Definitions&lt;/h2&gt; 
&lt;p&gt;For the rest of this post, let’s standardize on some terms so we don’t get tangled up:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Task&lt;/strong&gt; – A single unit of work submitted for processing.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Job&lt;/strong&gt; – A logical group of tasks that are submitted and executed together, often with interdependencies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Workload – &lt;/strong&gt;A larger group of jobs that are executed to deliver a business outcome, for example, a complete nightly batch.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Worker&lt;/strong&gt; – A process of some kind running on EC2 that can execute a task. An EC2 instance can facilitate support multiple workers, consuming different amounts of resource, each launching processes or containers.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Scheduling overhead&lt;/h2&gt; 
&lt;p&gt;Scheduling overhead (or scheduling &lt;em&gt;latency&lt;/em&gt;) is the time it takes for a client application to submit a job to the grid and to receive a processing result, excluding the time for the actual computational processing itself.&lt;/p&gt; 
&lt;p&gt;Low scheduling overhead can let traders price instruments and assess risk exposure in close to real-time, which can be critical in many situations.&amp;nbsp;Furthermore, when processing short-running jobs, low scheduling overhead ensures high grid utilisation, with most of the compute time spent on job processing rather than job scheduling and book-keeping.&lt;/p&gt; 
&lt;p&gt;Let’s look at how YellowDog serves these near real-time intraday workloads. Figure 1 illustrates the concepts of scheduling overhead.&lt;/p&gt; 
&lt;div id="attachment_3910" style="width: 1003px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3910" loading="lazy" class="size-full wp-image-3910" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/24/IMG-2024-06-24-15.01.14.png" alt="Figure 1: The timeline depicting events from the moment a task is submitted to a grid until results are received by the client. Scheduling overhead is the difference between the completion time perceived by the client and the actual task execution time. &amp;nbsp;" width="993" height="339"&gt;
 &lt;p id="caption-attachment-3910" class="wp-caption-text"&gt;Figure 1: The timeline depicting events from the moment a task is submitted to a grid until results are received by the client. Scheduling overhead is the difference between the completion time perceived by the client and the actual task execution time.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;This figure presents a timeline depicting the sequence of events from the moment a task is submitted to a grid until results are received by the client. For the sake of simplicity, we’ll consider a job consisting of a single task.&lt;/p&gt; 
&lt;p&gt;At time &lt;em&gt;T1&lt;/em&gt;, a client application prepares a job and submits it to the grid at time &lt;em&gt;T2&lt;/em&gt;.&amp;nbsp;At this stage, the task enters the scheduling queue of the grid and waits to be assigned to a worker process.&amp;nbsp;At time &lt;em&gt;T3&lt;/em&gt;, a worker picks up the task and executes it until completion at &lt;em&gt;T4&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;Finally, at time T5, using push or pull mechanisms, the client application becomes aware that the job has completed, allowing the collection of results.&amp;nbsp;At time T6, the client application collects the results (though sometimes, notification and result retrieval are combined).&lt;/p&gt; 
&lt;p&gt;It’s important to note that while the actual execution time of the task spans from T3 to T4, the perceived time from the client’s perspective is T2 to T6. For this post, we’ll refer to the time difference between execution time &lt;em&gt;perceived&lt;/em&gt; by the client application and &lt;em&gt;actual&lt;/em&gt; job execution time as the &lt;strong&gt;scheduling overhead&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Based on these definitions, a client can submit &lt;em&gt;at most&lt;/em&gt; (1 / T2 – T6) tasks per second into the grid.&lt;/p&gt; 
&lt;h2&gt;Testing YellowDog’s scheduling&lt;/h2&gt; 
&lt;p&gt;We wanted to stress YellowDog’s scheduling components, so we used &lt;em&gt;zero-work tasks &lt;/em&gt;(tasks that did literally nothing), so the processing step would simply return a hard coded output. That way we minimised the T3 to T4 time interval, and focused on the scheduling overhead itself.&lt;/p&gt; 
&lt;p&gt;Figure 2 shows the cumulative distribution functions (CDFs) of the measured scheduling overhead. In this figure, the &lt;em&gt;x&lt;/em&gt;-axis shows the end-to-end latency from the moment of submission of the job until all the results of the task are returned to the client, while the &lt;em&gt;y&lt;/em&gt;-axis shows the percentile – the fraction of jobs that completed within the indicated latency.&lt;/p&gt; 
&lt;p&gt;We performed these measurements using jobs containing 1, 10, and 100 zero-work tasks, and plotted them using full, dashed, and dotted lines respectively in Figure 2.&lt;/p&gt; 
&lt;p&gt;We also considered two distinct job-completion notification mechanisms. The first mechanism relies on YellowDog’s task-level REST API. This API allows us to check the status of individual tasks.&lt;/p&gt; 
&lt;p&gt;The second mechanism is a custom extension we built with Amazon ElastiCache. This mechanism relies on the worker to mark task completion directly in the cache, with the client application pulling results from the cache. This is more consistent with how real-world tasks would usually be managed.&lt;/p&gt; 
&lt;div id="attachment_3911" style="width: 956px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3911" loading="lazy" class="size-full wp-image-3911" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/24/IMG-2024-06-24-15.01.51.png" alt="Figure 2: End-to-end task completion time as perceived by a client. Median latency for different notification mechanisms is around 400ms." width="946" height="597"&gt;
 &lt;p id="caption-attachment-3911" class="wp-caption-text"&gt;Figure 2: End-to-end task completion time as perceived by a client. Median latency for different notification mechanisms is around 400ms.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The results indicated that YellowDog can provide an end-to-end scheduling overhead of a few hundred milliseconds, which is sufficiently low to be unnoticeable by a user like a trader, requiring a more or less real-time response.&lt;/p&gt; 
&lt;h2&gt;Efficiency with short duration tasks&lt;/h2&gt; 
&lt;p&gt;For ‘long’ running tasks (that is, &lt;em&gt;minutes to hours&lt;/em&gt;) a scheduling latency of one second might not be relevant. But this changes when processing time is measured in seconds: a low scheduling overhead directly translates into efficiency when processing short running tasks.&lt;/p&gt; 
&lt;p&gt;YellowDog’s &lt;em&gt;Worker&lt;/em&gt; polls the YellowDog &lt;em&gt;Scheduler&lt;/em&gt; for tasks to execute. If there are no tasks for a Worker to process it will sleep for a randomly chosen, but &lt;em&gt;configurable,&lt;/em&gt; duration which defaults to be between 0-60 seconds.&lt;/p&gt; 
&lt;p&gt;Once a task becomes available, the Worker will retrieve it, execute it, and will then attempt to retrieve the next task from the queue. If there are no further tasks waiting, the Worker will sleep again until another task becomes available.&lt;/p&gt; 
&lt;p&gt;The duration of the Worker sleep interval is only relevant when there are no tasks in the Worker’s queue, and it can be adjusted at a system-wide level if required, to reduce the average interval.&lt;/p&gt; 
&lt;p&gt;By using a prepopulated task queue, a single YellowDog Worker can efficiently process up to 25 tasks per second, implying a base scheduling overhead of only 40 milliseconds. This processing speed translates into an efficiency rate of 98% for tasks that complete within two seconds.&lt;/p&gt; 
&lt;p&gt;That means a single YellowDog Worker can efficiently consume short tasks from a queue and deliver results in a timely fashion.&lt;/p&gt; 
&lt;h2&gt;Throughput&lt;/h2&gt; 
&lt;p&gt;Throughput refers to the rate at which tasks can be processed. Throughput is particularly important for nightly batches when millions of tasks need to be processed as a part of a larger workload, as efficiently as possible. The ability to schedule a large volume of tasks quickly and efficiently is of paramount importance for a successful FSI grid scheduler.&lt;/p&gt; 
&lt;p&gt;More importantly, a fast scheduler can scale &lt;em&gt;horizontally&lt;/em&gt; to take full advantage of the compute resources available at AWS, allowing workloads to complete faster, thereby providing an opportunity for additional computation or to re-run workloads that required correction.&lt;/p&gt; 
&lt;p&gt;To measure YellowDog’s throughout we configured 2,000 YellowDog Workers (each with 1vCPU and 4GiB RAM), and we used 16 clients to submit zero-work jobs comprising of 5,000 to 10,000 tasks with 10,000,000 tasks in total.&lt;/p&gt; 
&lt;p&gt;We measured the total time from the moment we initiated submission, until the last task was complete.&lt;/p&gt; 
&lt;p&gt;Using this approach, we measured the average tasks per second (TPS) to be 3,000. This translates to 85 million tasks per typical overnight eight-hour window.&lt;/p&gt; 
&lt;h2&gt;Integration with AWS services&lt;/h2&gt; 
&lt;p&gt;When it comes to running large scale grids on AWS, it’s critical to follow the best practices and use the correct APIs to obtain the necessary compute capacity quickly – and cost effectively.&lt;/p&gt; 
&lt;p&gt;YellowDog supports the Amazon Elastic Computer Cloud (Amazon EC2) Fleet API, including important options like EC2 Spot, Allocation Strategies (including the recommended ‘price-capacity-optimized’). It also supports attribute-based instance selection, which greatly simplifies finding suitable Amazon EC2 instances for your workload to include in that mix for Spot.&lt;/p&gt; 
&lt;p&gt;Using ‘dynamic templates’ YellowDog automatically selects the best source of compute based on your requirements and preferences, especially useful when your workload characteristics can change. This automation is fed by YellowDog Insights which measures Amazon EC2 instance types across price, performance, benchmarking, location, and energy efficiency. Included in this data is a comparison of spot and on-demand pricing and YellowDog’s managed compute requirements ensure that if you experience spot interruptions, compute can be re-provisioned and tasks can be re-tried on other workers.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;We saw a median scheduling overhead of 400ms, and a throughput greater than 3,000 tasks per second. In our previous post we showed YellowDog scaling to more than 3.2 million vCPUs, and maintaining a high utilization for that fleet. Taken together, these results indicate that YellowDog has the capability to handle a wide range of FSI workloads.&lt;/p&gt; 
&lt;p&gt;Based on these results, we think YellowDog has the necessary scale, efficiency, and low scheduling latency to handle both batch &lt;em&gt;and&lt;/em&gt; intraday jobs effectively for financial services customers. It’s also well integrated with AWS services, which allows it to take full advantage of the power of AWS when provisioning compute resources at scale.&lt;/p&gt; 
&lt;p&gt;YellowDog’s existing FSI customers are made up of large asset managers and hedge funds running complex hybrid-cloud workloads, at scale. You can &lt;a href="https://aws.amazon.com/marketplace/pp/prodview-q3kswq4btig6o?sr=0-1&amp;amp;ref_=beagle&amp;amp;applicationId=AWSMPContessa"&gt;access YellowDog&lt;/a&gt; through AWS Marketplace, which will include the cost of the solution in your monthly AWS bill.&lt;/p&gt; 
&lt;p&gt;You can &lt;a href="https://yellowdog.co/get-started/"&gt;get in touch with YellowDog&lt;/a&gt; directly to arrange a discovery call, discuss your business requirements, and learn how they can support you. Alternatively, reach out to the FSI HPC experts at AWS by emailing us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Improve HPC workloads on AWS for environmental sustainability</title>
		<link>https://aws.amazon.com/blogs/hpc/improve-hpc-workloads-on-aws-for-environmental-sustainability/</link>
		
		<dc:creator><![CDATA[Sam Mokhtari]]></dc:creator>
		<pubDate>Tue, 02 Jul 2024 12:35:15 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<category><![CDATA[visualization]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">da98b13a916f8f924ea8af83da8c8373caefe598</guid>

					<description>Need to cut your carbon footprint without sacrificing productivity? Migrating HPC workloads to the cloud allowed Baker Hughes to reduce emissions by 99%! Get tips for optimizing compute, storage, networking so you can do better.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3870" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/18/boofla88_a_supercomputer_sitting_in_the_middle_of_a_rainforest._013a2b55-5560-40a6-bf1e-9ee121f4dd3c.png" alt="Improve HPC workloads on AWS for environmental sustainability" width="380" height="212"&gt;HPC workloads are used across industries from finance and engineering to genomics and chemistry – all to enable opportunities for innovation and growth. Because the compute is so intensive, these systems can consume large amounts of energy and generate serious carbon emissions. Many businesses are looking to reduce their carbon footprint to meet the Paris accord, and on-premises HPC clusters constitute a noticeable portion of their total data center impact. But by migrating their HPC workloads to AWS &lt;a href="https://aws.amazon.com/solutions/case-studies/baker-hughes-case-study/"&gt;Baker Hughes&lt;/a&gt; was able to reduce their carbon footprint by 99%, setting an example for the community.&lt;/p&gt; 
&lt;p&gt;If you want to achieve a similar result, it’s critical to optimize your cluster for resource efficiency &lt;em&gt;and&lt;/em&gt; effectiveness. HPC architectures are built on five main pillars: compute, storage, networking, visualization and orchestration.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll focus on each one of these pillars, and discuss tips and best practices that can help you optimize resource usage and the environmental impact of your workloads, while also keeping your productivity goals.&lt;/p&gt; 
&lt;p&gt;To demonstrate these ideas, we’ll assume the use case of an automotive customer who needs to run computer-aided engineering (CAE). These types of jobs need large volumes of compute, so they’re representative of most simulations you’re likely to be doing in your environment, too.&lt;/p&gt; 
&lt;h2&gt;Compute&lt;/h2&gt; 
&lt;h3&gt;Choose the right instance type for your HPC workload&lt;/h3&gt; 
&lt;p&gt;In an on-premises cluster, the computing nodes are usually homogeneous – they have the same CPU type and the quantity of memory per core. However, HPC workloads don’t all have the same requirements. For example, structural analysis can benefit from having a low latency NVMe scratch disk to access, computational fluid dynamics (CFD) jobs usually span across a greater number of cores, and finite element analysis (FEA) simulations need more memory per core, compared to other types of simulations.&lt;/p&gt; 
&lt;p&gt;To reduce your carbon footprint, you need to optimize your consumption of computing resources, so it’s important to match your workload to the &lt;strong&gt;right compute instance types&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;In AWS there are more than 750 types of Amazon Elastic Compute Cloud (Amazon EC2) instances to choose from, each one with different hardware characteristics. Recently, we launched a new family of instances dedicated to HPC workloads.&lt;/p&gt; 
&lt;p&gt;For our CAE use case, you can use the new Amazon EC2 Hpc7a instance. We’ve seen them perform very well for workloads that can be accelerated by increasing the number of compute cores. You can read about some of our &lt;a href="https://aws.amazon.com/blogs/hpc/deep-dive-into-hpc7a-the-newest-amd-powered-member-of-the-hpc-instance-family/"&gt;benchmark tests in another post&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Use AWS Graviton instances if you can&lt;/h3&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/ec2/graviton/"&gt;AWS Graviton&lt;/a&gt;&amp;nbsp;is a custom-designed processor created by AWS, based on the Arm64 architecture and which provides a balance of efficiency, performance, and cost. This makes it a good fit to lower the environmental impact of many HPC workloads. &lt;a href="https://aws.amazon.com/ec2/graviton/graviton-sustainability/"&gt;AWS Graviton-based Amazon EC2 instances use up to 60% less energy&lt;/a&gt; than comparable EC2 instances for the same performance. If your HPC application can be compiled for Arm64, you can use Graviton instances to enable greater energy efficiency for your compute intensive workloads.&lt;/p&gt; 
&lt;p&gt;Hpc7g Instances are powered by AWS Graviton3E processors, with up to 35% higher vector instruction processing performance than the Graviton3. They are designed to give you the best price/performance for tightly coupled compute-intensive HPC and distributed computing workloads, and deliver 200 Gbps of dedicated network bandwidth that’s optimized for traffic between instances in the same subnet.&lt;br&gt; For CAE, customers can take advantage of open source CFD applications, like OpenFOAM, that can be compiled for Arm architecture processors and are well suited for the hpc7g instance type. But ISVs like Siemens, Cadence, and Ansys, now make versions of their solvers available for Graviton in the same way they do for x86.&lt;/p&gt; 
&lt;p&gt;The table below summarizes some of the key HPC instance types with their characteristics, and provides a recommendation of the target workload.&lt;/p&gt; 
&lt;div id="attachment_3924" style="width: 1003px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3924" loading="lazy" class="size-full wp-image-3924" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/07/03/IMG-2024-07-03-12.57.09.png" alt="Table 1 – Comparison of Amazon EC2 HPC specific instance types showing example workloads and attributes" width="993" height="446"&gt;
 &lt;p id="caption-attachment-3924" class="wp-caption-text"&gt;Table 1 – Comparison of Amazon EC2 HPC specific instance types showing example workloads and attributes&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Choose the right AWS Region for your HPC workload&lt;/h3&gt; 
&lt;p&gt;AWS Well-Architected framework helps customers to build secure, highly-performant, resilient, and efficient architecture, and is built around six pillars: operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. The framework provides a consistent approach for customers and partners to evaluate architectures and implement scalable designs.&lt;/p&gt; 
&lt;p&gt;The &lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/region-selection.html"&gt;SUS01-BP01&lt;/a&gt; best practices in the AWS Well-Architected &lt;em&gt;Sustainability&lt;/em&gt; pillar, recommends that you choose the AWS Regions for your workload based on both business requirements and sustainability goals.&lt;br&gt; As explained in another &lt;a href="https://aws.amazon.com/blogs/architecture/how-to-select-a-region-for-your-workload-based-on-sustainability-goals/"&gt;blog post&lt;/a&gt;, this process includes two key steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Assess and shortlist potential Regions for your workload based on your business requirements.&lt;/li&gt; 
 &lt;li&gt;Choose Regions near Amazon renewable energy projects and Region(s) where the grid has a lower published carbon intensity.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;As explained in the previous step, for the sample CAE workload, Hpc7a instances can accelerate this workload and help you to achieve your business goals.&lt;/p&gt; 
&lt;p&gt;By using this simple script, you can assess that this instance type is available, for example, in the Stockholm Region (eu-north-1):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;#!/bin/sh
instance_type=hpc7a.96xlarge

# Get a list of all AWS regions
regions=$(aws ec2 describe-regions --all-regions --query 'Regions[].RegionName' --output text)

echo "AWS Regions where $instance_type is available:"

# Loop through each region and check if the instance type is available
for region in $regions; do
  available=$(aws ec2 describe-instance-type-offerings --filters Name=instance-type,Values=$instance_type --region $region --query 'InstanceTypeOfferings[].InstanceType' --output text 2&amp;gt;/dev/null)

  if [ "$available" = "$instance_type" ]; then
    echo "- $region"
  fi
done
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Moreover, the electricity consumed in the Stockholm Region is attributable to 100% renewable energy as listed on the &lt;a href="https://sustainability.aboutamazon.com/environment/the-cloud?energyType=true#renewable-energy-map"&gt;Amazon sustainability website&lt;/a&gt;. So, this Region is a good candidate for running your CAE workloads, and also achieving your sustainability goals.&lt;/p&gt; 
&lt;h3&gt;Use a mix of EC2 purchase options&lt;/h3&gt; 
&lt;p&gt;HPC workloads usually have different resource requirements and business priorities. You can use a combination of purchase options to address your specific business needs, balancing instance flexibility, scalability, and efficiency.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html"&gt;Compute Savings Plans&lt;/a&gt; for predictable, steady-state workloads that allow flexibility if your needs (like target Region, instance families, or instance types) change frequently.&lt;/li&gt; 
 &lt;li&gt;Use&amp;nbsp;&lt;a href="https://alpha-docs-aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html"&gt;On-Demand Instances&lt;/a&gt; for new, and unpredictable workloads&lt;/li&gt; 
 &lt;li&gt;Use&amp;nbsp;&lt;a href="https://alpha-docs-aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html"&gt;Spot Instances&lt;/a&gt; to supplement the other options for applications that are fault tolerant and flexible.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Our example use case may require periods of intense activity to meet project milestones or client deadlines, where the elasticity of the cloud facilitates access to large amounts of On-Demand compute resources in order to meet tight deadlines. AWS HPC orchestration tools like AWS Batch and AWS ParallelCluster both have support for Spot, On-demand and Saving Plans resources.&lt;/p&gt; 
&lt;h2&gt;Storage&lt;/h2&gt; 
&lt;p&gt;AWS provides several types of storage and file system technologies that can be used for HPC workloads. Similar to compute, optimizing your storage choices can help you to accelerate your workload &lt;em&gt;and&lt;/em&gt; help to reduce your carbon emissions.&lt;/p&gt; 
&lt;p&gt;Every workload has a different data strategy, but we can split them across different stages:&lt;/p&gt; 
&lt;h4&gt;On-premises storage&lt;/h4&gt; 
&lt;p&gt;Amazon’s goal is to power our operations with 100% renewable energy by 2025 – five years ahead of our original 2030 target, so migrating your entire portfolio of workloads to AWS is usually a great option to reduce your carbon footprint.&lt;/p&gt; 
&lt;p&gt;In some cases, this is not possible because your data is generated from on-site instruments or labs. In this case, you need an efficient mechanism to move data to AWS to process in the cloud. Some of our managed file system offerings, such as &lt;a href="https://docs.aws.amazon.com/fsx/latest/FileCacheGuide/what-is.html"&gt;Amazon File Cache&lt;/a&gt;, allow you to mount the on-premises file system to the cloud. By doing that, you’ll avoid unnecessarily replicating your data in the cloud and the results of your simulations will also be accessible on-premises.&lt;/p&gt; 
&lt;h3&gt;Local Disk&lt;/h3&gt; 
&lt;p&gt;Use EBS volumes to store your data on the computing nodes. If your applications need access to high-speed, low-latency local storage for scratch files, you can use local NVMe-based SSD block storage physically connected to the host server. Many Amazon EC2 instances – like the Hpc6id – have multiple NVMe disks to accelerate the performance of the most IO demanding workloads. Keep in mind that these local disks are ephemeral, and their content will be automatically erased when you stop the instances. You need to identify which files you need to save when the job has been completed and move them to a persistent storage area, like a shared POSIX file system or Amazon S3.&lt;/p&gt; 
&lt;h3&gt;Shared POSIX file system&lt;/h3&gt; 
&lt;p&gt;If your applications need to access a shared file system, you can use one of the AWS managed file systems. For example, Amazon FSx for Lustre is a fully-managed service that provides high-performance storage for compute workloads. Powered by Lustre, the world’s most popular high-performance file system, FSx for Lustre offers shared storage with sub-millisecond latencies, up to &lt;em&gt;terabytes per second&lt;/em&gt; of throughput, and millions of IOPS.&lt;/p&gt; 
&lt;h3&gt;Long term storage&lt;/h3&gt; 
&lt;p&gt;Once the jobs are completed, you can migrate your data to a long-term storage in Amazon S3. This will help you to reduce the amount of data in the shared file system. FSx for Lustre also integrates with Amazon S3 via the Data Repository mechanism that allows seamless access to objects in Amazon S3 that can be lazy loaded into the high-performance file system layer. This approach delivers the required access semantics and performance for scalable HPC applications when they’re needed, while also providing the usual capacity, cost, data protection, lifecycle, and sustainability benefits of Amazon S3.&lt;/p&gt; 
&lt;h3&gt;Use life cycle policies to move and delete data&lt;/h3&gt; 
&lt;p&gt;HPC workloads have datasets with different retention and access requirements. For example, your HPC application may need frequent access to&amp;nbsp;some datasets for a limited period of time. After that, those datasets can be archived or deleted.&lt;/p&gt; 
&lt;p&gt;To efficiently manage your HPC datasets, you can configure lifecycle policies, which are rules that define how to handle datasets. You can set automated lifecycle policies to enforce lifecycle rules. You can set up automated lifecycle policies for &lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html"&gt;Amazon S3&lt;/a&gt;, &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html"&gt;Elastic Block Storeage (EBS)&lt;/a&gt;, and &lt;a href="https://docs.aws.amazon.com/efs/latest/ug/lifecycle-management-efs.html"&gt;Amazon EFS&lt;/a&gt;&lt;u&gt;.&lt;/u&gt;&lt;/p&gt; 
&lt;p&gt;Our example CAE application is well suited to the FSx for Lustre file system since it allows high performance concurrent access from multiple compute instances. You can use the &lt;a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-dra-linked-data-repo.html"&gt;S3 Data Repository&lt;/a&gt; settings to automate the migration of the data to Amazon S3 and then the S3 lifecycle policies to move the data from different&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html"&gt;storage classes&lt;/a&gt;&amp;nbsp;like&amp;nbsp;&lt;strong&gt;S3 Standard-IA &lt;/strong&gt;and&amp;nbsp;&lt;strong&gt;S3 Glacier Deep Archive.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Networking&lt;/h2&gt; 
&lt;p&gt;For tightly coupled workloads, it’s important to remove all network bottlenecks to guarantee the best utilization of your computing resources. These applications are sensitive to network latency so we recommended using &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html"&gt;&lt;strong&gt;Cluster Placement Group&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;s (CPG)&lt;/strong&gt; and &lt;a href="https://aws.amazon.com/hpc/efa/"&gt;&lt;strong&gt;Elastic Fabric Adapter (EFA)&lt;/strong&gt;&lt;/a&gt;. Deploying your compute nodes in the same &lt;strong&gt;CPG&lt;/strong&gt;, will allow for reliable low-latency communication between the instances, and will help your tightly-coupled application to scale as desired.&lt;br&gt; &lt;strong&gt;Elastic Fabric Adapter (EFA)&lt;/strong&gt; is a network interface for Amazon EC2 instances that enables customers to run HPC applications requiring high levels of inter-instance communications, like computational fluid dynamics, weather modelling, and reservoir simulation, at scale on AWS.&lt;/p&gt; 
&lt;p&gt;Coming back to our example, most of the HPC applications used in the automotive industry implement distributed memory parallelism using the Message Passing Interface (MPI), allowing a single simulation to employ parallel processing using the cores and memory of multiple instances to increase simulation speed. MPI applications perform best using instances which feature EFA because it reduces latency and increases throughput for MPI parallel communications.&lt;/p&gt; 
&lt;h2&gt;Orchestration&lt;/h2&gt; 
&lt;p&gt;If you want to reduce the environmental impact of HPC workloads, it’s important to right size your compute resources and deploy only when needed. There is usually no need to have all your compute always up and running when there are no jobs to run.&lt;/p&gt; 
&lt;p&gt;On AWS, you can start and stop the computing nodes based on your users’ needs: they submit their jobs and AWS HPC infrastructure scales capacity up (or down) to aim for the right capacity when needed.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; is a fully managed batch computing service that plans, schedules, and runs your containerized HPC or Machine Learning workloads across the full range of AWS compute offerings, such as Amazon ECS, Amazon EKS, and AWS Fargate, using Spot or On-Demand instances. If you select the allocation strategy, AWS Batch will automatically select the Spot instances that are large enough to meet the requirements of the jobs and are less likely to be interrupted. Using Spot instances is a great mechanism to run your compute intensive workload leveraging our unused capacity.&lt;/li&gt; 
 &lt;li&gt;&lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; is an open-source cluster management tool that makes it easy for you to deploy and manage HPC clusters on AWS. ParallelCluster uses a graphical interface to let you model and provision all the resources needed for your HPC applications in an automated and secure manner. It supports multiple instance types and job submission queues using SLURM.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For example, automotive customers tend to deploy their HPC infrastructure using AWS ParallelCluster because it can configure EFA automatically and mount an FSx for Lustre file system for the high-performance scratch area.&lt;/p&gt; 
&lt;h2&gt;Remote visualization&lt;/h2&gt; 
&lt;p&gt;If you need an interactive graphical application to generate the input files for your jobs, it’s more convenient to also run the pre-processing GUI on AWS. Customers use remote visualization technologies such as &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;NICE DCV&lt;/a&gt; to stream the graphics from EC2 nodes back to the end-user laptop. By using a remote visualization technology, you remove the need to copy large datasets to and from the cloud.&lt;/p&gt; 
&lt;p&gt;For our automotive use case, NICE DCV is an excellent choice for the graphics intensive pre-processing and post-processing stages of the HPC workflows. NICE DCV allows you to use graphics instances with powerful GPUs and direct access to the same data used by the simulation stages, allowing high performance and interactive model setup and visualization of results.&lt;/p&gt; 
&lt;h2&gt;Evaluating sustainability improvements for your workloads&lt;/h2&gt; 
&lt;p&gt;The best way to evaluate success when optimizing HPC workloads for sustainability is to use&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/evaluate-specific-improvements.html"&gt;proxy measures and unit of work KPIs&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The aggregate cost of your computational resources can be a good “&lt;em&gt;rule of thumb&lt;/em&gt;” proxy for their energy consumption. Generally, compute resources (EC2 instance types and sizes) with more processing power, use commensurately more energy and mostly have a higher price.&lt;/p&gt; 
&lt;p&gt;To get a measure of where you currently stand, you could potentially analyze the total volume of simulation or modelling work that you have accomplished within say a typical one-month period, from which you can derive a metric for the average cost per job. Of course, different applications and workloads can have quite different computational resource requirements, so you might be better served deriving an average &lt;em&gt;cost per job&lt;/em&gt; for &lt;em&gt;each job type&lt;/em&gt;. Your challenge is to then try to reduce this average cost per job, by trying different EC2 instance types or sizes that can deliver the required performance at further reduced cost.&lt;/p&gt; 
&lt;p&gt;For each job type, it’s also important to define a service level agreement (SLA) represented by the maximum allowable run time (cut-off time) to maintain business operations or staff productivity objectives. This defines the practical limit, beyond which further parallelism, or performance optimization doesn’t help.&lt;/p&gt; 
&lt;p&gt;For the type and quantity of computing resources that you &lt;em&gt;do&lt;/em&gt; select, it’s also important to pay attention to their resource utilization while running the workload. For compute-intensive HPC workloads, you should generally not have any idle instances, and all CPU cores should generally be running at close to 100%, unless explainable by phases of parallel communication or storage I/O. Remember that if network transfers or I/O time are significant it may be beneficial to select EC2 instances with higher speed networking, or perhaps faster local or shared storage options.&lt;/p&gt; 
&lt;p&gt;To better understand how to use proxy measures to optimize workloads for efficiency, you can read&amp;nbsp;&lt;a href="https://wellarchitectedlabs.com/sustainability/300_labs/300_cur_reports_as_efficiency_reports/"&gt;Sustainability Well-Architected Lab&lt;/a&gt;&amp;nbsp;on&amp;nbsp;&lt;em&gt;Turning the Cost and Usage Report into Efficiency Reports&lt;/em&gt;.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Reducing the environmental impact of HPC workloads is a journey. Moving your workloads to AWS will help you to accelerate the reduction of your carbon footprint emissions quickly. In this post, we provided guidance and recommendations on how to improve your HPC workload on AWS without sacrificing your business outcomes.&lt;/p&gt; 
&lt;p&gt;To learn more, check out the&amp;nbsp;&lt;a href="https://docs.aws.amazon.com/wellarchitected/latest/sustainability-pillar/sustainability-pillar.html"&gt;Sustainability Pillar of the AWS Well-Architected Framework&lt;/a&gt;&amp;nbsp;and other blog posts on&amp;nbsp;&lt;a href="https://aws.amazon.com/blogs/architecture/tag/sustainability/"&gt;architecting for sustainability&lt;/a&gt;. For more architecture content, refer to&amp;nbsp;the &lt;a href="https://aws.amazon.com/architecture/"&gt;AWS Architecture Center&lt;/a&gt;&amp;nbsp;for reference architecture diagrams, vetted architecture solutions,&amp;nbsp;&lt;a href="https://aws.amazon.com/architecture/well-architected/"&gt;Well-Architected&lt;/a&gt;&amp;nbsp;best practices, patterns, icons, and more.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Simulating autonomous mining operations using Robotec.ai on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/simulating-autonomous-mining-operations-using-robotec-ai-on-aws/</link>
		
		<dc:creator><![CDATA[Matt Hansen]]></dc:creator>
		<pubDate>Mon, 01 Jul 2024 16:28:55 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">7e189a5a1112ae1b83b595746cad451281851d11</guid>

					<description>Big changes are underway in mining - see how the Boliden Group simulates fleets of autonomous trucks using AWS Batch - for safety and efficiency.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3749" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/30/Simulating-autonomous-mining-operations-using-Robotec.ai-on-AWS-1.png" alt="" width="380" height="212"&gt;This post was contributed by Matthew Hansen, Principal SA, Advanced Computing &amp;amp; Simulation, Ryan Qi, Principal BD/GTM, Advanced Computing from AWS and Dominik Jargot, Software Engineer, Bartłomiej Boczek, Senior Machine Learning Engineer from Robotec.ai, and Peter Burman, Program Manager, Boliden.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Autonomous mining operations represent a significant leap forward in the mining industry, offering enhanced safety, efficiency, and productivity. The introduction of autonomous trucks is at the forefront of this transformation. &lt;a href="https://www.boliden.com/"&gt;Boliden AB&lt;/a&gt;, a Swedish multinational company specializing in precious metals, is an AWS customer using autonomous trucking technology for mining operations. These self-driving heavy-duty vehicles are equipped with advanced sensors and navigation systems, and operate around the clock without human intervention – reducing the risk of accidents and increasing uptime. These trucks follow pre-determined routes to move materials from the mining site to storage or processing sites, communicating real-time data back to a central control system.&lt;/p&gt; 
&lt;p&gt;Simulations are instrumental in developing and optimizing autonomous mining operations. They create a virtual environment for testing various scenarios and conditions, minimizing the risks and costs of real-world trials. Engineers at Boliden use simulations to evaluate the operations and performance of autonomous trucks, refine control strategies, and ensure systems can adapt to unexpected events or changes in the mining environment. This predictive capability is essential for maintaining safety, enhancing efficiency, and ensuring continuous production, all of which contribute to the long-term sustainability and profitability of mining operations.&lt;/p&gt; 
&lt;p&gt;In this post, you’ll learn how Boliden simulates multiple autonomous trucks in a mine, and scales those simulations to run dozens of scenarios simultaneously using the recently added multi-container jobs feature in AWS Batch.&lt;/p&gt; 
&lt;h2&gt;Building autonomous mining simulations&lt;/h2&gt; 
&lt;p&gt;&lt;a href="https://robotec.ai/"&gt;Robotec.ai&lt;/a&gt; is a technology leader in the realm of robotics and autonomous vehicles. They have a diverse team of experts in robotics, electrical engineering, software development, AI/ML, and human-technology interaction. Their simulation platform, &lt;a href="https://robotec.ai/products/rosi-simulation-platform/"&gt;RoSi&lt;/a&gt;, is engineered to create hyper-realistic simulations of confined spaces such as mining sites, warehouses, and large agricultural fields. The simulation of virtual mining environments enables accurate modeling of mining equipment and experimentation with many traffic scenarios.&lt;/p&gt; 
&lt;p&gt;Boliden uses RoSi to simulate the real-time operations of their mines over 40 distinct scenarios that cover a wide spectrum of machine interactions. This rigorous testing regime demands in excess of 1,000 hours of computational simulation every time Boliden changes their mining operations, to ensure robustness and reliability.&lt;/p&gt; 
&lt;p&gt;Figure 1 illustrates a scenario where two autonomous mining trucks, operating underground and simulated using the RoSi platform, navigate a narrow tunnel while avoiding obstacles and each other. One truck is using an underground turnout to wait for another to pass going the opposite direction. The time spent waiting is idle time that Boliden wants to reduce. The number of turnouts is also an expense they want to minimize. They run scenarios with varying numbers of trucks and turnouts and measure the idle time of the trucks, to find the optimal number of turnouts in the mine.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 1 - Underground autonomous mining simulations using Robotec.aiâ&#128;&#153;s RoSi platform" width="500" height="375" src="https://www.youtube-nocookie.com/embed/wWVtjBrhf_g?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 1 – Underground autonomous mining simulations using Robotec.ai’s RoSi platform&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Multi-container simulations&lt;/h2&gt; 
&lt;p&gt;Traditionally, when Boliden used &lt;a href="https://aws.amazon.com/batch"&gt;AWS Batch&lt;/a&gt; to simulate these types of scenarios, the service allowed only jobs with a single container. Consequently, engineers had to spend time on extra job preparation steps, merging all of the simulation components into a large, complex, monolithic container which also made division of work across teams difficult. Any code changes required engineers to rebuild the entire monolithic container, complicating software development (Dev), IT operations (Ops), and debugging.&lt;/p&gt; 
&lt;p&gt;Recently, though, AWS Batch added multi-container jobs, a feature that streamlines the process for customers to conduct large-scale simulation for complex systems in autonomous vehicles and robotics. This enhancement allows teams like those at Robotec.ai and Boliden to leverage Batch’s sophisticated scaling, scheduling, and cost management features without the added complexity of converting their systems into monolithic containers. Figure 2 illustrates the architecture for simulating mining operations with RoSi, using these multi-container job features in Batch.&lt;/p&gt; 
&lt;div id="attachment_3724" style="width: 918px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3724" loading="lazy" class="size-full wp-image-3724" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.23.17.png" alt="Figure 2 - Architecture for running mining operation simulations on RoSi and AWS Batch multi-container feature" width="908" height="544"&gt;
 &lt;p id="caption-attachment-3724" class="wp-caption-text"&gt;Figure 2 – Architecture for running mining operation simulations on RoSi and AWS Batch multi-container feature&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;As shown in Figure 2, engineers can use an array of smaller, independent containers, each representing a mining truck, alongside the RoSi simulator. This approach not only accelerates development by eliminating redundant steps in job setup, but also reduces the need for engineers to develop additional in-house tools, and eases collaboration between software development and IT operations teams.&lt;/p&gt; 
&lt;h2&gt;How you can run simulations on AWS Batch with RoSi&lt;/h2&gt; 
&lt;h3&gt;Step 1 – Configure your Job inputs in RoSi&lt;/h3&gt; 
&lt;p&gt;Using RoSi, you can configure experiment properties specific to each use case. In the Boliden example, the configuration includes the main scenario setup, traffic properties, events, and safety system parameters. Configuration enables testing of the entire underground mine in the simulated environment against the complex system of systems, which typically consists of autonomous vehicles, a traffic management system, and a safety system.&lt;/p&gt; 
&lt;p&gt;RoSi offers a lot of flexibility in the system configuration, allowing engineers to create isolated tests for individual system parameters, or large-scale system-wide stress tests. The typical flow of batch experiments configuration in RoSi goes like this:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Configure the types and paths of vehicles in the &lt;strong&gt;Events&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Configure traffic properties in the &lt;strong&gt;Traffic&lt;/strong&gt; tab, for example, the localization of the turnouts (places where vehicles can freely pass each other) in the narrow tunnels of the mine.&lt;/li&gt; 
 &lt;li&gt;In the &lt;strong&gt;Main&lt;/strong&gt; tab, configure the scenario details, like the number of vehicles, their positions, and speeds, to simulate various situations.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Figures 3 and 4 show the detail for some of the configuration capabilities of RoSi.&lt;/p&gt; 
&lt;div id="attachment_3725" style="width: 1195px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3725" loading="lazy" class="size-full wp-image-3725" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.23.56.png" alt="Figure 3 – Turnouts and other traffic parameters configuration in the underground mine." width="1185" height="622"&gt;
 &lt;p id="caption-attachment-3725" class="wp-caption-text"&gt;Figure 3 – Turnouts and other traffic parameters configuration in the underground mine.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3726" style="width: 1198px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3726" loading="lazy" class="size-full wp-image-3726" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.24.28.png" alt="Figure 4 – A scenario configuration with six autonomous mine vehicles starting in different points of the mine." width="1188" height="623"&gt;
 &lt;p id="caption-attachment-3726" class="wp-caption-text"&gt;Figure 4 – A scenario configuration with six autonomous mine vehicles starting in different points of the mine.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Step 2 – Submit multi-container jobs from RoSi&lt;/h3&gt; 
&lt;p&gt;RoSi lets you to run multiple real-time experiments in parallel, which greatly reduces the time needed to test a large number of scenarios, significantly improving efficiency. Engineers can select the configurations required for the batch of experiments from the simulation setup screen (Figure 5).&lt;/p&gt; 
&lt;p&gt;For convenience, a set of configurations can be stored as a preset that can be reused. After pressing the &lt;strong&gt;Start&lt;/strong&gt; button, RoSi Simulations Manager will perform a cartesian product of selected configs, configure a batch of experiments, and submit them to the AWS Batch job queue. By increasing the number of workers, you can speed up the experiment execution rate – by running them in parallel. Once the jobs are submitted, you can also view them in the Batch console (Figure 6).&lt;/p&gt; 
&lt;div id="attachment_3727" style="width: 1193px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3727" loading="lazy" class="size-full wp-image-3727" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.24.56.png" alt="Figure 5 – The simulation setup screen, where users can select configurations and start a batch of experiments." width="1183" height="621"&gt;
 &lt;p id="caption-attachment-3727" class="wp-caption-text"&gt;Figure 5 – The simulation setup screen, where users can select configurations and start a batch of experiments.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3728" style="width: 1249px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3728" loading="lazy" class="size-full wp-image-3728" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.25.25.png" alt="Figure 6 - AWS Batch console while a multi-container job is running." width="1239" height="654"&gt;
 &lt;p id="caption-attachment-3728" class="wp-caption-text"&gt;Figure 6 – AWS Batch console while a multi-container job is running.&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Step 3 – Viewing the simulation data&lt;/h3&gt; 
&lt;p&gt;Viewing real-time simulation data from the RoSi experiment manager (Figure 7) helps you gain insights before the experiments end. In the mining example, Boliden can evaluate the productivity of each experiment to assess the impact of changing mining variables. Data is streamed, so when the multi-container job is submitted to Batch, you can monitor the job status, and view the logs from each container. All the data is stored in log files saved in an Amazon Simple Storage Service (Amazon S3) bucket, which you can use for post-processing, analysis, visualization, and debugging.&lt;/p&gt; 
&lt;div id="attachment_3729" style="width: 1193px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3729" loading="lazy" class="size-full wp-image-3729" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.25.53.png" alt="Figure 7 - RoSi dashboard provides status of each experiment in real-time" width="1183" height="670"&gt;
 &lt;p id="caption-attachment-3729" class="wp-caption-text"&gt;Figure 7 – RoSi dashboard provides status of each experiment in real-time&lt;/p&gt;
&lt;/div&gt; 
&lt;h3&gt;Step 4 – Viewing and analyzing the results&lt;/h3&gt; 
&lt;p&gt;RoSi enables viewing real-time data to automate and optimize multiple aspects of mining operations.&lt;/p&gt; 
&lt;p&gt;Boliden can track metrics they need in RoSi, and post-process them on AWS. This enables them to effectively manage the mining fleet and reduce the time vehicles wait between operations. The result is a cost-effective use of fuel and optimization of working shifts. Figure 8 depicts wait times (in minutes) of six vehicles running across multiple simulation experiments.&lt;/p&gt; 
&lt;div id="attachment_3730" style="width: 1214px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3730" loading="lazy" class="size-full wp-image-3730" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/IMG-2024-05-29-14.26.18.png" alt="Figure 8 - Waiting times distribution across the mining tunnel with six turnouts." width="1204" height="369"&gt;
 &lt;p id="caption-attachment-3730" class="wp-caption-text"&gt;Figure 8 – Waiting times distribution across the mining tunnel with six turnouts.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;In this sample data, the vehicles spend the most time waiting at turnout ID 1. In a 24-hour simulation, vehicle six spent the most time waiting at turnout ID 1, waiting approximately 55 minutes of total time.&lt;/p&gt; 
&lt;p&gt;We can compare this data with other scenarios – with different locations and numbers of turnouts – to find the optimal turnout configuration within the mine, &lt;em&gt;before creating the mine in the real world&lt;/em&gt;, thus ensuring smooth operation and minimizing costs.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post, we’ve explored running multi-container simulations on AWS to enhance autonomous mining operations, using the advanced capabilities of Robotec.ai’s RoSi simulator, in tandem with AWS Batch.&lt;/p&gt; 
&lt;p&gt;By partitioning complex systems into containers, such as individual mining trucks, we can analyze their interactions to increase efficiency. The integration of RoSi with Batch means simulation engineers and DevOps teams can orchestrate large-scale simulations, scaling of their compute resources more easily, and getting fast results that help them optimize their operations.&lt;/p&gt; 
&lt;p&gt;Most of all, this approach paves the way for more streamlined underground mining operations by reducing waiting times and operational costs in &lt;em&gt;the real world&lt;/em&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>How vertical scaling and GPUs can accelerate mixed media modelling for marketing analytics</title>
		<link>https://aws.amazon.com/blogs/hpc/how-vertical-scaling-and-gpus-can-accelerate-mixed-media-modelling-for-marketing-analytics/</link>
		
		<dc:creator><![CDATA[Niro Amerasinghe]]></dc:creator>
		<pubDate>Wed, 26 Jun 2024 13:58:36 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">d3547a4030d4f5ce527c3b726e59fa0d2c80abc1</guid>

					<description>In marketing analytics, mixed media modeling (MMM) is a machine learning technique that combines information from various sources, like TV ads, online ads and social media to measure the impact of marketing and advertising campaigns. By using these techniques, businesses can make smarter decisions about where to invest their money for advertising, helping them get […]</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3812" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/AdobeStock_814815853.png" alt="How vertical scaling and GPUs can accelerate mixed media modelling for marketing analytics " width="380" height="212"&gt;In marketing analytics, &lt;em&gt;mixed media modeling&lt;/em&gt; (MMM) is a machine learning technique that combines information from various sources, like TV ads, online ads and social media to measure the impact of marketing and advertising campaigns. By using these techniques, businesses can make smarter decisions about where to invest their money for advertising, helping them get the best return on investment. It’s a bit like having a treasure map that guides you to the most valuable marketing strategies.&lt;/p&gt; 
&lt;p&gt;A key challenge for marketing analytics teams is the compute-heavy requirement for running these models. With popular libraries like &lt;a href="https://github.com/google/lightweight_mmm"&gt;LightweightMMM&lt;/a&gt;, &lt;a href="https://facebookexperimental.github.io/Robyn/"&gt;Robyn&lt;/a&gt;, and &lt;a href="https://www.pymc-marketing.io/en/stable/"&gt;PyMC Marketing&lt;/a&gt; not being designed to scale horizontally, jobs with a granular breakdown of demographic and geographic regions will take more than 24 hours to complete on workstations with limited compute, leading to delayed insight and hence delayed marketing optimization.&lt;/p&gt; 
&lt;p&gt;In this blog post, I’ll demonstrate how to accelerate your mixed media modeling (MMM) jobs using AWS Batch. Using the &lt;a href="https://github.com/google/lightweight_mmm"&gt;LightweightMMM&lt;/a&gt; open-source library as an example, I’ll use AWS Batch to help with two key challenges: 1) give teams access to larger GPU and CPU compute resources; and 2) efficiently provision and manage infrastructure while reducing the risk of cost overruns. Using larger compute instances overcomes the inability of these MMM libraries to scale horizontally and results in a big reduction in model training time. At the end of the post, I’ll also go through a time and cost analysis to show you the benefits of using more expensive vertically-scaled instances for these types of workloads.&lt;/p&gt; 
&lt;h2&gt;Overview of our sample application&lt;/h2&gt; 
&lt;p&gt;First let’s take a look at a sample application (Figure 1) that is also published in &lt;a href="https://github.com/aws-samples/mixed-media-model-portal"&gt;AWS Samples repo here&lt;/a&gt; (see the link for the deployment method).&lt;/p&gt; 
&lt;p&gt;You can quickly submit training jobs for MMMs through this application while AWS Batch takes care of the provisioning and, importantly, the removal and cleanup of compute resource. You submit jobs using the &lt;strong&gt;Train new model&lt;/strong&gt; button. The &lt;strong&gt;Details&lt;/strong&gt; button shown in the completed jobs table allows the user to visualize the results and run further inference to optimize a proposed marketing budget.&lt;/p&gt; 
&lt;div id="attachment_3805" style="width: 950px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3805" loading="lazy" class="size-full wp-image-3805" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-08.53.08.png" alt="Figure 1 – The web frontend for our sample application. Data scientists can submit training jobs for mixed media models leveraging high end compute and obtain results for further analysis after completion." width="940" height="601"&gt;
 &lt;p id="caption-attachment-3805" class="wp-caption-text"&gt;Figure 1 – The web frontend for our sample application. Data scientists can submit training jobs for mixed media models leveraging high end compute and obtain results for further analysis after completion.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Architecture: AWS Batch integrated with web frontend&lt;/h2&gt; 
&lt;p&gt;The architectural diagram for the sample application is shown in Figure 2. A &lt;a href="#_Web_front-end"&gt;web application&lt;/a&gt; provides users with a simple user interface (1). When you submit a job, the web application calls an API in the processing layer with the model-training parameters. The processing layer uses AWS Batch to run a training job (2). AWS Batch will automatically provision the required compute and execute the code for the training job. The training job accesses the specified tables in the &lt;a href="#_Data_lake_for"&gt;data lake&lt;/a&gt; to acquire its training data (3). On completion, the trained model is saved back to the data lake (4). Once training is complete, AWS Batch will make sure the compute resources are shutdown so you don’t pay for compute that is not being used. Finally, the processing layer provides an API for inference requests back to the web application by loading the saved model from Amazon Simple Storage Service (Amazon S3) on demand (5).&lt;/p&gt; 
&lt;div id="attachment_3806" style="width: 952px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3806" loading="lazy" class="size-full wp-image-3806" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-08.53.40.png" alt="Figure 2 - Architectural diagram of a web application to train Mixed Media Models. When a training request is submitted from the web frontend, the job is submitted to AWS Batch via Amazon API Gateway/AWS Lambda. Amazon Athena provides the training data directly to the training job. All the components are serverless with no requirement for pre provisioned unmanaged infrastructure." width="942" height="403"&gt;
 &lt;p id="caption-attachment-3806" class="wp-caption-text"&gt;Figure 2 – Architectural diagram of a web application to train Mixed Media Models. When a training request is submitted from the web frontend, the job is submitted to AWS Batch via Amazon API Gateway/AWS Lambda. Amazon Athena provides the training data directly to the training job. All the components are serverless with no requirement for pre provisioned unmanaged infrastructure.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Deep dive on components in the architecture&lt;/h2&gt; 
&lt;h3&gt;Web front-end&lt;/h3&gt; 
&lt;p&gt;We used Amazon CloudFront and Amazon S3 to provide a simple, cost-effective way to serve a &lt;a href="https://react.dev/"&gt;React&lt;/a&gt; based front end, with Amazon Cognito providing authentication and authorization to control access. The combination of Amazon API Gateway, AWS Lambda, and Amazon DynamoDB provide a job management function exposed as a REST API that can be called by the web application.&lt;/p&gt; 
&lt;h3&gt;Data lake for storage&lt;/h3&gt; 
&lt;p&gt;By using Amazon Athena and Amazon S3, a data lake allows team members to prepare variations of the model input data using standard SQL queries. We also have a Lambda function that contains code to generate sample data that is a feature of the &lt;a href="https://github.com/google/lightweight_mmm"&gt;LightweightMMM&lt;/a&gt; framework.&lt;/p&gt; 
&lt;h3&gt;Processing layer&lt;/h3&gt; 
&lt;p&gt;The processing layer provides the compute required to both train the model’s and run inference requests. By using Batch, we create a number of different job types, queues, and compute environments to allow training the model on small / medium and large GPU/CPU instances. Batch will automatically provision the required compute using the Amazon Elastic Compute Cloud (Amazon EC2) service with the specified &lt;a href="https://aws.amazon.com/ec2/instance-types/"&gt;instance types&lt;/a&gt; per job. This allows us to expose a simple job submission system via the web front end while using Batch to manage the complexity of the underlying compute infrastructure. The trained jobs are saved into the S3 bucket in the data lake, and we use Lambda to expose inference requests on these trained models.&lt;/p&gt; 
&lt;h2&gt;The LightweightMMM framework&lt;/h2&gt; 
&lt;p&gt;The &lt;a href="https://github.com/google/lightweight_mmm"&gt;LightweightMMM&lt;/a&gt; library is written in Python and uses open-source frameworks Jax and NumPyro to create a Bayesian approach to MMM using the &lt;em&gt;Markov Chain Monte Carlo&lt;/em&gt; method.&lt;/p&gt; 
&lt;p&gt;There are some key input parameters for training the model that can influence the run time.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Historical Data and Granularit&lt;strong&gt;y&lt;/strong&gt; – the length and granularity of historical data used can impact the time it takes a model to complete. For this blog post we will use 3 years’ worth of data with a weekly granularity leading to 160 data points per feature.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;ul&gt; 
 &lt;li&gt;Number of Geographies – the number of Geographies multiplies the complexity of the calculation as each data point above must be repeated for each geography as well as the number of Marketing channels. For this blog post we will use different geography numbers to show the impact on run time.&lt;/li&gt; 
 &lt;li&gt;Marketing Channels – each data point multiplied by the number of Geographies is further multiplied for each marketing channel included in the analysis. For this blog post we will compare 3 and 6 channels.&lt;/li&gt; 
 &lt;li&gt;Chains – each additional chain requires running the MCMC algorithm independently. This means that you need to perform the sampling and calculations for each chain separately. For this blog post we will use a varying number of Chains to demonstrate the impact on run time.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to maximize GPU memory&lt;/h3&gt; 
&lt;p&gt;The current implementation of LightweightMMM runs each &lt;em&gt;Chain&lt;/em&gt; on a dedicated GPU, the default settings will pre-allocate GPU memory for each chain and we found that this can lead to &lt;code&gt;Out of Memory&lt;/code&gt; errors when running large MMM models where each chain has a large number of data points (calculated as &lt;code&gt;historical data x geographies x channels&lt;/code&gt;).&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-python"&gt;os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = ".50"
os.environ["XLA_PYTHON_CLIENT_ALLOCATOR"] = "platform"
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This configuration inside you python application code can help maximize the GPUs for larger model training jobs. You can refer to &lt;a href="https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html"&gt;Jax Documentation&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;CUDA and cuDNN on AWS Batch&lt;/h3&gt; 
&lt;p&gt;For JAX to work with an NVIDIA GPU we needed to ensure:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The host machine’s CUDA drivers are on the same major version as the container image, and the minor version must be the same or newer.&lt;/li&gt; 
 &lt;li&gt;The newer NVIDIA GPUs (e.g. H100) perform much better when using CUDA 12.&lt;/li&gt; 
 &lt;li&gt;Jax has a prebuilt &lt;code&gt;jaxlib&lt;/code&gt; python wheel to support CUDA 12.2 with cuDNN 8.9 and so it is best to align a container image that contains this.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;The simplest solution is to use a pre-built NVIDIA container image with the required libraries, and compiler tools. You can find these in &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda"&gt;the NVIDIA container catalog&lt;/a&gt;. We recommend using the developer releases with cuDNN included.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;FROM nvcr.io/NVIDIA/cuda:12.2.2-cudnn8-devel-ubuntu22.04&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;You can find a full example of the Docker file we used in our sample application in our &lt;a href="http://github/"&gt;AWS samples repository&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Time and cost analysis&lt;/h2&gt; 
&lt;p&gt;To understand the impact of scaling compute when training the models, we ran an experiment with LightweightMMM on a number of different compute configurations and model complexities. The results below have been compiled based on average run times and approximate costs (using On Demand pricing for a consistent baseline) for Amazon EC2 C6i and C7i instance types for CPU, and G5 and P5 instance types for GPUs. We used the &lt;code&gt;us-east-1&lt;/code&gt; Region.&lt;/p&gt; 
&lt;p&gt;For our experiment, we compared the results against a base configuration of 16-cores which is reflective of the performance of a typical desktop workstation.&lt;/p&gt; 
&lt;div id="attachment_3807" style="width: 956px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3807" loading="lazy" class="size-full wp-image-3807" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-08.56.37.png" alt="Figure 3 – Shows the average runtime of training a smaller model with 3 marketing channels, 100 geographies with 2 chains using different levels of compute" width="946" height="537"&gt;
 &lt;p id="caption-attachment-3807" class="wp-caption-text"&gt;Figure 3 – Shows the average runtime of training a smaller model with 3 marketing channels, 100 geographies with 2 chains using different levels of compute&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For smaller models, increasing the level of processing power using a greater number of CPU cores did not have a significant impact on training time. We think this is due to the overheads of splitting the processing across a larger number of threads. We found that on the extreme end, a 192-core instance with high-parallelism can actually have a &lt;em&gt;negative impact&lt;/em&gt; on the training time.&lt;/p&gt; 
&lt;p&gt;Using GPUs had a significant positive impact on the training job: a G5 instance with 4 x NVIDIA A10 GPUs ran 4x faster compared to a typical cloud desktop configuration despite being about the same cost to train the model.&lt;/p&gt; 
&lt;div id="attachment_3808" style="width: 959px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3808" loading="lazy" class="size-full wp-image-3808" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-08.57.03.png" alt="Figure 4 – Shows the average runtime of training a larger model with 6 Marketing channels, 300 Geographies using different levels of compute" width="949" height="545"&gt;
 &lt;p id="caption-attachment-3808" class="wp-caption-text"&gt;Figure 4 – Shows the average runtime of training a larger model with 6 Marketing channels, 300 Geographies using different levels of compute&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For larger complex models, a 128-core C6i CPU-based instance provided more than a 2x time reduction before higher parallelism started to become detrimental. We found that G5 instances with A10 GPUs didn’t have enough memory to run the larger training configuration. However, the P5 instances with NVIDIA H100 GPUs ran &lt;em&gt;58x faster&lt;/em&gt;, with training completing in approximately 45 minutes. For an approximate cost increase of $50 the run time dropped from 1.5 days to 45 mins. This is a great trade-off from if you’re short on time.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post I covered how you can use AWS Batch to run mixed media models (MMM) faster with Amazon EC2 &lt;a href="https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing"&gt;accelerated compute&lt;/a&gt; instances. I explained how to configure NVIDIA CUDA libraries to enable frameworks like Jax to work on Batch. And I walked you through a time and cost analysis, showing that while larger CPU and GPU instances do come with a higher per-hour price tag than the cheapest options, the time savings can enable data scientists to iterate faster – and build better MMM models – more efficiently.&lt;/p&gt; 
&lt;p&gt;To get started running mixed media models on AWS Batch, have a look at our &lt;a href="https://github.com/aws-samples/mixed-media-model-portal"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Harnessing the scale of AWS for financial simulations</title>
		<link>https://aws.amazon.com/blogs/hpc/harnessing-the-scale-of-aws-for-financial-simulations/</link>
		
		<dc:creator><![CDATA[Yusong Wang]]></dc:creator>
		<pubDate>Tue, 25 Jun 2024 14:57:18 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[Financial Services]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Industries]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[FSI]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">80cee173cb118b7e6b59dd5783c9980d9be60f84</guid>

					<description>Struggling with long compute times for numerical simulations in finance? See how AWS makes it simple to leverage the cloud for large-scale financial modeling. We walk through a real example using QuantLib and Monte Carlo methods.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3768" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/05/boofla88_a_room_of_financial_traders._Its_chaotic_and_busy_with_802574df-b747-498d-9a1e-d94cae8b61a0-copy.png" alt="Harnessing the scale of AWS for financial simulations" width="380" height="212"&gt;In the dynamic world of finance, numerical simulations have become indispensable tools for tackling complex problems where analytical solutions are elusive. However, these simulations can be incredibly time-consuming and computationally intensive, especially when dealing with large-scale tasks.&lt;/p&gt; 
&lt;p&gt;This is where the benefits of cloud computing come into play. Clouds like AWS offer a plethora of advantages that have made us increasingly popular in the financial industry. Flexible pricing models, massive compute environments, and versatile instance choices allow financial institutions to scale their simulations seamlessly and make use of multiple AWS Regions and Availability Zones for both scale and resiliency.&lt;/p&gt; 
&lt;p&gt;In this blog post, we’ll show how you can use AWS to execute large-scale financial simulations more easily. We’ll focus on a specific example: calculating option prices using Monte Carlo simulations with the &lt;a href="https://www.quantlib.org/"&gt;QuantLib&lt;/a&gt; open-source library. QuantLib is a powerful tool for modelling, trading, and risk management in the financial industry. Finally, there’s a link to a workshop you can follow – step-by-step – to deploy this in your own AWS account.&lt;/p&gt; 
&lt;h2&gt;Optimizing financial simulations with AWS Batch and AWS Lambda&lt;/h2&gt; 
&lt;p&gt;When it comes to running large-scale financial simulations, the choice of compute service can have a significant impact on the overall efficiency and cost-effectiveness of the workflow. In this regard, &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt; and &lt;a href="https://aws.amazon.com/pm/lambda"&gt;AWS Lambda&lt;/a&gt; offer complementary solutions that can be tailored to different workload requirements.&lt;/p&gt; 
&lt;p&gt;AWS Batch, a job scheduler and resource orchestrator, is well-suited for running batch-oriented workflows at scale. By automatically provisioning the appropriate compute resources and managing the execution of jobs, Batch enables financial institutions to run their simulations in a cost-effective and efficient manner, particularly for workloads that require longer durations or need to handle high concurrency.&lt;/p&gt; 
&lt;p&gt;On the other hand, for smaller jobs that require a fast turnaround, typically within a couple of minutes, AWS Lambda can be the preferred choice. Lambda’s serverless architecture and rapid response times make it an ideal solution for scenarios where the low latency of less than a second is a critical requirement, like in real-time risk analysis or portfolio optimization.&lt;/p&gt; 
&lt;p&gt;The selection of the appropriate compute service depends on the specific workload requirements. For fast response time (&amp;lt;1s) or high throughput (over 500 transactions per second), Lambda is the choice we’d recommend. However, for workloads with durations longer than 5 minutes, or the need to scale quickly to over 20,000 concurrent jobs, Batch is a more suitable option. Additionally, for managing multiple workloads with a scheduler, Batch is the recommended service.&lt;/p&gt; 
&lt;h2&gt;Solution architecture&lt;/h2&gt; 
&lt;p&gt;Figure 1 illustrates a solution architecture for running cloud-native financial simulations on AWS in multiple regions. The simulation workflow to calculate &lt;em&gt;American Option&lt;/em&gt; prices using QuantLib is described like this:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Upload an input CSV file with financial asset information to the &lt;em&gt;input&lt;/em&gt; Amazon Simple Storage Service (Amazon S3) buckets for AWS Batch or AWS Lambda to compute.&lt;/li&gt; 
 &lt;li&gt;Apply &lt;a href="https://aws.amazon.com/pm/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt; to monitor the designated Amazon S3 buckets.&lt;/li&gt; 
 &lt;li&gt;Once the input data has been fed into the Amazon S3 input bucket, an AWS Batch job is triggered through EventBridge, or a Lambda job is invoked through event-driven process of the S3 bucket, depending on the location of the input file.&lt;/li&gt; 
 &lt;li&gt;The job splits the input file to multiple files and put on S3 to run in parallel. For Lambda, we use the same event-driven process to run the job. For Batch, we run jobs in parallel using Array Jobs, for efficiency. The input file is processed directly if the number of assets is under a threshold (configurable through an environment variable).&lt;/li&gt; 
 &lt;li&gt;The result files are put on the &lt;em&gt;result&lt;/em&gt; S3 bucket with the same path as the input file. Users get the result back by copying from the result S3 bucket.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;div id="attachment_3765" style="width: 934px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3765" loading="lazy" class="size-full wp-image-3765" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/05/IMG-2024-06-05-09.13.33.png" alt="Figure 1: Reference solution architecture for running cloud-native financial simulations" width="924" height="641"&gt;
 &lt;p id="caption-attachment-3765" class="wp-caption-text"&gt;Figure 1: Reference solution architecture for running cloud-native financial simulations&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The common workflow in financial simulations is to keep experimenting with – and optimizing – a compute algorithm. Adopting continuous integration and continuous deployment (CI/CD) is a natural fit. Figure 2 illustrates the high-level workflow for CI/CD development.&lt;/p&gt; 
&lt;p&gt;&lt;a href="https://aws.amazon.com/codepipeline/"&gt;AWS CodePipeline&lt;/a&gt; is a continuous delivery service that allows you to model, visualize, and automate the steps required to release application software. With it, we model the full release process for building the code, deploying to pre-production environments, testing the application and releasing it to production every time there is a code change.&lt;/p&gt; 
&lt;div id="attachment_3766" style="width: 899px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3766" loading="lazy" class="size-full wp-image-3766" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/05/IMG-2024-06-05-09.14.06.png" alt="Figure 2: High-level workflow for the continuous integration and continuous deployment. When a developer pushes code changes to an AWS CodeCommit repository, the AWS CodePipeline will automatically start to build the container image with AWS CodeBuild and push the image to the Amazon ECR. The same image is used by both AWS Lambda and AWS Batch at runtime. " width="889" height="350"&gt;
 &lt;p id="caption-attachment-3766" class="wp-caption-text"&gt;Figure 2: High-level workflow for the continuous integration and continuous deployment. When a developer pushes code changes to an AWS CodeCommit repository, the AWS CodePipeline will automatically start to build the container image with AWS CodeBuild and push the image to the Amazon Elastic Container Registry (ECR). The same image is used by both AWS Lambda and AWS Batch at runtime.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;You can do this&lt;/h2&gt; 
&lt;p&gt;We’ve &lt;a href="https://catalog.workshops.aws/batch-lambda-fsi"&gt;published a workshop&lt;/a&gt; that offers a quick and effective implementation of this, which will take you about 20 minutes to set up.&lt;/p&gt; 
&lt;p&gt;You’ll have the opportunity to see for yourself the impressive performance of AWS Lambda to process &lt;strong&gt;hundreds of equities in about a minute&lt;/strong&gt;, and AWS Batch processing &lt;strong&gt;tens of thousands of equities in less than ten minutes&lt;/strong&gt; by chomping through 10 to 100 equities for each individual job.&lt;/p&gt; 
&lt;h2&gt;Unlock the future of financial simulations with AWS&lt;/h2&gt; 
&lt;p&gt;The cloud-native Monte Carlo simulation for &lt;em&gt;American Option&lt;/em&gt; pricing using QuantLib we’ve outlined here is just the beginning. By using AWS, financial institutions can unlock new levels of efficiency, scalability, and innovation in their simulation workflows.&lt;/p&gt; 
&lt;p&gt;Ready to get started? Check out &lt;a href="https://catalog.workshops.aws/batch-lambda-fsi"&gt;the published workshop&lt;/a&gt; to dive deeper into the step-by-step implementation details and see how you can apply these cloud-native techniques to your own financial simulation.&lt;/p&gt; 
&lt;p&gt;This hands-on experience empowers financial professionals to harness the full potential of cloud technologies, revolutionizing the approaches to complex financial simulations.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>A library of HPC Applications Best Practices on AWS</title>
		<link>https://aws.amazon.com/blogs/hpc/a-library-of-hpc-applications-best-practices-on-aws/</link>
		
		<dc:creator><![CDATA[Nicola Venuti]]></dc:creator>
		<pubDate>Mon, 24 Jun 2024 12:35:25 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[FEA]]></category>
		<category><![CDATA[Finite Element Analysis]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">8da82d09564c8acf86902c1e2a56895df6c02376</guid>

					<description>Want insights on running HPC codes efficiently on AWS? Our HPC specialists compiled their know-how into a new public GitHub repo. Get best practices, templates, scripts and more to optimize your workloads.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3836" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/HPC-Applications-Best-Practices-on-AWS-1.png" alt="" width="380" height="213"&gt;Being an HPC specialist in AWS comes with some critical responsibilities. Key among them is to help customers run their applications as fast as possible, but also in the most cost efficient way, too. We aim to help customers find the most appropriate services for every workload with optimal drivers, settings, and options to get great outcomes.&lt;/p&gt; 
&lt;p&gt;But the number of HPC related services – and their capabilities sometimes grows fast enough that it’s not easy for you to ensure you’re using AWS effectively.&lt;/p&gt; 
&lt;p&gt;So today we’re announcing a resource containing the best practices from our HPC Specialist Solution Architect (SSA) community to help you get the most from running your workloads on AWS. We’re hosting these in a &lt;a href="https://github.com/aws-samples/hpc-applications"&gt;GitHub repository&lt;/a&gt; that is publicly available starting today. In addition to Application Best Practices, this repo also contains CloudFormation Templates to create clusters, and launch scripts (with some benchmark results) for selected applications.&lt;/p&gt; 
&lt;p&gt;We expect to be regularly updating and expanding the list of HPC applications included in the repo, based on your feedback, and participation form our teams. You can propose new applications that would benefit from being included using &lt;a href="https://github.com/aws-samples/hpc-applications/issues"&gt;GitHub Issues&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;Today AWS has more than &lt;a href="https://aws.amazon.com/ec2/"&gt;750 different instance types&lt;/a&gt;, but only some of these are helpful for HPC applications. Even if you just include the &lt;a href="https://www.youtube.com/watch?v=i2T7Hi2Yjoc"&gt;HPC-specific instances&lt;/a&gt;, and the &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types"&gt;instance types that support EFA&lt;/a&gt; (the Elastic Fabric Adapter), there’s more than a hundred options.&lt;/p&gt; 
&lt;p&gt;Customers often benchmark their HPC applications on AWS to understand which of these instance types is best for their code, and to ensure that the way the application is installed and run is aligned with their business needs (e.g., runs as fast as possible, lowest price, and so forth).&lt;/p&gt; 
&lt;p&gt;Sometimes customers wonder whether they are running a specific application ‘properly’ – are they achieving the best performance?&lt;/p&gt; 
&lt;p&gt;Sometimes it’s more than idle interest – customers often request benchmarks as part of formal procurement exercises, or before they start a &lt;em&gt;Proof of Concept&lt;/em&gt; (PoC) – with help from our teams.&lt;/p&gt; 
&lt;p&gt;Running HPC application benchmarks properly – and at scale – is a complex task. It requires preparation, experience, and strong domain knowledge. It’s more complex if you have to leave your comfort-zone and run applications in a new, possibly &lt;em&gt;unknown&lt;/em&gt; environment, too, because this might call for a deep understanding of how the new infrastructure works.&lt;/p&gt; 
&lt;p&gt;This is why we’ve created the resource we’re launching today. It’s maintained by AWS HPC Specialists Solution Architects, who will take care of updating and improving it as our services evolve, new application versions are released, or more optimal ways are found for running applications.&lt;/p&gt; 
&lt;p&gt;We’re starting with the most common applications for the computer-aided engineering (CAE) community. These are the codes most requested by our customers.&lt;/p&gt; 
&lt;h2&gt;Our goals for HPC Applications Best Practices on AWS&lt;/h2&gt; 
&lt;p&gt;Here’s what we’re aiming to accomplish with this initiative:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure our customers can achieve the best price/performance using our infrastructure and services for their HPC Applications&lt;/li&gt; 
 &lt;li&gt;Ensure you have references (like time to completion) and datapoints (benchmark metrics) for running these applications using public datasets.&lt;/li&gt; 
 &lt;li&gt;Share general guidance, settings, and tips that can be applicable to other applications, too.&lt;/li&gt; 
 &lt;li&gt;Lowering the level of cloud expertise needed to run these workloads in the cloud.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;While the repo isn’t a supported product or service, we’re aiming to give you access to our best thinking – and our experience – on the topics it covers.&lt;/p&gt; 
&lt;h2&gt;Let’s tour the GitHub repo&lt;/h2&gt; 
&lt;p&gt;At launch, we’ve structured the repo as follows:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;/apps&lt;/code&gt; contains a folder for the best practices for each of the included applications. Within this folder you’ll find:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;an &lt;strong&gt;example launch script&lt;/strong&gt;, in some cases multiple ones to cover different architectures (x86 vs GPUs vs Graviton), or different application versions (in case they require different settings in the launch script). The example launch scripts are working examples, that can be executed with minor changes, but we also know every end-user has their own peculiarities and might want to run a given application in a custom way. In this case we’d recommend starting from the working example launch script provided and adapt it to your specific needs.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;short documentation&lt;/strong&gt; about the best practice (README.md file + a few assets). It typically comprises of an introduction, tips and tricks, a deep dive into the architectural choices and the most important application and environment settings aiming at tuning the performance, and finally the benchmark results shared with one or more charts.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;/docs&lt;/code&gt; contains documentation, images, and charts. This doesn’t replace our official application, but complements them with explanations about architectural choices and details about specific applications and environment settings we’ve used.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;/ParallelCluster&lt;/code&gt; contains simple example config files for building an HPC cluster &lt;a href="https://aws.amazon.com/it/hpc/parallelcluster/"&gt;using AWS ParallelCluster&lt;/a&gt;. As we release new services or features for managing HPC resources, we’ll also update this section, accordingly. We’ve included some automated &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/cloudformation-v3.html"&gt;CloudFormation-based&lt;/a&gt; procedures to deploy a basic cluster (using ParallelCluster) in selected AWS Regions. This structure will change over time as our capabilities grow, and as we gather new material.&lt;/p&gt; 
&lt;p&gt;Each application included in the repository will be supported by slightly different sets of assets (for example, launch scripts, documentation, performance charts). But there’s a minimum set you’ll see for every included application.&lt;/p&gt; 
&lt;p&gt;Beginning with the &lt;code&gt;/apps&lt;/code&gt; folder, you’ll find the list of Best Practices available in the repo.&lt;/p&gt; 
&lt;div id="attachment_3827" style="width: 953px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3827" loading="lazy" class="size-full wp-image-3827" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-15.52.33.png" alt="Figure 1. The list of available best practices you can find in the GitHub repository under /apps/" width="943" height="559"&gt;
 &lt;p id="caption-attachment-3827" class="wp-caption-text"&gt;Figure 1. The list of available best practices you can find in the GitHub repository under /apps/&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Inside any application directory, you’ll find one or more launch scripts that you can use as-is – or customize based on your needs. In some cases, the launch scripts are in sub-directories, broken down by CPU or GPU architectures.&lt;/p&gt; 
&lt;div id="attachment_3828" style="width: 929px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3828" loading="lazy" class="size-full wp-image-3828" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-15.53.00.png" alt="Figure 2. Shows a typical example of the assets you can expect to find for an application." width="919" height="849"&gt;
 &lt;p id="caption-attachment-3828" class="wp-caption-text"&gt;Figure 2. Shows a typical example of the assets you can expect to find for an application.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Simple documentation, typically in a Readme.md file (or additional documents linked to it).&lt;/p&gt; 
&lt;div id="attachment_3829" style="width: 937px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3829" loading="lazy" class="size-full wp-image-3829" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-15.53.43.png" alt="Figure 3. An example of the documentation available." width="927" height="788"&gt;
 &lt;p id="caption-attachment-3829" class="wp-caption-text"&gt;Figure 3. An example of the documentation available.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The documentation is not meant to be exhaustive – it focuses on what is relevant for running the application in the most optimal way on AWS. For general purpose application documentation (like end-user or admin guides), refer to the official guides that come with the app itself.&lt;/p&gt; 
&lt;p&gt;Typically, the documentation in the repo will include the list of versions and architectures we’ve tested, application installation tips, general tips, and key settings relevant for tuning the performance, and some performance-related information with charts and metrics for the most relevant Instance types.&lt;/p&gt; 
&lt;div id="attachment_3830" style="width: 680px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3830" loading="lazy" class="size-full wp-image-3830" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-15.54.43.png" alt="Figure 4. An example of the performance charts we will provide." width="670" height="845"&gt;
 &lt;p id="caption-attachment-3830" class="wp-caption-text"&gt;Figure 4. An example of the performance charts we will provide.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;How to use these best practices&lt;/h2&gt; 
&lt;p&gt;If you already have a cluster up and running, you can try these best practices by cloning this repository:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;git clone https://github.com/aws-samples/hpc-applications.git &lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In case you don’t have an existing HPC Cluster, or if you want to deploy a new one for the scope of these tests, then you can follow &lt;a href="https://github.com/aws-samples/hpc-applications/blob/main/ParallelCluster/README.md"&gt;the guide to launch a new ParallelCluster&lt;/a&gt;. We’ve included a few simple CloudFormation templates that help you to create a new test HPC cluster with just a few clicks. If you want to assemble a more complex environment for testing, using modular templates designed to work together, check out the HPC Recipes Library, which is also available &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/"&gt;as a GitHub repo&lt;/a&gt; (and is well explained in our &lt;a href="https://hpc.news/recipes"&gt;blog post&lt;/a&gt; when we announced it).&lt;/p&gt; 
&lt;p&gt;To deploy one of the one-click stacks, select your preferred AWS Region among the ones shown in the table, and click the appropriate launch button. You’ll be asked a few questions about networking and storage. If you don’t know how to answer these, just leave the default values: AUTO. The one-click deployment procedure will take care of creating everything needed for your HPC Cluster to run properly.&lt;/p&gt; 
&lt;div id="attachment_3831" style="width: 459px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3831" loading="lazy" class="size-full wp-image-3831" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-15.55.37.png" alt="Figure 5. the links to the 1-Click CloudFormation templates." width="449" height="409"&gt;
 &lt;p id="caption-attachment-3831" class="wp-caption-text"&gt;Figure 5. the links to the 1-Click CloudFormation templates.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;After the CloudFormation stack is deployed, you can go to the &lt;strong&gt;Output&lt;/strong&gt; tab in the CloudFormation console and click on the &lt;strong&gt;SystemManagerUrl&lt;/strong&gt; link. This will let you securely access the head node using AWS Systems Manager (SSM), without needing a password or certificate. You’ll find a clone of the GitHub HPC Applications Best Practices repo under &lt;code&gt;/fsx&lt;/code&gt; on the cluster.&lt;/p&gt; 
&lt;div id="attachment_3832" style="width: 922px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3832" loading="lazy" class="size-full wp-image-3832" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-15.56.42.png" alt="Figure 6. The Outputs tab in the CloudFormation console shows a link to securely connect to the cluster through AWS Systems Manager (SSM), without needing a password or certificate." width="912" height="305"&gt;
 &lt;p id="caption-attachment-3832" class="wp-caption-text"&gt;Figure 6. The Outputs tab in the CloudFormation console shows a link to securely connect to the cluster through AWS Systems Manager (SSM), without needing a password or certificate.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Fine tuning your HPC applications to run well on any cluster is a complex task. We aim to keep this repository up-to-date with future versions of common applications and AWS services to provide you with the best experience possible with the fewest steps.&lt;/p&gt; 
&lt;p&gt;We’d love to receive your feedback (using &lt;a href="https://github.com/aws-samples/hpc-applications/issues"&gt;GitHub Issues&lt;/a&gt;) to let us know if this is useful for you, or if you need something else to support your business.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Job queue snapshots: see what’s at the head of your queues in AWS Batch</title>
		<link>https://aws.amazon.com/blogs/hpc/job-queue-snapshots-see-whats-at-the-head-of-your-queues-in-aws-batch/</link>
		
		<dc:creator><![CDATA[Angel Pizarro]]></dc:creator>
		<pubDate>Thu, 20 Jun 2024 14:52:30 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[EC2 Spot]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">f22c191e048a0cdce1210fc4a7b44187ffd3d053</guid>

					<description>AWS Batch just grew a neat new feature: Job queue snapshots. This gives you the visibility you need for managing throughput in a dynamic environment - with competing priorities - and across multiple queues and workloads. Today we give you the inside scoop on how this feature works - especially when you’re using fair share scheduling.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="size-full wp-image-3893 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/20/boofla88_Rows_of_mice_wearing_lab_coats_lined_up_in_orderly_que_05a3c6ff-1f52-4929-b5a0-d81619ecacd2.png" alt="Job queue snapshots: see what’s at the head of your queues in AWS Batch" width="380" height="212"&gt;In 2021, AWS Batch introduced &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/job_scheduling.html"&gt;fair share job queues&lt;/a&gt; which allowed customers to create &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/scheduling-policies.html"&gt;scheduling policies&lt;/a&gt; for a job queue so you can control how to split resources and prioritize jobs &lt;em&gt;that belong to different workloads&lt;/em&gt; (differentiated by their jobs having different &lt;a href="https://docs.aws.amazon.com/batch/latest/userguide/scheduling-policy-parameters.html"&gt;share identifiers&lt;/a&gt;). Before this, all Batch job queues acted as independent first-in-first-out (FIFO) queues. That meant if you had multiple groups or workloads in the same AWS account, you needed separate job queues (JQs) and compute environments (CEs) for each business need.&lt;/p&gt; 
&lt;p&gt;This in turn meant you needed to manage the distribution of the underlying compute resource across all these CEs. By introducing Fair Share Scheduling (FSS), customers like Amazon Search could consolidate their environments, which reduced their operational overhead, drove up their fleet utilization – and greatly improved their throughput.&lt;/p&gt; 
&lt;p&gt;But moving away from FIFO queues introduced a different challenge — how to reliably tell what was going to run next across the set of possible jobs at the head of the queue across different share identifiers.&lt;/p&gt; 
&lt;p&gt;Today we’ll explain a recent addition to AWS Batch that we think will address this: Job queue snapshots. This is a new API &lt;a href="https://aws.amazon.com/about-aws/whats-new/2024/06/aws-batch-job-queue-snapshot-jobs-front-job-queues/"&gt;we launched a few weeks ago&lt;/a&gt; to query the jobs that are at the head of the job queue. We’ll work through the details and give you an practical example of how to use this new information.&lt;/p&gt; 
&lt;h2&gt;Inspecting the head of the queue&lt;/h2&gt; 
&lt;p&gt;Using the &lt;a href="https://console.aws.amazon.com/batch/home?#queues"&gt;AWS Batch management console&lt;/a&gt;, the AWS &lt;a href="https://docs.aws.amazon.com/batch/latest/APIReference/API_GetJobQueueSnapshot.html"&gt;SDK&lt;/a&gt;, or AWS &lt;a href="https://docs.aws.amazon.com/cli/latest/reference/batch/get-job-queue-snapshot.html"&gt;CLI&lt;/a&gt;, you can now list the first 100 &lt;code&gt;RUNNABLE&lt;/code&gt; jobs for a single job queue by calling the &lt;code&gt;&lt;a href="https://docs.aws.amazon.com/batch/latest/APIReference/API_GetJobQueueSnapshot.html"&gt;GetJobQueueSnapshot&lt;/a&gt;&lt;/code&gt; API. For FIFO job queues, jobs are ordered based on their submission time. For FSS job queues, jobs are ordered based on their share’s usage and, within a share, the job share priority. You can read more about how job priority and share usage effect job scheduling in our &lt;a href="https://aws.amazon.com/blogs/hpc/deep-dive-on-fair-share-scheduling-in-aws-batch/"&gt;fair share deep dive&lt;/a&gt; blog post.&lt;/p&gt; 
&lt;p&gt;Job queue snapshots are a great visibility tool for customers that need to make on-the-fly modifications to jobs in the queue. Let’s take a look at a practical example.&lt;/p&gt; 
&lt;p&gt;We’ve created a &lt;em&gt;fair share job queue&lt;/em&gt; that uses AWS Fargate for the compute environment. For the purposes of this experiment, we’ve temporarily disabled the compute environment so that jobs stay in the job queue and we can see the results of the queue manipulations. Finally, the fair share policy gives equal weight to the two active shares, “pink” and “blue”, meaning that you should see an interleaving of set of jobs with each share (Figure 1).&lt;/p&gt; 
&lt;div id="attachment_3881" style="width: 909px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3881" loading="lazy" class="size-full wp-image-3881" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/19/IMG-2024-06-19-09.35.15.png" alt="Figure 1:&amp;nbsp; The AWS Batch management console showing the Job queue snapshot tab. The tab lists an interleaving of pink and blue jobs that are at the head of the job queue." width="899" height="456"&gt;
 &lt;p id="caption-attachment-3881" class="wp-caption-text"&gt;Figure 1: The AWS Batch management console showing the Job queue snapshot tab. The tab lists an interleaving of pink and blue jobs that are at the head of the job queue.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Bob from team &lt;em&gt;pink&lt;/em&gt; comes in with an urgent request to run high-priority jobs as soon as possible to meet an important deadline. You submit these jobs with a priority=10, and this results in the high priority jobs moving ahead of all other &lt;em&gt;pink&lt;/em&gt; jobs, but there are still some &lt;em&gt;blue&lt;/em&gt; jobs ahead of one or more high priority &lt;em&gt;pink&lt;/em&gt; jobs (Figure 2). That’s because job priority is only applicable &lt;em&gt;within each share&lt;/em&gt;, and doesn’t affect the overall placement of jobs &lt;em&gt;across shares&lt;/em&gt;. At this point you can determine whether the high-priority pink jobs can finish by the deadline without affecting team &lt;em&gt;blue’s&lt;/em&gt; workload.&lt;/p&gt; 
&lt;div id="attachment_3882" style="width: 899px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3882" loading="lazy" class="size-full wp-image-3882" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/19/IMG-2024-06-19-09.35.50.png" alt="Figure 2: The AWS Batch management console showing the job queue snapshot tab. The tab shows that high-priority pink jobs have moved ahead of lower priority pink jobs, but are still interleaved with blue jobs." width="889" height="454"&gt;
 &lt;p id="caption-attachment-3882" class="wp-caption-text"&gt;Figure 2: The AWS Batch management console showing the job queue snapshot tab. The tab shows that high-priority pink jobs have moved ahead of lower priority pink jobs, but are still interleaved with blue jobs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;If you think that the high priority jobs will still not finish by the deadline, then you have two options:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;You can temporarily adjust the share policy to prefer &lt;em&gt;pink&lt;/em&gt; jobs over &lt;em&gt;blue&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;You can cancel team &lt;em&gt;blue&lt;/em&gt;’s jobs and then resubmit them once the high priority jobs are &lt;code&gt;RUNNING&lt;/code&gt;. Note that, in this case, &lt;em&gt;blue&lt;/em&gt; jobs will keep their place in the queue, even though Batch has marked them for cancellation. When they reach the head of the queue, their state will immediately switch to &lt;code&gt;FAILED&lt;/code&gt; and won’t take up any compute resources.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Since option 1 is less destructive, you change the scheduling policy to preference placement of team &lt;em&gt;pink&lt;/em&gt; jobs by adjusting the weight factor to a lower value (lower weight factor &lt;a href="https://docs.aws.amazon.com/batch/latest/APIReference/API_ShareAttributes.html#Batch-Type-ShareAttributes-weightFactor"&gt;means&lt;/a&gt; a share gets more compute resources over time). This places &lt;em&gt;most&lt;/em&gt; of the high priority pink jobs ahead of any &lt;em&gt;blue&lt;/em&gt; ones (Figure 3).&lt;/p&gt; 
&lt;div id="attachment_3883" style="width: 911px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3883" loading="lazy" class="size-full wp-image-3883" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/19/IMG-2024-06-19-09.36.16.png" alt="Figure 3: The AWS Batch management console showing the job queue snapshot tab. The tab shows that high-priority pink jobs have moved ahead of lower priority pink jobs, but are still interleaved with blue jobs." width="901" height="459"&gt;
 &lt;p id="caption-attachment-3883" class="wp-caption-text"&gt;Figure 3: The AWS Batch management console showing the job queue snapshot tab. The tab shows that high-priority pink jobs have moved ahead of lower priority pink jobs, but are still interleaved with blue jobs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The underlying reason &lt;em&gt;blue&lt;/em&gt; still gets some allocation before pink is that the &lt;em&gt;fair share&lt;/em&gt; algorithm will try to give blue jobs &lt;em&gt;some&lt;/em&gt; allocation of resources (it’s called “fair share” for a reason). Figure 3 shows that most of the upcoming resources are for &lt;em&gt;pink&lt;/em&gt; jobs, even the regular priority ones. At the low weight factor we set, the next &lt;em&gt;blue&lt;/em&gt; job comes after the last &lt;em&gt;pink&lt;/em&gt; job in our queue (not shown in the figure).&lt;/p&gt; 
&lt;p&gt;If at this point you’re still not sure that the &lt;em&gt;pink&lt;/em&gt; jobs will complete by your deadline, you may need to take the more drastic option number 2. In either case, once the high-priority workloads are &lt;code&gt;RUNNING&lt;/code&gt; be sure to re-instate the previous share policy allocations. Otherwise Batch will keep prioritizing team &lt;em&gt;pink&lt;/em&gt;’s jobs over team blue’s – forever.&lt;/p&gt; 
&lt;p&gt;Before this new feature to see what’s at the head of the queue, you may have needed to be indiscriminate about “clearing the queue” – cancelling all scheduled jobs to make room for the high priority request.&lt;/p&gt; 
&lt;p&gt;Now with job queue snapshots you can be more prescriptive about how to adjust the job queue to allow high-priority workloads to run in the time you need.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this post we described a new feature for AWS Batch: &lt;em&gt;job queue snapshots&lt;/em&gt;. This feature improves the customer and user experience with job queues by giving insight into what is at the head of the queue for both FIFO and fair share job queues. We also described a scenario to use job queue snapshots to help make decisions about managing a queue to reorder workloads based on an urgent priority.&lt;/p&gt; 
&lt;p&gt;Job queue snapshots are available now in to the &lt;a href="https://console.aws.amazon.com/batch/home?"&gt;AWS Batch console&lt;/a&gt;, or through the CLI, or API – whichever you prefer. We think you’ll love this new feature and welcome feedback on how you use it. Also &lt;a href="mailto:ask-hpc@amazon.com?subject=Re:%20GetJobQueueuSnapshot%20and%20job%20cancellation%20features"&gt;let us know&lt;/a&gt; if there’s more we can do to make the job of managing jobs easier.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>An agent-based simulation of Amazon’s inbound supply chain</title>
		<link>https://aws.amazon.com/blogs/hpc/an-agent-based-simulation-of-amazons-inbound-supply-chain/</link>
		
		<dc:creator><![CDATA[Siva Veluchamy]]></dc:creator>
		<pubDate>Wed, 19 Jun 2024 14:47:06 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">897cb050a52ce3f07883cdad8b87aedea2e5ba9c</guid>

					<description>Hundreds of millions of products, the entire *first-mile* of distribution - learn how Amazon simulated their massive US supply chain, end-to-end, with help from a company called Simudyne.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright wp-image-3762 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/30/An-agent-based-simulation-of-Amazons-inbound-supply-chain-2.png" alt="An agent-based simulation of Amazon's inbound supply chain" width="380" height="212"&gt;&lt;/p&gt; 
&lt;p&gt;Amazon has a complex inbound supply chain network for our retail operations in the United States, and we’re always seeking innovative ways to make it more efficient.&lt;/p&gt; 
&lt;p&gt;Recently, we embarked on an ambitious project: simulating our entire US inbound supply chain. To put this into perspective, this is the entire “first-mile” of distribution, tracking the movement of hundreds of millions of individual products through the network. This was a significant first step towards enhancing the network’s efficiency, and was made possible by using &lt;a href="https://docs.simudyne.com/overview"&gt;Simudyne’s Software Development Kit&lt;/a&gt; and the compute power of AWS.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk you through the story of our supply chain, and how we came to pull of what we think is quite an amazing feat. The supply chain toolkit used to build this is available on AWS Marketplace, so if you’re reading this – and interested in supply chain simulation – you can just dive right in.&lt;/p&gt; 
&lt;h2&gt;Amazon’s inbound network: a complex system&lt;/h2&gt; 
&lt;p&gt;Amazon’s inbound network in the US is a multi-stage, complex, and dynamic system where goods flow from a myriad of suppliers to strategically-located cross-dock facilities (we call them IXDs), and ultimately to Fulfillment Centers.&lt;/p&gt; 
&lt;p&gt;Suppliers range from local startups to international conglomerates, each feeding into the diverse Amazon inventory. The IXDs are vital in this system, acting as high-efficiency transit hubs where products are sorted and prepped for their next leg to the Fulfillment Centers.&lt;/p&gt; 
&lt;p&gt;Fulfillment Centers are large facilities, outfitted with advanced technologies like artificial intelligence (AI) systems and robotics. Items are stored, cataloged, and eventually dispatched to customers from these locations. This segment of the supply chain exemplifies the need for balance in optimization – a blend of efficiency, cost management, and technological innovation.&lt;/p&gt; 
&lt;p&gt;Optimizing a network like this is challenging. We need to balance cost efficiency and customer satisfaction, align operations with environmental sustainability, ensure adaptability to future market shifts, and deal manage technological advancements.&lt;/p&gt; 
&lt;p&gt;There must also be an understanding of the interface between the &lt;em&gt;micro&lt;/em&gt; aspects of fulfillment centers (i.e., what’s happening within individual centers/facilities) and the &lt;em&gt;macro&lt;/em&gt; aspects of a supply chain network (i.e., phenomena that emerge when examining the web of relationships between facilities).&lt;/p&gt; 
&lt;p&gt;One way to approach this optimization problem is by building and executing computer simulations, or mathematical models that aim to mimic systems, like supply chain networks, so the simulations can be used to study how those systems work. Building a supply chain simulation that can represent this variety of scales needs an intricate understanding of the supply chain’s dynamics – from warehousing logistics to the last-mile delivery intricacies.&lt;/p&gt; 
&lt;h2&gt;Solving for complexity: the power of agent-based models&lt;/h2&gt; 
&lt;p&gt;Simudyne’s Agent-Based Modeling (ABM) technology and SDK turned out to be a game-changer in our optimization toolkit. This technology allows us to simulate the Amazon supply chain with high fidelity, with each agent representing a distinct component like a warehouse or delivery vehicle. By mimicking real-world behavior and decision-making processes, we can analyze and optimize various aspects of supply chains like topology, inventory placement, resource allocation, and so forth.&lt;/p&gt; 
&lt;p&gt;Simudyne’s SDK is designed to strike a balance between handling the multi-scale system representation problem, the effort needed to build and implement simulations, and the ability of simulations to take advantage of large-scale computing that can speed up the business decision-making process.&lt;/p&gt; 
&lt;p&gt;Key to this balance is Simudyne’s advanced approach to agent-based modeling, which leverages a graph-based approach designed for more accurate and efficient simulations. Unlike traditional agent-based modeling tools that might use graph representation primarily for data visualization, Simudyne’s method involves using the graph as the core computational structure.&lt;/p&gt; 
&lt;p&gt;In this setup, each agent, whether a facility, vehicle, or another entity, is a node with connections to other nodes, representing their interactions and relationships. This structure allows agents to make independent observations and decisions based on their interactions, leading to a more dynamic and realistic simulation of the real world.&lt;/p&gt; 
&lt;h2&gt;Sumdyne’s SDK&lt;/h2&gt; 
&lt;p&gt;Simudyne uses a &lt;a href="https://docs.simudyne.com/features/graph-computation"&gt;Pregel-like framework for processing large-scale graphs&lt;/a&gt;. Pregel supports parallelized processing of massive graphs and graph computations by distributing the workload across multiple cores, for instance across large Amazon EC2 instances like the r6a.32xlarge, with 128 vCPUs. This technique is highly efficient for handling the complex, large-scale supply chain simulations that Amazon requires.&lt;/p&gt; 
&lt;p&gt;Simudyne’s Java-based SDK employs structures like &lt;em&gt;Parallel Streams&lt;/em&gt; and &lt;em&gt;Concurrent Lists&lt;/em&gt; that unlock additional intra-simulation parallelization potential. The SDK is designed for scalability from small-scale tests to large-scale production without needing significant code rewrites, ensuring that our simulation models are not only accurate, but flexible and adaptable.&lt;/p&gt; 
&lt;p&gt;Simudyne recently released a &lt;a href="https://docs.simudyne.com/supply_chain_toolkit"&gt;Supply Chain Toolkit&lt;/a&gt; that specifically enhances the SDK with specialized tools for modeling supply chain networks. The new toolkit includes some useful classes for representing the network:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Facility&lt;/code&gt; agent&lt;strong&gt; &amp;nbsp;&lt;/strong&gt;– a class which &lt;a href="https://docs.simudyne.com/supply_chain_toolkit/facility"&gt;users can extend&lt;/a&gt; to represent various types of facilities within a supply chain. This class includes essential functions for managing the flow of products, and users can add multiple &lt;code&gt;LoadingBay&lt;/code&gt; classes for different types of docks.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;TransportMessages&lt;/code&gt; – a class that &lt;a href="https://docs.simudyne.com/supply_chain_toolkit/transport_message"&gt;facilitates the movement&lt;/a&gt; of inventory between facilities, with options for cargo to be represented either as &lt;code&gt;Product&lt;/code&gt; class or as a string for memory efficiency.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;LoadingBay&lt;/code&gt; – a class that &lt;a href="https://docs.simudyne.com/supply_chain_toolkit/loading_bay"&gt;represents loading docks&lt;/a&gt; in the simulation, with a transport queue and a specified capacity, allowing for various types of docks and custom priority handling.&lt;/p&gt; 
&lt;p&gt;For visualizing supply chain networks, the &lt;a href="https://docs.simudyne.com/supply_chain_toolkit/python_visualization"&gt;Simudyne SDK integrates with Python tools like Plotly&lt;/a&gt;, so we can create interactive maps to display facility information. We can customize this visualization to display additional data points and network connections.&lt;/p&gt; 
&lt;p&gt;Overall, the toolkit is designed to provide a comprehensive suite for simulating and optimizing supply chain networks, offering operational researchers and supply chain analysts a powerful tool for modeling complex dynamics and managing high-throughput scenarios.&lt;/p&gt; 
&lt;div id="attachment_3794" style="width: 1396px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3794" loading="lazy" class="size-full wp-image-3794" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/07/IMG-2024-06-07-14.51.33.png" alt="Fig 1: Sample Dashboard Connecting Simudyne SDK to Custom Visualization Layer" width="1386" height="581"&gt;
 &lt;p id="caption-attachment-3794" class="wp-caption-text"&gt;Fig 1: Sample Dashboard Connecting Simudyne SDK to Custom Visualization Layer&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Running supply chain simulations on AWS&lt;/p&gt; 
&lt;p&gt;Using the Simudyne SDK and the Supply Chain Toolkit, Amazon’s Worldwide Design Engineering team, Simudyne, and AWS’s Emerging Technologies &amp;amp; Workloads team worked together to build a simulation of Amazon’s US inbound supply chain on AWS.&lt;/p&gt; 
&lt;p&gt;We’ve shown the reference architecture in Figure 2. The simulation code connects to an Amazon Redshift data warehouse to pull relevant supply chain data that’s used in the simulations.&lt;/p&gt; 
&lt;p&gt;Typically, a full-scale simulation of the US inbound supply chain can be executed on a r6a.32xlarge EC2 instance, which not only has 128 vCPUs, but also comes with over 1 TiB of RAM. Agent-based simulations are often memory-intensive applications and more performant when the state of agents during simulations can be saved and operated in memory, making high-memory instances found on AWS very handy. On an r6a.32xlarge instance, three (3) simulation weeks of the full US inbound supply chain ran in about an hour.&lt;/p&gt; 
&lt;div id="attachment_3755" style="width: 1004px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3755" loading="lazy" class="size-full wp-image-3755" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/30/IMG-2024-05-30-10.36.03.png" alt="Figure 2: AWS reference architecture used to run simulations of the Amazon US inbound supply chain." width="994" height="599"&gt;
 &lt;p id="caption-attachment-3755" class="wp-caption-text"&gt;Figure 2: AWS reference architecture used to run simulations of the Amazon US inbound supply chain.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Using Simudyne’s technology to simulate our supply chain has opened new avenues for us. We think It can provide insights into cost reduction, service improvement, environmental impact, and strategic adaptability.&lt;/p&gt; 
&lt;p&gt;This project is not just a testament to Amazon’s commitment to innovation but also an example for people exploring the frontiers of supply chain optimization. We’ll continue to refine our models and strategies because the potential for groundbreaking improvements in e-commerce logistics remains vast and exciting.&lt;/p&gt; 
&lt;p&gt;Now that we’ve successfully modeled our inbound supply chain network helping with the Inbound regionalization initiative as outlined in our CEO’s letter to shareholders, our focus is now shifting to simulating the outbound network – the critical journey of products from our Fulfillment Centers to customers. This new phase of modeling will let us explore and experiment with various strategies – including intermodal transport – within the simulated environment.&lt;/p&gt; 
&lt;p&gt;This allows us to refine and validate our strategies before we implement them in the real world. In our upcoming posts, we plan to share insights from our experience extending our simulations and using AI to further optimize the network, offering a glimpse into the cutting-edge methods driving supply chain efficiency.&lt;/p&gt; 
&lt;p&gt;Simudyne currently offers a 30-day free trial of its SDK, which you can find in &lt;a href="https://aws.amazon.com/marketplace/seller-profile?id=seller-2hr6jozikusxs"&gt;AWS Marketplace&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Call for participation: HPC tutorial series from the HPCIC</title>
		<link>https://aws.amazon.com/blogs/hpc/call-for-participation-hpc-tutorial-series-from-the-hpcic/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Tue, 11 Jun 2024 17:09:26 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Public Sector]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<category><![CDATA[Storage]]></category>
		<category><![CDATA[Sustainability]]></category>
		<category><![CDATA[visualization]]></category>
		<category><![CDATA[Weather]]></category>
		<guid isPermaLink="false">7634a0bf6ed82010fb48611fdb16217bc1cbb185</guid>

					<description>Interested in getting hands-on experience with cutting-edge HPC tools? Check out this blog post on an upcoming virtual training series from @LLNL and @AWSCloud. Learn emerging technologies from the experts this August.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3849" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/IMG-2024-06-11-16.56.36.png" alt="Call for participation: HPC tutorial series from the HPCIC" width="380" height="161"&gt;Lawrence Livermore National Laboratory (LLNL) and AWS are joining forces to provide a training opportunity for emerging HPC tools and application. This takes the form of a series of tutorials over the summer by the HPC Innovation Center (HPCIC) at the Lab, which focuses on a broad suite of open-source software projects originating from LLNL. The series aims to give attendees experience with these cutting-edge technologies.&lt;/p&gt; 
&lt;p&gt;This &lt;a href="https://hpcic.llnl.gov/tutorials/2024-hpc-tutorials"&gt;virtual series&lt;/a&gt; consists of nine distinct tutorials through the month of August. Each tutorial focuses on a specific package from the HPCIC. You’ll get hands-on experience with each project and learn more about running HPC on AWS. Each event will be hosted on AWS resources and use &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt; to provide the environment for attendees.&lt;/p&gt; 
&lt;p&gt;LLNL has a long history of delivering some of the world’s largest supercomputers and the supporting software to effectively use them. This collaboration between LLNL and AWS brings these projects and industry best practices to a broader community. Together we are democratizing access to next generation computational research tools and HPC resources.&lt;/p&gt; 
&lt;h2&gt;How to Participate&lt;/h2&gt; 
&lt;p&gt;Come join us in learning how these projects can help advance your HPC research and improve efficiency. These tutorials provide a great opportunity to learn modern techniques and tools in an adaptive cloud environment.&lt;/p&gt; 
&lt;p&gt;To register your interest, use the &lt;a href="https://llnlfed.webex.com/webappng/sites/llnlfed/webinar/webinarSeries/register/f0f129eba81946dc8a30552fc657ee94"&gt;signup form&lt;/a&gt;. Details of each event and registration deadlines are listed in the following table.&lt;/p&gt; 
&lt;table style="width: 90%"&gt; 
 &lt;thead&gt; 
  &lt;tr style="background-color: #000000;height: 20px"&gt; 
   &lt;th style="width: 20%"&gt;&lt;span style="color: #ffffff"&gt;Date&lt;/span&gt;&lt;/th&gt; 
   &lt;th style="width: 20%" width="72"&gt;&lt;span style="color: #ffffff"&gt;&lt;strong&gt;Time (PDT, GMT-7)&lt;/strong&gt;&lt;/span&gt;&lt;/th&gt; 
   &lt;td width="406"&gt;&lt;span style="color: #ffffff"&gt;&lt;strong&gt;Tutorial&lt;/strong&gt;&lt;/span&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu Aug 1&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;BLT:&lt;/strong&gt;&amp;nbsp;build, link, and test large-scale applications&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tue Aug 6&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;8–11:30am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Spack (part 1 of 2):&lt;/strong&gt;&amp;nbsp;learn to install your software quickly&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Wed Aug 7&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;8–11:30am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Spack (part 2 of 2)&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu Aug 8&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–12pm&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Caliper:&lt;/strong&gt;&amp;nbsp;integrate performance profiling capabilities into your applications&lt;br&gt; &lt;strong&gt;Hatchet:&lt;/strong&gt;&amp;nbsp;analyze hierarchical performance data&lt;br&gt; &lt;strong&gt;Thicket:&lt;/strong&gt;&amp;nbsp;optimize application performance on supercomputers&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tue Aug 13&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9– 11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;RAJA:&lt;/strong&gt;&amp;nbsp;run and port codes across different GPUs&lt;br&gt; &lt;strong&gt;Umpire:&lt;/strong&gt;&amp;nbsp;discover, provision, and manage HPC memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu Aug 15&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Axom:&lt;/strong&gt;&amp;nbsp;leverage robust, flexible software components for scientific applications&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tue Aug 20&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Ascent:&lt;/strong&gt;&amp;nbsp;visualize and analyze your simulations in situ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu Aug 22&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;MFEM:&lt;/strong&gt;&amp;nbsp;use scalable finite element discretizations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Tue Aug 27&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;WEAVE:&lt;/strong&gt;&amp;nbsp;analyze runs of your code&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Thu 29&lt;/strong&gt;&lt;/td&gt; 
   &lt;td width="72"&gt;9–11am&lt;/td&gt; 
   &lt;td width="406"&gt;&lt;strong&gt;Flux:&lt;/strong&gt;&amp;nbsp;run thousands of jobs in a workflow&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;We’re excited about this opportunity to give back to the HPC community alongside LLNL and look forward to meeting you all over the duration of the event.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Integrating Research and Engineering Studio with AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/research-and-engineering-studio-integration-with-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Doug Morand]]></dc:creator>
		<pubDate>Tue, 11 Jun 2024 13:38:50 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[DCV]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[VDI]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">975dc9ecff3f91bdf9cbec7ea6b4ab9cc4b3a431</guid>

					<description>Researchers, engineers &amp;amp; scientists - learn how to leverage AWS ParallelCluster with Research &amp;amp; Engineering Studio for a full-featured cloud workspace. Read this post for details on this new integration.</description>
										<content:encoded>&lt;p&gt;&lt;a href="https://aws.amazon.com/hpc/res/"&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3787" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/06/Integrating-Research-and-Engineering-Studio-with-AWS-ParallelCluster-1.png" alt="Integrating Research and Engineering Studio with AWS ParallelCluster" width="380" height="213"&gt;Research and Engineering Studio on AWS (RES)&lt;/a&gt; is an easy-to-use self-service portal for researchers and engineers to access and manage their cloud-based workspaces with persistent storage and secure virtual desktops to access their data and run interactive applications.&lt;/p&gt; 
&lt;p&gt;The people who use RES are also frequently users of HPC clusters. RES today does not ship with direct integration to other AWS solutions for HPC, like &lt;a href="https://aws.amazon.com/hpc/parallelcluster/"&gt;AWS ParallelCluster&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;So today, we’re happy to announce a new HPC recipe which creates a RES-compatible &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/login-nodes-v3.html"&gt;ParallelCluster login node&lt;/a&gt; software stack. This solution uses new features added to RES 2024.04 including &lt;a href="https://docs.aws.amazon.com/res/latest/ug/res-ready-ami.html"&gt;RES-ready AMIs&lt;/a&gt; and project &lt;a href="https://docs.aws.amazon.com/res/latest/ug/projects.html#project-launch-template"&gt;launch templates&lt;/a&gt;. The RES-ready AMI allows you to pre-install RES dependencies for your virtual desktop instances (VDIs) to pre-bake software and configuration into your images, and improve boot times for your end users.&lt;/p&gt; 
&lt;p&gt;These AMIs can be registered as a RES &lt;a href="https://docs.aws.amazon.com/res/latest/ug/evdi.html#software-stacks"&gt;software stack&lt;/a&gt; for your end users to create a VDI, which joins a ParallelCluster to become their own personal login node. Using this recipe, you’ll see how to create customized software stacks that can be used as conduits to access other native AWS cloud services.&lt;/p&gt; 
&lt;h2&gt;ParallelCluster login node for RES&lt;/h2&gt; 
&lt;p&gt;Starting from version 3.7.0, ParallelCluster provides login nodes for users to access the Slurm-based environment in the cloud. Both RES and ParallelCluster support multi-user access using Active Directory (AD) integration. With that mechanism in place, shared storage and user authentication becomes easier to manage.&lt;/p&gt; 
&lt;p&gt;We start with an existing ParallelCluster that’s integrated with the same AD and using the same &lt;code&gt;ldap_id_mapping&lt;/code&gt; setting. We take a snapshot of a login node of the cluster, turn it into an Amazon Machine Image (AMI), and use as the Base image within an EC2 Image Builder recipe to create a RES-ready AMI. The &lt;code&gt;ldap_ip_mapping&lt;/code&gt; setting is an installation-level setting used by System Security Services Daemon (SSSD). This setting (when enabled) allows SSSD to generate a UID/GID.&lt;/p&gt; 
&lt;p&gt;This setting is used in both RES and ParallelCluster. In RES, it’s enabled through the &lt;code&gt;EnableLdapIDMapping&lt;/code&gt; AWS CloudFormation parameter which you use to &lt;a href="https://docs.aws.amazon.com/res/latest/ug/launch-the-product.html"&gt;launch the product&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In ParallelCluster, it’s configured in the &lt;code&gt;DirectoryService&lt;/code&gt; section of the configuration file, as part of &lt;code&gt;AdditionalSssdConfigs&lt;/code&gt; (more on this &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/DirectoryService-v3.html#yaml-DirectoryService-AdditionalSssdConfigs"&gt;in our docs&lt;/a&gt;), like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;DirectoryService:
…
  AdditionalSssdConfigs:
    ldap_id_mapping: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The RES-compatible login node AMI is created through an &lt;a href="https://aws.amazon.com/systems-manager/"&gt;AWS Systems Manager&lt;/a&gt; (SSM) automation document. SSM allows you to gather insights and automate tasks across your AWS accounts.&lt;/p&gt; 
&lt;p&gt;This automation process performs two tasks:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It creates an Amazon Machine Image (AMI) from the login node&lt;/li&gt; 
 &lt;li&gt;Updates the ParallelCluster &lt;em&gt;head node security group&lt;/em&gt; by adding ingress rules to allow connections from RES virtual desktops. Slurm (6819-6829) and NFS (2049) ingress connections are allowed via the &lt;code&gt;RESPCLoginNodeSG&lt;/code&gt; security group. This group will be used later to associate to RES project(s) when creating login node VDI instances.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;After the RES-compatible login node AMI has been created, we can create the RES-ready AMI using &lt;a href="https://aws.amazon.com/image-builder/"&gt;EC2 Image Builder&lt;/a&gt;. EC2 Image Builder simplifies the build process of building and maintaining “golden images”. Our SSM automation process will create an Image Builder recipe that includes the base AMI, and an additional component for the RES login node to configure the image to work as a RES VDI. The resulting AMI from the Image Builder pipeline will be used to create a RES software stack.&lt;/p&gt; 
&lt;p&gt;Once the RES-ready AMI has been created by Image Builder, a RES administrator can login to create the Project and Software Stack by in two steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Update a RES Project to modify the Launch template (or create a project if it’s their first time using RES). The &lt;code&gt;RESPCLoginNodeSG&lt;/code&gt; must be added to any project that requires access to the ParallelCluster. This can be added in the RES project &lt;em&gt;Resource Configurations -&amp;gt; Advanced Options -&amp;gt; Add Security Groups&lt;/em&gt; configuration section.&lt;/li&gt; 
 &lt;li&gt;Create a new Software Stack using the RES-ready AMI created from the Image Builder pipeline.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Once the software stack has been created, end-users that have access to it as part of a Project can create their own, dedicated, login node virtual desktops.&lt;/p&gt; 
&lt;h2&gt;Virtual desktop login node in action&lt;/h2&gt; 
&lt;p&gt;End-users will access the login node virtual desktop in the same way they access other virtual desktops.&lt;/p&gt; 
&lt;div id="attachment_3776" style="width: 957px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3776" loading="lazy" class="size-full wp-image-3776" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/06/IMG-2024-06-06-13.46.01.png" alt="Figure 1 – The Virtual Desktops contain a virtual desktop (PC-LoginNode372) which is a based on a LoginNode instance compatible with ParallelCluster." width="947" height="550"&gt;
 &lt;p id="caption-attachment-3776" class="wp-caption-text"&gt;Figure 1 – The Virtual Desktops contain a virtual desktop (PC-LoginNode372) which is a based on a LoginNode instance compatible with ParallelCluster.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Once end users have access to the login node VDI they can interact with ParallelCluster in the same way they’re accustomed. They’ll have access to the same shared storage in the ParallelCluster Login Node &lt;em&gt;and&lt;/em&gt; in the RES VDI. The next couple of screenshots show examples of shared storage accessed from both the Login Node and RES VDI.&lt;/p&gt; 
&lt;div id="attachment_3777" style="width: 998px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3777" loading="lazy" class="size-full wp-image-3777" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/06/IMG-2024-06-06-13.46.34.png" alt="Figure 2 – A terminal session on a ParallelCluster Login Node showing the user shared storage directory listing" width="988" height="257"&gt;
 &lt;p id="caption-attachment-3777" class="wp-caption-text"&gt;Figure 2 – A terminal session on a ParallelCluster Login Node showing the user shared storage directory listing&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3778" style="width: 996px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3778" loading="lazy" class="size-full wp-image-3778" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/06/IMG-2024-06-06-13.46.54.png" alt="Figure 3 – A RES virtual desktop session showing the same shared storage directory accessible from the ParallelCluster Login Node" width="986" height="428"&gt;
 &lt;p id="caption-attachment-3778" class="wp-caption-text"&gt;Figure 3 – A RES virtual desktop session showing the same shared storage directory accessible from the ParallelCluster Login Node&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;End users can now interact with a ParallelCluster (PC) from a RES VDI. This VDI acts similarly to a ParallelCluster login node with the added benefit that end-users in the RES environment can launch their own login node, with VDI, any time they need one.&lt;/p&gt; 
&lt;div id="attachment_3791" style="width: 1448px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3791" loading="lazy" class="size-full wp-image-3791" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/07/res-loginnode-screenvideo.gif" alt="Figure 4 – A virtual desktop session showing examples of Slurm commands demonstrating the integration with ParallelCluster." width="1438" height="544"&gt;
 &lt;p id="caption-attachment-3791" class="wp-caption-text"&gt;Figure 4 – A virtual desktop session showing examples of Slurm commands demonstrating the integration with ParallelCluster.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Getting started with RES-compatible ParallelCluster login nodes&lt;/h2&gt; 
&lt;p&gt;You can follow the steps to create a RES-compatible login node for your ParallelCluster by heading to the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/login_node_for_res"&gt;Login Node for Research and Engineering Studio&lt;/a&gt; recipe that’s part of the &lt;a href="https://hpc.news/recipes"&gt;HPC Recipes Library&lt;/a&gt; (a great resource, if you’re unfamiliar with it until now).&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Integrating AWS ParallelCluster and Research and Engineering Studio unlocks the ability for end users using interactive desktops to process large amounts of data when HPC is necessary. It’s a great experience, because not only does this put large-scale computational power in the hands of scientists, it does so in a way that’s friendly to use, and accessible any time they need it.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Securing HPC on AWS: implementing STIGs in AWS ParallelCluster</title>
		<link>https://aws.amazon.com/blogs/hpc/securing-hpc-on-aws-implementing-stigs-in-aws-parallelcluster/</link>
		
		<dc:creator><![CDATA[Alex Domijan]]></dc:creator>
		<pubDate>Tue, 04 Jun 2024 11:35:47 +0000</pubDate>
				<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Security]]></category>
		<category><![CDATA[Security, Identity, & Compliance]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">a5572dfa24f9456e8fb3e01a8a7cc20766ce2442</guid>

					<description>Want to accelerate creating compliant Amazon EC2 images? Learn how HPC users can leverage cloud-native methods for applying STIG security standards.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3698" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/Securing-HPC.png" alt="" width="380" height="212"&gt;&lt;/em&gt;Today, we’ll discuss cloud-native methods that HPC customers can use to accelerate their process for creating Amazon Elastic Compute Cloud (Amazon EC2) images for AWS ParallelCluster that are compliant with Security Technical Implementation Guides (STIGs), a set of standards maintained by the US government.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll walk you through the process of applying STIGs to your ParallelCluster environment, help you identify the decisions you need to make on the way, and show you some of the tools you can use to make it all easier.&lt;/p&gt; 
&lt;h2&gt;What’s a STIG?&lt;/h2&gt; 
&lt;p&gt;STIGs are maintained by &lt;a href="https://public.cyber.mil/stigs/downloads/"&gt;a US government organization&lt;/a&gt;, and are simply a set of security standards that can be applied to different environments, like &lt;a href="https://aws.amazon.com/pm/ec2/?gclid=Cj0KCQjwiMmwBhDmARIsABeQ7xQS_X3FGRFdQeMzTWl84Gi1PUFg8U7-Y2A8g30P269Hh4BW5wxBXvIaAmfOEALw_wcB&amp;amp;trk=9cd376cd-1c18-46f2-9f75-0e1cdbca94c5&amp;amp;sc_channel=ps&amp;amp;ef_id=Cj0KCQjwiMmwBhDmARIsABeQ7xQS_X3FGRFdQeMzTWl84Gi1PUFg8U7-Y2A8g30P269Hh4BW5wxBXvIaAmfOEALw_wcB:G:s&amp;amp;s_kwcid=AL!4422!3!651751059309!e!!g!!amazon%20ec2!19852662176!145019189697"&gt;Amazon EC2&lt;/a&gt;. Think of STIGs as a checklist of items to apply to your EC2 instances where each checklist item has a corresponding severity level attached to it that says, “&lt;em&gt;the risk of not doing x is a low, medium, or high security risk&lt;/em&gt;”. You’ll also see these security levels referred to as Category Codes (CAT) where CAT 1 corresponds to a high security risk, CAT 2 to medium, and CAT 3 to low.&lt;/p&gt; 
&lt;p&gt;For example, one high security-risk STIG checklist item for Red Hat Enterprise Linux (RHEL) 8 is to not allow accounts configured with blank or null passwords. To resolve this, an administrator can login to the operating system and manually configure accounts to not have a blank or null password. With hundreds of checklist items it is easy to see why this can quickly become a burdensome task. The process described in this post automates up to 87% of the otherwise manual STIG remediation process.&lt;/p&gt; 
&lt;h2&gt;Why do customers want to implement STIGs?&lt;/h2&gt; 
&lt;p&gt;In short, some want to, and some need to. Customers such as the U.S. Department of Defense (DoD) must adhere to stringent compliance standards for operating system hardening. Other customers may prefer to use STIGs as a benchmark to improve their security posture.&lt;/p&gt; 
&lt;p&gt;Customers like the DoD often operate in AWS without any access to the Internet. Organizational policy dictates the reason for why, which is usually to reduce the risk of sensitive data going places it shouldn’t. We address how customers with these network restrictions can accelerate STIG hardening using AWS cloud native tools.&lt;/p&gt; 
&lt;p&gt;Once you’ve “&lt;em&gt;STIG’d&lt;/em&gt;” your ParallelCluster instances, how can you verify which checklist items you have crossed off? This is where&amp;nbsp;&lt;a href="https://www.open-scap.org/"&gt;OpenSCAP&lt;/a&gt;, an open-source security and compliance tool, comes into play. OpenSCAP automates continuous monitoring, vulnerability management, and reporting of security policy compliance data. While OpenSCAP is primarily designed to align with DoD security standards, it’s used to establish security baselines across many industries.&lt;/p&gt; 
&lt;p&gt;This post will focus on some supported ParallelCluster operating systems (OS): RHEL8, Amazon Linux 2 (AL2), and Ubuntu 20.04 (at the time of writing this, the &lt;a href="https://public.cyber.mil/stigs/downloads/"&gt;DISA STIG document library&lt;/a&gt; didn’t contain a benchmark for Ubuntu 22.04 – which is why it’s not mentioned).&lt;/p&gt; 
&lt;p&gt;We worked through the process defined in this post using the &lt;a href="https://aws.amazon.com/govcloud-us/?whats-new-ess.sort-by=item.additionalFields.postDateTime&amp;amp;whats-new-ess.sort-order=desc"&gt;AWS GovCloud West&lt;/a&gt; region, but you should be able to repeat it in other AWS regions.&lt;/p&gt; 
&lt;p&gt;For HPC customers completely new to AWS, we recommend reviewing this &lt;a href="https://aws.amazon.com/blogs/hpc/the-plumbing-best-practice-infrastructure-to-facilitate-hpc-on-aws/"&gt;blog post&lt;/a&gt; which speaks about best practices for setting up a foundation in AWS to build your HPC workloads on.&lt;/p&gt; 
&lt;h2&gt;AMIs for AWS ParallelCluster&lt;/h2&gt; 
&lt;p&gt;An &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html"&gt;Amazon Machine Image&lt;/a&gt; (AMI) is a template that contains a software configuration (for example, an OS, an application server, and applications). From an AMI, you launch an&amp;nbsp;EC2 instance, which is a copy of the AMI running as a virtual server in the cloud. AMIs used for ParallelCluster are unique because they have software installed on them necessary for operating the cluster management tool.&lt;/p&gt; 
&lt;p&gt;Customers can optionally choose to create custom AMIs for ParallelCluster using &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/custom-ami-v3.html"&gt;two methods&lt;/a&gt;, both of which we can use for achieving STIG compliance, depending on factors like Internet connectivity and OS choice.&lt;/p&gt; 
&lt;p&gt;The first option is the build image configuration process which you can trigger from a ParallelCluster CLI &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcluster.build-image-v3.html"&gt;command&lt;/a&gt;: &lt;code&gt;pcluster build-image&lt;/code&gt;. This process uses &lt;a href="https://aws.amazon.com/image-builder/"&gt;Amazon EC2 Image Builder&lt;/a&gt; to launch a build instance, apply the &lt;a href="https://github.com/aws/aws-parallelcluster-cookbook"&gt;ParallelCluster cookbook&lt;/a&gt;, install the ParallelCluster software stack, and perform other necessary configuration tasks.&lt;/p&gt; 
&lt;p&gt;The second option involves taking a baseline ParallelCluster AMI (one produced by the ParallelCluster team themselves) and customizing it by performing manual modifications through &lt;a href="https://aws.amazon.com/systems-manager/"&gt;AWS Systems Manager&lt;/a&gt; (SSM).&lt;/p&gt; 
&lt;h2&gt;Process comparison&lt;/h2&gt; 
&lt;p&gt;Should you take a baseline ParallelCluster image and then apply STIGs, or take an image that already has STIGs applied (a “golden image”), and then install ParallelCluster on top? The end result is fundamentally similar, but there are some trade-offs depending on which route you choose.&lt;/p&gt; 
&lt;p&gt;The benefit of applying STIGs after a ParallelCluster image is created is that you can minimize permissions attached to the EC2 instance’s role. There &lt;em&gt;are&lt;/em&gt; additional &lt;a href="https://aws.amazon.com/iam/"&gt;AWS Identity and Access Management&lt;/a&gt; (IAM) permissions required to trigger the build image process and you can find them in &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/iam-roles-in-parallelcluster-v3.html#iam-roles-in-parallelcluster-v3-user-policy-build-image"&gt;our documentation&lt;/a&gt;. The tradeoff you’re making is that you would be standing up a new image build pipeline to accommodate security policy (STIG) enforcement starting from a baseline ParallelCluster image.&lt;/p&gt; 
&lt;p&gt;An advantage of taking a golden image and installing ParallelCluster is that you can maintain an already established image build pipeline that may accelerate internal compliance processes. However, this would require a wider permissions boundary in comparison to the previous example. There’s also a chance that installing new software could impact how STIG compliant your images are. For customers interested in trying this process on your own AMIs, you can follow along with any of the sections below depending on Internet connectivity and operating system requirements as the process is the same. In either case, we recommend performing compliance scans on your images.&lt;/p&gt; 
&lt;h2&gt;Accelerating RHEL8, AL2, and Ubuntu 20.04 STIG compliance&lt;/h2&gt; 
&lt;p&gt;Apart from the OS your use cases require, the process to achieve&amp;nbsp;STIG compliance is determined by whether your Amazon EC2 instances have Internet connectivity or not. If your compliance requirements allow you the flexibility to choose, then it’s easier with Internet connectivity.&lt;/p&gt; 
&lt;p&gt;For users &lt;strong&gt;with&lt;/strong&gt; Internet connectivity who want to use RHEL8 or AL2 operating systems, refer to the instructions in our &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/stig#rhel8-and-al2-instances-with-internet-connectivity"&gt;GitHub repo&lt;/a&gt; that’s part of the HPC Recipes Library which will guide you through the EC2 Image Builder process.&lt;/p&gt; 
&lt;p&gt;For users &lt;strong&gt;without&lt;/strong&gt; Internet connectivity who want to use RHEL8 or AL2 operating systems, refer to &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/stig#rhel8-and-al2-instances-without-internet-connectivity"&gt;these instructions&lt;/a&gt; in the same repository. This type of connectivity scenario is perhaps more common amongst customers with STIG requirements. These customers can take advantage of &lt;a href="https://aws.amazon.com/privatelink/"&gt;AWS PrivateLink&lt;/a&gt; which is a feature of Virtual Private Cloud (&lt;a href="https://aws.amazon.com/vpc/"&gt;VPC&lt;/a&gt;) and allows for private connectivity to AWS services. To take advantage of this technology for purposes of accelerating STIG compliance, ensure that you configure the &lt;a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-create-vpc.html"&gt;required VPC endpoints&lt;/a&gt; to allow connectivity from your private subnet to SSM. You’ll also need the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/network-configuration-v3.html#aws-parallelcluster-in-a-single-public-subnet-no-internet-v3"&gt;required VPC endpoints&lt;/a&gt; for ParallelCluster which will be used to launch your cluster with the resulting AMI.&lt;/p&gt; 
&lt;p&gt;The process for Ubuntu 20.04 includes an extra step compared to RHEL8 and AL2 operating systems because there are a couple of findings that Systems Manager cannot rectify during its run command. Due to this, we launch a baseline ParallelCluster Ubuntu 20.04 EC2 instance with a user data script that resolves findings &lt;a href="https://www.stigviewer.com/stig/canonical_ubuntu_18.04_lts/2022-08-25/finding/V-219166"&gt;V-219166&lt;/a&gt;, &lt;a href="https://www.stigviewer.com/stig/canonical_ubuntu_20.04_lts/2023-09-08/finding/V-238237"&gt;V-238237&lt;/a&gt;, and &lt;a href="https://www.stigviewer.com/stig/canonical_ubuntu_20.04_lts/2021-03-23/finding/V-238218"&gt;V-238218.&lt;/a&gt; As with RHEL8 and AL2 operating systems, customers without Internet connectivity should ensure they configure the required VPC endpoints to allow connectivity from your private subnet to SSM, and the required VPC endpoints for ParallelCluster. Instructions for Ubuntu 20.04 can be found in our &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/stig#ubuntu-2004-instances-with-or-without-internet-connectivity"&gt;HPC samples repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;As previously mentioned, there are corresponding severity levels (high, medium, low) associated with STIG checklist items. Customers can choose which security level they want to apply to their Amazon EC2 instances which is described in our &lt;a href="https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/awsec2-configure-stig.html"&gt;SSM documentation&lt;/a&gt;. We used the STIG High baseline which includes any vulnerability that can result in loss of confidentiality, availability, or integrity. Customers can optionally choose to make additional modifications to the AMIs after the STIG process of their choosing has been performed. In any event, we recommend testing AMI compatibility with your application prior to deploying to a production environment.&lt;/p&gt; 
&lt;h2&gt;Results&lt;/h2&gt; 
&lt;p&gt;Customers may be interested to find out what the effects of running the EC2 Image Builder STIG High component or Systems Manager STIG High document has on their respective operating systems.&lt;/p&gt; 
&lt;p&gt;We used OpenSCAP to perform compliance scanning to assess the security posture of our instances. It also uses the concept of profiles to determine which checks it will run and the profile can vary on mission requirements and OS.&lt;/p&gt; 
&lt;p&gt;For the purposes of maintaining a consistent benchmark for before and after assessments, we used the &lt;code&gt;xccdf_mil.disa.stig_profile_MAC-2_Sensitive&lt;/code&gt; profile for RHEL8 and Ubuntu 20.04 operating systems, and &lt;code&gt;stig-rhel7-disa&lt;/code&gt; on AL2.&lt;/p&gt; 
&lt;p&gt;Each of the ‘Baseline’ AMIs in the screenshots that follow refer to the baseline ParallelCluster AMI. In other words, these are the AMIs you would find by typing the CLI &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/pcluster.list-official-images-v3.html"&gt;command&lt;/a&gt;: &lt;code&gt;pcluster list-official-images&lt;/code&gt;. Note that the baseline and subsequent STIG high AMI results may change in future ParallelCluster releases.&lt;/p&gt; 
&lt;div id="attachment_3684" style="width: 1253px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3684" loading="lazy" class="size-full wp-image-3684" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.46.20.png" alt="Figure 1 - RHEL8 baseline ParallelCluster AMI OpenSCAP results from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 83 checks and failing 148 for a result of being 35.93% compliant with this profile." width="1243" height="451"&gt;
 &lt;p id="caption-attachment-3684" class="wp-caption-text"&gt;Figure 1 – RHEL8 baseline ParallelCluster AMI OpenSCAP results from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 83 checks and failing 148 for a result of being 35.93% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3685" style="width: 1270px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3685" loading="lazy" class="size-full wp-image-3685" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.46.41.png" alt="Figure 2 - RHEL8 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 201 checks and failing 30 for a result of being 87.01% compliant with this profile." width="1260" height="447"&gt;
 &lt;p id="caption-attachment-3685" class="wp-caption-text"&gt;Figure 2 – RHEL8 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 201 checks and failing 30 for a result of being 87.01% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3686" style="width: 1243px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3686" loading="lazy" class="size-full wp-image-3686" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.47.16.png" alt="Figure 3 - Amazon Linux 2 baseline ParallelCluster AMI OpenSCAP results from running the stig-rhel7-disa profile. This shows the EC2 instance as passing 54 checks and failing 160 for a result of being 58.88% compliant with this profile." width="1233" height="446"&gt;
 &lt;p id="caption-attachment-3686" class="wp-caption-text"&gt;Figure 3 – Amazon Linux 2 baseline ParallelCluster AMI OpenSCAP results from running the stig-rhel7-disa profile. This shows the EC2 instance as passing 54 checks and failing 160 for a result of being 58.88% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3687" style="width: 1263px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3687" loading="lazy" class="size-full wp-image-3687" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.47.33.png" alt="Figure 4 - Amazon Linux 2 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the stig-rhel7-disa profile. This shows the EC2 instance as passing 142 checks and failing 72 for a result of being 68.09% compliant with this profile." width="1253" height="449"&gt;
 &lt;p id="caption-attachment-3687" class="wp-caption-text"&gt;Figure 4 – Amazon Linux 2 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the stig-rhel7-disa profile. This shows the EC2 instance as passing 142 checks and failing 72 for a result of being 68.09% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3688" style="width: 1273px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3688" loading="lazy" class="size-full wp-image-3688" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.48.01.png" alt="Figure 5 - Ubuntu 20.04 baseline ParallelCluster AMI OpenSCAP results from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 17 checks and failing 92 for a result of being 15.6% compliant with this profile." width="1263" height="454"&gt;
 &lt;p id="caption-attachment-3688" class="wp-caption-text"&gt;Figure 5 – Ubuntu 20.04 baseline ParallelCluster AMI OpenSCAP results from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 17 checks and failing 92 for a result of being 15.6% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;div id="attachment_3689" style="width: 1259px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3689" loading="lazy" class="size-full wp-image-3689" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/21/IMG-2024-05-21-12.48.17.png" alt="Figure 6 - Ubuntu 20.04 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 42 checks and failing 67 for a result of being 38.53% compliant with this profile." width="1249" height="446"&gt;
 &lt;p id="caption-attachment-3689" class="wp-caption-text"&gt;Figure 6 – Ubuntu 20.04 ParallelCluster AMI after running the Amazon STIG High runbook. OpenSCAP results are from running the xccdf_mil.disa.stig_profile_MAC-2_Sensitive profile. This shows the EC2 instance as passing 42 checks and failing 67 for a result of being 38.53% compliant with this profile.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Running your own OpenSCAP scans&lt;/h2&gt; 
&lt;p&gt;If you want to perform additional STIGs on ParallelCluster AMIs, you may want to run those images through the same OpenSCAP profiles used for this blog post.&lt;/p&gt; 
&lt;p&gt;We’ve stored the scripts for &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/blob/main/recipes/pcluster/stig/assets/EC2_RHEL8_SCAP_Assessment.sh"&gt;RHEL8&lt;/a&gt;, &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/blob/main/recipes/pcluster/stig/assets/EC2_AL2_SCAP_Assessment.sh"&gt;AL2&lt;/a&gt;, and &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/blob/main/recipes/pcluster/stig/assets/EC2_UBUNTU_2004_SCAP_Assessment.sh"&gt;Ubuntu 20.04&lt;/a&gt; in our GitHub repo. These scripts &lt;em&gt;do&lt;/em&gt; require Internet connectivity to run because they download a series of tools like the AWS CLI and OpenSCAP, and STIG benchmarks to the EC2 instance being evaluated.&lt;/p&gt; 
&lt;p&gt;You’ll need to &lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html"&gt;create an S3 bucket&lt;/a&gt;, and update the name of the bucket inside the script where it saves the results of the evaluations. The scripts use EC2 instance &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html"&gt;metadata&lt;/a&gt;&amp;nbsp;to dynamically name the output files in Amazon S3 after the instance, so they’re not overwritten as new instances are tested.&lt;/p&gt; 
&lt;p&gt;To run these scripts with minimal effort, you can run them as a user-data script upon launch and have the HTML results automatically sent to your S3 bucket. Inputting a user-data script follows the same logic as described under step 3 of the &lt;a href="https://github.com/aws-samples/aws-hpc-recipes/tree/main/recipes/pcluster/stig#ubuntu-2004-instances-with-or-without-internet-connectivity"&gt;Ubuntu 20.04 section&lt;/a&gt;. For RHEL8 and Ubuntu 20.04 operating systems, it takes approximately 10 minutes from instance launch to see the results uploaded to your Amazon S3 bucket. AL2 takes approximately 20-25 minutes.&lt;/p&gt; 
&lt;h2&gt;Using the resulting images&lt;/h2&gt; 
&lt;p&gt;The STIG’d AMIs can be found in the EC2 section of the Management Console and referenced in a ParallelCluster configuration file. You can create clusters using the ParallelCluster &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-configuring.html"&gt;CLI&lt;/a&gt; or the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/configure-create-pcui-v3.html"&gt;UI&lt;/a&gt;. For purposes of this post, we’ll show an example of placing the STIG’d AMI ID into the ParallelCluster configuration file for a cluster in GovCloud West.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Region: us-gov-west-1
Image:
  Os: rhel8
HeadNode:
  InstanceType: c5a.4xlarge
  Networking:
    SubnetId: {your-subnet-id}
  Ssh:
    KeyName: {your-keypair}
  Image:
    CustomAmi: {your-AMI-id}
SharedStorage:
  - MountDir: /fsx  
    Name: FSxExtData
    StorageType: FsxLustre
    FsxLustreSettings:
      StorageCapacity: 1200
      DeploymentType: PERSISTENT_1
      PerUnitStorageThroughput: 50
      DeletionPolicy: Delete
Scheduling:
  Scheduler: slurm
  SlurmSettings:
    QueueUpdateStrategy: DRAIN
  SlurmQueues:
  - Name: queue1
    ComputeResources:
    - Name: compute
      Instances:
      - InstanceType: hpc7a.48xlarge
      MinCount: 1
      MaxCount: 10
      Efa:
       Enabled: true
    Networking:
      SubnetIds:
      - {your-subnet-id}
      PlacementGroup:
        Enabled: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should edit the Items enclosed in the {} to include your identifiers.&lt;/p&gt; 
&lt;p&gt;Once you’ve created the file you can launch the cluster using the command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster --cluster-name &amp;lt;name&amp;gt; --cluster-configuration &amp;lt;file-name&amp;gt;.yml&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see validation warning messages because you are using a custom AMI, however these messages can be ignored and will not impact the creation of the cluster. You can track the cluster creation status through the &lt;a href="https://aws.amazon.com/cloudformation/"&gt;AWS CloudFormation&lt;/a&gt; console or by using the ParallelCluster CLI command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster list-clusters&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Verifying levels of compliance for compute resources is a requirement in some industries, and desired in others. Throughout this post, we’ve discussed several different cloud-native methods HPC customers with compliance requirements can choose from to accelerate their STIG process in AWS ParallelCluster depending on their Internet connectivity (or lack thereof) and operating system choice.&lt;/p&gt; 
&lt;p&gt;We recommend validating that your output images work with your application in a development environment prior to running in production.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Large scale training with NVIDIA NeMo Megatron on AWS ParallelCluster using P5 instances</title>
		<link>https://aws.amazon.com/blogs/hpc/large-scale-training-with-nemo-megatron-on-aws-parallelcluster-using-p5-instances/</link>
		
		<dc:creator><![CDATA[Aman Shanbhag]]></dc:creator>
		<pubDate>Wed, 29 May 2024 13:52:05 +0000</pubDate>
				<category><![CDATA[Amazon Machine Learning]]></category>
		<category><![CDATA[Artificial Intelligence]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[FEA]]></category>
		<category><![CDATA[GPU]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[Slurm]]></category>
		<guid isPermaLink="false">9f2f940ec2831d85025f4b4f1d755f1419ed79d4</guid>

					<description>Launching distributed GPT training? See how AWS ParallelCluster sets up a fast shared filesystem, SSH keys, host files, and more between nodes. Our guide has the details for creating a Slurm-managed cluster to train NeMo Megatron at scale.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;This post was contributed by Akshit Arora (NVIDIA), Peter Dykas (NVIDIA), Aman Shanbhag (AWS), Sean Smith (AWS), Pierre-Yves (AWS)&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Today we’ll take you on a step-by-step guide to help you to create a cluster of p5.48xlarge instances, using &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html"&gt;AWS ParallelCluster&lt;/a&gt; to launch GPT training through the NVIDIA NeMo Megatron framework, using Slurm. We’ve put detailed information about this &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher"&gt;in our GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We use ParallelCluster to execute NVIDIA NeMo Megatron across multiple nodes, because it takes care of mounting a fast shared filesystem between the nodes, synchronizing the SSH keys, creating a host file, and all the other overheads that make job submission possible.&lt;/p&gt; 
&lt;p&gt;&lt;a class="c-link" href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html" target="_blank" rel="noopener noreferrer" data-stringify-link="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html" data-sk="tooltip_parent"&gt;AWS ParallelCluster&lt;/a&gt; is a supported, open-source cluster management tool that makes it easy to create, scale, and manage clusters of accelerated&amp;nbsp;instances based on the open-source Slurm scheduler. It uses a YAML configuration file to stand up a head node,&amp;nbsp;accelerated&amp;nbsp;compute nodes, and a file system. Users can login and submit jobs to pre-provisioned nodes, or dynamically spin-up Amazon Elastic Compute Cloud (Amazon EC2) instances using On-Demand or Spot. ParallelCluster also offers a&amp;nbsp;&lt;a class="c-link" href="https://aws.amazon.com/blogs/hpc/large-scale-training-with-nemo-megatron-on-aws-parallelcluster-using-p5-instances/ParallelCluster%20also%20offers%20a%20web-based%20user%20interface%20that%20serves%20as%20a%20dashboard%20for%20creating,%20monitoring,%20and%20managing%20clusters." target="_blank" rel="noopener noreferrer" data-stringify-link="https://aws.amazon.com/blogs/hpc/large-scale-training-with-nemo-megatron-on-aws-parallelcluster-using-p5-instances/ParallelCluster%20also%20offers%20a%20web-based%20user%20interface%20that%20serves%20as%20a%20dashboard%20for%20creating,%20monitoring,%20and%20managing%20clusters." data-sk="tooltip_parent"&gt;web-based user interface&lt;/a&gt;&amp;nbsp;that serves as a dashboard for creating, monitoring, and managing clusters.&lt;/p&gt; 
&lt;h2&gt;Introducing NVIDIA NeMO Framework&lt;/h2&gt; 
&lt;p&gt;The NVIDIA NeMo Framework (or just &lt;em&gt;NeMo FW&lt;/em&gt; for the rest of this post) focuses on foundation model-training for generative AI models. Large language model (LLM) pre-training typically needs a lot of compute and model parallelism to efficiently scale training. NeMo FW’s model training scales to thousands of NVIDIA GPUs and can be used for training LLMs on trillions of tokens.&lt;/p&gt; 
&lt;p&gt;The &lt;em&gt;&lt;a href="https://github.com/NVIDIA/NeMo-Megatron-Launcher"&gt;NVIDIA NeMo Megatron Launcher&lt;/a&gt;&lt;/em&gt; &lt;em&gt;(NeMo Launcher)&lt;/em&gt; is a cloud-native tool for launching end-to-end NeMo FW training jobs. The Launcher is designed to be a simple and easy-to-use tool for launching NeMo FW training jobs on CSPs or on-prem clusters.&lt;/p&gt; 
&lt;p&gt;The launcher is typically used from a head node and only requires a minimal python installation. Launcher will generate (and launch) the submission scripts needed by the cluster scheduler and will also organize and store the job results. Launcher includes tested configuration files, but anything in a configuration file can be modified by the user. Launcher supports many functionalities, from cluster setup and configuration, data downloading, curating and model training setup, evaluation and deployment.&lt;/p&gt; 
&lt;h2&gt;Steps to create cluster and launch jobs&lt;/h2&gt; 
&lt;p&gt;This guide assumes that Amazon EC2 P5 instances are available in us-east-2 for you, but this may vary depending on your account / capacity reservation and the region you plan to use.&lt;/p&gt; 
&lt;h3&gt;Step 0: Install ParallelCluster CLI&lt;/h3&gt; 
&lt;p&gt;Installing CLI Instructions: &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-virtual-environment.html"&gt;https://docs.aws.amazon.com/parallelcluster/latest/ug/install-v3-virtual-environment.html&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If &lt;code&gt;virtualenv&lt;/code&gt; is not installed, install &lt;code&gt;virtualenv&lt;/code&gt; using &lt;code&gt;pip3&lt;/code&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;python3 -m pip install --upgrade pip
python3 -m pip install --user --upgrade virtualenv
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="2"&gt; 
 &lt;li&gt;Create a virtual environment, name it, and activate it.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;python3 -m virtualenv ~/apc-ve
source ~/apc-ve/bin/activate
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Install ParallelCluster into your virtual environment.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;(apc-ve)~$ python3 -m pip install "aws-parallelcluster" --upgrade --user&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start="4"&gt; 
 &lt;li&gt;Install Node Version Manager (&lt;code&gt;nvm&lt;/code&gt;) and the latest Long-Term Support (LTS) &lt;code&gt;Node.js&lt;/code&gt; version.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.38.0/install.sh | bash
$ chmod ug+x ~/.nvm/nvm.sh
$ source ~/.nvm/nvm.sh
$ nvm install --lts
$ node --version
$ export PATH=$PATH:~/.local/bin
$ pcluster version
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Step 1: Create a VPC and Security Groups&lt;/h3&gt; 
&lt;div id="attachment_3704" style="width: 931px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3704" loading="lazy" class="size-full wp-image-3704" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/22/IMG-2024-05-22-13.06.54.png" alt="Figure 1 – A VPC configuration in a new account with one public subnet and one private subnet in the target region. The P5 instance topology is defined to have 32 ENI cards of 100Gbps each." width="921" height="433"&gt;
 &lt;p id="caption-attachment-3704" class="wp-caption-text"&gt;Figure 1 – A VPC configuration in a new account with one public subnet and one private subnet in the target region. The P5 instance topology is defined to have 32 ENI cards of 100Gbps each.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;If you’re using a new AWS account, your VPC configuration will consist of one public subnet and a private subnet in the target region. We &lt;a href="https://github.com/aws/aws-ofi-nccl/blob/master/topology/p5.48xl-topo.xml"&gt;define the P5 instance topology&lt;/a&gt; to have a total of 32 Elastic Network Interfaces (ENI) cards of 100Gbps each. To handle 32 ENIs, compute instances need to be placed into a private subnet, otherwise your cluster will fail in creation because a public IP is not automatically assigned on instances with multiple NICs. You can find more information about deploying a VPC for ParallelCluster &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/1.architectures/1.vpc_network#vpc-cloudformation-stacks"&gt;in our GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unless you’re comfortable deploying a private subnet and setting the routes and security groups, we recommend that you deploy a custom VPC using the CloudFormation template called ML-VPC. This template is region-agnostic and enables you to create a VPC with the required network architecture to run your workloads.&lt;/p&gt; 
&lt;p&gt;You can follow the steps to deploy your new VPC:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Deploy this &lt;a href="https://console.aws.amazon.com/cloudformation/home?#/stacks/quickcreate?templateURL=https%3A%2F%2Fawsome-distributed-training.s3.amazonaws.com%2Ftemplates%2F1.vpc-multi-az.yaml&amp;amp;stackName=ML-VPC"&gt;CloudFormation template&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;You’ll see a list of parameters: 
  &lt;ul&gt; 
   &lt;li&gt;In &lt;strong&gt;Name of your VPC&lt;/strong&gt;, you can leave it as default LargeScaleVPC.&lt;/li&gt; 
   &lt;li&gt;For &lt;strong&gt;Availability zones&lt;/strong&gt; (AZ’s), select your desired AZ. This will deploy a public and private subnet in that AZ. &lt;em&gt;If you’re using a capacity reservation (CR), use the AZ specific to the CR.&lt;/em&gt;&lt;/li&gt; 
   &lt;li&gt;Keep the &lt;strong&gt;S3 Endpoint&lt;/strong&gt;, &lt;strong&gt;Public Subnet&lt;/strong&gt; and &lt;strong&gt;DynamoDB Endpoint&lt;/strong&gt; as true.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;ol start="3"&gt; 
 &lt;li&gt;Check the acknowledgement box in the &lt;strong&gt;Capabilities&lt;/strong&gt; section and create the stack.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;It’ll take a few minutes to deploy your network architecture. The stack outputs tab will contain IDs of your security groups and subnets. You’ll need to keep this information handy for the next step.&lt;/p&gt; 
&lt;h3&gt;Step 2: Build ParallelCluster custom AMI&lt;/h3&gt; 
&lt;p&gt;We used the following configuration (which we saved as image_build.yaml), for adding ParallelCluster dependencies on top of the &lt;a href="https://aws.amazon.com/releasenotes/aws-deep-learning-base-gpu-ami-ubuntu-22-04/"&gt;AWS Deep Learning Base GPU AMI&lt;/a&gt;. This Deep Learning AMI page also contains a command for retrieving the AMI ID (search for “Query AMI-ID with AWSCLI”). You can specify which AMI to use as base depending on your requirements, and then install ParallelCluster dependencies on top of it following &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/building-custom-ami-v3.html"&gt;the tutorial in our service documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Now ensure that the &lt;code&gt;SubnetId&lt;/code&gt;, &lt;code&gt;ParentImage&lt;/code&gt;, and &lt;code&gt;SecurityGroupIds&lt;/code&gt; are set to the values exported when deploying your network architecture in Step 1. Save the configuration to the file &lt;code&gt;image_build.yaml&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Build:
 InstanceType: p5.48xlarge
 SubnetId: subnet-xxxxxx
 ParentImage: ami-xxxxxx
 SecurityGroupIds:
 - sg-xxxxxx
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We specify security groups and subnet specific to private subnet (in the required AZ) created as a result of Step 1.&lt;/p&gt; 
&lt;p&gt;Now launch the ParallelCluster custom AMI creation job like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster build-image --image-id p5-pcluster-dlgpu-baseami --image-configuration image_build.yaml --region us-east-2&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This step takes about 30-45 minutes to complete.&lt;/p&gt; 
&lt;h3&gt;Step 3: Launch ParallelCluster&lt;/h3&gt; 
&lt;p&gt;Once the AMI is ready, it’s time to launch your cluster.&lt;/p&gt; 
&lt;p&gt;Here’s a reference configuration:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;Image:
 Os: ubuntu2204
HeadNode:
 InstanceType: m5.8xlarge
 LocalStorage:
   RootVolume:
     Size: 200
     DeleteOnTermination: true
 Networking:
   SubnetId: subnet-xxxxxx
 Ssh:
   KeyName: &amp;lt;key-name&amp;gt;
 Iam:
   S3Access:
     - BucketName: &amp;lt;s3-bucket-name&amp;gt;
 CustomActions:
   OnNodeConfigured:
     Script: https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/authentication-credentials/multi-runner/postinstall.sh
     Args:
       - https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/authentication-credentials/pyxis/postinstall.sh
       - -/fsx
       - https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/main/docker/postinstall.sh
Scheduling:
 Scheduler: slurm
 SlurmQueues:
 - Name: compute
   ComputeSettings:
     LocalStorage:
       RootVolume:
         Size: 200
   ComputeResources:
   - Name: compute
     InstanceType: p5.48xlarge
     MinCount: 2
     MaxCount: 2
     CapacityReservationTarget:
           CapacityReservationId: cr-xxxxxx
     Efa:
       Enabled: true
   Networking:
     PlacementGroup:
       Enabled: true
     SubnetIds:
       - subnet-xxxxxx
   CustomActions:
     OnNodeConfigured:
       Script: https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/authentication-credentials/multi-runner/postinstall.sh
       Args:
         - https://raw.githubusercontent.com/aws-samples/aws-parallelcluster-post-install-scripts/authentication-credentials/pyxis/postinstall.sh
         - -/fsx
   Image:
     CustomAmi: ami-xxxxxx
SharedStorage:
 - MountDir: /fsx
   Name: FSxDataMount
   StorageType: FsxLustre
   FsxLustreSettings:
     StorageCapacity: 1200
     DeploymentType: PERSISTENT_2
Monitoring:
 DetailedMonitoring: true
 Logs:
   CloudWatch:
     Enabled: true # good for debug
 Dashboards:
   CloudWatch:
     Enabled: false # provide basic dashboards
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, you should:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Update the &lt;code&gt;Region&lt;/code&gt; to your intended region&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;Networking:SubnetId&lt;/code&gt; (for the head node) to the &lt;strong&gt;public subnet&lt;/strong&gt; you created in Step 1.&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;Ssh:KeyName&lt;/code&gt;, to your specific key.&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;MinCount&lt;/code&gt; and &lt;code&gt;MaxCount&lt;/code&gt; to the desired numbers of instances you’d like in the cluster.&lt;/li&gt; 
 &lt;li&gt;Set the &lt;code&gt;CapacityReservationId&lt;/code&gt;, if any.&lt;/li&gt; 
 &lt;li&gt;Update the compute node &lt;code&gt;Networking:SubnetId&lt;/code&gt; to private subnet you created in Step 1.&lt;/li&gt; 
 &lt;li&gt;Optionally: 
  &lt;ol&gt; 
   &lt;li&gt;Set the &lt;code&gt;Iam:S3Access:BucketName&lt;/code&gt;, if you’d like the compute instances to be able to access an Amazon Simple Storage Service (Amazon S3) bucket.&lt;/li&gt; 
   &lt;li&gt;Update the &lt;code&gt;ImportPath&lt;/code&gt; within &lt;code&gt;SharedStorage&lt;/code&gt; to an Amazon S3 bucket URI, if you’d like to initialize your Lustre storage with data from an S3 bucket.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There’s more information about postinstall scripts and a library of especially useful ones &lt;a href="https://github.com/aws-samples/aws-parallelcluster-post-install-scripts/tree/main"&gt;in our GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can launch the cluster-creation process using a command like this once you’ve chosen a name for your cluster:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;pcluster create-cluster --cluster-name &amp;lt;cluster-name&amp;gt; --cluster-configuration pcluster_config.yaml --region us-east-2  --rollback-on-failure False&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Some additional commands you’ll need later:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To destroy a cluster: &lt;code&gt;pcluster delete-cluster -n &amp;lt;cluster-name&amp;gt; -r us-east-2&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;To SSH to the head node: &lt;code&gt;pcluster ssh -n &amp;lt;cluster-name&amp;gt; -r us-east-2 -i ssh_key.pem&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;You can use &lt;code&gt;sinfo&lt;/code&gt; on the head node to validate the cluster.&lt;/li&gt; 
 &lt;li&gt;You can get cluster status, too: &lt;code&gt;pcluster describe-cluster -n &amp;lt;cluster-name&amp;gt; -r us-east-2&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once your cluster is launched, you can validate some important elements by checking package versions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Elastic fabric adapter (EFA) – this is the custom-built, high-speed network interface into the Amazon EC2 fabric for running HPC and distributed machine-learning codes:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ fi_info --version
fi_info: 1.18.2amzn1.0
libfabric: 1.18.2amzn1.0
libfabric api: 1.18
 
$ cat /opt/amazon/efa_installed_packages
# EFA installer version: 1.24.1
# Debug packages installed: no
# Packages installed:
efa-config_1.15_all efa-profile_1.5_all libfabric-aws-bin_1.18.1_amd64 libfabric-aws-dev_1.18.1_amd64 libfabric1-aws_1.18.1_amd64 openmpi40-aws_4.1.5-1_amd64 ibacm_46.0-1_amd64 ibverbs-providers_46.0-1_amd64 ibverbs-utils_46.0-1_amd64 infiniband-diags_46.0-1_amd64 libibmad-dev_46.0-1_amd64 libibmad5_46.0-1_amd64 libibnetdisc-dev_46.0-1_amd64 libibnetdisc5_46.0-1_amd64 libibumad-dev_46.0-1_amd64 libibumad3_46.0-1_amd64 libibverbs-dev_46.0-1_amd64 libibverbs1_46.0-1_amd64 librdmacm-dev_46.0-1_amd64 librdmacm1_46.0-1_amd64 rdma-core_46.0-1_amd64 rdmacm-utils_46.0-1_amd64 efa_2.5.0-1.amzn1_amd64
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We have a &lt;a href="https://github.com/aws/aws-ofi-nccl/blob/master/doc/efa-env-var.md"&gt;document&lt;/a&gt; to help streamline EFA environment variables in your Docker image and scripts, along with some additional guidance.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Message-passing interface (MPI)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;$ mpirun --version
mpirun (Open MPI) 4.1.6&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;AWS-OFI-NCCL plugin&lt;/li&gt; 
 &lt;li style="list-style-type: none"&gt; 
  &lt;ol&gt; 
   &lt;li&gt;NCCL is the NVIDIA Collective Communications Library; it provides inter-GPU communication primitives. &lt;a href="https://github.com/aws/aws-ofi-nccl"&gt;AWS-OFI-NCCL&lt;/a&gt; is a plug-in that enables developers on AWS to use libfabric as a network provider while running NCCL based applications.&lt;/li&gt; 
   &lt;li&gt;The plugin is the easiest way to get the version is by running a NCCL (NVIDIA Collective Communications Library) test. You can build the tests using &lt;a href="https://github.com/NVIDIA/nccl-tests"&gt;these instructions&lt;/a&gt;, and look for logs reporting the version. You should see something like: &lt;code&gt;Initializing aws-ofi-nccl 1.7.4-aws&lt;/code&gt;&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;NCCL version – you should also find &lt;code&gt;NCCL version 2.18.5+cuda12.2&lt;/code&gt; in the logs of the NCCL test.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To resolve issues regarding Cluster Creation, please refer &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/troubleshooting-v3-cluster-deployment.html"&gt;to our troubleshooting documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 4: Cluster validation&lt;/h3&gt; 
&lt;p&gt;NeMo Launcher offers a &lt;a href="https://docs.nvidia.com/nemo-framework/user-guide/latest/cloudserviceproviders.html#cluster-validation"&gt;cluster validation script&lt;/a&gt; which runs NVIDIA &lt;a href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cloud-native/containers/dcgm"&gt;DCGM&lt;/a&gt; (Data Center GPU Manager) tests and NCCL tests. The DCGM is a suite of tools for managing and monitoring NVIDIA GPUs in cluster environments. All DCGM functionality is available via the dcgmi, which is the DCGM command-line utility.&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd /path/to/NeMoMegatronLauncher/csp_tools/aws &amp;amp;&amp;amp; bash cluster_validation.sh --nodes=2&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To resolve issues regarding Cluster Validation, please refer to the “Troubleshooting” section later.&lt;/p&gt; 
&lt;h3&gt;Step 5: Launch GPT training job&lt;/h3&gt; 
&lt;p&gt;Use &lt;code&gt;enroot&lt;/code&gt; to import the container to local:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;enroot import --output nemo_megatron_training.sqsh dockerd://nvcr.io/nvidia/nemo:dev&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, download the vocab and merges files:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json
wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here is the &lt;code&gt;config.yaml&lt;/code&gt; for a GPT 20B training job. Make a copy of this file and make changes as we describe next:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-yaml"&gt;defaults:
  - _self_
  - cluster: bcm  # Leave it as bcm even if using bcp. It will be ignored for bcp.
  - data_preparation: gpt3/download_gpt3_pile
  - training: gpt3/20b
  - conversion: gpt3/convert_gpt3
  - fine_tuning: null
  - prompt_learning: null
  - adapter_learning: null
  - ia3_learning: null
  - evaluation: gpt3/evaluate_all
  - export: gpt3/export_gpt3
  - override hydra/job_logging: stdout

hydra:
  run:
    dir: .
  output_subdir: null

debug: False

stages:
  - training
  # - conversion
  # - evaluation
  # - export

cluster_type: bcm  # bcm or bcp. If bcm, it must match - cluster above.
launcher_scripts_path: /home/ubuntu/NeMo-Megatron-Launcher/launcher_scripts  # Path to NeMo Megatron Launch scripts, should ends with /launcher_scripts
data_dir: /fsx/gpt3_dataset  # Location to store and read the data.
base_results_dir: /fsx/gpt3_dataset/results  # Location to store the results, checkpoints and logs.
container_mounts:
  - /home/ubuntu/NeMo-Megatron-Launcher/csp_tools/aws/:/nccl
container: /home/ubuntu/NeMo-Megatron-Launcher/nemo_megatron_training.sqsh

wandb_api_key_file: null  # File where the w&amp;amp;B api key is stored. Key must be on the first line.

env_vars:
  NCCL_TOPO_FILE: /nccl/topo.xml # Should be a path to an XML file describing the topology
  UCX_IB_PCI_RELAXED_ORDERING: null # Needed to improve Azure performance
  NCCL_IB_PCI_RELAXED_ORDERING: null # Needed to improve Azure performance
  NCCL_IB_TIMEOUT: null # InfiniBand Verbs Timeout. Set to 22 for Azure
  NCCL_DEBUG: INFO # Logging level for NCCL. Set to "INFO" for debug information
  NCCL_PROTO: simple # Protocol NCCL will use. Set to "simple" for AWS
  TRANSFORMERS_OFFLINE: 1
  NCCL_AVOID_RECORD_STREAMS: 1

# GPU Mapping
numa_mapping:
  enable: True  # Set to False to disable all mapping (performance will suffer).
  mode: unique_contiguous  # One of: all, single, single_unique, unique_interleaved or unique_contiguous.
  scope: node  # Either node or socket.
  cores: all_logical  # Either all_logical or single_logical.
  balanced: True  # Whether to assing an equal number of physical cores to each process.
  min_cores: 1  # Minimum number of physical cores per process.
  max_cores: 8  # Maximum number of physical cores per process. Can be null to use all available cores.

# Do not modify below, use the values above instead.
data_preparation_config: ${hydra:runtime.choices.data_preparation}
training_config: ${hydra:runtime.choices.training}
fine_tuning_config: ${hydra:runtime.choices.fine_tuning}
prompt_learning_config: ${hydra:runtime.choices.prompt_learning}
adapter_learning_config: ${hydra:runtime.choices.adapter_learning}
ia3_learning_config: ${hydra:runtime.choices.ia3_learning}
evaluation_config: ${hydra:runtime.choices.evaluation}
conversion_config: ${hydra:runtime.choices.conversion}
export_config: ${hydra:runtime.choices.export}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In this &lt;code&gt;config.yaml&lt;/code&gt; file:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Update the &lt;code&gt;launcher_scripts_path&lt;/code&gt; to the absolute path for NeMo Launcher’s launcher_scripts&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;data_dir&lt;/code&gt; to wherever the data is residing.&lt;/li&gt; 
 &lt;li&gt;Update container with path to &lt;code&gt;sqsh&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Update the &lt;code&gt;base_results_dir&lt;/code&gt; to point to the directory where you’d like to organize the results.&lt;/li&gt; 
 &lt;li&gt;Update &lt;code&gt;NCCL_TOPO_FILE&lt;/code&gt; to point to a xml &lt;a href="https://github.com/aws/aws-ofi-nccl/blob/master/topology/p5.48xl-topo.xml"&gt;specific to P5&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Optionally update &lt;code&gt;container_mounts&lt;/code&gt; to mount a specific directory from host into container.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can find some example configuration files &lt;a href="https://github.com/NVIDIA/NeMo-Framework-Launcher/blob/main/launcher_scripts/conf/config.yaml"&gt;in our GitHub repo&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To launch the job:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;cd /path/to/NeMoMegatronLauncher/launcher_scripts &amp;amp;&amp;amp; python main.py &amp;amp;&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once you launch this job, you can look at the &lt;code&gt;.log&lt;/code&gt; file (of format &lt;code&gt;log-nemo-megatron-&amp;lt;model_name&amp;gt;_&amp;lt;date&amp;gt;.log&lt;/code&gt;) to track the logs of the training job. Additionally, you can use a &lt;code&gt;.err&lt;/code&gt; file (of format &lt;code&gt;log-nemo-megatron-&amp;lt;model_name&amp;gt;_&amp;lt;date&amp;gt;.err&lt;/code&gt;) to track the errors and warnings, if any, of your training job. If you set up TensorBoard, you can also check the events file (of format &lt;code&gt;events.out.tfevents.&amp;lt;compute_details&amp;gt;&lt;/code&gt;) to look over the loss curves, learning rates and other parameters that NeMo tracks. For more information on this, refer to &lt;a href="https://www.tensorflow.org/tensorboard/get_started"&gt;the TensorBoard documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note: The files mentioned above are located in the directory specified by you in the &lt;code&gt;base_results_dir&lt;/code&gt; field in the &lt;code&gt;config.yaml&lt;/code&gt; file above.&lt;/p&gt; 
&lt;p&gt;This is what an example &lt;code&gt;.log&lt;/code&gt; file looks like. Note, this is only a part of the entire log file (“…” below entails omitted parts of the output), until step 3 of the training job (the actual logs contain logs until step 60000000):&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;    ************** Experiment configuration ***********
…
[NeMo I 2024-01-18 00:20:43 exp_manager:394] Experiments will be logged at /shared/backup120820231021/gpt_results/gpt3_126m_8_fp8_01172024_1619/results
[NeMo I 2024-01-18 00:20:43 exp_manager:835] TensorboardLogger has been set up
…
[NeMo I 2024-01-18 00:21:13 lr_scheduler:910] Scheduler "&amp;lt;nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fe75b3387f0&amp;gt;" 
    will be used during training (effective maximum steps = 60000000) - 
    Parameters : 
…
Sanity Checking DataLoader 0:   0%|                | 0/2 [00:00&amp;lt;?, ?it/s]
Sanity Checking DataLoader 0: 100%|████████| 2/2 [00:07&amp;lt;00:00,  3.92s/it]
…                                                      
Epoch 0: :   0%| | 1/60000000 [00:35&amp;lt;583621:51:56, v_num=, reduced_train_
Epoch 0: :   0%| | 2/60000000 [00:36&amp;lt;303128:00:21, v_num=, reduced_train_
Epoch 0: :   0%| | 3/60000000 [00:36&amp;lt;202786:11:11, v_num=, reduced_train_
…
&lt;/code&gt;&lt;/pre&gt; 
&lt;div id="attachment_3705" style="width: 949px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3705" loading="lazy" class="size-full wp-image-3705" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/22/IMG-2024-05-22-13.25.45.png" alt="Figure 2 – Sample output graphs from our run using TensorBoard." width="939" height="301"&gt;
 &lt;p id="caption-attachment-3705" class="wp-caption-text"&gt;Figure 2 – Sample output graphs from our run using TensorBoard.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Cluster creation failed&lt;/h3&gt; 
&lt;p&gt;Bringing up a cluster can fail for many reasons. The easiest way to debug is to create a cluster with &lt;code&gt;--rollback-on-failure False&lt;/code&gt;. Then you can see information in the AWS CloudFormation console detailing why the cluster creation failed. Even more detailed information will be in the logs on the head node which you can find in: &lt;code&gt;/var/log/cfn-init.log /var/log/cloud-init.log /var/log/cloud-init-output.log&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;The most common reason for cluster failure is that you may not have access to the target EC2 capacity. You’ll see this in the &lt;code&gt;/var/log/parallelcluster/clustermgtd&lt;/code&gt; log on the head node, or in CloudFormation.&lt;/p&gt; 
&lt;h3&gt;Cluster Validation Issues&lt;/h3&gt; 
&lt;h3&gt;1 – DCGMI output&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;Error: Unable to complete diagnostic for group 2147483647. Return: (-21) Host engine connection invalid/disconnected.
srun: error: compute-st-compute-1: task 0: Exited with exit code 235
Error: Unable to complete diagnostic for group 2147483647. Return: (-21) Host engine connection invalid/disconnected.
srun: error: compute-st-compute-2: task 1: Exited with exit code 235
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Resolution&lt;/strong&gt;: The DCGM container may not be accessible from &lt;a href="https://docs.nvidia.com/ngc/index.html"&gt;NGC&lt;/a&gt;. Try converting the DCGM container to a local &lt;code&gt;.sqsh&lt;/code&gt; file using &lt;code&gt;enroot&lt;/code&gt; and pointing the validation script (&lt;code&gt;csp_tools/aws/dcgmi_diag.sh&lt;/code&gt;) to this local file, like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;enroot import --output dcgm.sqsh 'docker://$oauthtoken@nvcr.io#nvidia/cloud-native/dcgm:2.3.5-1-ub
i8' 
srun --container-image=dcgm.sqsh bash -c "dcgmi diag -r 3"
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2 – PMIX Error in NCCL Logs&lt;/h3&gt; 
&lt;pre&gt;&lt;code class="lang-bash"&gt;[compute-st-compute-2:201457] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Resolution&lt;/strong&gt;: This is a non-fatal error. Try adding export &lt;code&gt;PMIX_MCA_gds=^ds12&lt;/code&gt; to the &lt;code&gt;csp_tools/aws/nccl.sh&lt;/code&gt; script.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;In this blog post, we’ve shown you how to leverage the AWS ParallelCluster and the NVIDIA NeMo Megatron Framework to enable large-scale Large Language Model (LLM) training on AWS P5 instances. Together, AWS ParallelCluster and the NVIDIA NeMo Megatron Framework can empower researchers and developers to train LLMs on trillions of tokens, scaling to thousands of GPUs, which means accelerating time-to-market for cutting-edge natural language processing (NLP) applications.&lt;/p&gt; 
&lt;p&gt;To learn more about training GPT3 NeMo Megatron on Slurm, refer to &lt;a href="https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher"&gt;AWS Sample&lt;/a&gt;s. To learn more about ParallelCluster and Nemo Megatron, check out the &lt;a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html"&gt;ParallelCluster User Guide&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA/NeMo-Megatron-Launcher"&gt;NeMo Megatron Launcher&lt;/a&gt;, and &lt;a href="https://www.mlworkshops.com/01-getting-started/05-pcui-connect.html"&gt;Parallel Cluster UI&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Building an AI simulation assistant with agentic workflows</title>
		<link>https://aws.amazon.com/blogs/hpc/building-an-ai-simulation-assistant-with-agentic-workflows/</link>
		
		<dc:creator><![CDATA[Sam Bydlon]]></dc:creator>
		<pubDate>Tue, 28 May 2024 14:57:10 +0000</pubDate>
				<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[simulations]]></category>
		<guid isPermaLink="false">a1c68c7ee69e75af5f1ce9d202a580c62bf691db</guid>

					<description>Simulations provide critical insights but running them takes specialized people, which can slow everyone down. We show how a Simulation Assistant can use LLMs and agents to start these workflows via chat so you can get results sooner.</description>
										<content:encoded>&lt;p&gt;&lt;img loading="lazy" class="alignright size-full wp-image-3673" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/20/building-an-AI-simulation-assistant.png" alt="" width="380" height="212"&gt;Simulations have become indispensable tools which enable organizations to predict outcomes, evaluate risks, and make informed decisions. Simulations provide valuable insights that drive strategic decision-making – running the gamut from supply chain optimization to exploration of design alternatives for products and processes.&lt;/p&gt; 
&lt;p&gt;But running and analyzing simulations can be a time-consuming task because it requires specialized teams of data scientists, analysts, and subject matter experts. In the manufacturing sector, these experts are in high demand to model and optimize complex production processes. This regularly leads to backlogs and delays in obtaining critical insights. In the healthcare industry, specialized teams of epidemiologists and statisticians run the simulations for infectious disease modeling that public health officials need to make decisions. The limited bandwidth of these specialists creates bottlenecks and inefficiencies that impact the ability to rapidly respond to emerging health crises in a data-driven manner.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll examine an generative AI-based “Simulation Assistant” demo application built using &lt;a href="https://python.langchain.com/v0.1/docs/modules/agents/"&gt;LangChain Agents&lt;/a&gt; and &lt;a href="https://www.anthropic.com/"&gt;Anthropic’s&lt;/a&gt; recently released Claude V3 large language model (LLM) on Amazon Bedrock.&lt;/p&gt; 
&lt;p&gt;By leveraging the latest advancements in LLMs and AWS, we’ll show you how to streamline and democratize your simulation workflows using a scalable and serverless architecture for an application with a chatbot-style interface. This will allow users to launch and interact with simulations using natural language prompts.&lt;/p&gt; 
&lt;h2&gt;How does this help experts?&lt;/h2&gt; 
&lt;p&gt;The Simulation Assistant demo offers a blueprint for providing significant value to organizations in two key ways. It:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Democratizes simulation-driven problem solving: &lt;/strong&gt;While regulated industries may still require certified personnel for final sign-off, this solution demonstrates a way to democratize simulation use beyond specialist teams. By enabling knowledgeable personnel across functions, such as analysts, managers, and decision-makers, to launch and analyze simulations under the guidance of experts, organizations can increase the utilization of simulation capabilities and free up the bandwidth of their simulation experts.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhances efficiency for simulation experts: &lt;/strong&gt;Allowing a wider user-base to run routine simulations lets experts focus on high value tasks like performance tuning or building new simulations. Streamlined, automated workflows accessible through a single chatbot interface improves productivity, standardizes processes, enables knowledge sharing, and increases result reliability.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Whether you’re a business analyst, product manager, researcher, or a simulation expert, this demonstration offers an intuitive and efficient way to harness the power of simulations by leveraging the capabilities of generative AI through Amazon Bedrock and the scalability of AWS – driving innovation and operational excellence across diverse industries.&lt;/p&gt; 
&lt;h2&gt;Solution overview&lt;/h2&gt; 
&lt;div id="attachment_3742" style="width: 1511px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3742" loading="lazy" class="size-full wp-image-3742" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/29/Sim_Assist_Demo_May24update-AWS-Architecture.drawio.png" alt="Figure 1 – Architectural diagram for Simulation Assistant application. A containerized Streamlit web app is deployed via a load-balanced AWS Fargate service. The web app instantiates an LLM-based agent with access to seven tools. The retriever tool provides RAG capabilities using Amazon Kendra. Others tools enable the agent to perform a custom mathematical transform, invoke a simple inflation simulation executed using AWS Lambda, invoke a set of containerized investment portfolio simulations using AWS Batch that store results in an Amazon DynamoDB table, and analyze a batch of simulation results by generating plots." width="1501" height="1163"&gt;
 &lt;p id="caption-attachment-3742" class="wp-caption-text"&gt;Figure 1 – Architectural diagram for Simulation Assistant application. A containerized Streamlit web app is deployed via a load-balanced AWS Fargate service. The web app instantiates an LLM-based agent with access to seven tools. The retriever tool provides RAG capabilities using Amazon Kendra. Others tools enable the agent to perform a custom mathematical transform, invoke a simple inflation simulation executed using AWS Lambda, invoke a set of containerized investment portfolio simulations using AWS Batch that store results in an Amazon DynamoDB table, and analyze a batch of simulation results by generating plots.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Figure 1 depicts the architecture of the Simulation Assistant application. A web application, built using &lt;a href="https://streamlit.io/"&gt;Streamlit&lt;/a&gt;, serves as the user interface. Streamlit is an open-source Python library that allows you to create interactive web applications for machine learning and data science use cases. We’ve containerized this app using Docker and stored it in an &lt;a href="https://aws.amazon.com/ecr/"&gt;Amazon Elastic Container Registry (ECR)&lt;/a&gt; repository.&lt;/p&gt; 
&lt;p&gt;The containerized web application is deployed as a load-balanced AWS Fargate Service within an &lt;a href="https://aws.amazon.com/ecs/"&gt;Amazon Elastic Container Service (ECS)&lt;/a&gt; cluster. &lt;a href="https://aws.amazon.com/fargate/"&gt;AWS Fargate&lt;/a&gt; is a serverless compute engine that allows you to run containers without managing servers or clusters. By using Fargate, the Simulation Assistant application can scale its compute resources up or down automatically based on the incoming traffic, ensuring optimal performance and cost-efficiency.&lt;/p&gt; 
&lt;p&gt;The web application is fronted by an &lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html"&gt;Application Load Balancer (ALB)&lt;/a&gt;. The ALB distributes incoming traffic across multiple targets, like Fargate tasks, in a balanced manner. This load balancing mechanism ensures that user requests are efficiently handled, even during periods of high traffic, by dynamically routing requests to available container instances.&lt;/p&gt; 
&lt;h2&gt;Life cycle of a request&lt;/h2&gt; 
&lt;p&gt;When a user accesses the Simulation Assistant application, their request is received by the ALB, which then forwards the request to one of the healthy Fargate tasks running the Streamlit web application. This serverless deployment approach, combined with the load-balancing capabilities of the ALB, provides a highly available and scalable architecture for the Simulation Assistant, allowing it to handle varying levels of user traffic without the need for manually provisioning and managing servers.&lt;/p&gt; 
&lt;p&gt;The Streamlit web application acts as the central hub, orchestrating the interaction between different AWS services to enable seamless simulation capabilities for users. Within the Streamlit app, we’ve used &lt;a href="https://aws.amazon.com/bedrock/"&gt;Amazon Bedrock&lt;/a&gt; to process user queries by leveraging state-of-the-art language models. Bedrock is a fully-managed service that makes these art foundation models from both Amazon and leading AI startups available via a unified API, while abstracting away complex model management.&lt;/p&gt; 
&lt;p&gt;For simple simulations, like price inflation scenarios, the Streamlit app integrates with &lt;a href="https://aws.amazon.com/lambda/"&gt;AWS Lambda&lt;/a&gt; functions. These serverless functions can encapsulate lightweight simulation logic, allowing for efficient execution and scalability without the need for provisioning and managing dedicated servers.&lt;/p&gt; 
&lt;p&gt;Additionally, we’re also leveraging &lt;a href="https://aws.amazon.com/kendra/"&gt;Amazon Kendra&lt;/a&gt;, an intelligent search service, to enable retrieval augmented generation (RAG). Amazon Kendra indexes and searches through documents stored in an &lt;a href="https://aws.amazon.com/s3/"&gt;Amazon S3&lt;/a&gt; bucket, acting as a source repository. This integration empowers the application to provide relevant information from existing documents, enhancing the simulation capabilities and enabling more informed decision-making.&lt;/p&gt; 
&lt;p&gt;For more computationally intensive simulations, like running sets of investment portfolio simulations in a Monte Carlo-style manner, the Simulation Assistant uses &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;. AWS Batch is a fully managed batch processing service that efficiently runs batch computing workloads across AWS resources. The Simulation Assistant submits jobs to AWS Batch, which then dynamically provisions the compute resources needed to run them in parallel, enabling faster execution times and scalability.&lt;/p&gt; 
&lt;p&gt;Once the simulations are complete, the results are stored in an &lt;a href="https://aws.amazon.com/dynamodb/"&gt;Amazon DynamoDB&lt;/a&gt; database, a fully managed NoSQL database service. DynamoDB provides fast and predictable performance with seamless scalability, making it well-suited for storing and retrieving simulation data efficiently. Furthermore, the application integrates with &lt;a href="https://aws.amazon.com/eventbridge/"&gt;Amazon EventBridge&lt;/a&gt;, a serverless event bus service. When a simulation batch is finished, EventBridge triggers a notification, which is sent to the user via email using &lt;a href="https://aws.amazon.com/sns/"&gt;Amazon Simple Notification Service (SNS)&lt;/a&gt;. This notification system keeps users informed about the completion of their simulation requests, allowing them to promptly access and analyze the results.&lt;/p&gt; 
&lt;h2&gt;LLM-based “agents” with “tools”&lt;/h2&gt; 
&lt;p&gt;The Streamlit web application houses the logic of the key technological concept underlying the Simulation Assistant application, the enablement of what is called “&lt;em&gt;agentic behavior&lt;/em&gt;” of an LLM. The precise definition of an LLM agent is elusive, because the field is relatively new and rapidly evolving. But the general idea is to augment the capabilities of LLMs by enabling the models to break down tasks into individual steps, make plans, and take actions including the use of tools to solve specific tasks, and even work together as a team of multiple agents that can collaborate and influence each other.&lt;/p&gt; 
&lt;p&gt;One design pattern for enabling agentic behavior of is called “&lt;a href="https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/?ref=dl-staging-website.ghost.io"&gt;Tool Use&lt;/a&gt;”, in which an LLM is taught (through prompt engineering or fine-tuning) how to trigger pieces of additional software, wrapped in a standardized form like a function call. These additional pieces of software are called “tools”. The Simulation Assistant employs tools to augment the behavior of an underlying foundation model. In our demo, the underlying foundation model is &lt;a href="https://www.anthropic.com/news/claude-3-family"&gt;Claude V3 Sonnet&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Tools help LLMs solve problems that are not reliably solved by direct generation using the underlying transformer network. LLMs are demonstrating incredible ability at solving problems and performing mathematical reasoning – try asking Claude V3 Sonnet to “simulate the price of milk over the next 20 years with a dynamic inflation rate where the mean is 4% and standard deviation 2%”. But their ability to simulate more complex systems like financial markets or the spread of wildfires is still (for now) best left to trusted simulation codes. By introducing tools that can execute those trusted codes and instructing the LLM on how and when tools should be used, LLMs can gain profound new abilities.&lt;/p&gt; 
&lt;p&gt;There are many possibilities for what tools can &lt;em&gt;be&lt;/em&gt; and &lt;em&gt;do&lt;/em&gt;, and the application of tools and agentic behavior in general is a concept that will have wide ranging uses cases far beyond simulation. Tools can help LLMs to perform web searches, query a database, or schedule a meeting using your calendar system, just to name a few options.&lt;/p&gt; 
&lt;h2&gt;How we applied agents and tools&lt;/h2&gt; 
&lt;p&gt;The Simulation Assistant instantiates a LangChain agent, based on Claude V3 Sonnet. &lt;a href="https://aws.amazon.com/what-is/langchain/"&gt;LangChain&lt;/a&gt; is an open-source framework for building applications with LLMs. With LangChain &lt;a href="https://python.langchain.com/v0.1/docs/modules/agents/"&gt;Agents&lt;/a&gt; and &lt;a href="https://python.langchain.com/v0.1/docs/modules/tools/"&gt;Tools&lt;/a&gt;, developers can create powerful generative AI-based applications that leverage LLM agentic behavior and integrate with existing systems and workflows.&lt;/p&gt; 
&lt;p&gt;There are &lt;a href="https://python.langchain.com/docs/modules/agents/agent_types/"&gt;several kinds of agents&lt;/a&gt; that can be constructed with LangChain, but not all agent types support tools with multiple inputs. Since simulations often require users to specify an array of input parameters to define the system mechanics to be simulated, we’ll want to choose an agent type that supports multi-input tools and can be used in concert with Amazon Bedrock. The Simulation Assistant demo uses a &lt;a href="https://python.langchain.com/docs/modules/agents/agent_types/structured_chat/"&gt;&lt;em&gt;structured chat agent&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;/em&gt;which satisfies both of these requirements.&lt;/p&gt; 
&lt;p&gt;Building the Simulation Assistant involved addressing several key technical challenges:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Designing and implementing an agent-tool architecture:&lt;/strong&gt; To create an effective agent-tool system, careful design of the tool interfaces and integration with the LangChain agent framework was required. We handled multi-input tools by defining structured input schemas. We enabled communication between the LLM and the tools using LangChain’s &lt;em&gt;tool abstraction layer&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Prompt engineering for agentic behavior:&lt;/strong&gt; Crafting prompts that guide the LLM to exhibit desired agentic behavior was an iterative process. The approach involved exploring the capabilities and limitations of Claude V3 Sonnet, designing prompts that promoted appropriate tool selection and usage, and refining the prompts based on performance in test scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Scaling and managing simulation workloads:&lt;/strong&gt; To handle computationally intensive simulations, we designed a scalable architecture using AWS Batch for running simulation jobs. This allowed us to efficiently manage compute resources and reliably execute simulations with varying workloads.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Interpreting and visualizing simulation results:&lt;/strong&gt; To help users interpret and visualize simulation outputs, we developed tools that process and summarize simulation data, and integrated visualizations within Simulation Assistant.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We gave the Simulation Assistant agent access to seven tools, including ones designed to perform a custom mathematical transform defined within the Streamlit application, perform simple inflation simulations housed in an AWS Lambda function, launch Monte Carlo-style simulation ensembles via AWS Batch to mimic an investment portfolio and visualize results, and to perform RAG over an internal database of documents.&lt;/p&gt; 
&lt;p&gt;These are simple examples, showing how tools can be built to handle some common elements of simulation workflows. But tools can do a lot more: they can also be designed to prepare configuration files, trigger pre- or post-processing jobs on related data – or even call other specialized LLMs that are able to interpret and summarize the results of simulations.&lt;/p&gt; 
&lt;h2&gt;A sample workflow&lt;/h2&gt; 
&lt;p&gt;When a user enters a natural language query into Simulation Assistant like, “&lt;em&gt;run a set of 100 investment simulations starting at $10,000, with cash flow of $250, for 50 steps&lt;/em&gt;,” we pass the query to Amazon Bedrock inside a prompt &lt;em&gt;specifically engineered&lt;/em&gt; to work well in an agent-tool setting. You can find examples of polished prompts you can use at the &lt;a href="https://smith.langchain.com/hub"&gt;LangChain Hub&lt;/a&gt; – including prompts that &lt;a href="https://smith.langchain.com/hub/hwchase17/structured-chat-agent?organizationId=6e7cb68e-d5eb-56c1-8a8a-5a32467e2996"&gt;work well with structured chat agents&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Behind the scenes, the agentic LLM breaks down the request into steps, decides whether each step can be completed with “bare hands” (i.e. without tools), or whether one of its tools can be used to complete the step. For our example request, the agentic LLM determines that it needs to use a tool that allows it to run batches of investment portfolio simulations, performs the entity extraction of the key parameters like cash flow, and uses these parameters to trigger an AWS Batch job to run the simulations. It does this completely independently.&lt;/p&gt; 
&lt;p&gt;The agent then responds to the user telling them they’ll receive an email when the simulations are complete and provides some helpful information that can be used to analyze the simulation results.&lt;/p&gt; 
&lt;p&gt;Figure 2 is a graphical depiction of this process.&lt;/p&gt; 
&lt;div id="attachment_3671" style="width: 1198px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3671" loading="lazy" class="size-full wp-image-3671" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/20/IMG-2024-05-20-09.11.00.png" alt="Figure 2 – Workflow of an LLM-based agent with tools for batch simulation execution. User requests are formatted into prompts by the application and sent to the LLM, which decomposes the request, selects and triggers the appropriate tool with extracted parameters (run_batch_simulations tool with simulation inputs). Tool results are returned to the LLM for generating a final response displayed to the user, including information for accessing and analyzing simulation outputs." width="1188" height="627"&gt;
 &lt;p id="caption-attachment-3671" class="wp-caption-text"&gt;Figure 2 – Workflow of an LLM-based agent with tools for batch simulation execution. User requests are formatted into prompts by the application and sent to the LLM, which decomposes the request, selects and triggers the appropriate tool with extracted parameters (run_batch_simulations tool with simulation inputs). Tool results are returned to the LLM for generating a final response displayed to the user, including information for accessing and analyzing simulation outputs.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;For a deeper look into how we used these tools and to see the Simulation Assistant demo in action (including the batch simulation query described above) you can check out the video demo below. This will walk you through some potential interactions with the application, and shows the LLM-based agent interacting with various tools representing different aspects of a simulation workflow. You’ll see the agent list the available tools and provide instructions on how to use them when prompted.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure 3 â&#128;&#147; A video showing the Simulation Assistant demo in action. Click to play." width="500" height="281" src="https://www.youtube-nocookie.com/embed/chmq52YX84A?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 3 – A video showing the Simulation Assistant demo in action. Click to play.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;The core portion of the demo involves the user asking the agent to run a set of 100 investment portfolio simulations with specific parameters like starting amount, cash flow, and number of steps. The agent interprets this natural language query, extracts the necessary parameters, and triggers an AWS Batch job to execute the simulations in parallel. Once the simulations complete, the agent retrieves the results from a database and visualizes them using another tool.&lt;/p&gt; 
&lt;p&gt;Additionally, the video contrasts the agent’s capabilities with a standard LLM’s response to the same query, so you can see the enhanced abilities provided by the agent-tool architecture. You’ll also notice the agent breaking down a complex problem into steps and leveraging multiple tools in different orders to solve it, demonstrating the flexibility of the approach.&lt;/p&gt; 
&lt;h2&gt;Future Work&lt;/h2&gt; 
&lt;p&gt;While the Simulation Assistant demo achieves a lot, we want to extend the demo in the future to tackle some more technical challenges:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Integrating with existing simulation codebases:&lt;/strong&gt; A key goal of ours is to integrate existing simulation codebases as tools within the agent-tool architecture. This will require a deep understanding of the codebases and, in some cases, modifying them to fit the tool interface requirements. For example, to integrate &lt;a href="https://www.openfoam.com/"&gt;OpenFOAM&lt;/a&gt; (a popular open-source computational fluid dynamics software) as a tool, the approach would involve wrapping OpenFOAM’s solver and utility executables as Python functions, defining input and output schemas, and potentially modifying the codebase to enable programmatic execution and data exchange.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Ensuring simulation reproducibility and traceability:&lt;/strong&gt; We’d like to enable comprehensive reproducibility and traceability of the simulation runs by implementing logging mechanisms that track input parameters, intermediate steps, and provide detailed documentation for each simulation execution.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Establishing guardrails:&lt;/strong&gt; Guardrails and safeguards are crucial to ensure the secure and responsible use of the simulation framework. This may involve setting limits on compute resources, enforcing access controls, and implementing validation checks to prevent potential misuse or unintended consequences. Additionally, ethical considerations should be considered, like ensuring privacy and data protection, avoiding biases, and promoting transparency in the simulation processes.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;This post introduces an AWS-native Simulation Assistant demo that we hope will provide inspiration, and a blueprint, for organizations looking to leverage generative AI and other cutting-edge techniques like agentic LLM behavior to help their businesses.&lt;/p&gt; 
&lt;p&gt;The demo shows the potential of these technologies for revolutionizing simulation workflows across various industries. Using LangChain Agents, Amazon Bedrock, and the scalability of AWS services, the Simulation Assistant demo can offer a glimpse into a future where simulations might be more accessible and interactive. We hope this will let organizations unlock new insights and drive better decision-making.&lt;/p&gt; 
&lt;p&gt;The application of agentic LLM frameworks extend far beyond simulations, and the concept of tools can be applied to a countless number of domains or workflows. By enabling LLMs to interact with external systems, perform computations, and trigger actions, organizations can augment their existing processes in this way, fostering innovation and operational excellence.&lt;/p&gt; 
&lt;p&gt;Solutions like this can also pave the way for new paradigms in human-machine collaboration, amplifying human capabilities and accelerating the pace of discovery.&lt;/p&gt; 
&lt;p&gt;If your organization is interested in exploring how to implement these techniques in concert with your workflows, we encourage you to reach out to your AWS account team, or send an email to &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Announcing: Seqera Containers for the bioinformatics community</title>
		<link>https://aws.amazon.com/blogs/hpc/announcing-seqera-containers-for-the-bioinformatics-community/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Thu, 23 May 2024 15:35:06 +0000</pubDate>
				<category><![CDATA[Featured]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Life Sciences]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[Bioinformatics]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[Genomics]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<guid isPermaLink="false">771f9023418ab35b6435bfa6a8543c7119414297</guid>

					<description>Genomics community: rejoice! Seqera and AWS have teamed up to announce Seqera Containers, an open-source, no cost, reliable way to generate containers.</description>
										<content:encoded>&lt;div id="attachment_3815" style="width: 390px" class="wp-caption alignright"&gt;
 &lt;a href="https://youtu.be/SE6ZCvh1ThQ" target="_new" rel="noopener"&gt;&lt;img aria-describedby="caption-attachment-3815" loading="lazy" class="wp-image-3815 size-full" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/YT-Containers-just-got-easier.png" alt="Announcing: Seqera Containers for the bioinformatics community" width="380" height="213"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-3815" class="wp-caption-text"&gt;You can watch a demo led by Phil Ewels on our Tech Shorts channel on YouTube.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;em&gt;This post was contributed by Brendan Bouffler, Head of Developer Relations, HPC Engineering at AWS, Phil Ewels, Snr Product Manager for Open Source at Seqera, and Paolo Di Tommaso, the CTO &amp;amp; co-founder of Seqera.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;Life sciences is a rapidly-moving area of research, and you don’t need to look too far for amazing examples where the practitioners of this field have adopted new tools and techniques to solve ever harder problems. Within this domain, bioinformatics has emerged as a crucial discipline, bridging the gap between biology and computer science. Many of the features in AWS Batch have been driven by this community, leading to a close relationship between our teams and the people working at the front lines of research.&lt;/p&gt; 
&lt;p&gt;But while biological data has become more complex and massive, it has also become increasingly personal, and inevitably regulated. The world needed researchers across borders and geographies to share insights and techniques without necessarily moving the data, or sharing the same infrastructure. The conditions were set for another uptake of new technology.&lt;/p&gt; 
&lt;p&gt;Adopting containers for packaging applications turned out to be pivotal for this shift: it revolutionized the way bioinformatics workflows are &lt;a href="https://seqera.io/blog/nextflow-and-aws-batch-inside-the-integration-part-1-of-3/"&gt;developed&lt;/a&gt;, &lt;a href="https://aws.amazon.com/blogs/hpc/leveraging-seqera-platform-on-aws-batch-for-machine-learning-workflows-part-1-of-2/"&gt;deployed&lt;/a&gt;, &lt;em&gt;and &lt;/em&gt;&lt;a href="https://nf-co.re/"&gt;&lt;em&gt;shared&lt;/em&gt;&lt;/a&gt; – increasing the reproducibility of an analysis. But while they’ve unquestionably simplified the life of bioinformaticians, their creation and usage is not without some friction.&lt;/p&gt; 
&lt;p&gt;Today, in conjunction with our friends at &lt;a href="https://seqera.io/about/"&gt;Seqera&lt;/a&gt;, we’re announcing a project we’re supporting called &lt;strong&gt;Seqera Containers. &lt;/strong&gt;This is a freely-available resource for the entire bioinformatics community (in the cloud or not) which simplifies the container experience – allowing researchers to generate a container for any combination of Conda and PyPI packages at the click of a button. Best of all, Seqera Containers are publicly accessible to everyone, at no-cost.&lt;/p&gt; 
&lt;h2&gt;Building containers on-demand – powered by Wave and AWS&lt;/h2&gt; 
&lt;p&gt;Seqera Containers is not a traditional container registry: users are not expected to browse existing container images or push local images to a remote. Instead, Seqera Containers provides a simple form to choose a combination of packages from Conda or the Python Package Index (PyPI). Clicking “Get Container” returns a Docker or Singularity image URI which can be used immediately.&lt;/p&gt; 
&lt;div id="attachment_3716" style="width: 1328px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3716" loading="lazy" class="size-full wp-image-3716" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/23/IMG-2024-05-23-07.44.30-1.png" alt="Figure 1 - the community wave registry is easy to access, and even easier to use." width="1318" height="942"&gt;
 &lt;p id="caption-attachment-3716" class="wp-caption-text"&gt;Figure 1 – the community wave registry is easy to access, and even easier to use.&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Wave containerization service&lt;/h2&gt; 
&lt;p&gt;The beating heart of Seqera Containers is &lt;a href="https://seqera.io/wave/"&gt;Wave&lt;/a&gt; – an &lt;a href="https://github.com/seqeralabs/wave"&gt;open-source&lt;/a&gt; technology built by Seqera for next-generation container provisioning that aims to simplify the usage of containers. Instead of writing Dockerfile scripts to build images, a developer or end user can request a just-in-time container image tailored for the target execution platform using only package names and version numbers from popular software packaging tools &lt;a href="https://conda.io/"&gt;Conda&lt;/a&gt; (including &lt;a href="https://bioconda.github.io/"&gt;Bioconda&lt;/a&gt;&lt;u&gt;),&lt;/u&gt; and the &lt;a href="https://pypi.org/"&gt;Python Package Index&lt;/a&gt; and later, &lt;a href="https://packages.spack.io/"&gt;Spack&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Wave can also compile images on the fly to match your compute infrastructure – including x86 and Arm64-based processors (where upstream packages are available).&lt;/p&gt; 
&lt;p&gt;Wave supports both Docker and Singularity images – and thus any other container technologies that use Docker images, like Podman, Shifter, Sarus, or Charliecloud. Seqera Containers provides an OCI compliant registry for native Singularity image builds and even allows direct .sif image download as a flat file, without needing Singularity installed locally.&lt;/p&gt; 
&lt;p&gt;As part of the build process, Wave conducts a vulnerability scan using the &lt;a href="https://trivy.dev/"&gt;Trivy&lt;/a&gt; security scanner and is able to generate &lt;a href="https://en.wikipedia.org/wiki/Software_supply_chain"&gt;SBOM manifests&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Reproducible container URIs&lt;/h2&gt; 
&lt;p&gt;Building containers on demand is convenient, but for the sake of performance, reproducibility, and provenance, it’s important to use the exact same image for every run. To do this, Wave generates a checksum for the build and can push the built image to a traditional OCI registry. Subsequent requests with the same checksum will be returned directly from the registry.&lt;/p&gt; 
&lt;p&gt;So, when you request an image through the Seqera Containers web interface, most of the time the images will come from the cache – meaning virtually instant access and consistent reuse by you, or anyone in the community. That image cache doesn’t expire, so those images will still be there when you need to reproduce that analysis in a few years’ time.&lt;/p&gt; 
&lt;h2&gt;Built-in support for Nextflow&lt;/h2&gt; 
&lt;p&gt;We hope that this service and these containers will be of use to the entire bioinformatics community. However, the experience of using Seqera Containers will be particularly good for Nextflow users.&lt;/p&gt; 
&lt;p&gt;Using the &lt;a href="https://nextflow.io/docs/latest/wave.html"&gt;Nextflow wave plugin&lt;/a&gt;, pipeline developers can avoid specifying container URIs in their pipeline code entirely. Instead, just naming the software packages in the conda (or, later, spack) declarations and then setting wave.enabled = true and wave.freeze = true is all you need. This instructs Nextflow to request an image from Wave for these packages, store it in the Seqera Containers public registry, and then use this at run time.&lt;/p&gt; 
&lt;p&gt;Wave isn’t restricted to working with Nextflow, there is also a &lt;a href="https://github.com/seqeralabs/wave-cli"&gt;Wave CLI&lt;/a&gt; which anyone can use to generate containers as well as an &lt;a href="https://docs.seqera.io/wave/api"&gt;API&lt;/a&gt;, both of which have all the same functionality as the Nextflow plugin and Seqera Containers web interface.&lt;/p&gt; 
&lt;h2&gt;It’s available now&lt;/h2&gt; 
&lt;div id="attachment_3818" style="width: 390px" class="wp-caption alignright"&gt;
 &lt;a href="https://youtu.be/HjNwpJPMvWQ" target="new" rel="noopener"&gt;&lt;img aria-describedby="caption-attachment-3818" loading="lazy" class="size-full wp-image-3818" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/06/11/YT-on-demand-containers-for-bioinformatics.png" alt="Paolo Di Tommaso talks about Nextflow and the revolution it started." width="380" height="213"&gt;&lt;/a&gt;
 &lt;p id="caption-attachment-3818" class="wp-caption-text"&gt;Paolo Di Tommaso talks about Nextflow and the revolution it started.&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;The impact of container technology on the progress of life sciences research just can’t be overstated.&lt;/p&gt; 
&lt;p&gt;It addresses big challenges in bioinformatics, like reproducibility, scalability, and collaboration. It eases software management, and empowers researchers to focus on their core scientific endeavors. This has accelerated the pace of discoveries, fostered collaboration, and enhanced the overall quality and reproducibility of bioinformatics research.&lt;/p&gt; 
&lt;p&gt;We think today’s announcement pushes containers, and bioinformatics one step further, by making it dramatically easier to get your hands on containers in the right shape, size, and form-factor to meet the needs of your pipelines. Batch users will definitely enjoy using this. But, we’re pretty sure this will be a community resource everyone will love, and AWS is thrilled to be able to support the Seqera team to deliver this for the entire community.&lt;/p&gt; 
&lt;p&gt;If you still need convincing, try it out yourself now – head to &lt;a href="https://seqera.io/containers/"&gt;https://seqera.io/containers/&lt;/a&gt; and type in some of your favorite bioinformatics tool names before clicking “Build”. Pull your Docker image and get to work.&lt;/p&gt; 
&lt;p&gt;And if you have ideas for Seqera or AWS, don’t hesitate to reach out to us at &lt;a href="mailto:ask-hpc@amazon.com"&gt;ask-hpc@amazon.com&lt;/a&gt;.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Using machine learning to drive faster automotive design cycles</title>
		<link>https://aws.amazon.com/blogs/hpc/using-machine-learning-to-drive-faster-automotive-design-cycles/</link>
		
		<dc:creator><![CDATA[Steven Miller]]></dc:creator>
		<pubDate>Mon, 13 May 2024 14:40:10 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[AI]]></category>
		<category><![CDATA[Amazon FSx for Lustre]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[CFD]]></category>
		<category><![CDATA[Computational Fluid Dynamics]]></category>
		<category><![CDATA[EFA]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Machine Learning]]></category>
		<category><![CDATA[ML]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[simulations]]></category>
		<category><![CDATA[visualization]]></category>
		<guid isPermaLink="false">2c1df43a0e4299fe6554a7b7f672836e27c25a46</guid>

					<description>Aerospace and automotive companies are speeding up their product design using AI. In this post we'll discuss how they're using machine learning to shift design cycles from hours to seconds using surrogate models.</description>
										<content:encoded>&lt;p&gt;The automotive product engineering process involves months or years of iterative design reviews and refinement, with back-and-forth feedback between stakeholders regularly to adjust designs and evaluate the impact of design changes on engineering metrics like the coefficient of drag. Between each design iteration, engineers wait hours or days for simulations to complete, which means they can only execute a handful of design decisions each week.&lt;/p&gt; 
&lt;p&gt;In this post, we’ll show how automakers can reduce cycle times from hours to seconds by leveraging surrogate machine-learning (ML) models in place of HPC, physics-based simulations and create subtle design variations for non-parametric geometries.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;In short&lt;/strong&gt;: automotive engineers can use emerging ML methodologies to speed up the product engineering process.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;Typically, the automotive product design process involves conceptualization, computer-aided design (CAD) geometry creation, and engineering simulations for validating the designs to ensure aerodynamic efficiency and desired structural stability.&lt;/p&gt; 
&lt;p&gt;Recent advances in AI/ML and Generative AI technology including algorithms such as MeshGraphNets, U-Nets and Variational Autoencoders, have provided a path towards accelerating the design process through fast ML inferences which allow us to run fewer full-fidelity physics-based HPC simulations, which can take hours, while gathering insight into large design spaces.&lt;/p&gt; 
&lt;p&gt;In this work, we’ll focus on computational fluid dynamics (CFD) simulations for intelligent aerodynamic surface design, though there are often other design considerations like impact response, noise, vibration and harshness.&lt;/p&gt; 
&lt;p&gt;The goal of this work is &lt;em&gt;not&lt;/em&gt; to develop a comprehensive toolkit for aerodynamic design – we want to present a simple user experience to show how engineers can use scientific ML methods, together with AWS services to deliver business value. We’ll mainly use &lt;a href="https://aws.amazon.com/batch/"&gt;AWS Batch&lt;/a&gt;, &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;Nice DCV&lt;/a&gt;, &lt;a href="https://aws.amazon.com/pm/serv-s3/"&gt;Amazon S3&lt;/a&gt; and &lt;a href="https://aws.amazon.com/pm/sagemaker/"&gt;Amazon SageMaker&lt;/a&gt;. We’ll also explain how to build a web application that uses ML and generative design to enhance these development processes.&lt;/p&gt; 
&lt;div id="attachment_3604" style="width: 857px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3604" loading="lazy" class="size-full wp-image-3604" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.33.24.png" alt="Figure 1 – Overview of Workflow Structure incorporating Web Application UI overview, AI/ML Methodology and AWS Implementation" width="847" height="433"&gt;
 &lt;p id="caption-attachment-3604" class="wp-caption-text"&gt;Figure 1 – Overview of Workflow Structure incorporating Web Application UI overview, AI/ML Methodology and AWS Implementation&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Roadmap for this post&lt;/h2&gt; 
&lt;p&gt;Much like the workflow chart in Figure 1, we’ll divide this post into three parts.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;, we’ll start by explaining how to build a minimally viable web application including the end user experience and core components starting from an AWS architecture blueprint, highlighting some key components.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Next&lt;/strong&gt;, we’ll dive deep into the underlying ML models including the training methodology and inference deployment.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Finally&lt;/strong&gt;, we’ll wrap up with a brief discussion about how we imagine automakers using workflows like this in practice to augment and accelerate the product design lifecycles.&lt;/p&gt; 
&lt;h2&gt;Cloud implementation and AWS architecture&lt;/h2&gt; 
&lt;p&gt;Let’s look at how to quickly and securely implement a minimally-viable web application. Using that, we’ll generate ground truth meshes, run OpenFOAM simulations, and train the machine learning models using AWS HPC services.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;AWS architecture.&lt;/strong&gt; We don’t want to completely replace HPC simulations – these are necessary for verification and validation. We want to enable engineers to quickly build a web application inside their environment, access it securely, and store all their related artifacts. For this reason, we’ve chosen a minimalist architecture and we’ll leave it up to the automaker to decide how to integrate this into their existing environment.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Figure 2: AWS Architecture diagram for deployment on a g5 instance using DCV, with endpoints to retrieve data or optionally submit compute jobs on self-managed ML endpoints (AWS Batch) or Amazon SageMaker&lt;/em&gt;&lt;/p&gt; 
&lt;div id="attachment_3605" style="width: 665px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3605" loading="lazy" class="size-full wp-image-3605" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.34.05.png" alt="Figure 2: AWS Architecture diagram for deployment on a g5 instance using DCV, with endpoints to retrieve data or optionally submit compute jobs on self-managed ML endpoints (AWS Batch) or Amazon SageMaker" width="655" height="345"&gt;
 &lt;p id="caption-attachment-3605" class="wp-caption-text"&gt;Figure 2: AWS Architecture diagram for deployment on a g5 instance using DCV, with endpoints to retrieve data or optionally submit compute jobs on self-managed ML endpoints (AWS Batch) or Amazon SageMaker&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;We’re hosting the web application on a g5.12xlarge Amazon Elastic Compute Cloud (Amazon EC2) instance. The &lt;a href="https://aws.amazon.com/ec2/instance-types/g5/"&gt;G5&lt;/a&gt; has 4 x A10G NVIDIA GPUs to serve the machine learning models and we’re using the &lt;a href="https://aws.amazon.com/hpc/dcv/"&gt;NICE DCV&lt;/a&gt; streaming protocol to securely stream the desktop to the end.&lt;/p&gt; 
&lt;p&gt;We’re also keeping the EC2 instance secure inside of a VPC and we only allow TCP traffic to flow from the instance to a single end user.&lt;/p&gt; 
&lt;p&gt;We train drag prediction models using Amazon SageMaker, and we securely store the inputs and outputs (including model artifacts) in Amazon Simple Storage Service (Amazon S3).&lt;/p&gt; 
&lt;p&gt;We use AWS Batch to launch the CFD simulations to create the ground truth data.&lt;/p&gt; 
&lt;p&gt;To keep all communications secure, we use VPC endpoints for all key services.&lt;/p&gt; 
&lt;h2&gt;Application workflow and user experience&lt;/h2&gt; 
&lt;p&gt;Now we can build the minimally viable application to show this new workflow. The web application layer consists of five main modules: application, inference configuration, pretrained models, processed meshes, and utilities.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Application.&lt;/strong&gt; The core application, built on the open-source &lt;a href="https://streamlit.io/"&gt;Streamlit&lt;/a&gt; library, serves the user interface and all the orchestration to deliver the user experience. We used a 3D plotting library (&lt;a href="https://docs.pyvista.org/version/stable/"&gt;PyVista&lt;/a&gt;) to display 3D visualizations and enable user interaction with STL meshes (original and design targets) and ML and physics-based computational fluid dynamics results, including pressure cross-sections and 3D streamlines.&lt;/p&gt; 
&lt;p&gt;In this example, we’ll begin with a choice of a base car model (coupe or estate/station-wagon), and then provide the user with design modification options. In this case, some features of the car (like front bumper shape, windscreen angle, trunk depth, and cabin height) are provided, but we could easily extend this to other design features we’re interested in.&lt;/p&gt; 
&lt;p&gt;For each of these combinations, the design variations are produced through non-parametric “morphing” of the base meshes using &lt;em&gt;radial basis function (RBF) interpolation&lt;/em&gt;. If we wanted to explore parametric design, and the base CAD geometries were available, we could potentially incorporate them into the workflow. The user interface is shown in Figure 3.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Figure3 - Using machine learning to drive faster automotive design cycles." width="500" height="375" src="https://www.youtube-nocookie.com/embed/2Z06wjAPVkA?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Radial basis function (RBF) – &lt;/strong&gt;RBF interpolation lets us distort meshes at specific parts of the car, while keeping other features constant – including the chassis platform and tire radius. RBF relies on the concept of a deformation map between the original mesh and the deformed mesh and we get to control the extent of deformation using the slider in the user interface.&lt;/p&gt; 
&lt;p&gt;To enable engineers to show specifically how the mesh has been deformed, we’ve provided a side-by-side comparison of the original mesh and the new mesh in the UI (which you can see in Figure 4) before we pass the deformed mesh along to the ML model for inference.&lt;/p&gt; 
&lt;div id="attachment_3607" style="width: 831px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3607" loading="lazy" class="size-full wp-image-3607" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.34.57.png" alt="Figure 4 – Deformed geometry (left) with a wireframe overlay of original mesh highlighting change and original mesh (right)" width="821" height="348"&gt;
 &lt;p id="caption-attachment-3607" class="wp-caption-text"&gt;Figure 4 – Deformed geometry (left) with a wireframe overlay of original mesh highlighting change and original mesh (right)&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;Inference configuration.&lt;/strong&gt; We trained a hierarchical machine learning model for a CFD flow field and drag predictions – we’ll explain that more soon. At runtime, we hosted these on different GPUs which allowed us to parallelize the inference of individual sub-models. To get inferences from the ML model, we specify the key inputs and parameters using inference configurations. These inputs include the decimated surface meshes of the car and a geometry for volume calculations. Additional input parameters include the batch size for inference, number of workers and other neural network associated hyperparameters.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Processed mesh.&lt;/strong&gt; When a user requests an inference, the application generates a processed &lt;a href="https://www.hdfgroup.org/solutions/hdf5/"&gt;HDF5&lt;/a&gt; file which contains the new mesh and the corresponding flow fields. These HDF5 files are then exposed to the end user via an interactive 3D visualization. There are some extra components that support the application: a function to convert meshes to HDF5 files, another function to apply the RBF to morph meshes based on user inputs, and finally a function to run inferences and gather outputs.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deployment.&lt;/strong&gt; Since this application is meant for demonstration purposes only, we designed it to run on a single compute instance, and chose a g5.12xlarge that includes 4 x GPUs that’s able to run the entire application. To speed the application, we distribute individual ML sub-models across the GPUs. To keep it all secure, we isolate the instances from the public internet in a private VPC. And we used NICE DCV to access the instance for an easier remote desktop experience.&lt;/p&gt; 
&lt;p&gt;Geometric and ML models&lt;/p&gt; 
&lt;p&gt;To enable engineers to rapidly iterate, potentially even &lt;em&gt;during&lt;/em&gt; design conversations, our web application predicts the &lt;em&gt;coefficient of drag&lt;/em&gt; (Cd) and the flow fields on unseen geometries. We used a hierarchy of ML models for the flow fields and drag predictions.&lt;/p&gt; 
&lt;p&gt;In both cases, we trained our models using synthetic data that we generated by morphing the open source &lt;a href="https://www.epc.ed.tum.de/en/aer/research-groups/automotive/drivaer/"&gt;DrivAer&lt;/a&gt; data set, and then ran full-fidelity CFD simulations on all these geometries using AWS Batch with the open-source &lt;a href="https://github.com/aws-samples/twingraph"&gt;TwinGraph framework&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Synthetic data generation&lt;/strong&gt;. To create training data for this project, we used the RBF to strategically morph the underlying mesh of both the coupe and the SUV. This method has broad applicability and can be generically applied to many parametric and non-parametric STL meshes.&lt;/p&gt; 
&lt;p&gt;For each area of the car, we specified points in an overlayed 1000-point (10x10x10) cubic lattice, bounded by the dimensions of the car. This allowed us to create 400 variations (100 for each of the area of the car) that we used as the basis for ground-truth simulations. Figure 5 shows how we used the RBF to deform the mesh &lt;em&gt;and&lt;/em&gt; create the training data for the ML model. Each time frame in the video corresponds to a different deformed mesh.&lt;/p&gt; 
&lt;p&gt;&lt;iframe loading="lazy" title="Using machine learning to drive faster automotive design cycles - Figure5" width="500" height="281" src="https://www.youtube-nocookie.com/embed/aBcO03OBTm0?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen sandbox="allow-scripts allow-same-origin"&gt;&lt;/iframe&gt;&lt;/p&gt; 
&lt;p style="text-align: left"&gt;&lt;em&gt;Figure 5 – Synthetic mesh generation using morphing of individual features of a car shown in sequence&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;After we created 400 variations of the base meshes, we used OpenFOAM to create CFD simulations with steady-state RANS simulations using the k-ω steady-state turbulence model with a fixed inlet velocity of 20m/s.&lt;/p&gt; 
&lt;p&gt;We ran these simulations in parallel using AWS Batch on c5.24xlarge instances using MPI for parallelism – this took around 7 hours to run. We could have chosen larger mesh sizes, but we settled on a mesh size that allowed resolving fine geometry – in a reasonable compute time.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Coefficient of drag with MeshGraphNets (MGN) –&lt;/strong&gt; MGN is a deep-learning framework for learning mesh-based simulations which represents the mesh as a graph where information is propagated across the graph’s nodes and edges. MGN excels at efficiently capturing unstructured mesh topologies and it’s a great fit for predicting Cd values. To train the MGN model, we used the synthetic data that we generated using the RBF, and OpenFOAM. During training, we provided the underlying mesh and the Coefficient of Drag (Cd) metrics as inputs.&lt;/p&gt; 
&lt;p&gt;We then trained a model on 320 samples, with each mesh decimated while preserving topological features for computational efficiency, containing approximately 6,000 nodes. We used a p3.2xlarge instance for 500 epochs, with a total training time of 46 minutes.&lt;/p&gt; 
&lt;p&gt;The model achieved a MAPE (&lt;em&gt;mean absolute percentage error&lt;/em&gt;) of 2.5% in drag coefficient (Cd) when predicting on unseen data, with an average inference time of 0.028 seconds per sample. Figure 6 shows the predicted vs. actual drag coefficient (Cd) plot for train, validation, and unseen test data (car meshes).&lt;/p&gt; 
&lt;div id="attachment_3609" style="width: 718px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3609" loading="lazy" class="size-full wp-image-3609" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.36.06.png" alt="Figure 6: Predicted vs. actual drag coefficient (Cd) plot for train, validation, and unseen test data (i.e. car meshes)." width="708" height="426"&gt;
 &lt;p id="caption-attachment-3609" class="wp-caption-text"&gt;Figure 6: Predicted vs. actual drag coefficient (Cd) plot for train, validation, and unseen test data (i.e. car meshes).&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Although the model didn’t match the ground truth CFD simulations exactly, this level of agreement from the MGN surrogate model may prove acceptable during the initial design iterations leading up to a final, high-fidelity simulation. After that comes real-world wind tunnel testing, which has an inherent uncertainty in measurements.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Flow Fields using deep convolutional neural networks (CNNs).&lt;/strong&gt; To predict pressure distributions and velocity fields for full 3D flow, we used a CNN architecture called &lt;a href="https://github.com/wolny/pytorch-3dunet"&gt;U-Net&lt;/a&gt; which has shown promise in high-resolution volume segmentation and regression tasks. This architecture included encoding and decoding convolutional layers with skip connections. We trained individual U-Nets for each of the pressure and velocity primary variables.&lt;/p&gt; 
&lt;p&gt;We converted the 3D volume outputs from 400 x OpenFOAM simulations into the training (375) and validation (25) ground-truth datasets. We used a p5.48xlarge instance with 8 x H100 GPUs for approximately 28 hours. We saw a volume RSME (&lt;em&gt;root-mean-square-error&lt;/em&gt;) of around 3% for pressure, and 5% for combined velocity magnitudes, for unseen data not in the training or validation sets – although this is dependent on the degree of mesh warping away from the initial baseline mesh. Figure 7 shows the differences in velocity slices an unseen mesh.&lt;/p&gt; 
&lt;div id="attachment_3610" style="width: 680px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3610" loading="lazy" class="size-full wp-image-3610" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.36.52.png" alt="Figure 7: ML Predicted (Top) vs Ground Truth OpenFOAM (Bottom) X-Component Velocity" width="670" height="516"&gt;
 &lt;p id="caption-attachment-3610" class="wp-caption-text"&gt;Figure 7: ML Predicted (Top) vs Ground Truth OpenFOAM (Bottom) X-Component Velocity&lt;/p&gt;
&lt;/div&gt; 
&lt;h2&gt;Business value creation&lt;/h2&gt; 
&lt;p&gt;We’ve now shown how to rapidly build a minimally viable web interface that engineers can use to take advantage of these ML models to speed up the product engineering process. But now we need to couple this with a new business process so automakers can realize actual business value.&lt;/p&gt; 
&lt;div id="attachment_3611" style="width: 855px" class="wp-caption aligncenter"&gt;
 &lt;img aria-describedby="caption-attachment-3611" loading="lazy" class="size-full wp-image-3611" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/04/18/CleanShot-2024-04-18-at-13.37.12.png" alt="Figure 8: Overall product design iterations using machine learning surrogate models to accelerate the cycles" width="845" height="168"&gt;
 &lt;p id="caption-attachment-3611" class="wp-caption-text"&gt;Figure 8: Overall product design iterations using machine learning surrogate models to accelerate the cycles&lt;/p&gt;
&lt;/div&gt; 
&lt;p&gt;Using the speed of the ML surrogate models, it’s possible to explore hundreds of designs per week, and we can now make meetings more collaborative.&lt;/p&gt; 
&lt;p&gt;Instead of reviewing results from prior runs, engineers can solicit feedback from their peers real-time0, and get the results in 20-60 seconds, avoiding costly wait times. This will require engineers to think differently about product design reviews and meetings. Engineers will need to set expectations accordingly and be prepared to guide conversations. But this enables engineers to explore a greater design space than they otherwise could – especially given budget or time constraints. We’re confident this approach will lead to new innovations.&lt;/p&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;With the right combination of machine-learning and process change, automakers can use the architecture explained here to build tailored solutions to speed up the product engineering process, reduce the cost of compute, and explore a greater number of design options in a shorter period of time. If you or your team want to discuss the business case, a recommended implementation path with our Professional Services team, or technical solution blueprint in more detail, you can reach out to us at ask-hpc@amazon.com.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
		<item>
		<title>Announcing the High Performance Software Foundation (HPSF)</title>
		<link>https://aws.amazon.com/blogs/hpc/announcing-the-high-performance-software-foundation/</link>
		
		<dc:creator><![CDATA[Brendan Bouffler]]></dc:creator>
		<pubDate>Mon, 13 May 2024 05:55:13 +0000</pubDate>
				<category><![CDATA[Compute]]></category>
		<category><![CDATA[High Performance Computing]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[AWS Batch]]></category>
		<category><![CDATA[AWS ParallelCluster]]></category>
		<category><![CDATA[Elastic Fabric Adapter]]></category>
		<category><![CDATA[HPC]]></category>
		<category><![CDATA[Research Computing]]></category>
		<category><![CDATA[Scientific Computing]]></category>
		<category><![CDATA[Sustainability]]></category>
		<guid isPermaLink="false">e91b47faf0b755001ca4998fd17206fcbdaf12f6</guid>

					<description>We're excited to share how we’re involved in launching the High Performance Software Foundation to increase access to and adoption of HPC. By bringing together key players to collaborate, we can lower barriers and accelerate development of portable HPC software stacks.</description>
										<content:encoded>&lt;p&gt;&lt;em&gt;&lt;img loading="lazy" class="size-full wp-image-3644 alignright" src="https://d2908q01vomqb2.cloudfront.net/e6c3dd630428fd54834172b8fd2735fed9416da4/2024/05/09/AdobeStock_144582125-resized.png" alt="Announcing the High Performance Software Foundation" width="380" height="253"&gt;&lt;/em&gt;In high performance computing (HPC), where speed and efficiency are paramount, open-source software provides the load-bearing, structural support. Our community has long recognized the immense potential of collaborative development, sharing of resources, and the power of community-driven innovation.&lt;/p&gt; 
&lt;p&gt;So we’re excited to announce that we’re a Premier founding member of the new &lt;strong&gt;High Performance Software Foundation&lt;/strong&gt; (HPSF). The HPSF was launched today in Hamburg at &lt;a href="https://www.isc-hpc.com/"&gt;ISC’24&lt;/a&gt;, by the Linux Foundation, the nonprofit organization that enables mass innovation through open-source. The HPSF has strong support across the HPC landscape.&lt;/p&gt; 
&lt;p&gt;Through a series of technical projects, the HPSF aims to build, promote, and advance a portable core software stack for HPC to increase adoption by lowering barriers to contribution, supporting open-source development efforts, and making HPC more accessible to all of us. We’ve seen from our work with RIKEN to &lt;a href="https://aws.amazon.com/blogs/publicsector/why-fugaku-japans-fastest-supercomputer-went-virtual-on-aws/"&gt;extend Fugaku to the cloud&lt;/a&gt;&amp;nbsp;how impactful this can be.&lt;/p&gt; 
&lt;p&gt;Recent years have seen extraordinary demand for HPC across domains which are critical to all of us, from climate modeling and genomics to drug-discovery and engineering design. With the emergence of machine learning and AI, there’s never been a more important task than to reinforce the structures which have made all of this possible.&lt;/p&gt; 
&lt;h2&gt;How can HPSF help?&lt;/h2&gt; 
&lt;p&gt;The HPSF aims to make life easier for HPC developers through a number of focused initiatives, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Continuous Integration (CI) resources tailored for HPC projects&lt;/li&gt; 
 &lt;li&gt;Continuously built, turnkey software stacks&lt;/li&gt; 
 &lt;li&gt;Architecture support&lt;/li&gt; 
 &lt;li&gt;Performance regression testing and benchmarking&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AWS has been supporting many of these efforts for the &lt;a href="https://spack.io/"&gt;Spack&lt;/a&gt; community since 2021, leading to the launch of the &lt;a href="https://aws.amazon.com/blogs/hpc/introducing-the-spack-rolling-binary-cache/"&gt;Spack Rolling Binary Cache&lt;/a&gt; two years ago. We’ve all learned a lot from this relationship, which we hope the wider HPC open-source community will be able to benefit from, through the foundation.&lt;/p&gt; 
&lt;h2&gt;First steps&lt;/h2&gt; 
&lt;p&gt;One of HPSF’s first jobs is to set up the technical advisory committee (TAC) which will manage working groups tackling a variety of HPC topics. Drawing from member organizations and community participants, the TAC will follow a governance model based on the &lt;a href="https://www.cncf.io/"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF).&lt;/p&gt; 
&lt;p&gt;The HPSF launches today with the following technical projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Spack&lt;/strong&gt;: the HPC package manager&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Kokkos&lt;/strong&gt;: a performance-portable programming model for writing modern C++ applications in a hardware-agnostic way&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Viskores (formerly VTK-m)&lt;/strong&gt;: a toolkit of scientific visualization algorithms for accelerator architectures&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HPCToolkit&lt;/strong&gt;: performance measurement and analysis tools for computers ranging from laptops to GPU-accelerated supercomputers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Apptainer&lt;/strong&gt;: Formerly known as Singularity, Apptainer is a Linux Foundation project providing a high performance, full featured HPC and computing optimized container subsystem&lt;/li&gt; 
 &lt;li&gt;&lt;b&gt;E4S: &lt;/b&gt;a curated, hardened distribution of scientific software packages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting involved&lt;/h2&gt; 
&lt;p&gt;The HPSF welcomes organizations from across the HPC community to become involved and help drive innovation in open-source HPC solutions.&lt;/p&gt; 
&lt;p&gt;To learn more about the HPSF, to contribute, or to become a member, you can visit the &lt;a href="https://hpsf.io/"&gt;HPSF website&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The HPSF’s commitment to supporting and nurturing these vital software packages will enable the HPC community to have access to the tools and resources necessary to push the boundaries of scientific discovery. We’re thrilled to be part of it.&lt;/p&gt;</content:encoded>
					
		
		
			</item>
	</channel>
</rss>